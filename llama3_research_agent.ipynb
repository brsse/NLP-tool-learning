{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c707a92-6f82-44d8-8de0-fa612064df5e",
   "metadata": {},
   "source": [
    "# Local Web Research Agent w/ Llama 3 8b\n",
    "\n",
    "### [Llama 3 Release](https://llama.meta.com/llama3/)\n",
    "\n",
    "### [Ollama Llama 3 Model](https://ollama.com/library/llama3)\n",
    "---\n",
    "\n",
    "![diagram](local_agent_diagram.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715059b3-857c-456d-a740-24e2551d739d",
   "metadata": {},
   "source": [
    "---\n",
    "[Llama 3 Prompt Format](https://llama.meta.com/docs/model-cards-and-prompt-formats/meta-llama-3/)\n",
    "\n",
    "### Special Tokens used with Meta Llama 3\n",
    "* **<|begin_of_text|>**: This is equivalent to the BOS token\n",
    "* **<|eot_id|>**: This signifies the end of the message in a turn.\n",
    "* **<|start_header_id|>{role}<|end_header_id|>**: These tokens enclose the role for a particular message. The possible roles can be: system, user, assistant.\n",
    "* **<|end_of_text|>**: This is equivalent to the EOS token. On generating this token, Llama 3 will cease to generate more tokens.\n",
    "A prompt should contain a single system message, can contain multiple alternating user and assistant messages, and always ends with the last user message followed by the assistant header."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ae902532",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --quiet feedparser langchain_community langgraph duckduckgo-search\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "35f2cb84-6abf-4a6c-8d1f-cdc6474b77ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying final output format\n",
    "from IPython.display import display, Markdown, Latex\n",
    "import feedparser\n",
    "from langchain.tools import BaseTool\n",
    "from urllib.parse import urlencode \n",
    "\n",
    "# LangChain Dependencies\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser, StrOutputParser\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_community.tools import DuckDuckGoSearchRun\n",
    "from langchain_community.utilities import DuckDuckGoSearchAPIWrapper\n",
    "from langgraph.graph import END, StateGraph\n",
    "# For State Graph \n",
    "from typing_extensions import TypedDict\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d39b8539-1bfe-4001-b7b2-6752a77846d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment Variables\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"L3 Research Agent\"\n",
    "LANGSMITH_ENDPOINT=\"https://api.smith.langchain.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9b341d1d-0a59-4c03-8558-759ea00171bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining LLM\n",
    "local_llm = 'llama3.2'\n",
    "llama3 = ChatOllama(model=local_llm, temperature=0)\n",
    "llama3_json = ChatOllama(model=local_llm, format='json', temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4c7813ac-791f-4035-a5ec-04810d5de5f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Web Search Tool\n",
    "\n",
    "wrapper = DuckDuckGoSearchAPIWrapper(max_results=25)\n",
    "web_search_tool = DuckDuckGoSearchRun(api_wrapper=wrapper)\n",
    "\n",
    "# Test Run\n",
    "# resp = web_search_tool.invoke(\"home depot news\")\n",
    "# resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1a2adc84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def arxiv_search(query: str, max_results: int = 5):\n",
    "    \"\"\"\n",
    "    Query arXiv’s API and return the top N most recent papers.\n",
    "    \"\"\"\n",
    "    base_url = \"http://export.arxiv.org/api/query\"\n",
    "    params = {\n",
    "        \"search_query\": query,\n",
    "        \"start\":        0,\n",
    "        \"max_results\":  max_results,\n",
    "        \"sortBy\":       \"submittedDate\",\n",
    "        \"sortOrder\":    \"descending\",\n",
    "    }\n",
    "    url = f\"{base_url}?{urlencode(params)}\"\n",
    "    feed = feedparser.parse(url)\n",
    "\n",
    "    results = []\n",
    "    for entry in feed.entries:\n",
    "        results.append({\n",
    "            \"title\": entry.title,\n",
    "            \"summary\": entry.summary,\n",
    "            \"authors\": [a.name for a in entry.authors],\n",
    "            \"published\": entry.published,\n",
    "            \"pdf_url\": next((l.href for l in entry.links if l.type==\"application/pdf\"), None)\n",
    "        })\n",
    "    return results\n",
    "\n",
    "class ArxivAPIWrapper(BaseTool):\n",
    "    name: str = \"research-search\"\n",
    "    description: str = \"Use this to fetch the latest arXiv papers on a topic.\"\n",
    "\n",
    "    def _run(self, query: str):\n",
    "        papers = arxiv_search(query, max_results=5)\n",
    "        return \"\\n\\n\".join(\n",
    "            f\"{p['title']} ({p['published']})\\n\"\n",
    "            f\"{p['summary'][:300]}…\\nPDF: {p['pdf_url']}\"\n",
    "            for p in papers\n",
    "        )\n",
    "\n",
    "    async def _arun(self, query: str):\n",
    "        return self._run(query)\n",
    "\n",
    "\n",
    "# instantiate\n",
    "research_tool = ArxivAPIWrapper()\n",
    "\n",
    "tools = [\n",
    "    web_search_tool,\n",
    "    research_tool,        \n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2d798a81-6ed6-4a4f-a1d9-93b4e3059fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generation Prompt\n",
    "\n",
    "generate_prompt = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "    \n",
    "    <|begin_of_text|>\n",
    "    \n",
    "    <|start_header_id|>system<|end_header_id|> \n",
    "    \n",
    "    You are an AI assistant for Research Question Tasks, that synthesizes web search results. \n",
    "    Strictly use the following pieces of web search context to answer the question. If you don't know the answer, just say that you don't know. \n",
    "    keep the answer concise, but provide all of the details you can in the form of a research report. \n",
    "    Only make direct references to material if provided in the context.\n",
    "    \n",
    "    <|eot_id|>\n",
    "    \n",
    "    <|start_header_id|>user<|end_header_id|>\n",
    "    \n",
    "    Question: {question} \n",
    "    Web Search Context: {context} \n",
    "    Answer: \n",
    "    \n",
    "    <|eot_id|>\n",
    "    \n",
    "    <|start_header_id|>assistant<|end_header_id|>\"\"\",\n",
    "    input_variables=[\"question\", \"context\"],\n",
    ")\n",
    "\n",
    "# Chain\n",
    "generate_chain = generate_prompt | llama3 | StrOutputParser()\n",
    "\n",
    "# Test Run\n",
    "# question = \"How are you?\"\n",
    "# context = \"\"\n",
    "# generation = generate_chain.invoke({\"context\": context, \"question\": question})\n",
    "# print(generation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "49fa1965-6bd4-4dfc-9eb8-96c6cff7b639",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to send compressed multipart ingest: langsmith.utils.LangSmithAuthError: Authentication failed for https://api.smith.langchain.com/runs/multipart. HTTPError('401 Client Error: Unauthorized for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Unauthorized\"}\\n')trace=d5447ff8-9e4c-49c7-acf3-4447825cd515,id=d5447ff8-9e4c-49c7-acf3-4447825cd515; trace=d5447ff8-9e4c-49c7-acf3-4447825cd515,id=54ffeab0-3e68-4e3d-9a90-b5fc6712066a; trace=d5447ff8-9e4c-49c7-acf3-4447825cd515,id=54ffeab0-3e68-4e3d-9a90-b5fc6712066a; trace=d5447ff8-9e4c-49c7-acf3-4447825cd515,id=c6d2b60e-e617-49c5-b663-8f25b35bfb77\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'choice': 'GENERAL'}\n"
     ]
    }
   ],
   "source": [
    "# Router\n",
    "\n",
    "router_prompt = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "    \n",
    "    <|begin_of_text|>\n",
    "    <|start_header_id|>system<|end_header_id|>\n",
    "    You are an expert at routing a user question into one of three categories:\n",
    "    1. RESEARCH – user needs the latest peer-reviewed papers (scholarly queries).\n",
    "    2. NEWS     – user is asking about current events or recent news.\n",
    "    3. GENERAL  – user should get a direct answer from the LLM without external lookups.\n",
    "\n",
    "    Return JSON with a single key \"choice\" whose value is exactly one of RESEARCH, NEWS, or GENERAL (no extra text).\n",
    "\n",
    "    Question to route: {question}\n",
    "    <|eot_id|>\n",
    "    <|start_header_id|>assistant<|end_header_id|>\n",
    "    \n",
    "    \"\"\",\n",
    "    input_variables=[\"question\"],\n",
    ")\n",
    "\n",
    "# Chain\n",
    "question_router = router_prompt | llama3_json | JsonOutputParser()\n",
    "\n",
    "# Test Run\n",
    "question = \"What's up?\"\n",
    "print(question_router.invoke({\"question\": question}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "33ab4128-e0b0-4f49-9f36-1d3bf5636715",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to send compressed multipart ingest: langsmith.utils.LangSmithAuthError: Authentication failed for https://api.smith.langchain.com/runs/multipart. HTTPError('401 Client Error: Unauthorized for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Unauthorized\"}\\n')trace=d5447ff8-9e4c-49c7-acf3-4447825cd515,id=c6d2b60e-e617-49c5-b663-8f25b35bfb77\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': 'Macom recent news'}\n"
     ]
    }
   ],
   "source": [
    "# Query Transformation\n",
    "\n",
    "query_prompt = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "    \n",
    "    <|begin_of_text|>\n",
    "    \n",
    "    <|start_header_id|>system<|end_header_id|> \n",
    "    \n",
    "    You are an expert at crafting web search queries for research questions.\n",
    "    More often than not, a user will ask a basic question that they wish to learn more about, however it might not be in the best format. \n",
    "    Reword their query to be the most effective web search string possible.\n",
    "    Return the JSON with a single key 'query' with no premable or explanation. \n",
    "    \n",
    "    Question to transform: {question} \n",
    "    \n",
    "    <|eot_id|>\n",
    "    \n",
    "    <|start_header_id|>assistant<|end_header_id|>\n",
    "    \n",
    "    \"\"\",\n",
    "    input_variables=[\"question\"],\n",
    ")\n",
    "\n",
    "# Chain\n",
    "query_chain = query_prompt | llama3_json | JsonOutputParser()\n",
    "\n",
    "# Test Run\n",
    "question = \"What's happened recently with Macom?\"\n",
    "print(query_chain.invoke({\"question\": question}))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a1c8e922-3f00-48d6-83cb-cc78a2292838",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph State\n",
    "class GraphState(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the state of our graph.\n",
    "\n",
    "    Attributes:\n",
    "        question: question\n",
    "        generation: LLM generation\n",
    "        search_query: revised question for web search\n",
    "        context: web_search result\n",
    "    \"\"\"\n",
    "    question : str\n",
    "    generation : str\n",
    "    search_query : str\n",
    "    context : str\n",
    "\n",
    "# Node - Generate\n",
    "\n",
    "def generate(state):\n",
    "    \"\"\"\n",
    "    Generate answer\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        dict: New key added to state, generation, that contains LLM generation\n",
    "    \"\"\"\n",
    "    print(\"Step: Generating Final Response\")\n",
    "    question = state[\"question\"]\n",
    "    # Safely grab context (empty if none)\n",
    "    context = state.get(\"context\", \"\")\n",
    "\n",
    "    # Answer Generation\n",
    "    generation = generate_chain.invoke({\"context\": context, \"question\": question})\n",
    "    return {\"generation\": generation}\n",
    "\n",
    "\n",
    "# Node - Query Transformation\n",
    "\n",
    "def transform_query(state):\n",
    "    \"\"\"\n",
    "    Transform user question to web search\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Appended search query\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Step: Optimizing Query for Web Search\")\n",
    "    question = state['question']\n",
    "    gen_query = query_chain.invoke({\"question\": question})\n",
    "    search_query = gen_query[\"query\"]\n",
    "    return {\"search_query\": search_query}\n",
    "\n",
    "\n",
    "# Node - Web Search\n",
    "\n",
    "def web_search(state):\n",
    "    \"\"\"\n",
    "    Web search based on the question\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Appended web results to context\n",
    "    \"\"\"\n",
    "\n",
    "    search_query = state['search_query']\n",
    "    print(f'Step: Searching the Web for: \"{search_query}\"')\n",
    "    \n",
    "    # Web search tool call\n",
    "    search_result = web_search_tool.invoke(search_query)\n",
    "    return {\"context\": search_result}\n",
    "\n",
    "def research_node(state):\n",
    "    \"\"\"\n",
    "    Call the ArXiv tool and stash results in state['context'].\n",
    "    \"\"\"\n",
    "    print(\"Step: Running Research Tool\")\n",
    "    q = state.get(\"query\") or state.get(\"question\")\n",
    "    papers = research_tool.invoke(q)\n",
    "    # research_tool.invoke returns a single string; if it were a list you'd join it\n",
    "    return {\"context\": papers}\n",
    "\n",
    "\n",
    "\n",
    "# Conditional Edge, Routing\n",
    "\n",
    "def route_question(state):\n",
    "    \"\"\"\n",
    "    Route question to one of the tags: RESEARCH, NEWS, or GENERAL.\n",
    "    \"\"\"\n",
    "    print(\"Step: Routing Query\")\n",
    "    q = state.get(\"query\") or state.get(\"question\")\n",
    "    choice = question_router.invoke({\"question\": q})[\"choice\"]\n",
    "\n",
    "    # Just return the tag\n",
    "    if choice == \"RESEARCH\":\n",
    "        print(\"→ Tag: RESEARCH\")\n",
    "        return \"RESEARCH\"\n",
    "    elif choice == \"NEWS\":\n",
    "        print(\"→ Tag: NEWS\")\n",
    "        return \"NEWS\"\n",
    "    else:\n",
    "        print(\"→ Tag: GENERAL\")\n",
    "        return \"GENERAL\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8f665713-e80b-4d86-8015-77ba55506004",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the nodes\n",
    "workflow = StateGraph(GraphState)\n",
    "workflow.add_node(\"websearch\",       web_search)\n",
    "workflow.add_node(\"transform_query\", transform_query)\n",
    "workflow.add_node(\"research\",        research_node)     \n",
    "workflow.add_node(\"generate\",        generate)\n",
    "\n",
    "# Build the edges\n",
    "workflow.set_conditional_entry_point(\n",
    "    route_question,\n",
    "    {\n",
    "        \"RESEARCH\": \"research\",       \n",
    "        \"NEWS\":     \"transform_query\",  \n",
    "        \"GENERAL\":  \"generate\",       \n",
    "    },\n",
    ")\n",
    "workflow.add_edge(\"transform_query\", \"websearch\")\n",
    "workflow.add_edge(\"websearch\",        \"generate\")\n",
    "workflow.add_edge(\"research\",         \"generate\")      \n",
    "workflow.add_edge(\"generate\",         END)\n",
    "\n",
    "# Compile the workflow\n",
    "local_agent = workflow.compile()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4f53aa05-20b2-420e-9a8f-bf12b1e547ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_agent(query):\n",
    "    output = local_agent.invoke({\"question\": query})\n",
    "    print(\"=======\")\n",
    "    display(Markdown(output[\"generation\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1b135d-131e-4276-b40c-12ea8b78c39c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: Routing Query\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to send compressed multipart ingest: langsmith.utils.LangSmithAuthError: Authentication failed for https://api.smith.langchain.com/runs/multipart. HTTPError('401 Client Error: Unauthorized for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Unauthorized\"}\\n')trace=2913463b-4ed3-43cd-9baa-c63a5f0a73f2,id=2913463b-4ed3-43cd-9baa-c63a5f0a73f2; trace=2913463b-4ed3-43cd-9baa-c63a5f0a73f2,id=7b3fe8db-9bb9-4c4f-828f-1b3b1d98435e; trace=2913463b-4ed3-43cd-9baa-c63a5f0a73f2,id=c184aa66-e21b-4345-8751-a80765149228; trace=2913463b-4ed3-43cd-9baa-c63a5f0a73f2,id=19ab7927-5a98-41af-9864-ab6af40ab740; trace=2913463b-4ed3-43cd-9baa-c63a5f0a73f2,id=bf53efc8-6472-47e1-96d0-6d075beb79e7; trace=2913463b-4ed3-43cd-9baa-c63a5f0a73f2,id=bf53efc8-6472-47e1-96d0-6d075beb79e7; trace=2913463b-4ed3-43cd-9baa-c63a5f0a73f2,id=a0e264ea-6d96-433f-942d-3561b28bbb7a\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Tag: RESEARCH\n",
      "Step: Running Research Tool\n",
      "Step: Generating Final Response\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to send compressed multipart ingest: langsmith.utils.LangSmithAuthError: Authentication failed for https://api.smith.langchain.com/runs/multipart. HTTPError('401 Client Error: Unauthorized for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Unauthorized\"}\\n')trace=2913463b-4ed3-43cd-9baa-c63a5f0a73f2,id=a0e264ea-6d96-433f-942d-3561b28bbb7a; trace=2913463b-4ed3-43cd-9baa-c63a5f0a73f2,id=05a552f7-cfdf-4454-a8c1-0419d5fbe8e4; trace=2913463b-4ed3-43cd-9baa-c63a5f0a73f2,id=05a552f7-cfdf-4454-a8c1-0419d5fbe8e4; trace=2913463b-4ed3-43cd-9baa-c63a5f0a73f2,id=19ab7927-5a98-41af-9864-ab6af40ab740; trace=2913463b-4ed3-43cd-9baa-c63a5f0a73f2,id=c184aa66-e21b-4345-8751-a80765149228; trace=2913463b-4ed3-43cd-9baa-c63a5f0a73f2,id=7b3fe8db-9bb9-4c4f-828f-1b3b1d98435e; trace=2913463b-4ed3-43cd-9baa-c63a5f0a73f2,id=805427f0-64b7-4baf-a799-a916cfd0083c; trace=2913463b-4ed3-43cd-9baa-c63a5f0a73f2,id=b34c8f84-b64f-474d-9a98-63ee8ad14b0c; trace=2913463b-4ed3-43cd-9baa-c63a5f0a73f2,id=b34c8f84-b64f-474d-9a98-63ee8ad14b0c; trace=2913463b-4ed3-43cd-9baa-c63a5f0a73f2,id=805427f0-64b7-4baf-a799-a916cfd0083c; trace=2913463b-4ed3-43cd-9baa-c63a5f0a73f2,id=4dca838e-7c8d-4c4b-95ff-f1291929185a; trace=2913463b-4ed3-43cd-9baa-c63a5f0a73f2,id=c55e9a5d-5311-4e0c-bab6-3c9b6f8069ba; trace=2913463b-4ed3-43cd-9baa-c63a5f0a73f2,id=649ba7e2-dd0e-437c-a29f-a71c7e6736c5; trace=2913463b-4ed3-43cd-9baa-c63a5f0a73f2,id=649ba7e2-dd0e-437c-a29f-a71c7e6736c5; trace=2913463b-4ed3-43cd-9baa-c63a5f0a73f2,id=d9afff85-1a89-43a0-95f0-51a50c9ab556\n"
     ]
    }
   ],
   "source": [
    "# Test it out!\n",
    "# run_agent(\"What's up with lope recently?\")\n",
    "#run_agent(\"How are you?\")\n",
    "run_agent(\"Find me the latest arXiv papers on graph neural networks?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
