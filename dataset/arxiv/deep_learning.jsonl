{"doi": "10.48550/arXiv.1805.08355", "date": "2018-05-22", "title": "Opening the black box of deep learning", "authors": "Dian Lei, Xiaoxiao Chen, Jianfei Zhao", "abstract": "The great success of deep learning shows that its technology contains\nprofound truth, and understanding its internal mechanism not only has important\nimplications for the development of its technology and effective application in\nvarious fields, but also provides meaningful insights into the understanding of\nhuman brain mechanism. At present, most of the theoretical research on deep\nlearning is based on mathematics. This dissertation proposes that the neural\nnetwork of deep learning is a physical system, examines deep learning from\nthree different perspectives: microscopic, macroscopic, and physical world\nviews, answers multiple theoretical puzzles in deep learning by using physics\nprinciples. For example, from the perspective of quantum mechanics and\nstatistical physics, this dissertation presents the calculation methods for\nconvolution calculation, pooling, normalization, and Restricted Boltzmann\nMachine, as well as the selection of cost functions, explains why deep learning\nmust be deep, what characteristics are learned in deep learning, why\nConvolutional Neural Networks do not have to be trained layer by layer, and the\nlimitations of deep learning, etc., and proposes the theoretical direction and\nbasis for the further development of deep learning now and in the future. The\nbrilliance of physics flashes in deep learning, we try to establish the deep\nlearning technology based on the scientific theory of physics.", "journal": ""}
{"doi": "10.48550/arXiv.1806.01756", "date": "2018-06-05", "title": "Concept-Oriented Deep Learning", "authors": "Daniel T Chang", "abstract": "Concepts are the foundation of human deep learning, understanding, and\nknowledge integration and transfer. We propose concept-oriented deep learning\n(CODL) which extends (machine) deep learning with concept representations and\nconceptual understanding capability. CODL addresses some of the major\nlimitations of deep learning: interpretability, transferability, contextual\nadaptation, and requirement for lots of labeled training data. We discuss the\nmajor aspects of CODL including concept graph, concept representations, concept\nexemplars, and concept representation learning systems supporting incremental\nand continual learning.", "journal": ""}
{"doi": "10.48550/arXiv.1908.02130", "date": "2019-07-30", "title": "Deep learning research landscape & roadmap in a nutshell: past, present and future -- Towards deep cortical learning", "authors": "Aras R. Dargazany", "abstract": "The past, present and future of deep learning is presented in this work.\nGiven this landscape & roadmap, we predict that deep cortical learning will be\nthe convergence of deep learning & cortical learning which builds an artificial\ncortical column ultimately.", "journal": ""}
{"doi": "10.48550/arXiv.1812.05448", "date": "2018-11-08", "title": "A First Look at Deep Learning Apps on Smartphones", "authors": "Mengwei Xu, Jiawei Liu, Yuanqiang Liu, Felix Xiaozhu Lin, Yunxin Liu, Xuanzhe Liu", "abstract": "We are in the dawn of deep learning explosion for smartphones. To bridge the\ngap between research and practice, we present the first empirical study on\n16,500 the most popular Android apps, demystifying how smartphone apps exploit\ndeep learning in the wild. To this end, we build a new static tool that\ndissects apps and analyzes their deep learning functions. Our study answers\nthreefold questions: what are the early adopter apps of deep learning, what do\nthey use deep learning for, and how do their deep learning models look like.\nOur study has strong implications for app developers, smartphone vendors, and\ndeep learning R\\&D. On one hand, our findings paint a promising picture of deep\nlearning for smartphones, showing the prosperity of mobile deep learning\nframeworks as well as the prosperity of apps building their cores atop deep\nlearning. On the other hand, our findings urge optimizations on deep learning\nmodels deployed on smartphones, the protection of these models, and validation\nof research ideas on these models.", "journal": ""}
{"doi": "10.48550/arXiv.1901.02354", "date": "2019-01-06", "title": "Geometrization of deep networks for the interpretability of deep learning systems", "authors": "Xiao Dong, Ling Zhou", "abstract": "How to understand deep learning systems remains an open problem. In this\npaper we propose that the answer may lie in the geometrization of deep\nnetworks. Geometrization is a bridge to connect physics, geometry, deep network\nand quantum computation and this may result in a new scheme to reveal the rule\nof the physical world. By comparing the geometry of image matching and deep\nnetworks, we show that geometrization of deep networks can be used to\nunderstand existing deep learning systems and it may also help to solve the\ninterpretability problem of deep learning systems.", "journal": ""}
{"doi": "10.48550/arXiv.1705.03921", "date": "2017-05-10", "title": "Why & When Deep Learning Works: Looking Inside Deep Learnings", "authors": "Ronny Ronen", "abstract": "The Intel Collaborative Research Institute for Computational Intelligence\n(ICRI-CI) has been heavily supporting Machine Learning and Deep Learning\nresearch from its foundation in 2012. We have asked six leading ICRI-CI Deep\nLearning researchers to address the challenge of \"Why & When Deep Learning\nworks\", with the goal of looking inside Deep Learning, providing insights on\nhow deep networks function, and uncovering key observations on their\nexpressiveness, limitations, and potential. The output of this challenge\nresulted in five papers that address different facets of deep learning. These\ndifferent facets include a high-level understating of why and when deep\nnetworks work (and do not work), the impact of geometry on the expressiveness\nof deep networks, and making deep networks interpretable.", "journal": ""}
{"doi": "10.48550/arXiv.2010.05125", "date": "2020-10-11", "title": "Learning Task-aware Robust Deep Learning Systems", "authors": "Keji Han, Yun Li, Xianzhong Long, Yao Ge", "abstract": "Many works demonstrate that deep learning system is vulnerable to adversarial\nattack. A deep learning system consists of two parts: the deep learning task\nand the deep model. Nowadays, most existing works investigate the impact of the\ndeep model on robustness of deep learning systems, ignoring the impact of the\nlearning task. In this paper, we adopt the binary and interval label encoding\nstrategy to redefine the classification task and design corresponding loss to\nimprove robustness of the deep learning system. Our method can be viewed as\nimproving the robustness of deep learning systems from both the learning task\nand deep model. Experimental results demonstrate that our learning task-aware\nmethod is much more robust than traditional classification while retaining the\naccuracy.", "journal": ""}
{"doi": "10.48550/arXiv.1805.04825", "date": "2018-05-13", "title": "Deep Learning in Software Engineering", "authors": "Xiaochen Li, He Jiang, Zhilei Ren, Ge Li, Jingxuan Zhang", "abstract": "Recent years, deep learning is increasingly prevalent in the field of\nSoftware Engineering (SE). However, many open issues still remain to be\ninvestigated. How do researchers integrate deep learning into SE problems?\nWhich SE phases are facilitated by deep learning? Do practitioners benefit from\ndeep learning? The answers help practitioners and researchers develop practical\ndeep learning models for SE tasks. To answer these questions, we conduct a\nbibliography analysis on 98 research papers in SE that use deep learning\ntechniques. We find that 41 SE tasks in all SE phases have been facilitated by\ndeep learning integrated solutions. In which, 84.7% papers only use standard\ndeep learning models and their variants to solve SE problems. The\npracticability becomes a concern in utilizing deep learning techniques. How to\nimprove the effectiveness, efficiency, understandability, and testability of\ndeep learning based solutions may attract more SE researchers in the future.", "journal": ""}
{"doi": "10.48550/arXiv.1901.09388", "date": "2019-01-27", "title": "Moving Deep Learning into Web Browser: How Far Can We Go?", "authors": "Yun Ma, Dongwei Xiang, Shuyu Zheng, Deyu Tian, Xuanzhe Liu", "abstract": "Recently, several JavaScript-based deep learning frameworks have emerged,\nmaking it possible to perform deep learning tasks directly in browsers.\nHowever, little is known on what and how well we can do with these frameworks\nfor deep learning in browsers. To bridge the knowledge gap, in this paper, we\nconduct the first empirical study of deep learning in browsers. We survey 7\nmost popular JavaScript-based deep learning frameworks, investigating to what\nextent deep learning tasks have been supported in browsers so far. Then we\nmeasure the performance of different frameworks when running different deep\nlearning tasks. Finally, we dig out the performance gap between deep learning\nin browsers and on native platforms by comparing the performance of\nTensorFlow.js and TensorFlow in Python. Our findings could help application\ndevelopers, deep-learning framework vendors and browser vendors to improve the\nefficiency of deep learning in browsers.", "journal": ""}
{"doi": "10.48550/arXiv.2108.01468", "date": "2021-08-02", "title": "Quantum Neural Networks: Concepts, Applications, and Challenges", "authors": "Yunseok Kwak, Won Joon Yun, Soyi Jung, Joongheon Kim", "abstract": "Quantum deep learning is a research field for the use of quantum computing\ntechniques for training deep neural networks. The research topics and\ndirections of deep learning and quantum computing have been separated for long\ntime, however by discovering that quantum circuits can act like artificial\nneural networks, quantum deep learning research is widely adopted. This paper\nexplains the backgrounds and basic principles of quantum deep learning and also\nintroduces major achievements. After that, this paper discusses the challenges\nof quantum deep learning research in multiple perspectives. Lastly, this paper\npresents various future research directions and application fields of quantum\ndeep learning.", "journal": ""}
{"doi": "10.48550/arXiv.2306.13586", "date": "2023-06-23", "title": "NetBooster: Empowering Tiny Deep Learning By Standing on the Shoulders of Deep Giants", "authors": "Zhongzhi Yu, Yonggan Fu, Jiayi Yuan, Haoran You, Yingyan Lin", "abstract": "Tiny deep learning has attracted increasing attention driven by the\nsubstantial demand for deploying deep learning on numerous intelligent\nInternet-of-Things devices. However, it is still challenging to unleash tiny\ndeep learning's full potential on both large-scale datasets and downstream\ntasks due to the under-fitting issues caused by the limited model capacity of\ntiny neural networks (TNNs). To this end, we propose a framework called\nNetBooster to empower tiny deep learning by augmenting the architectures of\nTNNs via an expansion-then-contraction strategy. Extensive experiments show\nthat NetBooster consistently outperforms state-of-the-art tiny deep learning\nsolutions.", "journal": ""}
{"doi": "10.48550/arXiv.1602.00203", "date": "2016-01-31", "title": "Greedy Deep Dictionary Learning", "authors": "Snigdha Tariyal, Angshul Majumdar, Richa Singh, Mayank Vatsa", "abstract": "In this work we propose a new deep learning tool called deep dictionary\nlearning. Multi-level dictionaries are learnt in a greedy fashion, one layer at\na time. This requires solving a simple (shallow) dictionary learning problem,\nthe solution to this is well known. We apply the proposed technique on some\nbenchmark deep learning datasets. We compare our results with other deep\nlearning tools like stacked autoencoder and deep belief network; and state of\nthe art supervised dictionary learning tools like discriminative KSVD and label\nconsistent KSVD. Our method yields better results than all.", "journal": ""}
{"doi": "10.48550/arXiv.2108.11510", "date": "2021-08-25", "title": "Deep Reinforcement Learning in Computer Vision: A Comprehensive Survey", "authors": "Ngan Le, Vidhiwar Singh Rathour, Kashu Yamazaki, Khoa Luu, Marios Savvides", "abstract": "Deep reinforcement learning augments the reinforcement learning framework and\nutilizes the powerful representation of deep neural networks. Recent works have\ndemonstrated the remarkable successes of deep reinforcement learning in various\ndomains including finance, medicine, healthcare, video games, robotics, and\ncomputer vision. In this work, we provide a detailed review of recent and\nstate-of-the-art research advances of deep reinforcement learning in computer\nvision. We start with comprehending the theories of deep learning,\nreinforcement learning, and deep reinforcement learning. We then propose a\ncategorization of deep reinforcement learning methodologies and discuss their\nadvantages and limitations. In particular, we divide deep reinforcement\nlearning into seven main categories according to their applications in computer\nvision, i.e. (i)landmark localization (ii) object detection; (iii) object\ntracking; (iv) registration on both 2D image and 3D image volumetric data (v)\nimage segmentation; (vi) videos analysis; and (vii) other applications. Each of\nthese categories is further analyzed with reinforcement learning techniques,\nnetwork design, and performance. Moreover, we provide a comprehensive analysis\nof the existing publicly available datasets and examine source code\navailability. Finally, we present some open issues and discuss future research\ndirections on deep reinforcement learning in computer vision", "journal": ""}
{"doi": "10.48550/arXiv.2106.00120", "date": "2021-05-31", "title": "Probabilistic Deep Learning with Probabilistic Neural Networks and Deep Probabilistic Models", "authors": "Daniel T. Chang", "abstract": "Probabilistic deep learning is deep learning that accounts for uncertainty,\nboth model uncertainty and data uncertainty. It is based on the use of\nprobabilistic models and deep neural networks. We distinguish two approaches to\nprobabilistic deep learning: probabilistic neural networks and deep\nprobabilistic models. The former employs deep neural networks that utilize\nprobabilistic layers which can represent and process uncertainty; the latter\nuses probabilistic models that incorporate deep neural network components which\ncapture complex non-linear stochastic relationships between the random\nvariables. We discuss some major examples of each approach including Bayesian\nneural networks and mixture density networks (for probabilistic neural\nnetworks), and variational autoencoders, deep Gaussian processes and deep mixed\neffects models (for deep probabilistic models). TensorFlow Probability is a\nlibrary for probabilistic modeling and inference which can be used for both\napproaches of probabilistic deep learning. We include its code examples for\nillustration.", "journal": ""}
{"doi": "10.48550/arXiv.2303.01980", "date": "2023-02-05", "title": "Towards energy-efficient Deep Learning: An overview of energy-efficient approaches along the Deep Learning Lifecycle", "authors": "Vanessa Mehlin, Sigurd Schacht, Carsten Lanquillon", "abstract": "Deep Learning has enabled many advances in machine learning applications in\nthe last few years. However, since current Deep Learning algorithms require\nmuch energy for computations, there are growing concerns about the associated\nenvironmental costs. Energy-efficient Deep Learning has received much attention\nfrom researchers and has already made much progress in the last couple of\nyears. This paper aims to gather information about these advances from the\nliterature and show how and at which points along the lifecycle of Deep\nLearning (IT-Infrastructure, Data, Modeling, Training, Deployment, Evaluation)\nit is possible to reduce energy consumption.", "journal": ""}
{"doi": "10.48550/arXiv.1805.03551", "date": "2018-05-09", "title": "A Unified Framework of Deep Neural Networks by Capsules", "authors": "Yujian Li, Chuanhui Shan", "abstract": "With the growth of deep learning, how to describe deep neural networks\nunifiedly is becoming an important issue. We first formalize neural networks\nmathematically with their directed graph representations, and prove a\ngeneration theorem about the induced networks of connected directed acyclic\ngraphs. Then, we set up a unified framework for deep learning with capsule\nnetworks. This capsule framework could simplify the description of existing\ndeep neural networks, and provide a theoretical basis of graphic designing and\nprogramming techniques for deep learning models, thus would be of great\nsignificance to the advancement of deep learning.", "journal": ""}
{"doi": "10.48550/arXiv.1901.04195", "date": "2019-01-14", "title": "Integrating Learning and Reasoning with Deep Logic Models", "authors": "Giuseppe Marra, Francesco Giannini, Michelangelo Diligenti, Marco Gori", "abstract": "Deep learning is very effective at jointly learning feature representations\nand classification models, especially when dealing with high dimensional input\npatterns. Probabilistic logic reasoning, on the other hand, is capable to take\nconsistent and robust decisions in complex environments. The integration of\ndeep learning and logic reasoning is still an open-research problem and it is\nconsidered to be the key for the development of real intelligent agents. This\npaper presents Deep Logic Models, which are deep graphical models integrating\ndeep learning and logic reasoning both for learning and inference. Deep Logic\nModels create an end-to-end differentiable architecture, where deep learners\nare embedded into a network implementing a continuous relaxation of the logic\nknowledge. The learning process allows to jointly learn the weights of the deep\nlearners and the meta-parameters controlling the high-level reasoning. The\nexperimental results show that the proposed methodology overtakes the\nlimitations of the other approaches that have been proposed to bridge deep\nlearning and reasoning.", "journal": ""}
{"doi": "10.48550/arXiv.2303.02715", "date": "2023-03-05", "title": "Deep Learning in the Field of Biometric Template Protection: An Overview", "authors": "Christian Rathgeb, Jascha Kolberg, Andreas Uhl, Christoph Busch", "abstract": "Today, deep learning represents the most popular and successful form of\nmachine learning. Deep learning has revolutionised the field of pattern\nrecognition, including biometric recognition. Biometric systems utilising deep\nlearning have been shown to achieve auspicious recognition accuracy, surpassing\nhuman performance. Apart from said breakthrough advances in terms of biometric\nperformance, the use of deep learning was reported to impact different\ncovariates of biometrics such as algorithmic fairness, vulnerability to\nattacks, or template protection. Technologies of biometric template protection\nare designed to enable a secure and privacy-preserving deployment of\nbiometrics. In the recent past, deep learning techniques have been frequently\napplied in biometric template protection systems for various purposes. This\nwork provides an overview of how advances in deep learning take influence on\nthe field of biometric template protection. The interrelation between improved\nbiometric performance rates and security in biometric template protection is\nelaborated. Further, the use of deep learning for obtaining feature\nrepresentations that are suitable for biometric template protection is\ndiscussed. Novel methods that apply deep learning to achieve various goals of\nbiometric template protection are surveyed along with deep learning-based\nattacks.", "journal": ""}
{"doi": "10.48550/arXiv.2401.02349", "date": "2024-01-04", "title": "A Survey Analyzing Generalization in Deep Reinforcement Learning", "authors": "Ezgi Korkmaz", "abstract": "Reinforcement learning research obtained significant success and attention\nwith the utilization of deep neural networks to solve problems in high\ndimensional state or action spaces. While deep reinforcement learning policies\nare currently being deployed in many different fields from medical applications\nto large language models, there are still ongoing questions the field is trying\nto answer on the generalization capabilities of deep reinforcement learning\npolicies. In this paper, we will formalize and analyze generalization in deep\nreinforcement learning. We will explain the fundamental reasons why deep\nreinforcement learning policies encounter overfitting problems that limit their\ngeneralization capabilities. Furthermore, we will categorize and explain the\nmanifold solution approaches to increase generalization, and overcome\noverfitting in deep reinforcement learning policies. From exploration to\nadversarial analysis and from regularization to robustness our paper provides\nan analysis on a wide range of subfields within deep reinforcement learning\nwith a broad scope and in-depth view. We believe our study can provide a\ncompact guideline for the current advancements in deep reinforcement learning,\nand help to construct robust deep neural policies with higher generalization\nskills.", "journal": ""}
{"doi": "10.48550/arXiv.1711.03577", "date": "2017-11-06", "title": "What Really is Deep Learning Doing?", "authors": "Chuyu Xiong", "abstract": "Deep learning has achieved a great success in many areas, from computer\nvision to natural language processing, to game playing, and much more. Yet,\nwhat deep learning is really doing is still an open question. There are a lot\nof works in this direction. For example, [5] tried to explain deep learning by\ngroup renormalization, and [6] tried to explain deep learning from the view of\nfunctional approximation. In order to address this very crucial question, here\nwe see deep learning from perspective of mechanical learning and learning\nmachine (see [1], [2]). From this particular angle, we can see deep learning\nmuch better and answer with confidence: What deep learning is really doing? why\nit works well, how it works, and how much data is necessary for learning. We\nalso will discuss advantages and disadvantages of deep learning at the end of\nthis work.", "journal": ""}
{"doi": "10.48550/arXiv.2201.05867", "date": "2022-01-15", "title": "Transferability in Deep Learning: A Survey", "authors": "Junguang Jiang, Yang Shu, Jianmin Wang, Mingsheng Long", "abstract": "The success of deep learning algorithms generally depends on large-scale\ndata, while humans appear to have inherent ability of knowledge transfer, by\nrecognizing and applying relevant knowledge from previous learning experiences\nwhen encountering and solving unseen tasks. Such an ability to acquire and\nreuse knowledge is known as transferability in deep learning. It has formed the\nlong-term quest towards making deep learning as data-efficient as human\nlearning, and has been motivating fruitful design of more powerful deep\nlearning algorithms. We present this survey to connect different isolated areas\nin deep learning with their relation to transferability, and to provide a\nunified and complete view to investigating transferability through the whole\nlifecycle of deep learning. The survey elaborates the fundamental goals and\nchallenges in parallel with the core principles and methods, covering recent\ncornerstones in deep architectures, pre-training, task adaptation and domain\nadaptation. This highlights unanswered questions on the appropriate objectives\nfor learning transferable knowledge and for adapting the knowledge to new tasks\nand domains, avoiding catastrophic forgetting and negative transfer. Finally,\nwe implement a benchmark and an open-source library, enabling a fair evaluation\nof deep learning methods in terms of transferability.", "journal": ""}
{"doi": "10.48550/arXiv.1710.06798", "date": "2017-10-17", "title": "Feature versus Raw Sequence: Deep Learning Comparative Study on Predicting Pre-miRNA", "authors": "Jaya Thomas, Sonia Thomas, Lee Sael", "abstract": "Should we input known genome sequence features or input sequence itself in\ndeep learning framework? As deep learning more popular in various applications,\nresearchers often come to question whether to generate features or use raw\nsequences for deep learning. To answer this question, we study the prediction\naccuracy of precursor miRNA prediction of feature-based deep belief network and\nsequence-based convolution neural network. Tested on a variant of six-layer\nconvolution neural net and three-layer deep belief network, we find the raw\nsequence input based convolution neural network model performs similar or\nslightly better than feature based deep belief networks with best accuracy\nvalues of 0.995 and 0.990, respectively. Both the models outperform existing\nbenchmarks models. The results shows us that if provided large enough data,\nwell devised raw sequence based deep learning models can replace feature based\ndeep learning models. However, construction of well behaved deep learning model\ncan be very challenging. In cased features can be easily extracted,\nfeature-based deep learning models may be a better alternative.", "journal": ""}
{"doi": "10.48550/arXiv.2212.00253", "date": "2022-12-01", "title": "Distributed Deep Reinforcement Learning: A Survey and A Multi-Player Multi-Agent Learning Toolbox", "authors": "Qiyue Yin, Tongtong Yu, Shengqi Shen, Jun Yang, Meijing Zhao, Kaiqi Huang, Bin Liang, Liang Wang", "abstract": "With the breakthrough of AlphaGo, deep reinforcement learning becomes a\nrecognized technique for solving sequential decision-making problems. Despite\nits reputation, data inefficiency caused by its trial and error learning\nmechanism makes deep reinforcement learning hard to be practical in a wide\nrange of areas. Plenty of methods have been developed for sample efficient deep\nreinforcement learning, such as environment modeling, experience transfer, and\ndistributed modifications, amongst which, distributed deep reinforcement\nlearning has shown its potential in various applications, such as\nhuman-computer gaming, and intelligent transportation. In this paper, we\nconclude the state of this exciting field, by comparing the classical\ndistributed deep reinforcement learning methods, and studying important\ncomponents to achieve efficient distributed learning, covering single player\nsingle agent distributed deep reinforcement learning to the most complex\nmultiple players multiple agents distributed deep reinforcement learning.\nFurthermore, we review recently released toolboxes that help to realize\ndistributed deep reinforcement learning without many modifications of their\nnon-distributed versions. By analyzing their strengths and weaknesses, a\nmulti-player multi-agent distributed deep reinforcement learning toolbox is\ndeveloped and released, which is further validated on Wargame, a complex\nenvironment, showing usability of the proposed toolbox for multiple players and\nmultiple agents distributed deep reinforcement learning under complex games.\nFinally, we try to point out challenges and future trends, hoping this brief\nreview can provide a guide or a spark for researchers who are interested in\ndistributed deep reinforcement learning.", "journal": "Machine Intelligence Research, 2024\n  (https://link.springer.com/article/10.1007/s11633-023-1454-4)"}
{"doi": "10.48550/arXiv.1807.06399", "date": "2018-07-17", "title": "Are Efficient Deep Representations Learnable?", "authors": "Maxwell Nye, Andrew Saxe", "abstract": "Many theories of deep learning have shown that a deep network can require\ndramatically fewer resources to represent a given function compared to a\nshallow network. But a question remains: can these efficient representations be\nlearned using current deep learning techniques? In this work, we test whether\nstandard deep learning methods can in fact find the efficient representations\nposited by several theories of deep representation. Specifically, we train deep\nneural networks to learn two simple functions with known efficient solutions:\nthe parity function and the fast Fourier transform. We find that using\ngradient-based optimization, a deep network does not learn the parity function,\nunless initialized very close to a hand-coded exact solution. We also find that\na deep linear neural network does not learn the fast Fourier transform, even in\nthe best-case scenario of infinite training data, unless the weights are\ninitialized very close to the exact hand-coded solution. Our results suggest\nthat not every element of the class of compositional functions can be learned\nefficiently by a deep network, and further restrictions are necessary to\nunderstand what functions are both efficiently representable and learnable.", "journal": ""}
{"doi": "10.48550/arXiv.1801.00631", "date": "2018-01-02", "title": "Deep Learning: A Critical Appraisal", "authors": "Gary Marcus", "abstract": "Although deep learning has historical roots going back decades, neither the\nterm \"deep learning\" nor the approach was popular just over five years ago,\nwhen the field was reignited by papers such as Krizhevsky, Sutskever and\nHinton's now classic (2012) deep network model of Imagenet. What has the field\ndiscovered in the five subsequent years? Against a background of considerable\nprogress in areas such as speech recognition, image recognition, and game\nplaying, and considerable enthusiasm in the popular press, I present ten\nconcerns for deep learning, and suggest that deep learning must be supplemented\nby other techniques if we are to reach artificial general intelligence.", "journal": ""}
{"doi": "10.48550/arXiv.1801.07883", "date": "2018-01-24", "title": "Deep Learning for Sentiment Analysis : A Survey", "authors": "Lei Zhang, Shuai Wang, Bing Liu", "abstract": "Deep learning has emerged as a powerful machine learning technique that\nlearns multiple layers of representations or features of the data and produces\nstate-of-the-art prediction results. Along with the success of deep learning in\nmany other application domains, deep learning is also popularly used in\nsentiment analysis in recent years. This paper first gives an overview of deep\nlearning and then provides a comprehensive survey of its current applications\nin sentiment analysis.", "journal": ""}
{"doi": "10.48550/arXiv.2310.19495", "date": "2023-10-30", "title": "Deep Learning for Visual Navigation of Underwater Robots", "authors": "M. Sunbeam", "abstract": "This paper aims to briefly survey deep learning methods for visual navigation\nof underwater robotics. The scope of this paper includes the visual perception\nof underwater robotics with deep learning methods, the available visual\nunderwater datasets, imitation learning, and reinforcement learning methods for\nnavigation. Additionally, relevant works will be categorized under the\nimitation learning or deep learning paradigm for underwater robots for clarity\nof the training methodologies in the current landscape. Literature that uses\ndeep learning algorithms to process non-visual data for underwater navigation\nwill not be considered, except as contrasting examples.", "journal": ""}
{"doi": "10.48550/arXiv.1807.04739", "date": "2018-07-12", "title": "When deep learning meets security", "authors": "Majd Latah", "abstract": "Deep learning is an emerging research field that has proven its effectiveness\ntowards deploying more efficient intelligent systems. Security, on the other\nhand, is one of the most essential issues in modern communication systems.\nRecently many papers have shown that using deep learning models can achieve\npromising results when applied to the security domain. In this work, we provide\nan overview for the recent studies that apply deep learning techniques to the\nfield of security.", "journal": ""}
{"doi": "10.48550/arXiv.2212.12597", "date": "2022-12-23", "title": "Deep Causal Learning for Robotic Intelligence", "authors": "Yangming Li", "abstract": "This invited review discusses causal learning in the context of robotic\nintelligence. The paper introduced the psychological findings on causal\nlearning in human cognition, then it introduced the traditional statistical\nsolutions on causal discovery and causal inference. The paper reviewed recent\ndeep causal learning algorithms with a focus on their architectures and the\nbenefits of using deep nets and discussed the gap between deep causal learning\nand the needs of robotic intelligence.", "journal": ""}
{"doi": "10.48550/arXiv.1802.08717", "date": "2018-02-10", "title": "Deep learning in radiology: an overview of the concepts and a survey of the state of the art", "authors": "Maciej A. Mazurowski, Mateusz Buda, Ashirbani Saha, Mustafa R. Bashir", "abstract": "Deep learning is a branch of artificial intelligence where networks of simple\ninterconnected units are used to extract patterns from data in order to solve\ncomplex problems. Deep learning algorithms have shown groundbreaking\nperformance in a variety of sophisticated tasks, especially those related to\nimages. They have often matched or exceeded human performance. Since the\nmedical field of radiology mostly relies on extracting useful information from\nimages, it is a very natural application area for deep learning, and research\nin this area has rapidly grown in recent years. In this article, we review the\nclinical reality of radiology and discuss the opportunities for application of\ndeep learning algorithms. We also introduce basic concepts of deep learning\nincluding convolutional neural networks. Then, we present a survey of the\nresearch in deep learning applied to radiology. We organize the studies by the\ntypes of specific tasks that they attempt to solve and review the broad range\nof utilized deep learning algorithms. Finally, we briefly discuss opportunities\nand challenges for incorporating deep learning in the radiology practice of the\nfuture.", "journal": ""}
{"doi": "10.48550/arXiv.1904.05526", "date": "2019-04-10", "title": "A Selective Overview of Deep Learning", "authors": "Jianqing Fan, Cong Ma, Yiqiao Zhong", "abstract": "Deep learning has arguably achieved tremendous success in recent years. In\nsimple words, deep learning uses the composition of many nonlinear functions to\nmodel the complex dependency between input features and labels. While neural\nnetworks have a long history, recent advances have greatly improved their\nperformance in computer vision, natural language processing, etc. From the\nstatistical and scientific perspective, it is natural to ask: What is deep\nlearning? What are the new characteristics of deep learning, compared with\nclassical methods? What are the theoretical foundations of deep learning? To\nanswer these questions, we introduce common neural network models (e.g.,\nconvolutional neural nets, recurrent neural nets, generative adversarial nets)\nand training techniques (e.g., stochastic gradient descent, dropout, batch\nnormalization) from a statistical point of view. Along the way, we highlight\nnew characteristics of deep learning (including depth and over-parametrization)\nand explain their practical and theoretical benefits. We also sample recent\nresults on theories of deep learning, many of which are only suggestive. While\na complete understanding of deep learning remains elusive, we hope that our\nperspectives and discussions serve as a stimulus for new statistical research.", "journal": ""}
{"doi": "10.48550/arXiv.1803.10862", "date": "2018-03-28", "title": "A Survey on Deep Learning Methods for Robot Vision", "authors": "Javier Ruiz-del-Solar, Patricio Loncomilla, Naiomi Soto", "abstract": "Deep learning has allowed a paradigm shift in pattern recognition, from using\nhand-crafted features together with statistical classifiers to using\ngeneral-purpose learning procedures for learning data-driven representations,\nfeatures, and classifiers together. The application of this new paradigm has\nbeen particularly successful in computer vision, in which the development of\ndeep learning methods for vision applications has become a hot research topic.\nGiven that deep learning has already attracted the attention of the robot\nvision community, the main purpose of this survey is to address the use of deep\nlearning in robot vision. To achieve this, a comprehensive overview of deep\nlearning and its usage in computer vision is given, that includes a description\nof the most frequently used neural models and their main application areas.\nThen, the standard methodology and tools used for designing deep-learning based\nvision systems are presented. Afterwards, a review of the principal work using\ndeep learning in robot vision is presented, as well as current and future\ntrends related to the use of deep learning in robotics. This survey is intended\nto be a guide for the developers of robot vision systems.", "journal": ""}
{"doi": "10.48550/arXiv.1906.06706", "date": "2019-06-16", "title": "Interpretations of Deep Learning by Forests and Haar Wavelets", "authors": "Changcun Huang", "abstract": "This paper presents a basic property of region dividing of ReLU (rectified\nlinear unit) deep learning when new layers are successively added, by which two\nnew perspectives of interpreting deep learning are given. The first is related\nto decision trees and forests; we construct a deep learning structure\nequivalent to a forest in classification abilities, which means that certain\nkinds of ReLU deep learning can be considered as forests. The second\nperspective is that Haar wavelet represented functions can be approximated by\nReLU deep learning with arbitrary precision; and then a general conclusion of\nfunction approximation abilities of ReLU deep learning is given. Finally,\ngeneralize some of the conclusions of ReLU deep learning to the case of\nsigmoid-unit deep learning.", "journal": ""}
{"doi": "10.48550/arXiv.1708.05866", "date": "2017-08-19", "title": "A Brief Survey of Deep Reinforcement Learning", "authors": "Kai Arulkumaran, Marc Peter Deisenroth, Miles Brundage, Anil Anthony Bharath", "abstract": "Deep reinforcement learning is poised to revolutionise the field of AI and\nrepresents a step towards building autonomous systems with a higher level\nunderstanding of the visual world. Currently, deep learning is enabling\nreinforcement learning to scale to problems that were previously intractable,\nsuch as learning to play video games directly from pixels. Deep reinforcement\nlearning algorithms are also applied to robotics, allowing control policies for\nrobots to be learned directly from camera inputs in the real world. In this\nsurvey, we begin with an introduction to the general field of reinforcement\nlearning, then progress to the main streams of value-based and policy-based\nmethods. Our survey will cover central algorithms in deep reinforcement\nlearning, including the deep $Q$-network, trust region policy optimisation, and\nasynchronous advantage actor-critic. In parallel, we highlight the unique\nadvantages of deep neural networks, focusing on visual understanding via\nreinforcement learning. To conclude, we describe several current areas of\nresearch within the field.", "journal": ""}
{"doi": "10.48550/arXiv.2302.03836", "date": "2023-02-08", "title": "Topological Deep Learning: A Review of an Emerging Paradigm", "authors": "Ali Zia, Abdelwahed Khamis, James Nichols, Zeeshan Hayder, Vivien Rolland, Lars Petersson", "abstract": "Topological data analysis (TDA) provides insight into data shape. The\nsummaries obtained by these methods are principled global descriptions of\nmulti-dimensional data whilst exhibiting stable properties such as robustness\nto deformation and noise. Such properties are desirable in deep learning\npipelines but they are typically obtained using non-TDA strategies. This is\npartly caused by the difficulty of combining TDA constructs (e.g. barcode and\npersistence diagrams) with current deep learning algorithms. Fortunately, we\nare now witnessing a growth of deep learning applications embracing\ntopologically-guided components. In this survey, we review the nascent field of\ntopological deep learning by first revisiting the core concepts of TDA. We then\nexplore how the use of TDA techniques has evolved over time to support deep\nlearning frameworks, and how they can be integrated into different aspects of\ndeep learning. Furthermore, we touch on TDA usage for analyzing existing deep\nmodels; deep topological analytics. Finally, we discuss the challenges and\nfuture prospects of topological deep learning.", "journal": ""}
{"doi": "10.48550/arXiv.1803.03772", "date": "2018-03-10", "title": "Generalization and Expressivity for Deep Nets", "authors": "Shao-Bo Lin", "abstract": "Along with the rapid development of deep learning in practice, the\ntheoretical explanations for its success become urgent. Generalization and\nexpressivity are two widely used measurements to quantify theoretical behaviors\nof deep learning. The expressivity focuses on finding functions expressible by\ndeep nets but cannot be approximated by shallow nets with the similar number of\nneurons. It usually implies the large capacity. The generalization aims at\nderiving fast learning rate for deep nets. It usually requires small capacity\nto reduce the variance. Different from previous studies on deep learning,\npursuing either expressivity or generalization, we take both factors into\naccount to explore the theoretical advantages of deep nets. For this purpose,\nwe construct a deep net with two hidden layers possessing excellent\nexpressivity in terms of localized and sparse approximation. Then, utilizing\nthe well known covering number to measure the capacity, we find that deep nets\npossess excellent expressive power (measured by localized and sparse\napproximation) without enlarging the capacity of shallow nets. As a\nconsequence, we derive near optimal learning rates for implementing empirical\nrisk minimization (ERM) on the constructed deep nets. These results\ntheoretically exhibit the advantage of deep nets from learning theory\nviewpoints.", "journal": ""}
{"doi": "10.48550/arXiv.1708.03704", "date": "2017-08-11", "title": "Deep Incremental Boosting", "authors": "Alan Mosca, George D Magoulas", "abstract": "This paper introduces Deep Incremental Boosting, a new technique derived from\nAdaBoost, specifically adapted to work with Deep Learning methods, that reduces\nthe required training time and improves generalisation. We draw inspiration\nfrom Transfer of Learning approaches to reduce the start-up time to training\neach incremental Ensemble member. We show a set of experiments that outlines\nsome preliminary results on some common Deep Learning datasets and discuss the\npotential improvements Deep Incremental Boosting brings to traditional Ensemble\nmethods in Deep Learning.", "journal": "Christoph Benzm\\\"uller, Geoff Sutcliffe and Raul Rojas (editors).\n  GCAI 2016. 2nd Global Conference on Artificial Intelligence, vol 41, pages\n  293--302"}
{"doi": "10.48550/arXiv.2207.03757", "date": "2022-07-08", "title": "Combining Deep Learning with Good Old-Fashioned Machine Learning", "authors": "Moshe Sipper", "abstract": "We present a comprehensive, stacking-based framework for combining deep\nlearning with good old-fashioned machine learning, called Deep GOld. Our\nframework involves ensemble selection from 51 retrained pretrained deep\nnetworks as first-level models, and 10 machine-learning algorithms as\nsecond-level models. Enabled by today's state-of-the-art software tools and\nhardware platforms, Deep GOld delivers consistent improvement when tested on\nfour image-classification datasets: Fashion MNIST, CIFAR10, CIFAR100, and Tiny\nImageNet. Of 120 experiments, in all but 10 Deep GOld improved the original\nnetworks' performance.", "journal": ""}
{"doi": "10.48550/arXiv.2007.14313", "date": "2020-07-28", "title": "Deep frequency principle towards understanding why deeper learning is faster", "authors": "Zhi-Qin John Xu, Hanxu Zhou", "abstract": "Understanding the effect of depth in deep learning is a critical problem. In\nthis work, we utilize the Fourier analysis to empirically provide a promising\nmechanism to understand why feedforward deeper learning is faster. To this end,\nwe separate a deep neural network, trained by normal stochastic gradient\ndescent, into two parts during analysis, i.e., a pre-condition component and a\nlearning component, in which the output of the pre-condition one is the input\nof the learning one. We use a filtering method to characterize the frequency\ndistribution of a high-dimensional function. Based on experiments of deep\nnetworks and real dataset, we propose a deep frequency principle, that is, the\neffective target function for a deeper hidden layer biases towards lower\nfrequency during the training. Therefore, the learning component effectively\nlearns a lower frequency function if the pre-condition component has more\nlayers. Due to the well-studied frequency principle, i.e., deep neural networks\nlearn lower frequency functions faster, the deep frequency principle provides a\nreasonable explanation to why deeper learning is faster. We believe these\nempirical studies would be valuable for future theoretical studies of the\neffect of depth in deep learning.", "journal": "AAAI-2021"}
{"doi": "10.48550/arXiv.1703.02910", "date": "2017-03-08", "title": "Deep Bayesian Active Learning with Image Data", "authors": "Yarin Gal, Riashat Islam, Zoubin Ghahramani", "abstract": "Even though active learning forms an important pillar of machine learning,\ndeep learning tools are not prevalent within it. Deep learning poses several\ndifficulties when used in an active learning setting. First, active learning\n(AL) methods generally rely on being able to learn and update models from small\namounts of data. Recent advances in deep learning, on the other hand, are\nnotorious for their dependence on large amounts of data. Second, many AL\nacquisition functions rely on model uncertainty, yet deep learning methods\nrarely represent such model uncertainty. In this paper we combine recent\nadvances in Bayesian deep learning into the active learning framework in a\npractical way. We develop an active learning framework for high dimensional\ndata, a task which has been extremely challenging so far, with very sparse\nexisting literature. Taking advantage of specialised models such as Bayesian\nconvolutional neural networks, we demonstrate our active learning techniques\nwith image data, obtaining a significant improvement on existing active\nlearning approaches. We demonstrate this on both the MNIST dataset, as well as\nfor skin cancer diagnosis from lesion images (ISIC2016 task).", "journal": ""}
{"doi": "10.48550/arXiv.2006.05579", "date": "2020-06-10", "title": "Deep reinforcement learning for optical systems: A case study of mode-locked lasers", "authors": "Chang Sun, Eurika Kaiser, Steven L. Brunton, J. Nathan Kutz", "abstract": "We demonstrate that deep reinforcement learning (deep RL) provides a highly\neffective strategy for the control and self-tuning of optical systems. Deep RL\nintegrates the two leading machine learning architectures of deep neural\nnetworks and reinforcement learning to produce robust and stable learning for\ncontrol. Deep RL is ideally suited for optical systems as the tuning and\ncontrol relies on interactions with its environment with a goal-oriented\nobjective to achieve optimal immediate or delayed rewards. This allows the\noptical system to recognize bi-stable structures and navigate, via trajectory\nplanning, to optimally performing solutions, the first such algorithm\ndemonstrated to do so in optical systems. We specifically demonstrate the deep\nRL architecture on a mode-locked laser, where robust self-tuning and control\ncan be established through access of the deep RL agent to its waveplates and\npolarizers. We further integrate transfer learning to help the deep RL agent\nrapidly learn new parameter regimes and generalize its control authority.\nAdditionally, the deep RL learning can be easily integrated with other control\nparadigms to provide a broad framework to control any optical system.", "journal": ""}
{"doi": "10.48550/arXiv.2111.12963", "date": "2021-11-25", "title": "Error Bounds for a Matrix-Vector Product Approximation with Deep ReLU Neural Networks", "authors": "Tilahun M. Getu", "abstract": "Among the several paradigms of artificial intelligence (AI) or machine\nlearning (ML), a remarkably successful paradigm is deep learning. Deep\nlearning's phenomenal success has been hoped to be interpreted via fundamental\nresearch on the theory of deep learning. Accordingly, applied research on deep\nlearning has spurred the theory of deep learning-oriented depth and breadth of\ndevelopments. Inspired by such developments, we pose these fundamental\nquestions: can we accurately approximate an arbitrary matrix-vector product\nusing deep rectified linear unit (ReLU) feedforward neural networks (FNNs)? If\nso, can we bound the resulting approximation error? In light of these\nquestions, we derive error bounds in Lebesgue and Sobolev norms that comprise\nour developed deep approximation theory. Guided by this theory, we have\nsuccessfully trained deep ReLU FNNs whose test results justify our developed\ntheory. The developed theory is also applicable for guiding and easing the\ntraining of teacher deep ReLU FNNs in view of the emerging teacher-student AI\nor ML paradigms that are essential for solving several AI or ML problems in\nwireless communications and signal processing; network science and graph signal\nprocessing; and network neuroscience and brain physics.", "journal": ""}
{"doi": "10.48550/arXiv.1212.2686", "date": "2012-12-12", "title": "Joint Training of Deep Boltzmann Machines", "authors": "Ian Goodfellow, Aaron Courville, Yoshua Bengio", "abstract": "We introduce a new method for training deep Boltzmann machines jointly. Prior\nmethods require an initial learning pass that trains the deep Boltzmann machine\ngreedily, one layer at a time, or do not perform well on classifi- cation\ntasks.", "journal": ""}
{"doi": "10.48550/arXiv.2003.03253", "date": "2020-02-29", "title": "Introduction to deep learning", "authors": "Lihi Shiloh-Perl, Raja Giryes", "abstract": "Deep Learning (DL) has made a major impact on data science in the last\ndecade. This chapter introduces the basic concepts of this field. It includes\nboth the basic structures used to design deep neural networks and a brief\nsurvey of some of its popular use cases.", "journal": ""}
{"doi": "10.48550/arXiv.2205.01069", "date": "2022-04-22", "title": "Deep Learning: From Basics to Building Deep Neural Networks with Python", "authors": "Milad Vazan", "abstract": "This book is intended for beginners who have no familiarity with deep\nlearning. Our only expectation from readers is that they already have the basic\nprogramming skills in Python.", "journal": ""}
{"doi": "10.48550/arXiv.2010.09465", "date": "2020-10-15", "title": "A Nesterov's Accelerated quasi-Newton method for Global Routing using Deep Reinforcement Learning", "authors": "S. Indrapriyadarsini, Shahrzad Mahboubi, Hiroshi Ninomiya, Takeshi Kamio, Hideki Asai", "abstract": "Deep Q-learning method is one of the most popularly used deep reinforcement\nlearning algorithms which uses deep neural networks to approximate the\nestimation of the action-value function. Training of the deep Q-network (DQN)\nis usually restricted to first order gradient based methods. This paper\nattempts to accelerate the training of deep Q-networks by introducing a second\norder Nesterov's accelerated quasi-Newton method. We evaluate the performance\nof the proposed method on deep reinforcement learning using double DQNs for\nglobal routing. The results show that the proposed method can obtain better\nrouting solutions compared to the DQNs trained with first order Adam and\nRMSprop methods.", "journal": ""}
{"doi": "10.48550/arXiv.1709.05067", "date": "2017-09-15", "title": "Deep Reinforcement Learning for Conversational AI", "authors": "Mahipal Jadeja, Neelanshi Varia, Agam Shah", "abstract": "Deep reinforcement learning is revolutionizing the artificial intelligence\nfield. Currently, it serves as a good starting point for constructing\nintelligent autonomous systems which offer a better knowledge of the visual\nworld. It is possible to scale deep reinforcement learning with the use of deep\nlearning and do amazing tasks such as use of pixels in playing video games. In\nthis paper, key concepts of deep reinforcement learning including reward\nfunction, differences between reinforcement learning and supervised learning\nand models for implementation of reinforcement are discussed. Key challenges\nrelated to the implementation of reinforcement learning in conversational AI\ndomain are identified as well as discussed in detail. Various conversational\nmodels which are based on deep reinforcement learning (as well as deep\nlearning) are also discussed. In summary, this paper discusses key aspects of\ndeep reinforcement learning which are crucial for designing an efficient\nconversational AI.", "journal": ""}
{"doi": "10.48550/arXiv.2006.05278", "date": "2020-06-09", "title": "An Overview of Deep Semi-Supervised Learning", "authors": "Yassine Ouali, C\u00e9line Hudelot, Myriam Tami", "abstract": "Deep neural networks demonstrated their ability to provide remarkable\nperformances on a wide range of supervised learning tasks (e.g., image\nclassification) when trained on extensive collections of labeled data (e.g.,\nImageNet). However, creating such large datasets requires a considerable amount\nof resources, time, and effort. Such resources may not be available in many\npractical cases, limiting the adoption and the application of many deep\nlearning methods. In a search for more data-efficient deep learning methods to\novercome the need for large annotated datasets, there is a rising research\ninterest in semi-supervised learning and its applications to deep neural\nnetworks to reduce the amount of labeled data required, by either developing\nnovel methods or adopting existing semi-supervised learning frameworks for a\ndeep learning setting. In this paper, we provide a comprehensive overview of\ndeep semi-supervised learning, starting with an introduction to the field,\nfollowed by a summarization of the dominant semi-supervised approaches in deep\nlearning.", "journal": ""}
{"doi": "10.48550/arXiv.1602.06183", "date": "2016-02-19", "title": "Node-By-Node Greedy Deep Learning for Interpretable Features", "authors": "Ke Wu, Malik Magdon-Ismail", "abstract": "Multilayer networks have seen a resurgence under the umbrella of deep\nlearning. Current deep learning algorithms train the layers of the network\nsequentially, improving algorithmic performance as well as providing some\nregularization. We present a new training algorithm for deep networks which\ntrains \\emph{each node in the network} sequentially. Our algorithm is orders of\nmagnitude faster, creates more interpretable internal representations at the\nnode level, while not sacrificing on the ultimate out-of-sample performance.", "journal": ""}
{"doi": "10.48550/arXiv.1605.01369", "date": "2016-05-04", "title": "Accelerating Deep Learning with Shrinkage and Recall", "authors": "Shuai Zheng, Abhinav Vishnu, Chris Ding", "abstract": "Deep Learning is a very powerful machine learning model. Deep Learning trains\na large number of parameters for multiple layers and is very slow when data is\nin large scale and the architecture size is large. Inspired from the shrinking\ntechnique used in accelerating computation of Support Vector Machines (SVM)\nalgorithm and screening technique used in LASSO, we propose a shrinking Deep\nLearning with recall (sDLr) approach to speed up deep learning computation. We\nexperiment shrinking Deep Learning with recall (sDLr) using Deep Neural Network\n(DNN), Deep Belief Network (DBN) and Convolution Neural Network (CNN) on 4 data\nsets. Results show that the speedup using shrinking Deep Learning with recall\n(sDLr) can reach more than 2.0 while still giving competitive classification\nperformance.", "journal": ""}
{"doi": "10.48550/arXiv.1802.00810", "date": "2018-02-02", "title": "Deep Learning for Genomics: A Concise Overview", "authors": "Tianwei Yue, Yuanxin Wang, Longxiang Zhang, Chunming Gu, Haoru Xue, Wenping Wang, Qi Lyu, Yujie Dun", "abstract": "Advancements in genomic research such as high-throughput sequencing\ntechniques have driven modern genomic studies into \"big data\" disciplines. This\ndata explosion is constantly challenging conventional methods used in genomics.\nIn parallel with the urgent demand for robust algorithms, deep learning has\nsucceeded in a variety of fields such as vision, speech, and text processing.\nYet genomics entails unique challenges to deep learning since we are expecting\nfrom deep learning a superhuman intelligence that explores beyond our knowledge\nto interpret the genome. A powerful deep learning model should rely on\ninsightful utilization of task-specific knowledge. In this paper, we briefly\ndiscuss the strengths of different deep learning models from a genomic\nperspective so as to fit each particular task with a proper deep architecture,\nand remark on practical considerations of developing modern deep learning\narchitectures for genomics. We also provide a concise review of deep learning\napplications in various aspects of genomic research, as well as pointing out\npotential opportunities and obstacles for future genomics applications.", "journal": ""}
{"doi": "10.48550/arXiv.2009.10568", "date": "2020-09-22", "title": "Adversarial Attack Based Countermeasures against Deep Learning Side-Channel Attacks", "authors": "Ruizhe Gu, Ping Wang, Mengce Zheng, Honggang Hu, Nenghai Yu", "abstract": "Numerous previous works have studied deep learning algorithms applied in the\ncontext of side-channel attacks, which demonstrated the ability to perform\nsuccessful key recoveries. These studies show that modern cryptographic devices\nare increasingly threatened by side-channel attacks with the help of deep\nlearning. However, the existing countermeasures are designed to resist\nclassical side-channel attacks, and cannot protect cryptographic devices from\ndeep learning based side-channel attacks. Thus, there arises a strong need for\ncountermeasures against deep learning based side-channel attacks. Although deep\nlearning has the high potential in solving complex problems, it is vulnerable\nto adversarial attacks in the form of subtle perturbations to inputs that lead\na model to predict incorrectly.\n  In this paper, we propose a kind of novel countermeasures based on\nadversarial attacks that is specifically designed against deep learning based\nside-channel attacks. We estimate several models commonly used in deep learning\nbased side-channel attacks to evaluate the proposed countermeasures. It shows\nthat our approach can effectively protect cryptographic devices from deep\nlearning based side-channel attacks in practice. In addition, our experiments\nshow that the new countermeasures can also resist classical side-channel\nattacks.", "journal": ""}
{"doi": "10.48550/arXiv.2108.10451", "date": "2021-08-24", "title": "Adversarial Robustness of Deep Learning: Theory, Algorithms, and Applications", "authors": "Wenjie Ruan, Xinping Yi, Xiaowei Huang", "abstract": "This tutorial aims to introduce the fundamentals of adversarial robustness of\ndeep learning, presenting a well-structured review of up-to-date techniques to\nassess the vulnerability of various types of deep learning models to\nadversarial examples. This tutorial will particularly highlight\nstate-of-the-art techniques in adversarial attacks and robustness verification\nof deep neural networks (DNNs). We will also introduce some effective\ncountermeasures to improve the robustness of deep learning models, with a\nparticular focus on adversarial training. We aim to provide a comprehensive\noverall picture about this emerging direction and enable the community to be\naware of the urgency and importance of designing robust deep learning models in\nsafety-critical data analytical applications, ultimately enabling the end-users\nto trust deep learning classifiers. We will also summarize potential research\ndirections concerning the adversarial robustness of deep learning, and its\npotential benefits to enable accountable and trustworthy deep learning-based\ndata analytical systems and applications.", "journal": ""}
{"doi": "10.48550/arXiv.2004.00993", "date": "2020-03-31", "title": "Augmented Q Imitation Learning (AQIL)", "authors": "Xiao Lei Zhang, Anish Agarwal", "abstract": "The study of unsupervised learning can be generally divided into two\ncategories: imitation learning and reinforcement learning. In imitation\nlearning the machine learns by mimicking the behavior of an expert system\nwhereas in reinforcement learning the machine learns via direct environment\nfeedback. Traditional deep reinforcement learning takes a significant time\nbefore the machine starts to converge to an optimal policy. This paper proposes\nAugmented Q-Imitation-Learning, a method by which deep reinforcement learning\nconvergence can be accelerated by applying Q-imitation-learning as the initial\ntraining process in traditional Deep Q-learning.", "journal": ""}
{"doi": "10.48550/arXiv.1512.03844", "date": "2015-12-11", "title": "Efficient Deep Feature Learning and Extraction via StochasticNets", "authors": "Mohammad Javad Shafiee, Parthipan Siva, Paul Fieguth, Alexander Wong", "abstract": "Deep neural networks are a powerful tool for feature learning and extraction\ngiven their ability to model high-level abstractions in highly complex data.\nOne area worth exploring in feature learning and extraction using deep neural\nnetworks is efficient neural connectivity formation for faster feature learning\nand extraction. Motivated by findings of stochastic synaptic connectivity\nformation in the brain as well as the brain's uncanny ability to efficiently\nrepresent information, we propose the efficient learning and extraction of\nfeatures via StochasticNets, where sparsely-connected deep neural networks can\nbe formed via stochastic connectivity between neurons. To evaluate the\nfeasibility of such a deep neural network architecture for feature learning and\nextraction, we train deep convolutional StochasticNets to learn abstract\nfeatures using the CIFAR-10 dataset, and extract the learned features from\nimages to perform classification on the SVHN and STL-10 datasets. Experimental\nresults show that features learned using deep convolutional StochasticNets,\nwith fewer neural connections than conventional deep convolutional neural\nnetworks, can allow for better or comparable classification accuracy than\nconventional deep neural networks: relative test error decrease of ~4.5% for\nclassification on the STL-10 dataset and ~1% for classification on the SVHN\ndataset. Furthermore, it was shown that the deep features extracted using deep\nconvolutional StochasticNets can provide comparable classification accuracy\neven when only 10% of the training data is used for feature learning. Finally,\nit was also shown that significant gains in feature extraction speed can be\nachieved in embedded applications using StochasticNets. As such, StochasticNets\nallow for faster feature learning and extraction performance while facilitate\nfor better or comparable accuracy performances.", "journal": ""}
{"doi": "10.48550/arXiv.1803.02323", "date": "2018-03-06", "title": "Deep Super Learner: A Deep Ensemble for Classification Problems", "authors": "Steven Young, Tamer Abdou, Ayse Bener", "abstract": "Deep learning has become very popular for tasks such as predictive modeling\nand pattern recognition in handling big data. Deep learning is a powerful\nmachine learning method that extracts lower level features and feeds them\nforward for the next layer to identify higher level features that improve\nperformance. However, deep neural networks have drawbacks, which include many\nhyper-parameters and infinite architectures, opaqueness into results, and\nrelatively slower convergence on smaller datasets. While traditional machine\nlearning algorithms can address these drawbacks, they are not typically capable\nof the performance levels achieved by deep neural networks. To improve\nperformance, ensemble methods are used to combine multiple base learners. Super\nlearning is an ensemble that finds the optimal combination of diverse learning\nalgorithms. This paper proposes deep super learning as an approach which\nachieves log loss and accuracy results competitive to deep neural networks\nwhile employing traditional machine learning algorithms in a hierarchical\nstructure. The deep super learner is flexible, adaptable, and easy to train\nwith good performance across different tasks using identical hyper-parameter\nvalues. Using traditional machine learning requires fewer hyper-parameters,\nallows transparency into results, and has relatively fast convergence on\nsmaller datasets. Experimental results show that the deep super learner has\nsuperior performance compared to the individual base learners, single-layer\nensembles, and in some cases deep neural networks. Performance of the deep\nsuper learner may further be improved with task-specific tuning.", "journal": ""}
{"doi": "10.48550/arXiv.1905.07187", "date": "2019-05-17", "title": "An Essay on Optimization Mystery of Deep Learning", "authors": "Eugene Golikov", "abstract": "Despite the huge empirical success of deep learning, theoretical\nunderstanding of neural networks learning process is still lacking. This is the\nreason, why some of its features seem \"mysterious\". We emphasize two mysteries\nof deep learning: generalization mystery, and optimization mystery. In this\nessay we review and draw connections between several selected works concerning\nthe latter.", "journal": ""}
{"doi": "10.48550/arXiv.1906.10025", "date": "2019-06-24", "title": "Modern Deep Reinforcement Learning Algorithms", "authors": "Sergey Ivanov, Alexander D'yakonov", "abstract": "Recent advances in Reinforcement Learning, grounded on combining classical\ntheoretical results with Deep Learning paradigm, led to breakthroughs in many\nartificial intelligence tasks and gave birth to Deep Reinforcement Learning\n(DRL) as a field of research. In this work latest DRL algorithms are reviewed\nwith a focus on their theoretical justification, practical limitations and\nobserved empirical properties.", "journal": ""}
{"doi": "10.48550/arXiv.2105.06868", "date": "2021-05-14", "title": "Priors in Bayesian Deep Learning: A Review", "authors": "Vincent Fortuin", "abstract": "While the choice of prior is one of the most critical parts of the Bayesian\ninference workflow, recent Bayesian deep learning models have often fallen back\non vague priors, such as standard Gaussians. In this review, we highlight the\nimportance of prior choices for Bayesian deep learning and present an overview\nof different priors that have been proposed for (deep) Gaussian processes,\nvariational autoencoders, and Bayesian neural networks. We also outline\ndifferent methods of learning priors for these models from data. We hope to\nmotivate practitioners in Bayesian deep learning to think more carefully about\nthe prior specification for their models and to provide them with some\ninspiration in this regard.", "journal": ""}
{"doi": "10.48550/arXiv.2202.01319", "date": "2022-02-02", "title": "Deep Learning for Epidemiologists: An Introduction to Neural Networks", "authors": "Stylianos Serghiou, Kathryn Rough", "abstract": "Deep learning methods are increasingly being applied to problems in medicine\nand healthcare. However, few epidemiologists have received formal training in\nthese methods. To bridge this gap, this article introduces to the fundamentals\nof deep learning from an epidemiological perspective. Specifically, this\narticle reviews core concepts in machine learning (overfitting, regularization,\nhyperparameters), explains several fundamental deep learning architectures\n(convolutional neural networks, recurrent neural networks), and summarizes\ntraining, evaluation, and deployment of models. We aim to enable the reader to\nengage with and critically evaluate medical applications of deep learning,\nfacilitating a dialogue between computer scientists and epidemiologists that\nwill improve the safety and efficacy of applications of this technology.", "journal": ""}
{"doi": "10.48550/arXiv.1804.05806", "date": "2018-04-16", "title": "Deep Embedding Kernel", "authors": "Linh Le, Ying Xie", "abstract": "In this paper, we propose a novel supervised learning method that is called\nDeep Embedding Kernel (DEK). DEK combines the advantages of deep learning and\nkernel methods in a unified framework. More specifically, DEK is a learnable\nkernel represented by a newly designed deep architecture. Compared with\npre-defined kernels, this kernel can be explicitly trained to map data to an\noptimized high-level feature space where data may have favorable features\ntoward the application. Compared with typical deep learning using SoftMax or\nlogistic regression as the top layer, DEK is expected to be more generalizable\nto new data. Experimental results show that DEK has superior performance than\ntypical machine learning methods in identity detection, classification,\nregression, dimension reduction, and transfer learning.", "journal": ""}
{"doi": "10.48550/arXiv.2403.19083", "date": "2024-03-28", "title": "Improving Cancer Imaging Diagnosis with Bayesian Networks and Deep Learning: A Bayesian Deep Learning Approach", "authors": "Pei Xi, Lin", "abstract": "With recent advancements in the development of artificial intelligence\napplications using theories and algorithms in machine learning, many accurate\nmodels can be created to train and predict on given datasets. With the\nrealization of the importance of imaging interpretation in cancer diagnosis,\nthis article aims to investigate the theory behind Deep Learning and Bayesian\nNetwork prediction models. Based on the advantages and drawbacks of each model,\ndifferent approaches will be used to construct a Bayesian Deep Learning Model,\ncombining the strengths while minimizing the weaknesses. Finally, the\napplications and accuracy of the resulting Bayesian Deep Learning approach in\nthe health industry in classifying images will be analyzed.", "journal": ""}
{"doi": "10.48550/arXiv.1710.00211", "date": "2017-09-30", "title": "The Deep Ritz method: A deep learning-based numerical algorithm for solving variational problems", "authors": "Weinan E, Bing Yu", "abstract": "We propose a deep learning based method, the Deep Ritz Method, for\nnumerically solving variational problems, particularly the ones that arise from\npartial differential equations. The Deep Ritz method is naturally nonlinear,\nnaturally adaptive and has the potential to work in rather high dimensions. The\nframework is quite simple and fits well with the stochastic gradient descent\nmethod used in deep learning. We illustrate the method on several problems\nincluding some eigenvalue problems.", "journal": ""}
{"doi": "10.48550/arXiv.2106.16088", "date": "2021-05-18", "title": "Application of deep reinforcement learning for Indian stock trading automation", "authors": "Supriya Bajpai", "abstract": "In stock trading, feature extraction and trading strategy design are the two\nimportant tasks to achieve long-term benefits using machine learning\ntechniques. Several methods have been proposed to design trading strategy by\nacquiring trading signals to maximize the rewards. In the present paper the\ntheory of deep reinforcement learning is applied for stock trading strategy and\ninvestment decisions to Indian markets. The experiments are performed\nsystematically with three classical Deep Reinforcement Learning models Deep\nQ-Network, Double Deep Q-Network and Dueling Double Deep Q-Network on ten\nIndian stock datasets. The performance of the models are evaluated and\ncomparison is made.", "journal": ""}
{"doi": "10.48550/arXiv.2005.02612", "date": "2020-05-06", "title": "Deep Divergence Learning", "authors": "Kubra Cilingir, Rachel Manzelli, Brian Kulis", "abstract": "Classical linear metric learning methods have recently been extended along\ntwo distinct lines: deep metric learning methods for learning embeddings of the\ndata using neural networks, and Bregman divergence learning approaches for\nextending learning Euclidean distances to more general divergence measures such\nas divergences over distributions. In this paper, we introduce deep Bregman\ndivergences, which are based on learning and parameterizing functional Bregman\ndivergences using neural networks, and which unify and extend these existing\nlines of work. We show in particular how deep metric learning formulations,\nkernel metric learning, Mahalanobis metric learning, and moment-matching\nfunctions for comparing distributions arise as special cases of these\ndivergences in the symmetric setting. We then describe a deep learning\nframework for learning general functional Bregman divergences, and show in\nexperiments that this method yields superior performance on benchmark datasets\nas compared to existing deep metric learning approaches. We also discuss novel\napplications, including a semi-supervised distributional clustering problem,\nand a new loss function for unsupervised data generation.", "journal": ""}
{"doi": "10.48550/arXiv.1409.3358", "date": "2014-09-11", "title": "Building Program Vector Representations for Deep Learning", "authors": "Lili Mou, Ge Li, Yuxuan Liu, Hao Peng, Zhi Jin, Yan Xu, Lu Zhang", "abstract": "Deep learning has made significant breakthroughs in various fields of\nartificial intelligence. Advantages of deep learning include the ability to\ncapture highly complicated features, weak involvement of human engineering,\netc. However, it is still virtually impossible to use deep learning to analyze\nprograms since deep architectures cannot be trained effectively with pure back\npropagation. In this pioneering paper, we propose the \"coding criterion\" to\nbuild program vector representations, which are the premise of deep learning\nfor program analysis. Our representation learning approach directly makes deep\nlearning a reality in this new field. We evaluate the learned vector\nrepresentations both qualitatively and quantitatively. We conclude, based on\nthe experiments, the coding criterion is successful in building program\nrepresentations. To evaluate whether deep learning is beneficial for program\nanalysis, we feed the representations to deep neural networks, and achieve\nhigher accuracy in the program classification task than \"shallow\" methods, such\nas logistic regression and the support vector machine. This result confirms the\nfeasibility of deep learning to analyze programs. It also gives primary\nevidence of its success in this new field. We believe deep learning will become\nan outstanding technique for program analysis in the near future.", "journal": ""}
{"doi": "10.48550/arXiv.2210.11237", "date": "2022-10-19", "title": "Emerging Threats in Deep Learning-Based Autonomous Driving: A Comprehensive Survey", "authors": "Hui Cao, Wenlong Zou, Yinkun Wang, Ting Song, Mengjun Liu", "abstract": "Since the 2004 DARPA Grand Challenge, the autonomous driving technology has\nwitnessed nearly two decades of rapid development. Particularly, in recent\nyears, with the application of new sensors and deep learning technologies\nextending to the autonomous field, the development of autonomous driving\ntechnology has continued to make breakthroughs. Thus, many carmakers and\nhigh-tech giants dedicated to research and system development of autonomous\ndriving. However, as the foundation of autonomous driving, the deep learning\ntechnology faces many new security risks. The academic community has proposed\ndeep learning countermeasures against the adversarial examples and AI backdoor,\nand has introduced them into the autonomous driving field for verification.\nDeep learning security matters to autonomous driving system security, and then\nmatters to personal safety, which is an issue that deserves attention and\nresearch.This paper provides an summary of the concepts, developments and\nrecent research in deep learning security technologies in autonomous driving.\nFirstly, we briefly introduce the deep learning framework and pipeline in the\nautonomous driving system, which mainly include the deep learning technologies\nand algorithms commonly used in this field. Moreover, we focus on the potential\nsecurity threats of the deep learning based autonomous driving system in each\nfunctional layer in turn. We reviews the development of deep learning attack\ntechnologies to autonomous driving, investigates the State-of-the-Art\nalgorithms, and reveals the potential risks. At last, we provides an outlook on\ndeep learning security in the autonomous driving field and proposes\nrecommendations for building a safe and trustworthy autonomous driving system.", "journal": ""}
{"doi": "10.48550/arXiv.2209.12014", "date": "2022-09-24", "title": "Asset Pricing and Deep Learning", "authors": "Chen Zhang", "abstract": "Traditional machine learning methods have been widely studied in financial\ninnovation. My study focuses on the application of deep learning methods on\nasset pricing. I investigate various deep learning methods for asset pricing,\nespecially for risk premia measurement. All models take the same set of\npredictive signals (firm characteristics, systematic risks and macroeconomics).\nI demonstrate high performance of all kinds of state-of-the-art (SOTA) deep\nlearning methods, and figure out that RNNs with memory mechanism and attention\nhave the best performance in terms of predictivity. Furthermore, I demonstrate\nlarge economic gains to investors using deep learning forecasts. The results of\nmy comparative experiments highlight the importance of domain knowledge and\nfinancial theory when designing deep learning models. I also show return\nprediction tasks bring new challenges to deep learning. The time varying\ndistribution causes distribution shift problem, which is essential for\nfinancial time series prediction. I demonstrate that deep learning methods can\nimprove asset risk premium measurement. Due to the booming deep learning\nstudies, they can constantly promote the study of underlying financial\nmechanisms behind asset pricing. I also propose a promising research method\nthat learning from data and figuring out the underlying economic mechanisms\nthrough explainable artificial intelligence (AI) methods. My findings not only\njustify the value of deep learning in blooming fintech development, but also\nhighlight their prospects and advantages over traditional machine learning\nmethods.", "journal": ""}
{"doi": "10.48550/arXiv.1810.08033", "date": "2018-10-18", "title": "Adaptivity of deep ReLU network for learning in Besov and mixed smooth Besov spaces: optimal rate and curse of dimensionality", "authors": "Taiji Suzuki", "abstract": "Deep learning has shown high performances in various types of tasks from\nvisual recognition to natural language processing, which indicates superior\nflexibility and adaptivity of deep learning. To understand this phenomenon\ntheoretically, we develop a new approximation and estimation error analysis of\ndeep learning with the ReLU activation for functions in a Besov space and its\nvariant with mixed smoothness. The Besov space is a considerably general\nfunction space including the Holder space and Sobolev space, and especially can\ncapture spatial inhomogeneity of smoothness. Through the analysis in the Besov\nspace, it is shown that deep learning can achieve the minimax optimal rate and\noutperform any non-adaptive (linear) estimator such as kernel ridge regression,\nwhich shows that deep learning has higher adaptivity to the spatial\ninhomogeneity of the target function than other estimators such as linear ones.\nIn addition to this, it is shown that deep learning can avoid the curse of\ndimensionality if the target function is in a mixed smooth Besov space. We also\nshow that the dependency of the convergence rate on the dimensionality is tight\ndue to its minimax optimality. These results support high adaptivity of deep\nlearning and its superior ability as a feature extractor.", "journal": ""}
{"doi": "10.48550/arXiv.1908.08843", "date": "2019-08-23", "title": "Fairness in Deep Learning: A Computational Perspective", "authors": "Mengnan Du, Fan Yang, Na Zou, Xia Hu", "abstract": "Deep learning is increasingly being used in high-stake decision making\napplications that affect individual lives. However, deep learning models might\nexhibit algorithmic discrimination behaviors with respect to protected groups,\npotentially posing negative impacts on individuals and society. Therefore,\nfairness in deep learning has attracted tremendous attention recently. We\nprovide a review covering recent progresses to tackle algorithmic fairness\nproblems of deep learning from the computational perspective. Specifically, we\nshow that interpretability can serve as a useful ingredient to diagnose the\nreasons that lead to algorithmic discrimination. We also discuss fairness\nmitigation approaches categorized according to three stages of deep learning\nlife-cycle, aiming to push forward the area of fairness in deep learning and\nbuild genuinely fair and reliable deep learning systems.", "journal": ""}
{"doi": "10.48550/arXiv.1908.10206", "date": "2019-08-25", "title": "The many faces of deep learning", "authors": "Raul Vicente", "abstract": "Deep learning has sparked a network of mutual interactions between different\ndisciplines and AI. Naturally, each discipline focuses and interprets the\nworkings of deep learning in different ways. This diversity of perspectives on\ndeep learning, from neuroscience to statistical physics, is a rich source of\ninspiration that fuels novel developments in the theory and applications of\nmachine learning. In this perspective, we collect and synthesize different\nintuitions scattered across several communities as for how deep learning works.\nIn particular, we will briefly discuss the different perspectives that\ndisciplines across mathematics, physics, computation, and neuroscience take on\nhow deep learning does its tricks. Our discussion on each perspective is\nnecessarily shallow due to the multiple views that had to be covered. The\ndeepness in this case should come from putting all these faces of deep learning\ntogether in the reader's mind, so that one can look at the same problem from\ndifferent angles.", "journal": ""}
{"doi": "10.48550/arXiv.2110.08611", "date": "2021-10-16", "title": "Deep Active Learning by Leveraging Training Dynamics", "authors": "Haonan Wang, Wei Huang, Ziwei Wu, Andrew Margenot, Hanghang Tong, Jingrui He", "abstract": "Active learning theories and methods have been extensively studied in\nclassical statistical learning settings. However, deep active learning, i.e.,\nactive learning with deep learning models, is usually based on empirical\ncriteria without solid theoretical justification, thus suffering from heavy\ndoubts when some of those fail to provide benefits in real applications. In\nthis paper, by exploring the connection between the generalization performance\nand the training dynamics, we propose a theory-driven deep active learning\nmethod (dynamicAL) which selects samples to maximize training dynamics. In\nparticular, we prove that the convergence speed of training and the\ngeneralization performance are positively correlated under the ultra-wide\ncondition and show that maximizing the training dynamics leads to better\ngeneralization performance. Furthermore, to scale up to large deep neural\nnetworks and data sets, we introduce two relaxations for the subset selection\nproblem and reduce the time complexity from polynomial to constant. Empirical\nresults show that dynamicAL not only outperforms the other baselines\nconsistently but also scales well on large deep learning models. We hope our\nwork would inspire more attempts on bridging the theoretical findings of deep\nnetworks and practical impacts of deep active learning in real applications.", "journal": ""}
{"doi": "10.48550/arXiv.2305.18357", "date": "2023-05-26", "title": "DeepSI: Interactive Deep Learning for Semantic Interaction", "authors": "Yali Bian, Chris North", "abstract": "In this paper, we design novel interactive deep learning methods to improve\nsemantic interactions in visual analytics applications. The ability of semantic\ninteraction to infer analysts' precise intents during sensemaking is dependent\non the quality of the underlying data representation. We propose the\n$\\text{DeepSI}_{\\text{finetune}}$ framework that integrates deep learning into\nthe human-in-the-loop interactive sensemaking pipeline, with two important\nproperties. First, deep learning extracts meaningful representations from raw\ndata, which improves semantic interaction inference. Second, semantic\ninteractions are exploited to fine-tune the deep learning representations,\nwhich then further improves semantic interaction inference. This feedback loop\nbetween human interaction and deep learning enables efficient learning of user-\nand task-specific representations. To evaluate the advantage of embedding the\ndeep learning within the semantic interaction loop, we compare\n$\\text{DeepSI}_{\\text{finetune}}$ against a state-of-the-art but more basic use\nof deep learning as only a feature extractor pre-processed outside of the\ninteractive loop. Results of two complementary studies, a human-centered\nqualitative case study and an algorithm-centered simulation-based quantitative\nexperiment, show that $\\text{DeepSI}_{\\text{finetune}}$ more accurately\ncaptures users' complex mental models with fewer interactions.", "journal": "IUI '21: 26th International Conference on Intelligent User\n  Interfaces, College Station, TX, USA, April 2021"}
{"doi": "10.48550/arXiv.1812.00564", "date": "2018-12-03", "title": "Split learning for health: Distributed deep learning without sharing raw patient data", "authors": "Praneeth Vepakomma, Otkrist Gupta, Tristan Swedish, Ramesh Raskar", "abstract": "Can health entities collaboratively train deep learning models without\nsharing sensitive raw data? This paper proposes several configurations of a\ndistributed deep learning method called SplitNN to facilitate such\ncollaborations. SplitNN does not share raw data or model details with\ncollaborating institutions. The proposed configurations of splitNN cater to\npractical settings of i) entities holding different modalities of patient data,\nii) centralized and local health entities collaborating on multiple tasks and\niii) learning without sharing labels. We compare performance and resource\nefficiency trade-offs of splitNN and other distributed deep learning methods\nlike federated learning, large batch synchronous stochastic gradient descent\nand show highly encouraging results for splitNN.", "journal": ""}
{"doi": "10.48550/arXiv.1611.07174", "date": "2016-11-22", "title": "Deep Recurrent Convolutional Neural Network: Improving Performance For Speech Recognition", "authors": "Zewang Zhang, Zheng Sun, Jiaqi Liu, Jingwen Chen, Zhao Huo, Xiao Zhang", "abstract": "A deep learning approach has been widely applied in sequence modeling\nproblems. In terms of automatic speech recognition (ASR), its performance has\nsignificantly been improved by increasing large speech corpus and deeper neural\nnetwork. Especially, recurrent neural network and deep convolutional neural\nnetwork have been applied in ASR successfully. Given the arising problem of\ntraining speed, we build a novel deep recurrent convolutional network for\nacoustic modeling and then apply deep residual learning to it. Our experiments\nshow that it has not only faster convergence speed but better recognition\naccuracy over traditional deep convolutional recurrent network. In the\nexperiments, we compare the convergence speed of our novel deep recurrent\nconvolutional networks and traditional deep convolutional recurrent networks.\nWith faster convergence speed, our novel deep recurrent convolutional networks\ncan reach the comparable performance. We further show that applying deep\nresidual learning can boost the convergence speed of our novel deep recurret\nconvolutional networks. Finally, we evaluate all our experimental networks by\nphoneme error rate (PER) with our proposed bidirectional statistical n-gram\nlanguage model. Our evaluation results show that our newly proposed deep\nrecurrent convolutional network applied with deep residual learning can reach\nthe best PER of 17.33\\% with the fastest convergence speed on TIMIT database.\nThe outstanding performance of our novel deep recurrent convolutional neural\nnetwork with deep residual learning indicates that it can be potentially\nadopted in other sequential problems.", "journal": ""}
{"doi": "10.48550/arXiv.2110.06901", "date": "2021-10-13", "title": "A Survey on Deep Learning for Skeleton-Based Human Animation", "authors": "L. Mourot, L. Hoyet, F. Le Clerc, Fran\u00e7ois Schnitzler, Pierre Hellier", "abstract": "Human character animation is often critical in entertainment content\nproduction, including video games, virtual reality or fiction films. To this\nend, deep neural networks drive most recent advances through deep learning and\ndeep reinforcement learning. In this article, we propose a comprehensive survey\non the state-of-the-art approaches based on either deep learning or deep\nreinforcement learning in skeleton-based human character animation. First, we\nintroduce motion data representations, most common human motion datasets and\nhow basic deep models can be enhanced to foster learning of spatial and\ntemporal patterns in motion data. Second, we cover state-of-the-art approaches\ndivided into three large families of applications in human animation pipelines:\nmotion synthesis, character control and motion editing. Finally, we discuss the\nlimitations of the current state-of-the-art methods based on deep learning\nand/or deep reinforcement learning in skeletal human character animation and\npossible directions of future research to alleviate current limitations and\nmeet animators' needs.", "journal": ""}
{"doi": "10.48550/arXiv.1807.08169", "date": "2018-07-21", "title": "Recent Advances in Deep Learning: An Overview", "authors": "Matiur Rahman Minar, Jibon Naher", "abstract": "Deep Learning is one of the newest trends in Machine Learning and Artificial\nIntelligence research. It is also one of the most popular scientific research\ntrends now-a-days. Deep learning methods have brought revolutionary advances in\ncomputer vision and machine learning. Every now and then, new and new deep\nlearning techniques are being born, outperforming state-of-the-art machine\nlearning and even existing deep learning techniques. In recent years, the world\nhas seen many major breakthroughs in this field. Since deep learning is\nevolving at a huge speed, its kind of hard to keep track of the regular\nadvances especially for new researchers. In this paper, we are going to briefly\ndiscuss about recent advances in Deep Learning for past few years.", "journal": ""}
{"doi": "10.48550/arXiv.1805.10451", "date": "2018-05-26", "title": "Geometric Understanding of Deep Learning", "authors": "Na Lei, Zhongxuan Luo, Shing-Tung Yau, David Xianfeng Gu", "abstract": "Deep learning is the mainstream technique for many machine learning tasks,\nincluding image recognition, machine translation, speech recognition, and so\non. It has outperformed conventional methods in various fields and achieved\ngreat successes. Unfortunately, the understanding on how it works remains\nunclear. It has the central importance to lay down the theoretic foundation for\ndeep learning.\n  In this work, we give a geometric view to understand deep learning: we show\nthat the fundamental principle attributing to the success is the manifold\nstructure in data, namely natural high dimensional data concentrates close to a\nlow-dimensional manifold, deep learning learns the manifold and the probability\ndistribution on it.\n  We further introduce the concepts of rectified linear complexity for deep\nneural network measuring its learning capability, rectified linear complexity\nof an embedding manifold describing the difficulty to be learned. Then we show\nfor any deep neural network with fixed architecture, there exists a manifold\nthat cannot be learned by the network. Finally, we propose to apply optimal\nmass transportation theory to control the probability distribution in the\nlatent space.", "journal": ""}
{"doi": "10.48550/arXiv.2004.05366", "date": "2020-04-11", "title": "In-Machine-Learning Database: Reimagining Deep Learning with Old-School SQL", "authors": "Len Du", "abstract": "In-database machine learning has been very popular, almost being a cliche.\nHowever, can we do it the other way around? In this work, we say \"yes\" by\napplying plain old SQL to deep learning, in a sense implementing deep learning\nalgorithms with SQL. Most deep learning frameworks, as well as generic machine\nlearning ones, share a de facto standard of multidimensional array operations,\nunderneath fancier infrastructure such as automatic differentiation. As SQL\ntables can be regarded as generalisations of (multi-dimensional) arrays, we\nhave found a way to express common deep learning operations in SQL, encouraging\na different way of thinking and thus potentially novel models. In particular,\none of the latest trend in deep learning was the introduction of sparsity in\nthe name of graph convolutional networks, whereas we take sparsity almost for\ngranted in the database world. As both databases and machine learning involve\ntransformation of datasets, we hope this work can inspire further works\nutilizing the large body of existing wisdom, algorithms and technologies in the\ndatabase field to advance the state of the art in machine learning, rather than\nmerely integerating machine learning into databases.", "journal": ""}
{"doi": "10.48550/arXiv.1802.03596", "date": "2018-02-10", "title": "Deep Meta-Learning: Learning to Learn in the Concept Space", "authors": "Fengwei Zhou, Bin Wu, Zhenguo Li", "abstract": "Few-shot learning remains challenging for meta-learning that learns a\nlearning algorithm (meta-learner) from many related tasks. In this work, we\nargue that this is due to the lack of a good representation for meta-learning,\nand propose deep meta-learning to integrate the representation power of deep\nlearning into meta-learning. The framework is composed of three modules, a\nconcept generator, a meta-learner, and a concept discriminator, which are\nlearned jointly. The concept generator, e.g. a deep residual net, extracts a\nrepresentation for each instance that captures its high-level concept, on which\nthe meta-learner performs few-shot learning, and the concept discriminator\nrecognizes the concepts. By learning to learn in the concept space rather than\nin the complicated instance space, deep meta-learning can substantially improve\nvanilla meta-learning, which is demonstrated on various few-shot image\nrecognition problems. For example, on 5-way-1-shot image recognition on\nCIFAR-100 and CUB-200, it improves Matching Nets from 50.53% and 56.53% to\n58.18% and 63.47%, improves MAML from 49.28% and 50.45% to 56.65% and 64.63%,\nand improves Meta-SGD from 53.83% and 53.34% to 61.62% and 66.95%,\nrespectively.", "journal": ""}
{"doi": "10.48550/arXiv.2101.08387", "date": "2021-01-21", "title": "A Survey on Ensemble Learning under the Era of Deep Learning", "authors": "Yongquan Yang, Haijun Lv, Ning Chen", "abstract": "Due to the dominant position of deep learning (mostly deep neural networks)\nin various artificial intelligence applications, recently, ensemble learning\nbased on deep neural networks (ensemble deep learning) has shown significant\nperformances in improving the generalization of learning system. However, since\nmodern deep neural networks usually have millions to billions of parameters,\nthe time and space overheads for training multiple base deep learners and\ntesting with the ensemble deep learner are far greater than that of traditional\nensemble learning. Though several algorithms of fast ensemble deep learning\nhave been proposed to promote the deployment of ensemble deep learning in some\napplications, further advances still need to be made for many applications in\nspecific fields, where the developing time and computing resources are usually\nrestricted or the data to be processed is of large dimensionality. An urgent\nproblem needs to be solved is how to take the significant advantages of\nensemble deep learning while reduce the required expenses so that many more\napplications in specific fields can benefit from it. For the alleviation of\nthis problem, it is essential to know about how ensemble learning has developed\nunder the era of deep learning. Thus, in this article, we present fundamental\ndiscussions focusing on data analyses of published works, methodologies, recent\nadvances and unattainability of traditional ensemble learning and ensemble deep\nlearning. We hope this article will be helpful to realize the intrinsic\nproblems and technical challenges faced by future developments of ensemble\nlearning under the era of deep learning.", "journal": "Artificial Intelligence Review, 2022"}
{"doi": "10.48550/arXiv.2403.17561", "date": "2024-03-26", "title": "A Survey on State-of-the-art Deep Learning Applications and Challenges", "authors": "Mohd Halim Mohd Noor, Ayokunle Olalekan Ige", "abstract": "Deep learning, a branch of artificial intelligence, is a data-driven method\nthat uses multiple layers of interconnected units or neurons to learn intricate\npatterns and representations directly from raw input data. Empowered by this\nlearning capability, it has become a powerful tool for solving complex problems\nand is the core driver of many groundbreaking technologies and innovations.\nBuilding a deep learning model is challenging due to the algorithm's complexity\nand the dynamic nature of real-world problems. Several studies have reviewed\ndeep learning concepts and applications. However, the studies mostly focused on\nthe types of deep learning models and convolutional neural network\narchitectures, offering limited coverage of the state-of-the-art deep learning\nmodels and their applications in solving complex problems across different\ndomains. Therefore, motivated by the limitations, this study aims to\ncomprehensively review the state-of-the-art deep learning models in computer\nvision, natural language processing, time series analysis and pervasive\ncomputing, and robotics. We highlight the key features of the models and their\neffectiveness in solving the problems within each domain. Furthermore, this\nstudy presents the fundamentals of deep learning, various deep learning model\ntypes and prominent convolutional neural network architectures. Finally,\nchallenges and future directions in deep learning research are discussed to\noffer a broader perspective for future researchers.", "journal": ""}
{"doi": "10.48550/arXiv.2404.19226", "date": "2024-04-30", "title": "A Survey of Deep Learning Based Software Refactoring", "authors": "Bridget Nyirongo, Yanjie Jiang, He Jiang, Hui Liu", "abstract": "Refactoring is one of the most important activities in software engineering\nwhich is used to improve the quality of a software system. With the advancement\nof deep learning techniques, researchers are attempting to apply deep learning\ntechniques to software refactoring. Consequently, dozens of deep learning-based\nrefactoring approaches have been proposed. However, there is a lack of\ncomprehensive reviews on such works as well as a taxonomy for deep\nlearning-based refactoring. To this end, in this paper, we present a survey on\ndeep learning-based software refactoring. We classify related works into five\ncategories according to the major tasks they cover. Among these categories, we\nfurther present key aspects (i.e., code smell types, refactoring types,\ntraining strategies, and evaluation) to give insight into the details of the\ntechnologies that have supported refactoring through deep learning. The\nclassification indicates that there is an imbalance in the adoption of deep\nlearning techniques for the process of refactoring. Most of the deep learning\ntechniques have been used for the detection of code smells and the\nrecommendation of refactoring solutions as found in 56.25\\% and 33.33\\% of the\nliterature respectively. In contrast, only 6.25\\% and 4.17\\% were towards the\nend-to-end code transformation as refactoring and the mining of refactorings,\nrespectively. Notably, we found no literature representation for the quality\nassurance for refactoring. We also observe that most of the deep learning\ntechniques have been used to support refactoring processes occurring at the\nmethod level whereas classes and variables attracted minimal attention.\nFinally, we discuss the challenges and limitations associated with the\nemployment of deep learning-based refactorings and present some potential\nresearch opportunities for future work.", "journal": ""}
{"doi": "10.48550/arXiv.1904.10337", "date": "2019-04-22", "title": "MinCall - MinION end2end convolutional deep learning basecaller", "authors": "Neven Miculini\u0107, Marko Ratkovi\u0107, Mile \u0160iki\u0107", "abstract": "The Oxford Nanopore Technologies's MinION is the first portable DNA\nsequencing device. It is capable of producing long reads, over 100 kBp were\nreported. However, it has significantly higher error rate than other methods.\nIn this study, we present MinCall, an end2end basecaller model for the MinION.\nThe model is based on deep learning and uses convolutional neural networks\n(CNN) in its implementation. For extra performance, it uses cutting edge deep\nlearning techniques and architectures, batch normalization and Connectionist\nTemporal Classification (CTC) loss. The best performing deep learning model\nachieves 91.4% median match rate on E. Coli dataset using R9 pore chemistry and\n1D reads.", "journal": ""}
{"doi": "10.48550/arXiv.1902.05148", "date": "2019-02-11", "title": "Probabilistic Generative Deep Learning for Molecular Design", "authors": "Daniel T. Chang", "abstract": "Probabilistic generative deep learning for molecular design involves the\ndiscovery and design of new molecules and analysis of their structure,\nproperties and activities by probabilistic generative models using the deep\nlearning approach. It leverages the existing huge databases and publications of\nexperimental results, and quantum-mechanical calculations, to learn and explore\nmolecular structure, properties and activities. We discuss the major components\nof probabilistic generative deep learning for molecular design, which include\nmolecular structure, molecular representations, deep generative models,\nmolecular latent representations and latent space, molecular structure-property\nand structure-activity relationships, molecular similarity and molecular\ndesign. We highlight significant recent work using or applicable to this new\napproach.", "journal": ""}
{"doi": "10.48550/arXiv.2103.05127", "date": "2021-03-08", "title": "Model Complexity of Deep Learning: A Survey", "authors": "Xia Hu, Lingyang Chu, Jian Pei, Weiqing Liu, Jiang Bian", "abstract": "Model complexity is a fundamental problem in deep learning. In this paper we\nconduct a systematic overview of the latest studies on model complexity in deep\nlearning. Model complexity of deep learning can be categorized into expressive\ncapacity and effective model complexity. We review the existing studies on\nthose two categories along four important factors, including model framework,\nmodel size, optimization process and data complexity. We also discuss the\napplications of deep learning model complexity including understanding model\ngeneralization, model optimization, and model selection and design. We conclude\nby proposing several interesting future directions.", "journal": ""}
{"doi": "10.48550/arXiv.2312.12904", "date": "2023-12-20", "title": "PGN: A perturbation generation network against deep reinforcement learning", "authors": "Xiangjuan Li, Feifan Li, Yang Li, Quan Pan", "abstract": "Deep reinforcement learning has advanced greatly and applied in many areas.\nIn this paper, we explore the vulnerability of deep reinforcement learning by\nproposing a novel generative model for creating effective adversarial examples\nto attack the agent. Our proposed model can achieve both targeted attacks and\nuntargeted attacks. Considering the specificity of deep reinforcement learning,\nwe propose the action consistency ratio as a measure of stealthiness, and a new\nmeasurement index of effectiveness and stealthiness. Experiment results show\nthat our method can ensure the effectiveness and stealthiness of attack\ncompared with other algorithms. Moreover, our methods are considerably faster\nand thus can achieve rapid and efficient verification of the vulnerability of\ndeep reinforcement learning.", "journal": ""}
{"doi": "10.48550/arXiv.1611.00336", "date": "2016-11-01", "title": "Stochastic Variational Deep Kernel Learning", "authors": "Andrew Gordon Wilson, Zhiting Hu, Ruslan Salakhutdinov, Eric P. Xing", "abstract": "Deep kernel learning combines the non-parametric flexibility of kernel\nmethods with the inductive biases of deep learning architectures. We propose a\nnovel deep kernel learning model and stochastic variational inference procedure\nwhich generalizes deep kernel learning approaches to enable classification,\nmulti-task learning, additive covariance structures, and stochastic gradient\ntraining. Specifically, we apply additive base kernels to subsets of output\nfeatures from deep neural architectures, and jointly learn the parameters of\nthe base kernels and deep network through a Gaussian process marginal\nlikelihood objective. Within this framework, we derive an efficient form of\nstochastic variational inference which leverages local kernel interpolation,\ninducing points, and structure exploiting algebra. We show improved performance\nover stand alone deep networks, SVMs, and state of the art scalable Gaussian\nprocesses on several classification benchmarks, including an airline delay\ndataset containing 6 million training points, CIFAR, and ImageNet.", "journal": ""}
{"doi": "10.48550/arXiv.1612.07640", "date": "2016-12-16", "title": "Deep Learning and Its Applications to Machine Health Monitoring: A Survey", "authors": "Rui Zhao, Ruqiang Yan, Zhenghua Chen, Kezhi Mao, Peng Wang, Robert X. Gao", "abstract": "Since 2006, deep learning (DL) has become a rapidly growing research\ndirection, redefining state-of-the-art performances in a wide range of areas\nsuch as object recognition, image segmentation, speech recognition and machine\ntranslation. In modern manufacturing systems, data-driven machine health\nmonitoring is gaining in popularity due to the widespread deployment of\nlow-cost sensors and their connection to the Internet. Meanwhile, deep learning\nprovides useful tools for processing and analyzing these big machinery data.\nThe main purpose of this paper is to review and summarize the emerging research\nwork of deep learning on machine health monitoring. After the brief\nintroduction of deep learning techniques, the applications of deep learning in\nmachine health monitoring systems are reviewed mainly from the following\naspects: Auto-encoder (AE) and its variants, Restricted Boltzmann Machines and\nits variants including Deep Belief Network (DBN) and Deep Boltzmann Machines\n(DBM), Convolutional Neural Networks (CNN) and Recurrent Neural Networks (RNN).\nFinally, some new trends of DL-based machine health monitoring methods are\ndiscussed.", "journal": ""}
{"doi": "10.48550/arXiv.1806.08874", "date": "2018-06-22", "title": "The Foundations of Deep Learning with a Path Towards General Intelligence", "authors": "Eray \u00d6zkural", "abstract": "Like any field of empirical science, AI may be approached axiomatically. We\nformulate requirements for a general-purpose, human-level AI system in terms of\npostulates. We review the methodology of deep learning, examining the explicit\nand tacit assumptions in deep learning research. Deep Learning methodology\nseeks to overcome limitations in traditional machine learning research as it\ncombines facets of model richness, generality, and practical applicability. The\nmethodology so far has produced outstanding results due to a productive synergy\nof function approximation, under plausible assumptions of irreducibility and\nthe efficiency of back-propagation family of algorithms. We examine these\nwinning traits of deep learning, and also observe the various known failure\nmodes of deep learning. We conclude by giving recommendations on how to extend\ndeep learning methodology to cover the postulates of general-purpose AI\nincluding modularity, and cognitive architecture. We also relate deep learning\nto advances in theoretical neuroscience research.", "journal": ""}
{"doi": "10.48550/arXiv.2004.12524", "date": "2020-04-27", "title": "Sequential Interpretability: Methods, Applications, and Future Direction for Understanding Deep Learning Models in the Context of Sequential Data", "authors": "Benjamin Shickel, Parisa Rashidi", "abstract": "Deep learning continues to revolutionize an ever-growing number of critical\napplication areas including healthcare, transportation, finance, and basic\nsciences. Despite their increased predictive power, model transparency and\nhuman explainability remain a significant challenge due to the \"black box\"\nnature of modern deep learning models. In many cases the desired balance\nbetween interpretability and performance is predominately task specific.\nHuman-centric domains such as healthcare necessitate a renewed focus on\nunderstanding how and why these frameworks are arriving at critical and\npotentially life-or-death decisions. Given the quantity of research and\nempirical successes of deep learning for computer vision, most of the existing\ninterpretability research has focused on image processing techniques.\nComparatively, less attention has been paid to interpreting deep learning\nframeworks using sequential data. Given recent deep learning advancements in\nhighly sequential domains such as natural language processing and physiological\nsignal processing, the need for deep sequential explanations is at an all-time\nhigh. In this paper, we review current techniques for interpreting deep\nlearning techniques involving sequential data, identify similarities to\nnon-sequential methods, and discuss current limitations and future avenues of\nsequential interpretability research.", "journal": ""}
{"doi": "10.48550/arXiv.2208.00203", "date": "2022-07-30", "title": "Adding Context to Source Code Representations for Deep Learning", "authors": "Fuwei Tian, Christoph Treude", "abstract": "Deep learning models have been successfully applied to a variety of software\nengineering tasks, such as code classification, summarisation, and bug and\nvulnerability detection. In order to apply deep learning to these tasks, source\ncode needs to be represented in a format that is suitable for input into the\ndeep learning model. Most approaches to representing source code, such as\ntokens, abstract syntax trees (ASTs), data flow graphs (DFGs), and control flow\ngraphs (CFGs) only focus on the code itself and do not take into account\nadditional context that could be useful for deep learning models. In this\npaper, we argue that it is beneficial for deep learning models to have access\nto additional contextual information about the code being analysed. We present\npreliminary evidence that encoding context from the call hierarchy along with\ninformation from the code itself can improve the performance of a\nstate-of-the-art deep learning model for two software engineering tasks. We\noutline our research agenda for adding further contextual information to source\ncode representations for deep learning.", "journal": ""}
{"doi": "10.48550/arXiv.2208.07643", "date": "2022-08-16", "title": "A Review of the Convergence of 5G/6G Architecture and Deep Learning", "authors": "Olusola T. Odeyomi, Olubiyi O. Akintade, Temitayo O. Olowu, Gergely Zaruba", "abstract": "The convergence of 5G architecture and deep learning has gained a lot of\nresearch interests in both the fields of wireless communication and artificial\nintelligence. This is because deep learning technologies have been identified\nto be the potential driver of the 5G technologies, that make up the 5G\narchitecture. Hence, there have been extensive surveys on the convergence of 5G\narchitecture and deep learning. However, most of the existing survey papers\nmainly focused on how deep learning can converge with a specific 5G technology,\nthus, not covering the full spectrum of the 5G architecture. Although there is\na recent survey paper that appears to be robust, a review of that paper shows\nthat it is not well structured to specifically cover the convergence of deep\nlearning and the 5G technologies. Hence, this paper provides a robust overview\nof the convergence of the key 5G technologies and deep learning. The challenges\nfaced by such convergence are discussed. In addition, a brief overview of the\nfuture 6G architecture, and how it can converge with deep learning is also\ndiscussed.", "journal": ""}
{"doi": "10.48550/arXiv.2309.08500", "date": "2023-09-15", "title": "Deep-learning-powered data analysis in plankton ecology", "authors": "Harshith Bachimanchi, Matthew I. M. Pinder, Chlo\u00e9 Robert, Pierre De Wit, Jonathan Havenhand, Alexandra Kinnby, Daniel Midtvedt, Erik Selander, Giovanni Volpe", "abstract": "The implementation of deep learning algorithms has brought new perspectives\nto plankton ecology. Emerging as an alternative approach to established\nmethods, deep learning offers objective schemes to investigate plankton\norganisms in diverse environments. We provide an overview of\ndeep-learning-based methods including detection and classification of phyto-\nand zooplankton images, foraging and swimming behaviour analysis, and finally\necological modelling. Deep learning has the potential to speed up the analysis\nand reduce the human experimental bias, thus enabling data acquisition at\nrelevant temporal and spatial scales with improved reproducibility. We also\ndiscuss shortcomings and show how deep learning architectures have evolved to\nmitigate imprecise readouts. Finally, we suggest opportunities where deep\nlearning is particularly likely to catalyze plankton research. The examples are\naccompanied by detailed tutorials and code samples that allow readers to apply\nthe methods described in this review to their own data.", "journal": ""}
{"doi": "10.48550/arXiv.2210.08367", "date": "2022-10-15", "title": "Active Learning with Neural Networks: Insights from Nonparametric Statistics", "authors": "Yinglun Zhu, Robert Nowak", "abstract": "Deep neural networks have great representation power, but typically require\nlarge numbers of training examples. This motivates deep active learning methods\nthat can significantly reduce the amount of labeled training data. Empirical\nsuccesses of deep active learning have been recently reported in the\nliterature, however, rigorous label complexity guarantees of deep active\nlearning have remained elusive. This constitutes a significant gap between\ntheory and practice. This paper tackles this gap by providing the first\nnear-optimal label complexity guarantees for deep active learning. The key\ninsight is to study deep active learning from the nonparametric classification\nperspective. Under standard low noise conditions, we show that active learning\nwith neural networks can provably achieve the minimax label complexity, up to\ndisagreement coefficient and other logarithmic terms. When equipped with an\nabstention option, we further develop an efficient deep active learning\nalgorithm that achieves $\\mathsf{polylog}(\\frac{1}{\\epsilon})$ label\ncomplexity, without any low noise assumptions. We also provide extensions of\nour results beyond the commonly studied Sobolev/H\\\"older spaces and develop\nlabel complexity guarantees for learning in Radon $\\mathsf{BV}^2$ spaces, which\nhave recently been proposed as natural function spaces associated with neural\nnetworks.", "journal": ""}
{"doi": "10.48550/arXiv.2211.03374", "date": "2022-11-07", "title": "Deep Causal Learning: Representation, Discovery and Inference", "authors": "Zizhen Deng, Xiaolong Zheng, Hu Tian, Daniel Dajun Zeng", "abstract": "Causal learning has garnered significant attention in recent years because it\nreveals the essential relationships that underpin phenomena and delineates the\nmechanisms by which the world evolves. Nevertheless, traditional causal\nlearning methods face numerous challenges and limitations, including\nhigh-dimensional, unstructured variables, combinatorial optimization problems,\nunobserved confounders, selection biases, and estimation inaccuracies. Deep\ncausal learning, which leverages deep neural networks, offers innovative\ninsights and solutions for addressing these challenges. Although numerous deep\nlearning-based methods for causal discovery and inference have been proposed,\nthere remains a dearth of reviews examining the underlying mechanisms by which\ndeep learning can enhance causal learning. In this article, we comprehensively\nreview how deep learning can contribute to causal learning by tackling\ntraditional challenges across three key dimensions: representation, discovery,\nand inference. We emphasize that deep causal learning is pivotal for advancing\nthe theoretical frontiers and broadening the practical applications of causal\nscience. We conclude by summarizing open issues and outlining potential\ndirections for future research.", "journal": ""}
{"doi": "10.48550/arXiv.2410.07564", "date": "2024-10-10", "title": "Boosting Deep Ensembles with Learning Rate Tuning", "authors": "Hongpeng Jin, Yanzhao Wu", "abstract": "The Learning Rate (LR) has a high impact on deep learning training\nperformance. A common practice is to train a Deep Neural Network (DNN) multiple\ntimes with different LR policies to find the optimal LR policy, which has been\nwidely recognized as a daunting and costly task. Moreover, multiple times of\nDNN training has not been effectively utilized. In practice, often only the\noptimal LR is adopted, which misses the opportunities to further enhance the\noverall accuracy of the deep learning system and results in a huge waste of\nboth computing resources and training time. This paper presents a novel\nframework, LREnsemble, to effectively leverage effective learning rate tuning\nto boost deep ensemble performance. We make three original contributions.\nFirst, we show that the LR tuning with different LR policies can produce highly\ndiverse DNNs, which can be supplied as base models for deep ensembles. Second,\nwe leverage different ensemble selection algorithms to identify high-quality\ndeep ensembles from the large pool of base models with significant accuracy\nimprovements over the best single base model. Third, we propose LREnsemble, a\nframework that utilizes the synergy of LR tuning and deep ensemble techniques\nto enhance deep learning performance. The experiments on multiple benchmark\ndatasets have demonstrated the effectiveness of LREnsemble, generating up to\n2.34% accuracy improvements over well-optimized baselines.", "journal": ""}
{"doi": "10.48550/arXiv.1305.0445", "date": "2013-05-02", "title": "Deep Learning of Representations: Looking Forward", "authors": "Yoshua Bengio", "abstract": "Deep learning research aims at discovering learning algorithms that discover\nmultiple levels of distributed representations, with higher levels representing\nmore abstract concepts. Although the study of deep learning has already led to\nimpressive theoretical results, learning algorithms and breakthrough\nexperiments, several challenges lie ahead. This paper proposes to examine some\nof these challenges, centering on the questions of scaling deep learning\nalgorithms to much larger models and datasets, reducing optimization\ndifficulties due to ill-conditioning or local minima, designing more efficient\nand powerful inference and sampling procedures, and learning to disentangle the\nfactors of variation underlying the observed data. It also proposes a few\nforward-looking research directions aimed at overcoming these challenges.", "journal": ""}
{"doi": "10.48550/arXiv.1905.10817", "date": "2019-05-26", "title": "Deep Online Learning with Stochastic Constraints", "authors": "Guy Uziel", "abstract": "Deep learning models are considered to be state-of-the-art in many offline\nmachine learning tasks. However, many of the techniques developed are not\nsuitable for online learning tasks. The problem of using deep learning models\nwith sequential data becomes even harder when several loss functions need to be\nconsidered simultaneously, as in many real-world applications. In this paper,\nwe, therefore, propose a novel online deep learning training procedure which\ncan be used regardless of the neural network's architecture, aiming to deal\nwith the multiple objectives case. We demonstrate and show the effectiveness of\nour algorithm on the Neyman-Pearson classification problem on several benchmark\ndatasets.", "journal": ""}
{"doi": "10.48550/arXiv.1909.04751", "date": "2019-09-10", "title": "Reinforcement Learning and Video Games", "authors": "Yue Zheng", "abstract": "Reinforcement learning has exceeded human-level performance in game playing\nAI with deep learning methods according to the experiments from DeepMind on Go\nand Atari games. Deep learning solves high dimension input problems which stop\nthe development of reinforcement for many years. This study uses both two\ntechniques to create several agents with different algorithms that successfully\nlearn to play T-rex Runner. Deep Q network algorithm and three types of\nimprovements are implemented to train the agent. The results from some of them\nare far from satisfactory but others are better than human experts. Batch\nnormalization is a method to solve internal covariate shift problems in deep\nneural network. The positive influence of this on reinforcement learning has\nalso been proved in this study.", "journal": ""}
{"doi": "10.48550/arXiv.1801.01968", "date": "2018-01-06", "title": "Faster Deep Q-learning using Neural Episodic Control", "authors": "Daichi Nishio, Satoshi Yamane", "abstract": "The research on deep reinforcement learning which estimates Q-value by deep\nlearning has been attracted the interest of researchers recently. In deep\nreinforcement learning, it is important to efficiently learn the experiences\nthat an agent has collected by exploring environment. We propose NEC2DQN that\nimproves learning speed of a poor sample efficiency algorithm such as DQN by\nusing good one such as NEC at the beginning of learning. We show it is able to\nlearn faster than Double DQN or N-step DQN in the experiments of Pong.", "journal": ""}
{"doi": "10.48550/arXiv.2105.07636", "date": "2021-05-17", "title": "DOC3-Deep One Class Classification using Contradictions", "authors": "Sauptik Dhar, Bernardo Gonzalez Torres", "abstract": "This paper introduces the notion of learning from contradictions (a.k.a\nUniversum learning) for deep one class classification problems. We formalize\nthis notion for the widely adopted one class large-margin loss, and propose the\nDeep One Class Classification using Contradictions (DOC3) algorithm. We show\nthat learning from contradictions incurs lower generalization error by\ncomparing the Empirical Rademacher Complexity (ERC) of DOC3 against its\ntraditional inductive learning counterpart. Our empirical results demonstrate\nthe efficacy of DOC3 compared to popular baseline algorithms on several\nreal-life data sets.", "journal": ""}
{"doi": "10.48550/arXiv.1906.01432", "date": "2019-05-31", "title": "Knowledge-augmented Column Networks: Guiding Deep Learning with Advice", "authors": "Mayukh Das, Devendra Singh Dhami, Yang Yu, Gautam Kunapuli, Sriraam Natarajan", "abstract": "Recently, deep models have had considerable success in several tasks,\nespecially with low-level representations. However, effective learning from\nsparse noisy samples is a major challenge in most deep models, especially in\ndomains with structured representations. Inspired by the proven success of\nhuman guided machine learning, we propose Knowledge-augmented Column Networks,\na relational deep learning framework that leverages human advice/knowledge to\nlearn better models in presence of sparsity and systematic noise.", "journal": ""}
{"doi": "10.48550/arXiv.2106.07798", "date": "2021-06-14", "title": "Poisoning Deep Reinforcement Learning Agents with In-Distribution Triggers", "authors": "Chace Ashcraft, Kiran Karra", "abstract": "In this paper, we propose a new data poisoning attack and apply it to deep\nreinforcement learning agents. Our attack centers on what we call\nin-distribution triggers, which are triggers native to the data distributions\nthe model will be trained on and deployed in. We outline a simple procedure for\nembedding these, and other, triggers in deep reinforcement learning agents\nfollowing a multi-task learning paradigm, and demonstrate in three common\nreinforcement learning environments. We believe that this work has important\nimplications for the security of deep learning models.", "journal": ""}
{"doi": "10.48550/arXiv.1912.03735", "date": "2019-12-08", "title": "Security of Deep Learning Methodologies: Challenges and Opportunities", "authors": "Shahbaz Rezaei, Xin Liu", "abstract": "Despite the plethora of studies about security vulnerabilities and defenses\nof deep learning models, security aspects of deep learning methodologies, such\nas transfer learning, have been rarely studied. In this article, we highlight\nthe security challenges and research opportunities of these methodologies,\nfocusing on vulnerabilities and attacks unique to them.", "journal": ""}
{"doi": "10.48550/arXiv.1711.06929", "date": "2017-11-18", "title": "Deep Gaussian Mixture Models", "authors": "Cinzia Viroli, Geoffrey J. McLachlan", "abstract": "Deep learning is a hierarchical inference method formed by subsequent\nmultiple layers of learning able to more efficiently describe complex\nrelationships. In this work, Deep Gaussian Mixture Models are introduced and\ndiscussed. A Deep Gaussian Mixture model (DGMM) is a network of multiple layers\nof latent variables, where, at each layer, the variables follow a mixture of\nGaussian distributions. Thus, the deep mixture model consists of a set of\nnested mixtures of linear models, which globally provide a nonlinear model able\nto describe the data in a very flexible way. In order to avoid\noverparameterized solutions, dimension reduction by factor models can be\napplied at each layer of the architecture thus resulting in deep mixtures of\nfactor analysers.", "journal": ""}
{"doi": "10.48550/arXiv.1812.08904", "date": "2018-12-21", "title": "Pre-training with Non-expert Human Demonstration for Deep Reinforcement Learning", "authors": "Gabriel V. de la Cruz, Yunshu Du, Matthew E. Taylor", "abstract": "Deep reinforcement learning (deep RL) has achieved superior performance in\ncomplex sequential tasks by using deep neural networks as function\napproximators to learn directly from raw input images. However, learning\ndirectly from raw images is data inefficient. The agent must learn feature\nrepresentation of complex states in addition to learning a policy. As a result,\ndeep RL typically suffers from slow learning speeds and often requires a\nprohibitively large amount of training time and data to reach reasonable\nperformance, making it inapplicable to real-world settings where data is\nexpensive. In this work, we improve data efficiency in deep RL by addressing\none of the two learning goals, feature learning. We leverage supervised\nlearning to pre-train on a small set of non-expert human demonstrations and\nempirically evaluate our approach using the asynchronous advantage actor-critic\nalgorithms (A3C) in the Atari domain. Our results show significant improvements\nin learning speed, even when the provided demonstration is noisy and of low\nquality.", "journal": ""}
{"doi": "10.48550/arXiv.1907.07543", "date": "2019-07-17", "title": "Low-Shot Classification: A Comparison of Classical and Deep Transfer Machine Learning Approaches", "authors": "Peter Usherwood, Steven Smit", "abstract": "Despite the recent success of deep transfer learning approaches in NLP, there\nis a lack of quantitative studies demonstrating the gains these models offer in\nlow-shot text classification tasks over existing paradigms. Deep transfer\nlearning approaches such as BERT and ULMFiT demonstrate that they can beat\nstate-of-the-art results on larger datasets, however when one has only 100-1000\nlabelled examples per class, the choice of approach is less clear, with\nclassical machine learning and deep transfer learning representing valid\noptions. This paper compares the current best transfer learning approach with\ntop classical machine learning approaches on a trinary sentiment classification\ntask to assess the best paradigm. We find that BERT, representing the best of\ndeep transfer learning, is the best performing approach, outperforming top\nclassical machine learning algorithms by 9.7% on average when trained with 100\nexamples per class, narrowing to 1.8% at 1000 labels per class. We also show\nthe robustness of deep transfer learning in moving across domains, where the\nmaximum loss in accuracy is only 0.7% in similar domain tasks and 3.2% cross\ndomain, compared to classical machine learning which loses up to 20.6%.", "journal": ""}
{"doi": "10.48550/arXiv.1803.07608", "date": "2018-03-20", "title": "A Survey of Deep Learning Techniques for Mobile Robot Applications", "authors": "Jahanzaib Shabbir, Tarique Anwer", "abstract": "Advancements in deep learning over the years have attracted research into how\ndeep artificial neural networks can be used in robotic systems. This research\nsurvey will present a summarization of the current research with a specific\nfocus on the gains and obstacles for deep learning to be applied to mobile\nrobotics.", "journal": ""}
{"doi": "10.48550/arXiv.1709.04083", "date": "2017-09-12", "title": "Pre-training Neural Networks with Human Demonstrations for Deep Reinforcement Learning", "authors": "Gabriel V. de la Cruz Jr, Yunshu Du, Matthew E. Taylor", "abstract": "Deep reinforcement learning (deep RL) has achieved superior performance in\ncomplex sequential tasks by using a deep neural network as its function\napproximator and by learning directly from raw images. A drawback of using raw\nimages is that deep RL must learn the state feature representation from the raw\nimages in addition to learning a policy. As a result, deep RL can require a\nprohibitively large amount of training time and data to reach reasonable\nperformance, making it difficult to use deep RL in real-world applications,\nespecially when data is expensive. In this work, we speed up training by\naddressing half of what deep RL is trying to solve --- learning features. Our\napproach is to learn some of the important features by pre-training deep RL\nnetwork's hidden layers via supervised learning using a small set of human\ndemonstrations. We empirically evaluate our approach using deep Q-network (DQN)\nand asynchronous advantage actor-critic (A3C) algorithms on the Atari 2600\ngames of Pong, Freeway, and Beamrider. Our results show that: 1) pre-training\nwith human demonstrations in a supervised learning manner is better at\ndiscovering features relative to pre-training naively in DQN, and 2)\ninitializing a deep RL network with a pre-trained model provides a significant\nimprovement in training time even when pre-training from a small number of\nhuman demonstrations.", "journal": ""}
{"doi": "10.48550/arXiv.2210.04142", "date": "2022-10-09", "title": "Deep Clustering: A Comprehensive Survey", "authors": "Yazhou Ren, Jingyu Pu, Zhimeng Yang, Jie Xu, Guofeng Li, Xiaorong Pu, Philip S. Yu, Lifang He", "abstract": "Cluster analysis plays an indispensable role in machine learning and data\nmining. Learning a good data representation is crucial for clustering\nalgorithms. Recently, deep clustering, which can learn clustering-friendly\nrepresentations using deep neural networks, has been broadly applied in a wide\nrange of clustering tasks. Existing surveys for deep clustering mainly focus on\nthe single-view fields and the network architectures, ignoring the complex\napplication scenarios of clustering. To address this issue, in this paper we\nprovide a comprehensive survey for deep clustering in views of data sources.\nWith different data sources and initial conditions, we systematically\ndistinguish the clustering methods in terms of methodology, prior knowledge,\nand architecture. Concretely, deep clustering methods are introduced according\nto four categories, i.e., traditional single-view deep clustering,\nsemi-supervised deep clustering, deep multi-view clustering, and deep transfer\nclustering. Finally, we discuss the open challenges and potential future\nopportunities in different fields of deep clustering.", "journal": ""}
{"doi": "10.48550/arXiv.1912.13171", "date": "2019-12-31", "title": "Deep Learning on Image Denoising: An overview", "authors": "Chunwei Tian, Lunke Fei, Wenxian Zheng, Yong Xu, Wangmeng Zuo, Chia-Wen Lin", "abstract": "Deep learning techniques have received much attention in the area of image\ndenoising. However, there are substantial differences in the various types of\ndeep learning methods dealing with image denoising. Specifically,\ndiscriminative learning based on deep learning can ably address the issue of\nGaussian noise. Optimization models based on deep learning are effective in\nestimating the real noise. However, there has thus far been little related\nresearch to summarize the different deep learning techniques for image\ndenoising. In this paper, we offer a comparative study of deep techniques in\nimage denoising. We first classify the deep convolutional neural networks\n(CNNs) for additive white noisy images; the deep CNNs for real noisy images;\nthe deep CNNs for blind denoising and the deep CNNs for hybrid noisy images,\nwhich represents the combination of noisy, blurred and low-resolution images.\nThen, we analyze the motivations and principles of the different types of deep\nlearning methods. Next, we compare the state-of-the-art methods on public\ndenoising datasets in terms of quantitative and qualitative analysis. Finally,\nwe point out some potential challenges and directions of future research.", "journal": ""}
{"doi": "10.48550/arXiv.1707.08325", "date": "2017-07-26", "title": "Asymmetric Deep Supervised Hashing", "authors": "Qing-Yuan Jiang, Wu-Jun Li", "abstract": "Hashing has been widely used for large-scale approximate nearest neighbor\nsearch because of its storage and search efficiency. Recent work has found that\ndeep supervised hashing can significantly outperform non-deep supervised\nhashing in many applications. However, most existing deep supervised hashing\nmethods adopt a symmetric strategy to learn one deep hash function for both\nquery points and database (retrieval) points. The training of these symmetric\ndeep supervised hashing methods is typically time-consuming, which makes them\nhard to effectively utilize the supervised information for cases with\nlarge-scale database. In this paper, we propose a novel deep supervised hashing\nmethod, called asymmetric deep supervised hashing (ADSH), for large-scale\nnearest neighbor search. ADSH treats the query points and database points in an\nasymmetric way. More specifically, ADSH learns a deep hash function only for\nquery points, while the hash codes for database points are directly learned.\nThe training of ADSH is much more efficient than that of traditional symmetric\ndeep supervised hashing methods. Experiments show that ADSH can achieve\nstate-of-the-art performance in real applications.", "journal": ""}
{"doi": "10.48550/arXiv.2205.06571", "date": "2022-05-13", "title": "Convergence Analysis of Deep Residual Networks", "authors": "Wentao Huang, Haizhang Zhang", "abstract": "Various powerful deep neural network architectures have made great\ncontribution to the exciting successes of deep learning in the past two\ndecades. Among them, deep Residual Networks (ResNets) are of particular\nimportance because they demonstrated great usefulness in computer vision by\nwinning the first place in many deep learning competitions. Also, ResNets were\nthe first class of neural networks in the development history of deep learning\nthat are really deep. It is of mathematical interest and practical meaning to\nunderstand the convergence of deep ResNets. We aim at characterizing the\nconvergence of deep ResNets as the depth tends to infinity in terms of the\nparameters of the networks. Toward this purpose, we first give a matrix-vector\ndescription of general deep neural networks with shortcut connections and\nformulate an explicit expression for the networks by using the notions of\nactivation domains and activation matrices. The convergence is then reduced to\nthe convergence of two series involving infinite products of non-square\nmatrices. By studying the two series, we establish a sufficient condition for\npointwise convergence of ResNets. Our result is able to give justification for\nthe design of ResNets. We also conduct experiments on benchmark machine\nlearning data to verify our results.", "journal": ""}
{"doi": "10.48550/arXiv.1605.06391", "date": "2016-05-20", "title": "Deep Multi-task Representation Learning: A Tensor Factorisation Approach", "authors": "Yongxin Yang, Timothy Hospedales", "abstract": "Most contemporary multi-task learning methods assume linear models. This\nsetting is considered shallow in the era of deep learning. In this paper, we\npresent a new deep multi-task representation learning framework that learns\ncross-task sharing structure at every layer in a deep network. Our approach is\nbased on generalising the matrix factorisation techniques explicitly or\nimplicitly used by many conventional MTL algorithms to tensor factorisation, to\nrealise automatic learning of end-to-end knowledge sharing in deep networks.\nThis is in contrast to existing deep learning approaches that need a\nuser-defined multi-task sharing strategy. Our approach applies to both\nhomogeneous and heterogeneous MTL. Experiments demonstrate the efficacy of our\ndeep multi-task representation learning in terms of both higher accuracy and\nfewer design choices.", "journal": ""}
{"doi": "10.48550/arXiv.2006.05612", "date": "2020-06-10", "title": "Deep Learning for Change Detection in Remote Sensing Images: Comprehensive Review and Meta-Analysis", "authors": "Lazhar Khelifi, Max Mignotte", "abstract": "Deep learning (DL) algorithms are considered as a methodology of choice for\nremote-sensing image analysis over the past few years. Due to its effective\napplications, deep learning has also been introduced for automatic change\ndetection and achieved great success. The present study attempts to provide a\ncomprehensive review and a meta-analysis of the recent progress in this\nsubfield. Specifically, we first introduce the fundamentals of deep learning\nmethods which arefrequently adopted for change detection. Secondly, we present\nthe details of the meta-analysis conducted to examine the status of change\ndetection DL studies. Then, we focus on deep learning-based change detection\nmethodologies for remote sensing images by giving a general overview of the\nexisting methods. Specifically, these deep learning-based methods were\nclassified into three groups; fully supervised learning-based methods, fully\nunsupervised learning-based methods and transfer learning-based techniques. As\na result of these investigations, promising new directions were identified for\nfuture research. This study will contribute in several ways to our\nunderstanding of deep learning for change detection and will provide a basis\nfor further research.", "journal": ""}
{"doi": "10.48550/arXiv.1801.00904", "date": "2018-01-03", "title": "ScreenerNet: Learning Self-Paced Curriculum for Deep Neural Networks", "authors": "Tae-Hoon Kim, Jonghyun Choi", "abstract": "We propose to learn a curriculum or a syllabus for supervised learning and\ndeep reinforcement learning with deep neural networks by an attachable deep\nneural network, called ScreenerNet. Specifically, we learn a weight for each\nsample by jointly training the ScreenerNet and the main network in an\nend-to-end self-paced fashion. The ScreenerNet neither has sampling bias nor\nrequires to remember the past learning history. We show the networks augmented\nwith the ScreenerNet achieve early convergence with better accuracy than the\nstate-of-the-art curricular learning methods in extensive experiments using\nthree popular vision datasets such as MNIST, CIFAR10 and Pascal VOC2012, and a\nCart-pole task using Deep Q-learning. Moreover, the ScreenerNet can extend\nother curriculum learning methods such as Prioritized Experience Replay (PER)\nfor further accuracy improvement.", "journal": ""}
{"doi": "10.48550/arXiv.1805.07504", "date": "2018-05-19", "title": "Deep Loopy Neural Network Model for Graph Structured Data Representation Learning", "authors": "Jiawei Zhang", "abstract": "Existing deep learning models may encounter great challenges in handling\ngraph structured data. In this paper, we introduce a new deep learning model\nfor graph data specifically, namely the deep loopy neural network.\nSignificantly different from the previous deep models, inside the deep loopy\nneural network, there exist a large number of loops created by the extensive\nconnections among nodes in the input graph data, which makes model learning an\ninfeasible task. To resolve such a problem, in this paper, we will introduce a\nnew learning algorithm for the deep loopy neural network specifically. Instead\nof learning the model variables based on the original model, in the proposed\nlearning algorithm, errors will be back-propagated through the edges in a group\nof extracted spanning trees. Extensive numerical experiments have been done on\nseveral real-world graph datasets, and the experimental results demonstrate the\neffectiveness of both the proposed model and the learning algorithm in handling\ngraph data.", "journal": ""}
{"doi": "10.48550/arXiv.1901.04355", "date": "2019-01-14", "title": "Iterative Deep Learning Based Unbiased Stereology With Human-in-the-Loop", "authors": "Saeed S. Alahmari, Dmitry Goldgof, Lawrence O. Hall, Palak Dave, Hady Ahmady Phoulady, Peter R. Mouton", "abstract": "Lack of enough labeled data is a major problem in building machine learning\nbased models when the manual annotation (labeling) is error-prone, expensive,\ntedious, and time-consuming. In this paper, we introduce an iterative deep\nlearning based method to improve segmentation and counting of cells based on\nunbiased stereology applied to regions of interest of extended depth of field\n(EDF) images. This method uses an existing machine learning algorithm called\nthe adaptive segmentation algorithm (ASA) to generate masks (verified by a\nuser) for EDF images to train deep learning models. Then an iterative deep\nlearning approach is used to feed newly predicted and accepted deep learning\nmasks/images (verified by a user) to the training set of the deep learning\nmodel. The error rate in unbiased stereology count of cells on an unseen test\nset reduced from about 3 % to less than 1 % after 5 iterations of the iterative\ndeep learning based unbiased stereology process.", "journal": ""}
{"doi": "10.48550/arXiv.1907.04490", "date": "2019-07-10", "title": "Deep Lagrangian Networks: Using Physics as Model Prior for Deep Learning", "authors": "Michael Lutter, Christian Ritter, Jan Peters", "abstract": "Deep learning has achieved astonishing results on many tasks with large\namounts of data and generalization within the proximity of training data. For\nmany important real-world applications, these requirements are unfeasible and\nadditional prior knowledge on the task domain is required to overcome the\nresulting problems. In particular, learning physics models for model-based\ncontrol requires robust extrapolation from fewer samples - often collected\nonline in real-time - and model errors may lead to drastic damages of the\nsystem. Directly incorporating physical insight has enabled us to obtain a\nnovel deep model learning approach that extrapolates well while requiring fewer\nsamples. As a first example, we propose Deep Lagrangian Networks (DeLaN) as a\ndeep network structure upon which Lagrangian Mechanics have been imposed. DeLaN\ncan learn the equations of motion of a mechanical system (i.e., system\ndynamics) with a deep network efficiently while ensuring physical plausibility.\nThe resulting DeLaN network performs very well at robot tracking control. The\nproposed method did not only outperform previous model learning approaches at\nlearning speed but exhibits substantially improved and more robust\nextrapolation to novel trajectories and learns online in real-time", "journal": ""}
{"doi": "10.48550/arXiv.1912.07464", "date": "2019-12-16", "title": "Realization of spatial sparseness by deep ReLU nets with massive data", "authors": "Charles K. Chui, Shao-Bo Lin, Bo Zhang, Ding-Xuan Zhou", "abstract": "The great success of deep learning poses urgent challenges for understanding\nits working mechanism and rationality. The depth, structure, and massive size\nof the data are recognized to be three key ingredients for deep learning. Most\nof the recent theoretical studies for deep learning focus on the necessity and\nadvantages of depth and structures of neural networks. In this paper, we aim at\nrigorous verification of the importance of massive data in embodying the\nout-performance of deep learning. To approximate and learn spatially sparse and\nsmooth functions, we establish a novel sampling theorem in learning theory to\nshow the necessity of massive data. We then prove that implementing the\nclassical empirical risk minimization on some deep nets facilitates in\nrealization of the optimal learning rates derived in the sampling theorem. This\nperhaps explains why deep learning performs so well in the era of big data.", "journal": ""}
{"doi": "10.48550/arXiv.2301.00802", "date": "2023-01-02", "title": "Deep Clustering of Tabular Data by Weighted Gaussian Distribution Learning", "authors": "Shourav B. Rabbani, Ivan V. Medri, Manar D. Samad", "abstract": "Deep learning methods are primarily proposed for supervised learning of\nimages or text with limited applications to clustering problems. In contrast,\ntabular data with heterogeneous features pose unique challenges in\nrepresentation learning, where deep learning has yet to replace traditional\nmachine learning. This paper addresses these challenges in developing one of\nthe first deep clustering methods for tabular data: Gaussian Cluster Embedding\nin Autoencoder Latent Space (G-CEALS). G-CEALS is an unsupervised deep\nclustering framework for learning the parameters of multivariate Gaussian\ncluster distributions by iteratively updating individual cluster weights. The\nG-CEALS method presents average rank orderings of 2.9(1.7) and 2.8(1.7) based\non clustering accuracy and adjusted Rand index (ARI) scores on sixteen tabular\ndata sets, respectively, and outperforms nine state-of-the-art clustering\nmethods. G-CEALS substantially improves clustering performance compared to\ntraditional K-means and GMM, which are still de facto methods for clustering\ntabular data. Similar computationally efficient and high-performing deep\nclustering frameworks are imperative to reap the myriad benefits of deep\nlearning on tabular data over traditional machine learning.", "journal": ""}
{"doi": "10.48550/arXiv.1603.06430", "date": "2016-03-21", "title": "Deep Learning in Bioinformatics", "authors": "Seonwoo Min, Byunghan Lee, Sungroh Yoon", "abstract": "In the era of big data, transformation of biomedical big data into valuable\nknowledge has been one of the most important challenges in bioinformatics. Deep\nlearning has advanced rapidly since the early 2000s and now demonstrates\nstate-of-the-art performance in various fields. Accordingly, application of\ndeep learning in bioinformatics to gain insight from data has been emphasized\nin both academia and industry. Here, we review deep learning in bioinformatics,\npresenting examples of current research. To provide a useful and comprehensive\nperspective, we categorize research both by the bioinformatics domain (i.e.,\nomics, biomedical imaging, biomedical signal processing) and deep learning\narchitecture (i.e., deep neural networks, convolutional neural networks,\nrecurrent neural networks, emergent architectures) and present brief\ndescriptions of each study. Additionally, we discuss theoretical and practical\nissues of deep learning in bioinformatics and suggest future research\ndirections. We believe that this review will provide valuable insights and\nserve as a starting point for researchers to apply deep learning approaches in\ntheir bioinformatics studies.", "journal": ""}
{"doi": "10.48550/arXiv.1711.11008", "date": "2017-11-29", "title": "Security Risks in Deep Learning Implementations", "authors": "Qixue Xiao, Kang Li, Deyue Zhang, Weilin Xu", "abstract": "Advance in deep learning algorithms overshadows their security risk in\nsoftware implementations. This paper discloses a set of vulnerabilities in\npopular deep learning frameworks including Caffe, TensorFlow, and Torch.\nContrast to the small code size of deep learning models, these deep learning\nframeworks are complex and contain heavy dependencies on numerous open source\npackages. This paper considers the risks caused by these vulnerabilities by\nstudying their impact on common deep learning applications such as voice\nrecognition and image classifications. By exploiting these framework\nimplementations, attackers can launch denial-of-service attacks that crash or\nhang a deep learning application, or control-flow hijacking attacks that cause\neither system compromise or recognition evasions. The goal of this paper is to\ndraw attention on the software implementations and call for the community\neffort to improve the security of deep learning frameworks.", "journal": ""}
{"doi": "10.48550/arXiv.1807.00297", "date": "2018-07-01", "title": "Exponential Convergence of the Deep Neural Network Approximation for Analytic Functions", "authors": "Weinan E, Qingcan Wang", "abstract": "We prove that for analytic functions in low dimension, the convergence rate\nof the deep neural network approximation is exponential.", "journal": ""}
{"doi": "10.48550/arXiv.1808.05077", "date": "2018-08-15", "title": "Exploiting Deep Learning for Persian Sentiment Analysis", "authors": "Kia Dashtipour, Mandar Gogate, Ahsan Adeel, Cosimo Ieracitano, Hadi Larijani, Amir Hussain", "abstract": "The rise of social media is enabling people to freely express their opinions\nabout products and services. The aim of sentiment analysis is to automatically\ndetermine subject's sentiment (e.g., positive, negative, or neutral) towards a\nparticular aspect such as topic, product, movie, news etc. Deep learning has\nrecently emerged as a powerful machine learning technique to tackle a growing\ndemand of accurate sentiment analysis. However, limited work has been conducted\nto apply deep learning algorithms to languages other than English, such as\nPersian. In this work, two deep learning models (deep autoencoders and deep\nconvolutional neural networks (CNNs)) are developed and applied to a novel\nPersian movie reviews dataset. The proposed deep learning models are analyzed\nand compared with the state-of-the-art shallow multilayer perceptron (MLP)\nbased machine learning model. Simulation results demonstrate the enhanced\nperformance of deep learning over state-of-the-art MLP.", "journal": ""}
{"doi": "10.48550/arXiv.1808.09772", "date": "2018-08-29", "title": "Notes on Deep Learning for NLP", "authors": "Antoine J. -P. Tixier", "abstract": "My notes on Deep Learning for NLP.", "journal": ""}
{"doi": "10.48550/arXiv.2302.09566", "date": "2023-02-19", "title": "Optimization Methods in Deep Learning: A Comprehensive Overview", "authors": "David Shulman", "abstract": "In recent years, deep learning has achieved remarkable success in various\nfields such as image recognition, natural language processing, and speech\nrecognition. The effectiveness of deep learning largely depends on the\noptimization methods used to train deep neural networks. In this paper, we\nprovide an overview of first-order optimization methods such as Stochastic\nGradient Descent, Adagrad, Adadelta, and RMSprop, as well as recent\nmomentum-based and adaptive gradient methods such as Nesterov accelerated\ngradient, Adam, Nadam, AdaMax, and AMSGrad. We also discuss the challenges\nassociated with optimization in deep learning and explore techniques for\naddressing these challenges, including weight initialization, batch\nnormalization, and layer normalization. Finally, we provide recommendations for\nselecting optimization methods for different deep learning tasks and datasets.\nThis paper serves as a comprehensive guide to optimization methods in deep\nlearning and can be used as a reference for researchers and practitioners in\nthe field.", "journal": ""}
{"doi": "10.48550/arXiv.2305.05959", "date": "2023-05-10", "title": "Survey of Code Search Based on Deep Learning", "authors": "Yutao Xie, Jiayi Lin, Hande Dong, Lei Zhang, Zhonghai Wu", "abstract": "Code writing is repetitive and predictable, inspiring us to develop various\ncode intelligence techniques. This survey focuses on code search, that is, to\nretrieve code that matches a given query by effectively capturing the semantic\nsimilarity between the query and code. Deep learning, being able to extract\ncomplex semantics information, has achieved great success in this field.\nRecently, various deep learning methods, such as graph neural networks and\npretraining models, have been applied to code search with significant progress.\nDeep learning is now the leading paradigm for code search. In this survey, we\nprovide a comprehensive overview of deep learning-based code search. We review\nthe existing deep learning-based code search framework which maps query/code to\nvectors and measures their similarity. Furthermore, we propose a new taxonomy\nto illustrate the state-of-the-art deep learning-based code search in a\nthree-steps process: query semantics modeling, code semantics modeling, and\nmatching modeling which involves the deep learning model training. Finally, we\nsuggest potential avenues for future research in this promising field.", "journal": "ACM Transactions on Software Engineering and Methodology 2023"}
{"doi": "10.48550/arXiv.1711.03386", "date": "2017-11-09", "title": "Performance Evaluation of Deep Learning Tools in Docker Containers", "authors": "Pengfei Xu, Shaohuai Shi, Xiaowen Chu", "abstract": "With the success of deep learning techniques in a broad range of application\ndomains, many deep learning software frameworks have been developed and are\nbeing updated frequently to adapt to new hardware features and software\nlibraries, which bring a big challenge for end users and system administrators.\nTo address this problem, container techniques are widely used to simplify the\ndeployment and management of deep learning software. However, it remains\nunknown whether container techniques bring any performance penalty to deep\nlearning applications. The purpose of this work is to systematically evaluate\nthe impact of docker container on the performance of deep learning\napplications. We first benchmark the performance of system components (IO, CPU\nand GPU) in a docker container and the host system and compare the results to\nsee if there's any difference. According to our results, we find that\ncomputational intensive jobs, either running on CPU or GPU, have small overhead\nindicating docker containers can be applied to deep learning programs. Then we\nevaluate the performance of some popular deep learning tools deployed in a\ndocker container and the host system. It turns out that the docker container\nwill not cause noticeable drawbacks while running those deep learning tools. So\nencapsulating deep learning tool in a container is a feasible solution.", "journal": ""}
{"doi": "10.48550/arXiv.1712.07805", "date": "2017-12-21", "title": "Wolf in Sheep's Clothing - The Downscaling Attack Against Deep Learning Applications", "authors": "Qixue Xiao, Kang Li, Deyue Zhang, Yier Jin", "abstract": "This paper considers security risks buried in the data processing pipeline in\ncommon deep learning applications. Deep learning models usually assume a fixed\nscale for their training and input data. To allow deep learning applications to\nhandle a wide range of input data, popular frameworks, such as Caffe,\nTensorFlow, and Torch, all provide data scaling functions to resize input to\nthe dimensions used by deep learning models. Image scaling algorithms are\nintended to preserve the visual features of an image after scaling. However,\ncommon image scaling algorithms are not designed to handle human crafted\nimages. Attackers can make the scaling outputs look dramatically different from\nthe corresponding input images.\n  This paper presents a downscaling attack that targets the data scaling\nprocess in deep learning applications. By carefully crafting input data that\nmismatches with the dimension used by deep learning models, attackers can\ncreate deceiving effects. A deep learning application effectively consumes data\nthat are not the same as those presented to users. The visual inconsistency\nenables practical evasion and data poisoning attacks to deep learning\napplications. This paper presents proof-of-concept attack samples to popular\ndeep-learning-based image classification applications. To address the\ndownscaling attacks, the paper also suggests multiple potential mitigation\nstrategies.", "journal": ""}
{"doi": "10.48550/arXiv.2006.03364", "date": "2020-06-05", "title": "Structure preserving deep learning", "authors": "Elena Celledoni, Matthias J. Ehrhardt, Christian Etmann, Robert I McLachlan, Brynjulf Owren, Carola-Bibiane Sch\u00f6nlieb, Ferdia Sherry", "abstract": "Over the past few years, deep learning has risen to the foreground as a topic\nof massive interest, mainly as a result of successes obtained in solving\nlarge-scale image processing tasks. There are multiple challenging mathematical\nproblems involved in applying deep learning: most deep learning methods require\nthe solution of hard optimisation problems, and a good understanding of the\ntradeoff between computational effort, amount of data and model complexity is\nrequired to successfully design a deep learning approach for a given problem. A\nlarge amount of progress made in deep learning has been based on heuristic\nexplorations, but there is a growing effort to mathematically understand the\nstructure in existing deep learning methods and to systematically design new\ndeep learning methods to preserve certain types of structure in deep learning.\nIn this article, we review a number of these directions: some deep neural\nnetworks can be understood as discretisations of dynamical systems, neural\nnetworks can be designed to have desirable properties such as invertibility or\ngroup equivariance, and new algorithmic frameworks based on conformal\nHamiltonian systems and Riemannian manifolds to solve the optimisation problems\nhave been proposed. We conclude our review of each of these topics by\ndiscussing some open problems that we consider to be interesting directions for\nfuture research.", "journal": ""}
{"doi": "10.48550/arXiv.1904.08483", "date": "2019-04-16", "title": "Deep learning for image segmentation: veritable or overhyped?", "authors": "Zhenzhou Wang", "abstract": "Deep learning has achieved great success as a powerful classification tool\nand also made great progress in sematic segmentation. As a result, many\nresearchers also believe that deep learning is the most powerful tool for pixel\nlevel image segmentation. Could deep learning achieve the same pixel level\naccuracy as traditional image segmentation techniques by mapping the features\nof the object into a non-linear function? This paper gives a short survey of\nthe accuracies achieved by deep learning so far in image classification and\nimage segmentation. Compared to the high accuracies achieved by deep learning\nin classifying limited categories in international vision challenges, the image\nsegmentation accuracies achieved by deep learning in the same challenges are\nonly about eighty percent. On the contrary, the image segmentation accuracies\nachieved in international biomedical challenges are close to ninty five\npercent. Why the difference is so big? Since the accuracies of the competitors\nmethods are only evaluated based on their submitted results instead of\nreproducing the results by submitting the source codes or the software, are the\nachieved accuracies verifiable or overhyped? We are going to find it out by\nanalyzing the working principle of deep learning. Finally, we compared the\naccuracies of state of the art deep learning methods with a threshold selection\nmethod quantitatively. Experimental results showed that the threshold selection\nmethod could achieve significantly higher accuracy than deep learning methods\nin image segmentation.", "journal": ""}
{"doi": "10.48550/arXiv.1906.01388", "date": "2019-06-03", "title": "A Comprehensive Study on Deep Learning Bug Characteristics", "authors": "Md Johirul Islam, Giang Nguyen, Rangeet Pan, Hridesh Rajan", "abstract": "Deep learning has gained substantial popularity in recent years. Developers\nmainly rely on libraries and tools to add deep learning capabilities to their\nsoftware. What kinds of bugs are frequently found in such software? What are\nthe root causes of such bugs? What impacts do such bugs have? Which stages of\ndeep learning pipeline are more bug prone? Are there any antipatterns?\nUnderstanding such characteristics of bugs in deep learning software has the\npotential to foster the development of better deep learning platforms,\ndebugging mechanisms, development practices, and encourage the development of\nanalysis and verification frameworks. Therefore, we study 2716 high-quality\nposts from Stack Overflow and 500 bug fix commits from Github about five\npopular deep learning libraries Caffe, Keras, Tensorflow, Theano, and Torch to\nunderstand the types of bugs, root causes of bugs, impacts of bugs, bug-prone\nstage of deep learning pipeline as well as whether there are some common\nantipatterns found in this buggy software. The key findings of our study\ninclude: data bug and logic bug are the most severe bug types in deep learning\nsoftware appearing more than 48% of the times, major root causes of these bugs\nare Incorrect Model Parameter (IPS) and Structural Inefficiency (SI) showing up\nmore than 43% of the times. We have also found that the bugs in the usage of\ndeep learning libraries have some common antipatterns that lead to a strong\ncorrelation of bug types among the libraries.", "journal": "The ACM Joint European Software Engineering Conference and\n  Symposium on the Foundations of Software Engineering (ESEC/FSE) (Aug. 2019)"}
{"doi": "10.48550/arXiv.2011.05627", "date": "2020-11-11", "title": "Skin disease diagnosis with deep learning: a review", "authors": "Hongfeng Li, Yini Pan, Jie Zhao, Li Zhang", "abstract": "Skin cancer is one of the most threatening diseases worldwide. However,\ndiagnosing skin cancer correctly is challenging. Recently, deep learning\nalgorithms have emerged to achieve excellent performance on various tasks.\nParticularly, they have been applied to the skin disease diagnosis tasks. In\nthis paper, we present a review on deep learning methods and their applications\nin skin disease diagnosis. We first present a brief introduction to skin\ndiseases and image acquisition methods in dermatology, and list several\npublicly available skin datasets for training and testing algorithms. Then, we\nintroduce the conception of deep learning and review popular deep learning\narchitectures. Thereafter, popular deep learning frameworks facilitating the\nimplementation of deep learning algorithms and performance evaluation metrics\nare presented. As an important part of this article, we then review the\nliterature involving deep learning methods for skin disease diagnosis from\nseveral aspects according to the specific tasks. Additionally, we discuss the\nchallenges faced in the area and suggest possible future research directions.\nThe major purpose of this article is to provide a conceptual and systematically\nreview of the recent works on skin disease diagnosis with deep learning. Given\nthe popularity of deep learning, there remains great challenges in the area, as\nwell as opportunities that we can explore in the future.", "journal": ""}
{"doi": "10.48550/arXiv.2110.01894", "date": "2021-10-05", "title": "Combining Physics and Deep Learning to learn Continuous-Time Dynamics Models", "authors": "Michael Lutter, Jan Peters", "abstract": "Deep learning has been widely used within learning algorithms for robotics.\nOne disadvantage of deep networks is that these networks are black-box\nrepresentations. Therefore, the learned approximations ignore the existing\nknowledge of physics or robotics. Especially for learning dynamics models,\nthese black-box models are not desirable as the underlying principles are well\nunderstood and the standard deep networks can learn dynamics that violate these\nprinciples. To learn dynamics models with deep networks that guarantee\nphysically plausible dynamics, we introduce physics-inspired deep networks that\ncombine first principles from physics with deep learning. We incorporate\nLagrangian mechanics within the model learning such that all approximated\nmodels adhere to the laws of physics and conserve energy. Deep Lagrangian\nNetworks (DeLaN) parametrize the system energy using two networks. The\nparameters are obtained by minimizing the squared residual of the\nEuler-Lagrange differential equation. Therefore, the resulting model does not\nrequire specific knowledge of the individual system, is interpretable, and can\nbe used as a forward, inverse, and energy model. Previously these properties\nwere only obtained when using system identification techniques that require\nknowledge of the kinematic structure. We apply DeLaN to learning dynamics\nmodels and apply these models to control simulated and physical rigid body\nsystems. The results show that the proposed approach obtains dynamics models\nthat can be applied to physical systems for real-time control. Compared to\nstandard deep networks, the physics-inspired models learn better models and\ncapture the underlying structure of the dynamics.", "journal": ""}
{"doi": "10.48550/arXiv.1501.04413", "date": "2015-01-19", "title": "Statistical-mechanical analysis of pre-training and fine tuning in deep learning", "authors": "Masayuki Ohzeki", "abstract": "In this paper, we present a statistical-mechanical analysis of deep learning.\nWe elucidate some of the essential components of deep learning---pre-training\nby unsupervised learning and fine tuning by supervised learning. We formulate\nthe extraction of features from the training data as a margin criterion in a\nhigh-dimensional feature-vector space. The self-organized classifier is then\nsupplied with small amounts of labelled data, as in deep learning. Although we\nemploy a simple single-layer perceptron model, rather than directly analyzing a\nmulti-layer neural network, we find a nontrivial phase transition that is\ndependent on the number of unlabelled data in the generalization error of the\nresultant classifier. In this sense, we evaluate the efficacy of the\nunsupervised learning component of deep learning. The analysis is performed by\nthe replica method, which is a sophisticated tool in statistical mechanics. We\nvalidate our result in the manner of deep learning, using a simple iterative\nalgorithm to learn the weight vector on the basis of belief propagation.", "journal": ""}
{"doi": "10.48550/arXiv.1701.07274", "date": "2017-01-25", "title": "Deep Reinforcement Learning: An Overview", "authors": "Yuxi Li", "abstract": "We give an overview of recent exciting achievements of deep reinforcement\nlearning (RL). We discuss six core elements, six important mechanisms, and\ntwelve applications. We start with background of machine learning, deep\nlearning and reinforcement learning. Next we discuss core RL elements,\nincluding value function, in particular, Deep Q-Network (DQN), policy, reward,\nmodel, planning, and exploration. After that, we discuss important mechanisms\nfor RL, including attention and memory, unsupervised learning, transfer\nlearning, multi-agent RL, hierarchical RL, and learning to learn. Then we\ndiscuss various applications of RL, including games, in particular, AlphaGo,\nrobotics, natural language processing, including dialogue systems, machine\ntranslation, and text generation, computer vision, neural architecture design,\nbusiness management, finance, healthcare, Industry 4.0, smart grid, intelligent\ntransportation systems, and computer systems. We mention topics not reviewed\nyet, and list a collection of RL resources. After presenting a brief summary,\nwe close with discussions.\n  Please see Deep Reinforcement Learning, arXiv:1810.06339, for a significant\nupdate.", "journal": ""}
{"doi": "10.48550/arXiv.1410.3831", "date": "2014-10-14", "title": "An exact mapping between the Variational Renormalization Group and Deep Learning", "authors": "Pankaj Mehta, David J. Schwab", "abstract": "Deep learning is a broad set of techniques that uses multiple layers of\nrepresentation to automatically learn relevant features directly from\nstructured data. Recently, such techniques have yielded record-breaking results\non a diverse set of difficult machine learning tasks in computer vision, speech\nrecognition, and natural language processing. Despite the enormous success of\ndeep learning, relatively little is understood theoretically about why these\ntechniques are so successful at feature learning and compression. Here, we show\nthat deep learning is intimately related to one of the most important and\nsuccessful techniques in theoretical physics, the renormalization group (RG).\nRG is an iterative coarse-graining scheme that allows for the extraction of\nrelevant features (i.e. operators) as a physical system is examined at\ndifferent length scales. We construct an exact mapping from the variational\nrenormalization group, first introduced by Kadanoff, and deep learning\narchitectures based on Restricted Boltzmann Machines (RBMs). We illustrate\nthese ideas using the nearest-neighbor Ising Model in one and two-dimensions.\nOur results suggests that deep learning algorithms may be employing a\ngeneralized RG-like scheme to learn relevant features from data.", "journal": ""}
{"doi": "10.48550/arXiv.2007.06183", "date": "2020-07-13", "title": "Data-driven geophysics: from dictionary learning to deep learning", "authors": "Siwei Yu, Jianwei Ma", "abstract": "Understanding the principles of geophysical phenomena is an essential and\nchallenging task. \"Model-driven\" approaches have supported the development of\ngeophysics for a long time; however, such methods suffer from the curse of\ndimensionality and may inaccurately model the subsurface. \"Data-driven\"\ntechniques may overcome these issues with increasingly available geophysical\ndata. In this article, we review the basic concepts of and recent advances in\ndata-driven approaches from dictionary learning to deep learning in a variety\nof geophysical scenarios. Explorational geophysics including data processing,\ninversion and interpretation will be mainly focused. Artificial intelligence\napplications on geoscience involving deep Earth, earthquake, water resource,\natmospheric science, satellite remoe sensing and space sciences are also\nreviewed. We present a coding tutorial and a summary of tips for beginners and\ninterested geophysical readers to rapidly explore deep learning. Some promising\ndirections are provided for future research involving deep learning in\ngeophysics, such as unsupervised learning, transfer learning, multimodal deep\nlearning, federated learning, uncertainty estimation, and activate learning.", "journal": ""}
{"doi": "10.48550/arXiv.2310.06557", "date": "2023-10-10", "title": "Data efficient deep learning for medical image analysis: A survey", "authors": "Suruchi Kumari, Pravendra Singh", "abstract": "The rapid evolution of deep learning has significantly advanced the field of\nmedical image analysis. However, despite these achievements, the further\nenhancement of deep learning models for medical image analysis faces a\nsignificant challenge due to the scarcity of large, well-annotated datasets. To\naddress this issue, recent years have witnessed a growing emphasis on the\ndevelopment of data-efficient deep learning methods. This paper conducts a\nthorough review of data-efficient deep learning methods for medical image\nanalysis. To this end, we categorize these methods based on the level of\nsupervision they rely on, encompassing categories such as no supervision,\ninexact supervision, incomplete supervision, inaccurate supervision, and only\nlimited supervision. We further divide these categories into finer\nsubcategories. For example, we categorize inexact supervision into multiple\ninstance learning and learning with weak annotations. Similarly, we categorize\nincomplete supervision into semi-supervised learning, active learning, and\ndomain-adaptive learning and so on. Furthermore, we systematically summarize\ncommonly used datasets for data efficient deep learning in medical image\nanalysis and investigate future research directions to conclude this survey.", "journal": ""}
{"doi": "10.48550/arXiv.2405.18281", "date": "2024-05-28", "title": "MODL: Multilearner Online Deep Learning", "authors": "Antonios Valkanas, Boris N. Oreshkin, Mark Coates", "abstract": "Online deep learning tackles the challenge of learning from data streams by\nbalancing two competing goals: fast learning and deep learning. However,\nexisting research primarily emphasizes deep learning solutions, which are more\nadept at handling the ``deep'' aspect than the ``fast'' aspect of online\nlearning. In this work, we introduce an alternative paradigm through a hybrid\nmultilearner approach. We begin by developing a fast online logistic regression\nlearner, which operates without relying on backpropagation. It leverages\nclosed-form recursive updates of model parameters, efficiently addressing the\nfast learning component of the online learning challenge. This approach is\nfurther integrated with a cascaded multilearner design, where shallow and deep\nlearners are co-trained in a cooperative, synergistic manner to solve the\nonline learning problem. We demonstrate that this approach achieves\nstate-of-the-art performance on standard online learning datasets. We make our\ncode available: https://github.com/AntonValk/MODL", "journal": ""}
{"doi": "10.48550/arXiv.2002.11816", "date": "2020-02-26", "title": "Streaming Active Deep Forest for Evolving Data Stream Classification", "authors": "Anh Vu Luong, Tien Thanh Nguyen, Alan Wee-Chung Liew", "abstract": "In recent years, Deep Neural Networks (DNNs) have gained progressive momentum\nin many areas of machine learning. The layer-by-layer process of DNNs has\ninspired the development of many deep models, including deep ensembles. The\nmost notable deep ensemble-based model is Deep Forest, which can achieve highly\ncompetitive performance while having much fewer hyper-parameters comparing to\nDNNs. In spite of its huge success in the batch learning setting, no effort has\nbeen made to adapt Deep Forest to the context of evolving data streams. In this\nwork, we introduce the Streaming Deep Forest (SDF) algorithm, a\nhigh-performance deep ensemble method specially adapted to stream\nclassification. We also present the Augmented Variable Uncertainty (AVU) active\nlearning strategy to reduce the labeling cost in the streaming context. We\ncompare the proposed methods to state-of-the-art streaming algorithms in a wide\nrange of datasets. The results show that by following the AVU active learning\nstrategy, SDF with only 70\\% of labeling budget significantly outperforms other\nmethods trained with all instances.", "journal": ""}
{"doi": "10.48550/arXiv.2403.18930", "date": "2024-02-17", "title": "Optimizing Wireless Networks with Deep Unfolding: Comparative Study on Two Deep Unfolding Mechanisms", "authors": "Abuzar B. M. Adam, Mohammed A. M. Elhassan, Elhadj Moustapha Diallo", "abstract": "In this work, we conduct a comparative study on two deep unfolding mechanisms\nto efficiently perform power control in the next generation wireless networks.\nThe power control problem is formulated as energy efficiency over multiple\ninterference links. The problem is nonconvex. We employ fractional programming\ntransformation to design two solutions for the problem. The first solution is a\nnumerical solution while the second solution is a closed-form solution. Based\non the first solution, we design a semi-unfolding deep learning model where we\ncombine the domain knowledge of the wireless communications and the recent\nadvances in the data-driven deep learning. Moreover, on the highlights of the\nclosed-form solution, fully deep unfolded deep learning model is designed in\nwhich we fully leveraged the expressive closed-form power control solution and\ndeep learning advances. In the simulation results, we compare the performance\nof the proposed deep learning models and the iterative solutions in terms of\naccuracy and inference speed to show their suitability for the real-time\napplication in next generation networks.", "journal": ""}
{"doi": "10.48550/arXiv.2004.11694", "date": "2020-04-18", "title": "Identifying Semantically Duplicate Questions Using Data Science Approach: A Quora Case Study", "authors": "Navedanjum Ansari, Rajesh Sharma", "abstract": "Identifying semantically identical questions on, Question and Answering\nsocial media platforms like Quora is exceptionally significant to ensure that\nthe quality and the quantity of content are presented to users, based on the\nintent of the question and thus enriching overall user experience. Detecting\nduplicate questions is a challenging problem because natural language is very\nexpressive, and a unique intent can be conveyed using different words, phrases,\nand sentence structuring. Machine learning and deep learning methods are known\nto have accomplished superior results over traditional natural language\nprocessing techniques in identifying similar texts. In this paper, taking Quora\nfor our case study, we explored and applied different machine learning and deep\nlearning techniques on the task of identifying duplicate questions on Quora's\ndataset. By using feature engineering, feature importance techniques, and\nexperimenting with seven selected machine learning classifiers, we demonstrated\nthat our models outperformed previous studies on this task. Xgboost model with\ncharacter level term frequency and inverse term frequency is our best machine\nlearning model that has also outperformed a few of the Deep learning baseline\nmodels. We applied deep learning techniques to model four different deep neural\nnetworks of multiple layers consisting of Glove embeddings, Long Short Term\nMemory, Convolution, Max pooling, Dense, Batch Normalization, Activation\nfunctions, and model merge. Our deep learning models achieved better accuracy\nthan machine learning models. Three out of four proposed architectures\noutperformed the accuracy from previous machine learning and deep learning\nresearch work, two out of four models outperformed accuracy from previous deep\nlearning study on Quora's question pair dataset, and our best model achieved\naccuracy of 85.82% which is close to Quora state of the art accuracy.", "journal": ""}
{"doi": "10.48550/arXiv.1312.5548", "date": "2013-12-19", "title": "My First Deep Learning System of 1991 + Deep Learning Timeline 1962-2013", "authors": "J\u00fcrgen Schmidhuber", "abstract": "Deep Learning has attracted significant attention in recent years. Here I\npresent a brief overview of my first Deep Learner of 1991, and its historic\ncontext, with a timeline of Deep Learning highlights.", "journal": ""}
{"doi": "10.48550/arXiv.1711.07655", "date": "2017-11-21", "title": "Genetic Algorithms for Evolving Deep Neural Networks", "authors": "Eli David, Iddo Greental", "abstract": "In recent years, deep learning methods applying unsupervised learning to\ntrain deep layers of neural networks have achieved remarkable results in\nnumerous fields. In the past, many genetic algorithms based methods have been\nsuccessfully applied to training neural networks. In this paper, we extend\nprevious work and propose a GA-assisted method for deep learning. Our\nexperimental results indicate that this GA-assisted approach improves the\nperformance of a deep autoencoder, producing a sparser neural network.", "journal": "ACM Genetic and Evolutionary Computation Conference (GECCO), pages\n  1451-1452, Vancouver, Canada, July 2014"}
{"doi": "10.48550/arXiv.2003.06520", "date": "2020-03-14", "title": "Symmetry Detection of Occluded Point Cloud Using Deep Learning", "authors": "Zhelun Wu, Hongyan Jiang, Siyun He", "abstract": "Symmetry detection has been a classical problem in computer graphics, many of\nwhich using traditional geometric methods. In recent years, however, we have\nwitnessed the arising deep learning changed the landscape of computer graphics.\nIn this paper, we aim to solve the symmetry detection of the occluded point\ncloud in a deep-learning fashion. To the best of our knowledge, we are the\nfirst to utilize deep learning to tackle such a problem. In such a deep\nlearning framework, double supervisions: points on the symmetry plane and\nnormal vectors are employed to help us pinpoint the symmetry plane. We\nconducted experiments on the YCB- video dataset and demonstrate the efficacy of\nour method.", "journal": ""}
{"doi": "10.48550/arXiv.1912.10382", "date": "2019-12-22", "title": "Deep Learning via Dynamical Systems: An Approximation Perspective", "authors": "Qianxiao Li, Ting Lin, Zuowei Shen", "abstract": "We build on the dynamical systems approach to deep learning, where deep\nresidual networks are idealized as continuous-time dynamical systems, from the\napproximation perspective. In particular, we establish general sufficient\nconditions for universal approximation using continuous-time deep residual\nnetworks, which can also be understood as approximation theories in $L^p$ using\nflow maps of dynamical systems. In specific cases, rates of approximation in\nterms of the time horizon are also established. Overall, these results reveal\nthat composition function approximation through flow maps present a new\nparadigm in approximation theory and contributes to building a useful\nmathematical framework to investigate deep learning.", "journal": ""}
{"doi": "10.48550/arXiv.2007.09637", "date": "2020-07-19", "title": "Survey on Deep Learning-based Kuzushiji Recognition", "authors": "Kazuya Ueki, Tomoka Kojima", "abstract": "Owing to the overwhelming accuracy of the deep learning method demonstrated\nat the 2012 image classification competition, deep learning has been\nsuccessfully applied to a variety of other tasks. The high-precision detection\nand recognition of Kuzushiji, a Japanese cursive script used for transcribing\nhistorical documents, has been made possible through the use of deep learning.\nIn recent years, competitions on Kuzushiji recognition have been held, and many\nresearchers have proposed various recognition methods. This study examines\nrecent research trends, current problems, and future prospects in Kuzushiji\nrecognition using deep learning.", "journal": ""}
{"doi": "10.48550/arXiv.2302.07503", "date": "2023-02-15", "title": "Excess risk bound for deep learning under weak dependence", "authors": "William Kengne", "abstract": "This paper considers deep neural networks for learning weakly dependent\nprocesses in a general framework that includes, for instance, regression\nestimation, time series prediction, time series classification. The $\\psi$-weak\ndependence structure considered is quite large and covers other conditions such\nas mixing, association,$\\ldots$ Firstly, the approximation of smooth functions\nby deep neural networks with a broad class of activation functions is\nconsidered. We derive the required depth, width and sparsity of a deep neural\nnetwork to approximate any H\\\"{o}lder smooth function, defined on any compact\nset $\\mx$. Secondly, we establish a bound of the excess risk for the learning\nof weakly dependent observations by deep neural networks. When the target\nfunction is sufficiently smooth, this bound is close to the usual\n$\\mathcal{O}(n^{-1/2})$.", "journal": ""}
{"doi": "10.48550/arXiv.1807.07987", "date": "2018-07-20", "title": "Deep Learning", "authors": "Nicholas G. Polson, Vadim O. Sokolov", "abstract": "Deep learning (DL) is a high dimensional data reduction technique for\nconstructing high-dimensional predictors in input-output models. DL is a form\nof machine learning that uses hierarchical layers of latent features. In this\narticle, we review the state-of-the-art of deep learning from a modeling and\nalgorithmic perspective. We provide a list of successful areas of applications\nin Artificial Intelligence (AI), Image Processing, Robotics and Automation.\nDeep learning is predictive in its nature rather then inferential and can be\nviewed as a black-box methodology for high-dimensional function estimation.", "journal": ""}
{"doi": "10.48550/arXiv.1808.08618", "date": "2018-08-26", "title": "Deep Learning: Computational Aspects", "authors": "Nicholas Polson, Vadim Sokolov", "abstract": "In this article we review computational aspects of Deep Learning (DL). Deep\nlearning uses network architectures consisting of hierarchical layers of latent\nvariables to construct predictors for high-dimensional input-output models.\nTraining a deep learning architecture is computationally intensive, and\nefficient linear algebra libraries is the key for training and inference.\nStochastic gradient descent (SGD) optimization and batch sampling are used to\nlearn from massive data sets.", "journal": ""}
{"doi": "10.48550/arXiv.1812.10747", "date": "2018-12-27", "title": "Off-the-grid model based deep learning (O-MODL)", "authors": "Aniket Pramanik, Hemant Kumar Aggarwal, Mathews Jacob", "abstract": "We introduce a model based off-the-grid image reconstruction algorithm using\ndeep learned priors. The main difference of the proposed scheme with current\ndeep learning strategies is the learning of non-linear annihilation relations\nin Fourier space. We rely on a model based framework, which allows us to use a\nsignificantly smaller deep network, compared to direct approaches that also\nlearn how to invert the forward model. Preliminary comparisons against image\ndomain MoDL approach demonstrates the potential of the off-the-grid\nformulation. The main benefit of the proposed scheme compared to structured\nlow-rank methods is the quite significant reduction in computational\ncomplexity.", "journal": ""}
{"doi": "10.48550/arXiv.1903.03614", "date": "2019-03-11", "title": "Gradient Descent based Optimization Algorithms for Deep Learning Models Training", "authors": "Jiawei Zhang", "abstract": "In this paper, we aim at providing an introduction to the gradient descent\nbased optimization algorithms for learning deep neural network models. Deep\nlearning models involving multiple nonlinear projection layers are very\nchallenging to train. Nowadays, most of the deep learning model training still\nrelies on the back propagation algorithm actually. In back propagation, the\nmodel variables will be updated iteratively until convergence with gradient\ndescent based optimization algorithms. Besides the conventional vanilla\ngradient descent algorithm, many gradient descent variants have also been\nproposed in recent years to improve the learning performance, including\nMomentum, Adagrad, Adam, Gadam, etc., which will all be introduced in this\npaper respectively.", "journal": ""}
{"doi": "10.48550/arXiv.1803.08416", "date": "2018-03-22", "title": "Demystifying Deep Learning: A Geometric Approach to Iterative Projections", "authors": "Ashkan Panahi, Hamid Krim, Liyi Dai", "abstract": "Parametric approaches to Learning, such as deep learning (DL), are highly\npopular in nonlinear regression, in spite of their extremely difficult training\nwith their increasing complexity (e.g. number of layers in DL). In this paper,\nwe present an alternative semi-parametric framework which foregoes the\nordinarily required feedback, by introducing the novel idea of geometric\nregularization. We show that certain deep learning techniques such as residual\nnetwork (ResNet) architecture are closely related to our approach. Hence, our\ntechnique can be used to analyze these types of deep learning. Moreover, we\npresent preliminary results which confirm that our approach can be easily\ntrained to obtain complex structures.", "journal": ""}
{"doi": "10.48550/arXiv.1804.02527", "date": "2018-04-07", "title": "Visual Analytics for Explainable Deep Learning", "authors": "Jaegul Choo, Shixia Liu", "abstract": "Recently, deep learning has been advancing the state of the art in artificial\nintelligence to a new level, and humans rely on artificial intelligence\ntechniques more than ever. However, even with such unprecedented advancements,\nthe lack of explanation regarding the decisions made by deep learning models\nand absence of control over their internal processes act as major drawbacks in\ncritical decision-making processes, such as precision medicine and law\nenforcement. In response, efforts are being made to make deep learning\ninterpretable and controllable by humans. In this paper, we review visual\nanalytics, information visualization, and machine learning perspectives\nrelevant to this aim, and discuss potential challenges and future research\ndirections.", "journal": ""}
{"doi": "10.48550/arXiv.1911.12461", "date": "2019-11-27", "title": "Two-Stage Learning for Uplink Channel Estimation in One-Bit Massive MIMO", "authors": "Eren Balevi, Jeffrey G. Andrews", "abstract": "We develop a two-stage deep learning pipeline architecture to estimate the\nuplink massive MIMO channel with one-bit ADCs. This deep learning pipeline is\ncomposed of two separate generative deep learning models. The first one is a\nsupervised learning model and designed to compensate for the quantization loss.\nThe second one is an unsupervised learning model and optimized for denoising.\nOur results show that the proposed deep learning-based channel estimator can\nsignificantly outperform other state-of-the-art channel estimators for one-bit\nquantized massive MIMO systems. In particular, our design provides 5-10 dB gain\nin channel estimation error. Furthermore, it requires a reasonable amount of\npilots, on the order of 20 per coherence time interval.", "journal": ""}
{"doi": "10.48550/arXiv.2104.05569", "date": "2021-04-12", "title": "Deep Learning for IoT", "authors": "Tao Lin", "abstract": "Deep learning and other machine learning approaches are deployed to many\nsystems related to Internet of Things or IoT. However, it faces challenges that\nadversaries can take loopholes to hack these systems through tampering history\ndata. This paper first presents overall points of adversarial machine learning.\nThen, we illustrate traditional methods, such as Petri Net cannot solve this\nnew question efficiently. To help IoT data analysis more efficient, we propose\na retrieval method based on deep learning (recurrent neural network). Besides,\nthis paper presents a research on data retrieval solution to avoid hacking by\nadversaries in the fields of adversary machine leaning. It further directs the\nnew approaches in terms of how to implementing this framework in IoT settings\nbased on adversarial deep learning.", "journal": ""}
{"doi": "10.48550/arXiv.1811.06622", "date": "2018-11-15", "title": "Concept-Oriented Deep Learning: Generative Concept Representations", "authors": "Daniel T. Chang", "abstract": "Generative concept representations have three major advantages over\ndiscriminative ones: they can represent uncertainty, they support integration\nof learning and reasoning, and they are good for unsupervised and\nsemi-supervised learning. We discuss probabilistic and generative deep\nlearning, which generative concept representations are based on, and the use of\nvariational autoencoders and generative adversarial networks for learning\ngenerative concept representations, particularly for concepts whose data are\nsequences, structured data or graphs.", "journal": ""}
{"doi": "10.48550/arXiv.1707.09905", "date": "2017-07-31", "title": "Deep Discrete Supervised Hashing", "authors": "Qing-Yuan Jiang, Xue Cui, Wu-Jun Li", "abstract": "Hashing has been widely used for large-scale search due to its low storage\ncost and fast query speed. By using supervised information, supervised hashing\ncan significantly outperform unsupervised hashing. Recently, discrete\nsupervised hashing and deep hashing are two representative progresses in\nsupervised hashing. On one hand, hashing is essentially a discrete optimization\nproblem. Hence, utilizing supervised information to directly guide discrete\n(binary) coding procedure can avoid sub-optimal solution and improve the\naccuracy. On the other hand, deep hashing, which integrates deep feature\nlearning and hash-code learning into an end-to-end architecture, can enhance\nthe feedback between feature learning and hash-code learning. The key in\ndiscrete supervised hashing is to adopt supervised information to directly\nguide the discrete coding procedure in hashing. The key in deep hashing is to\nadopt the supervised information to directly guide the deep feature learning\nprocedure. However, there have not existed works which can use the supervised\ninformation to directly guide both discrete coding procedure and deep feature\nlearning procedure in the same framework. In this paper, we propose a novel\ndeep hashing method, called deep discrete supervised hashing (DDSH), to address\nthis problem. DDSH is the first deep hashing method which can utilize\nsupervised information to directly guide both discrete coding procedure and\ndeep feature learning procedure, and thus enhance the feedback between these\ntwo important procedures. Experiments on three real datasets show that DDSH can\noutperform other state-of-the-art baselines, including both discrete hashing\nand deep hashing baselines, for image retrieval.", "journal": ""}
{"doi": "10.48550/arXiv.1702.05796", "date": "2017-02-19", "title": "Collaborative Deep Reinforcement Learning", "authors": "Kaixiang Lin, Shu Wang, Jiayu Zhou", "abstract": "Besides independent learning, human learning process is highly improved by\nsummarizing what has been learned, communicating it with peers, and\nsubsequently fusing knowledge from different sources to assist the current\nlearning goal. This collaborative learning procedure ensures that the knowledge\nis shared, continuously refined, and concluded from different perspectives to\nconstruct a more profound understanding. The idea of knowledge transfer has led\nto many advances in machine learning and data mining, but significant\nchallenges remain, especially when it comes to reinforcement learning,\nheterogeneous model structures, and different learning tasks. Motivated by\nhuman collaborative learning, in this paper we propose a collaborative deep\nreinforcement learning (CDRL) framework that performs adaptive knowledge\ntransfer among heterogeneous learning agents. Specifically, the proposed CDRL\nconducts a novel deep knowledge distillation method to address the\nheterogeneity among different learning tasks with a deep alignment network.\nFurthermore, we present an efficient collaborative Asynchronous Advantage\nActor-Critic (cA3C) algorithm to incorporate deep knowledge distillation into\nthe online training of agents, and demonstrate the effectiveness of the CDRL\nframework using extensive empirical evaluation on OpenAI gym.", "journal": ""}
{"doi": "10.48550/arXiv.2008.00766", "date": "2020-08-03", "title": "Tracking the Race Between Deep Reinforcement Learning and Imitation Learning -- Extended Version", "authors": "Timo P. Gros, Daniel H\u00f6ller, J\u00f6rg Hoffmann, Verena Wolf", "abstract": "Learning-based approaches for solving large sequential decision making\nproblems have become popular in recent years. The resulting agents perform\ndifferently and their characteristics depend on those of the underlying\nlearning approach. Here, we consider a benchmark planning problem from the\nreinforcement learning domain, the Racetrack, to investigate the properties of\nagents derived from different deep (reinforcement) learning approaches. We\ncompare the performance of deep supervised learning, in particular imitation\nlearning, to reinforcement learning for the Racetrack model. We find that\nimitation learning yields agents that follow more risky paths. In contrast, the\ndecisions of deep reinforcement learning are more foresighted, i.e., avoid\nstates in which fatal decisions are more likely. Our evaluations show that for\nthis sequential decision making problem, deep reinforcement learning performs\nbest in many aspects even though for imitation learning optimal decisions are\nconsidered.", "journal": ""}
{"doi": "10.48550/arXiv.1506.04477", "date": "2015-06-15", "title": "Dual Memory Architectures for Fast Deep Learning of Stream Data via an Online-Incremental-Transfer Strategy", "authors": "Sang-Woo Lee, Min-Oh Heo, Jiwon Kim, Jeonghee Kim, Byoung-Tak Zhang", "abstract": "The online learning of deep neural networks is an interesting problem of\nmachine learning because, for example, major IT companies want to manage the\ninformation of the massive data uploaded on the web daily, and this technology\ncan contribute to the next generation of lifelong learning. We aim to train\ndeep models from new data that consists of new classes, distributions, and\ntasks at minimal computational cost, which we call online deep learning.\nUnfortunately, deep neural network learning through classical online and\nincremental methods does not work well in both theory and practice. In this\npaper, we introduce dual memory architectures for online incremental deep\nlearning. The proposed architecture consists of deep representation learners\nand fast learnable shallow kernel networks, both of which synergize to track\nthe information of new data. During the training phase, we use various online,\nincremental ensemble, and transfer learning techniques in order to achieve\nlower error of the architecture. On the MNIST, CIFAR-10, and ImageNet image\nrecognition tasks, the proposed dual memory architectures performs much better\nthan the classical online and incremental ensemble algorithm, and their\naccuracies are similar to that of the batch learner.", "journal": ""}
{"doi": "10.48550/arXiv.1812.00602", "date": "2018-12-03", "title": "Examining Deep Learning Architectures for Crime Classification and Prediction", "authors": "Panagiotis Stalidis, Theodoros Semertzidis, Petros Daras", "abstract": "In this paper, a detailed study on crime classification and prediction using\ndeep learning architectures is presented. We examine the effectiveness of deep\nlearning algorithms on this domain and provide recommendations for designing\nand training deep learning systems for predicting crime areas, using open data\nfrom police reports. Having as training data time-series of crime types per\nlocation, a comparative study of 10 state-of-the-art methods against 3\ndifferent deep learning configurations is conducted. In our experiments with\nfive publicly available datasets, we demonstrate that the deep learning-based\nmethods consistently outperform the existing best-performing methods. Moreover,\nwe evaluate the effectiveness of different parameters in the deep learning\narchitectures and give insights for configuring them in order to achieve\nimproved performance in crime classification and finally crime prediction.", "journal": ""}
{"doi": "10.48550/arXiv.1908.03673", "date": "2019-08-10", "title": "Recent Advances in Deep Learning for Object Detection", "authors": "Xiongwei Wu, Doyen Sahoo, Steven C. H. Hoi", "abstract": "Object detection is a fundamental visual recognition problem in computer\nvision and has been widely studied in the past decades. Visual object detection\naims to find objects of certain target classes with precise localization in a\ngiven image and assign each object instance a corresponding class label. Due to\nthe tremendous successes of deep learning based image classification, object\ndetection techniques using deep learning have been actively studied in recent\nyears. In this paper, we give a comprehensive survey of recent advances in\nvisual object detection with deep learning. By reviewing a large body of recent\nrelated work in literature, we systematically analyze the existing object\ndetection frameworks and organize the survey into three major parts: (i)\ndetection components, (ii) learning strategies, and (iii) applications &\nbenchmarks. In the survey, we cover a variety of factors affecting the\ndetection performance in detail, such as detector architectures, feature\nlearning, proposal generation, sampling strategies, etc. Finally, we discuss\nseveral future directions to facilitate and spur future research for visual\nobject detection with deep learning. Keywords: Object Detection, Deep Learning,\nDeep Convolutional Neural Networks", "journal": ""}
{"doi": "10.48550/arXiv.1901.02144", "date": "2019-01-08", "title": "Guidelines and Benchmarks for Deployment of Deep Learning Models on Smartphones as Real-Time Apps", "authors": "Abhishek Sehgal, Nasser Kehtarnavaz", "abstract": "Deep learning solutions are being increasingly used in mobile applications.\nAlthough there are many open-source software tools for the development of deep\nlearning solutions, there are no guidelines in one place in a unified manner\nfor using these tools towards real-time deployment of these solutions on\nsmartphones. From the variety of available deep learning tools, the most suited\nones are used in this paper to enable real-time deployment of deep learning\ninference networks on smartphones. A uniform flow of implementation is devised\nfor both Android and iOS smartphones. The advantage of using multi-threading to\nachieve or improve real-time throughputs is also showcased. A benchmarking\nframework consisting of accuracy, CPU/GPU consumption and real-time throughput\nis considered for validation purposes. The developed deployment approach allows\ndeep learning models to be turned into real-time smartphone apps with ease\nbased on publicly available deep learning and smartphone software tools. This\napproach is applied to six popular or representative convolutional neural\nnetwork models and the validation results based on the benchmarking metrics are\nreported.", "journal": ""}
{"doi": "10.48550/arXiv.1912.10804", "date": "2019-12-11", "title": "Row-Sparse Discriminative Deep Dictionary Learning for Hyperspectral Image Classification", "authors": "Vanika Singhal, Angshul Majumdar", "abstract": "In recent studies in hyperspectral imaging, biometrics and energy analytics,\nthe framework of deep dictionary learning has shown promise. Deep dictionary\nlearning outperforms other traditional deep learning tools when training data\nis limited; therefore hyperspectral imaging is one such example that benefits\nfrom this framework. Most of the prior studies were based on the unsupervised\nformulation; and in all cases, the training algorithm was greedy and hence\nsub-optimal. This is the first work that shows how to learn the deep dictionary\nlearning problem in a joint fashion. Moreover, we propose a new discriminative\npenalty to the said framework. The third contribution of this work is showing\nhow to incorporate stochastic regularization techniques into the deep\ndictionary learning framework. Experimental results on hyperspectral image\nclassification shows that the proposed technique excels over all\nstate-of-the-art deep and shallow (traditional) learning based methods\npublished in recent times.", "journal": ""}
{"doi": "10.48550/arXiv.2001.08001", "date": "2020-01-22", "title": "Safety Concerns and Mitigation Approaches Regarding the Use of Deep Learning in Safety-Critical Perception Tasks", "authors": "Oliver Willers, Sebastian Sudholt, Shervin Raafatnia, Stephanie Abrecht", "abstract": "Deep learning methods are widely regarded as indispensable when it comes to\ndesigning perception pipelines for autonomous agents such as robots, drones or\nautomated vehicles. The main reasons, however, for deep learning not being used\nfor autonomous agents at large scale already are safety concerns. Deep learning\napproaches typically exhibit a black-box behavior which makes it hard for them\nto be evaluated with respect to safety-critical aspects. While there have been\nsome work on safety in deep learning, most papers typically focus on high-level\nsafety concerns. In this work, we seek to dive into the safety concerns of deep\nlearning methods and present a concise enumeration on a deeply technical level.\nAdditionally, we present extensive discussions on possible mitigation methods\nand give an outlook regarding what mitigation methods are still missing in\norder to facilitate an argumentation for the safety of a deep learning method.", "journal": ""}
{"doi": "10.48550/arXiv.2005.06068", "date": "2020-05-12", "title": "Deep Learning for Wireless Communications", "authors": "Tugba Erpek, Timothy J. O'Shea, Yalin E. Sagduyu, Yi Shi, T. Charles Clancy", "abstract": "Existing communication systems exhibit inherent limitations in translating\ntheory to practice when handling the complexity of optimization for emerging\nwireless applications with high degrees of freedom. Deep learning has a strong\npotential to overcome this challenge via data-driven solutions and improve the\nperformance of wireless systems in utilizing limited spectrum resources. In\nthis chapter, we first describe how deep learning is used to design an\nend-to-end communication system using autoencoders. This flexible design\neffectively captures channel impairments and optimizes transmitter and receiver\noperations jointly in single-antenna, multiple-antenna, and multiuser\ncommunications. Next, we present the benefits of deep learning in spectrum\nsituation awareness ranging from channel modeling and estimation to signal\ndetection and classification tasks. Deep learning improves the performance when\nthe model-based methods fail. Finally, we discuss how deep learning applies to\nwireless communication security. In this context, adversarial machine learning\nprovides novel means to launch and defend against wireless attacks. These\napplications demonstrate the power of deep learning in providing novel means to\ndesign, optimize, adapt, and secure wireless communications.", "journal": ""}
{"doi": "10.48550/arXiv.2010.12717", "date": "2020-10-23", "title": "Deep Learning for Radio-based Human Sensing: Recent Advances and Future Directions", "authors": "Isura Nirmal, Abdelwahed Khamis, Mahbub Hassan, Wen Hu, Xiaoqing Zhu", "abstract": "While decade-long research has clearly demonstrated the vast potential of\nradio frequency (RF) for many human sensing tasks, scaling this technology to\nlarge scenarios remained problematic with conventional approaches. Recently,\nresearchers have successfully applied deep learning to take radio-based sensing\nto a new level. Many different types of deep learning models have been proposed\nto achieve high sensing accuracy over a large population and activity set, as\nwell as in unseen environments. Deep learning has also enabled detection of\nnovel human sensing phenomena that were previously not possible. In this\nsurvey, we provide a comprehensive review and taxonomy of recent research\nefforts on deep learning based RF sensing. We also identify and compare several\npublicly released labeled RF sensing datasets that can facilitate such deep\nlearning research. Finally, we summarize the lessons learned and discuss the\ncurrent limitations and future directions of deep learning based RF sensing.", "journal": "23, 2021, 995-1019"}
{"doi": "10.48550/arXiv.2011.09857", "date": "2020-11-19", "title": "On tuning deep learning models: a data mining perspective", "authors": "M. M. Ozturk", "abstract": "Deep learning algorithms vary depending on the underlying connection\nmechanism of nodes of them. They have various hyperparameters that are either\nset via specific algorithms or randomly chosen. Meanwhile, hyperparameters of\ndeep learning algorithms have the potential to help enhance the performance of\nthe machine learning tasks. In this paper, a tuning guideline is provided for\nresearchers who cope with issues originated from hyperparameters of deep\nlearning models. To that end, four types of deep learning algorithms are\ninvestigated in terms of tuning and data mining perspective. Further, common\nsearch methods of hyperparameters are evaluated on four deep learning\nalgorithms. Normalization helps increase the performance of classification,\naccording to the results of this study. The number of features has not\ncontributed to the decline in the accuracy of deep learning algorithms. Even\nthough high sparsity results in low accuracy, a uniform distribution is much\nmore crucial to reach reliable results in terms of data mining.", "journal": ""}
{"doi": "10.48550/arXiv.2108.10828", "date": "2021-08-24", "title": "Physics-Informed Deep Learning: A Promising Technique for System Reliability Assessment", "authors": "Taotao Zhou, Enrique Lopez Droguett, Ali Mosleh", "abstract": "Considerable research has been devoted to deep learning-based predictive\nmodels for system prognostics and health management in the reliability and\nsafety community. However, there is limited study on the utilization of deep\nlearning for system reliability assessment. This paper aims to bridge this gap\nand explore this new interface between deep learning and system reliability\nassessment by exploiting the recent advances of physics-informed deep learning.\nParticularly, we present an approach to frame system reliability assessment in\nthe context of physics-informed deep learning and discuss the potential value\nof physics-informed generative adversarial networks for the uncertainty\nquantification and measurement data incorporation in system reliability\nassessment. The proposed approach is demonstrated by three numerical examples\ninvolving a dual-processor computing system. The results indicate the potential\nvalue of physics-informed deep learning to alleviate computational challenges\nand combine measurement data and mathematical models for system reliability\nassessment.", "journal": ""}
{"doi": "10.48550/arXiv.2309.13761", "date": "2023-09-24", "title": "Text Classification: A Perspective of Deep Learning Methods", "authors": "Zhongwei Wan", "abstract": "In recent years, with the rapid development of information on the Internet,\nthe number of complex texts and documents has increased exponentially, which\nrequires a deeper understanding of deep learning methods in order to accurately\nclassify texts using deep learning techniques, and thus deep learning methods\nhave become increasingly important in text classification. Text classification\nis a class of tasks that automatically classifies a set of documents into\nmultiple predefined categories based on their content and subject matter. Thus,\nthe main goal of text classification is to enable users to extract information\nfrom textual resources and process processes such as retrieval, classification,\nand machine learning techniques together in order to classify different\ncategories. Many new techniques of deep learning have already achieved\nexcellent results in natural language processing. The success of these learning\nalgorithms relies on their ability to understand complex models and non-linear\nrelationships in data. However, finding the right structure, architecture, and\ntechniques for text classification is a challenge for researchers. This paper\nintroduces deep learning-based text classification algorithms, including\nimportant steps required for text classification tasks such as feature\nextraction, feature reduction, and evaluation strategies and methods. At the\nend of the article, different deep learning text classification methods are\ncompared and summarized.", "journal": ""}
{"doi": "10.48550/arXiv.2410.20634", "date": "2024-10-27", "title": "Plastic Learning with Deep Fourier Features", "authors": "Alex Lewandowski, Dale Schuurmans, Marlos C. Machado", "abstract": "Deep neural networks can struggle to learn continually in the face of\nnon-stationarity. This phenomenon is known as loss of plasticity. In this\npaper, we identify underlying principles that lead to plastic algorithms. In\nparticular, we provide theoretical results showing that linear function\napproximation, as well as a special case of deep linear networks, do not suffer\nfrom loss of plasticity. We then propose deep Fourier features, which are the\nconcatenation of a sine and cosine in every layer, and we show that this\ncombination provides a dynamic balance between the trainability obtained\nthrough linearity and the effectiveness obtained through the nonlinearity of\nneural networks. Deep networks composed entirely of deep Fourier features are\nhighly trainable and sustain their trainability over the course of learning.\nOur empirical results show that continual learning performance can be\ndrastically improved by replacing ReLU activations with deep Fourier features.\nThese results hold for different continual learning scenarios (e.g., label\nnoise, class incremental learning, pixel permutations) on all major supervised\nlearning datasets used for continual learning research, such as CIFAR10,\nCIFAR100, and tiny-ImageNet.", "journal": ""}
{"doi": "10.48550/arXiv.2006.02724", "date": "2020-06-04", "title": "Characterizing the Weight Space for Different Learning Models", "authors": "Saurav Musunuru, Jay N. Paranjape, Rahul Kumar Dubey, Vijendran G. Venkoparao", "abstract": "Deep Learning has become one of the primary research areas in developing\nintelligent machines. Most of the well-known applications (such as Speech\nRecognition, Image Processing and NLP) of AI are driven by Deep Learning. Deep\nLearning algorithms mimic human brain using artificial neural networks and\nprogressively learn to accurately solve a given problem. But there are\nsignificant challenges in Deep Learning systems. There have been many attempts\nto make deep learning models imitate the biological neural network. However,\nmany deep learning models have performed poorly in the presence of adversarial\nexamples. Poor performance in adversarial examples leads to adversarial attacks\nand in turn leads to safety and security in most of the applications. In this\npaper we make an attempt to characterize the solution space of a deep neural\nnetwork in terms of three different subsets viz. weights belonging to exact\ntrained patterns, weights belonging to generalized pattern set and weights\nbelonging to adversarial pattern sets. We attempt to characterize the solution\nspace with two seemingly different learning paradigms viz. the Deep Neural\nNetworks and the Dense Associative Memory Model, which try to achieve learning\nvia quite different mechanisms. We also show that adversarial attacks are\ngenerally less successful against Associative Memory Models than Deep Neural\nNetworks.", "journal": ""}
{"doi": "10.48550/arXiv.2401.04305", "date": "2024-01-09", "title": "Advancing Deep Active Learning & Data Subset Selection: Unifying Principles with Information-Theory Intuitions", "authors": "Andreas Kirsch", "abstract": "At its core, this thesis aims to enhance the practicality of deep learning by\nimproving the label and training efficiency of deep learning models. To this\nend, we investigate data subset selection techniques, specifically active\nlearning and active sampling, grounded in information-theoretic principles.\nActive learning improves label efficiency, while active sampling enhances\ntraining efficiency. Supervised deep learning models often require extensive\ntraining with labeled data. Label acquisition can be expensive and\ntime-consuming, and training large models is resource-intensive, hindering the\nadoption outside academic research and \"big tech.\" Existing methods for data\nsubset selection in deep learning often rely on heuristics or lack a principled\ninformation-theoretic foundation. In contrast, this thesis examines several\nobjectives for data subset selection and their applications within deep\nlearning, striving for a more principled approach inspired by information\ntheory. We begin by disentangling epistemic and aleatoric uncertainty in single\nforward-pass deep neural networks, which provides helpful intuitions and\ninsights into different forms of uncertainty and their relevance for data\nsubset selection. We then propose and investigate various approaches for active\nlearning and data subset selection in (Bayesian) deep learning. Finally, we\nrelate various existing and proposed approaches to approximations of\ninformation quantities in weight or prediction space. Underpinning this work is\na principled and practical notation for information-theoretic quantities that\nincludes both random variables and observed outcomes. This thesis demonstrates\nthe benefits of working from a unified perspective and highlights the potential\nimpact of our contributions to the practical application of deep learning.", "journal": ""}
{"doi": "10.48550/arXiv.2206.07579", "date": "2022-06-15", "title": "A Comprehensive Survey on Deep Clustering: Taxonomy, Challenges, and Future Directions", "authors": "Sheng Zhou, Hongjia Xu, Zhuonan Zheng, Jiawei Chen, Zhao li, Jiajun Bu, Jia Wu, Xin Wang, Wenwu Zhu, Martin Ester", "abstract": "Clustering is a fundamental machine learning task which has been widely\nstudied in the literature. Classic clustering methods follow the assumption\nthat data are represented as features in a vectorized form through various\nrepresentation learning techniques. As the data become increasingly complicated\nand complex, the shallow (traditional) clustering methods can no longer handle\nthe high-dimensional data type. With the huge success of deep learning,\nespecially the deep unsupervised learning, many representation learning\ntechniques with deep architectures have been proposed in the past decade.\nRecently, the concept of Deep Clustering, i.e., jointly optimizing the\nrepresentation learning and clustering, has been proposed and hence attracted\ngrowing attention in the community. Motivated by the tremendous success of deep\nlearning in clustering, one of the most fundamental machine learning tasks, and\nthe large number of recent advances in this direction, in this paper we conduct\na comprehensive survey on deep clustering by proposing a new taxonomy of\ndifferent state-of-the-art approaches. We summarize the essential components of\ndeep clustering and categorize existing methods by the ways they design\ninteractions between deep representation learning and clustering. Moreover,\nthis survey also provides the popular benchmark datasets, evaluation metrics\nand open-source implementations to clearly illustrate various experimental\nsettings. Last but not least, we discuss the practical applications of deep\nclustering and suggest challenging topics deserving further investigations as\nfuture directions.", "journal": ""}
{"doi": "10.48550/arXiv.2310.16499", "date": "2023-10-25", "title": "Data Optimization in Deep Learning: A Survey", "authors": "Ou Wu, Rujing Yao", "abstract": "Large-scale, high-quality data are considered an essential factor for the\nsuccessful application of many deep learning techniques. Meanwhile, numerous\nreal-world deep learning tasks still have to contend with the lack of\nsufficient amounts of high-quality data. Additionally, issues such as model\nrobustness, fairness, and trustworthiness are also closely related to training\ndata. Consequently, a huge number of studies in the existing literature have\nfocused on the data aspect in deep learning tasks. Some typical data\noptimization techniques include data augmentation, logit perturbation, sample\nweighting, and data condensation. These techniques usually come from different\ndeep learning divisions and their theoretical inspirations or heuristic\nmotivations may seem unrelated to each other. This study aims to organize a\nwide range of existing data optimization methodologies for deep learning from\nthe previous literature, and makes the effort to construct a comprehensive\ntaxonomy for them. The constructed taxonomy considers the diversity of split\ndimensions, and deep sub-taxonomies are constructed for each dimension. On the\nbasis of the taxonomy, connections among the extensive data optimization\nmethods for deep learning are built in terms of four aspects. We probe into\nrendering several promising and interesting future directions. The constructed\ntaxonomy and the revealed connections will enlighten the better understanding\nof existing methods and the design of novel data optimization techniques.\nFurthermore, our aspiration for this survey is to promote data optimization as\nan independent subdivision of deep learning. A curated, up-to-date list of\nresources related to data optimization in deep learning is available at\n\\url{https://github.com/YaoRujing/Data-Optimization}.", "journal": ""}
{"doi": "10.48550/arXiv.1901.02291", "date": "2019-01-08", "title": "Spectral Clustering via Ensemble Deep Autoencoder Learning (SC-EDAE)", "authors": "Severine Affeldt, Lazhar Labiod, Mohamed Nadif", "abstract": "Recently, a number of works have studied clustering strategies that combine\nclassical clustering algorithms and deep learning methods. These approaches\nfollow either a sequential way, where a deep representation is learned using a\ndeep autoencoder before obtaining clusters with k-means, or a simultaneous way,\nwhere deep representation and clusters are learned jointly by optimizing a\nsingle objective function. Both strategies improve clustering performance,\nhowever the robustness of these approaches is impeded by several deep\nautoencoder setting issues, among which the weights initialization, the width\nand number of layers or the number of epochs. To alleviate the impact of such\nhyperparameters setting on the clustering performance, we propose a new model\nwhich combines the spectral clustering and deep autoencoder strengths in an\nensemble learning framework. Extensive experiments on various benchmark\ndatasets demonstrate the potential and robustness of our approach compared to\nstate-of-the-art deep clustering methods.", "journal": ""}
{"doi": "10.48550/arXiv.2010.06209", "date": "2020-10-13", "title": "Deep Reservoir Networks with Learned Hidden Reservoir Weights using Direct Feedback Alignment", "authors": "Matthew Evanusa, Cornelia Ferm\u00fcller, Yiannis Aloimonos", "abstract": "Deep Reservoir Computing has emerged as a new paradigm for deep learning,\nwhich is based around the reservoir computing principle of maintaining random\npools of neurons combined with hierarchical deep learning. The reservoir\nparadigm reflects and respects the high degree of recurrence in biological\nbrains, and the role that neuronal dynamics play in learning. However, one\nissue hampering deep reservoir network development is that one cannot\nbackpropagate through the reservoir layers. Recent deep reservoir architectures\ndo not learn hidden or hierarchical representations in the same manner as deep\nartificial neural networks, but rather concatenate all hidden reservoirs\ntogether to perform traditional regression. Here we present a novel Deep\nReservoir Network for time series prediction and classification that learns\nthrough the non-differentiable hidden reservoir layers using a\nbiologically-inspired backpropagation alternative called Direct Feedback\nAlignment, which resembles global dopamine signal broadcasting in the brain. We\ndemonstrate its efficacy on two real world multidimensional time series\ndatasets.", "journal": ""}
{"doi": "10.48550/arXiv.2012.00204", "date": "2020-12-01", "title": "How to fine-tune deep neural networks in few-shot learning?", "authors": "Peng Peng, Jiugen Wang", "abstract": "Deep learning has been widely used in data-intensive applications. However,\ntraining a deep neural network often requires a large data set. When there is\nnot enough data available for training, the performance of deep learning models\nis even worse than that of shallow networks. It has been proved that few-shot\nlearning can generalize to new tasks with few training samples. Fine-tuning of\na deep model is simple and effective few-shot learning method. However, how to\nfine-tune deep learning models (fine-tune convolution layer or BN layer?) still\nlack deep investigation. Hence, we study how to fine-tune deep models through\nexperimental comparison in this paper. Furthermore, the weight of the models is\nanalyzed to verify the feasibility of the fine-tuning method.", "journal": ""}
{"doi": "10.48550/arXiv.2104.02395", "date": "2021-04-06", "title": "Ensemble deep learning: A review", "authors": "M. A. Ganaie, Minghui Hu, A. K. Malik, M. Tanveer, P. N. Suganthan", "abstract": "Ensemble learning combines several individual models to obtain better\ngeneralization performance. Currently, deep learning architectures are showing\nbetter performance compared to the shallow or traditional models. Deep ensemble\nlearning models combine the advantages of both the deep learning models as well\nas the ensemble learning such that the final model has better generalization\nperformance. This paper reviews the state-of-art deep ensemble models and hence\nserves as an extensive summary for the researchers. The ensemble models are\nbroadly categorised into bagging, boosting, stacking, negative correlation\nbased deep ensemble models, explicit/implicit ensembles,\nhomogeneous/heterogeneous ensemble, decision fusion strategies based deep\nensemble models. Applications of deep ensemble models in different domains are\nalso briefly discussed. Finally, we conclude this paper with some potential\nfuture research directions.", "journal": "Engineering Applications of Artificial Intelligence, 2022"}
{"doi": "10.48550/arXiv.2210.05866", "date": "2022-10-12", "title": "Deep Learning for Iris Recognition: A Survey", "authors": "Kien Nguyen, Hugo Proen\u00e7a, Fernando Alonso-Fernandez", "abstract": "In this survey, we provide a comprehensive review of more than 200 papers,\ntechnical reports, and GitHub repositories published over the last 10 years on\nthe recent developments of deep learning techniques for iris recognition,\ncovering broad topics on algorithm designs, open-source tools, open challenges,\nand emerging research. First, we conduct a comprehensive analysis of deep\nlearning techniques developed for two main sub-tasks in iris biometrics:\nsegmentation and recognition. Second, we focus on deep learning techniques for\nthe robustness of iris recognition systems against presentation attacks and via\nhuman-machine pairing. Third, we delve deep into deep learning techniques for\nforensic application, especially in post-mortem iris recognition. Fourth, we\nreview open-source resources and tools in deep learning techniques for iris\nrecognition. Finally, we highlight the technical challenges, emerging research\ntrends, and outlook for the future of deep learning in iris recognition.", "journal": ""}
{"doi": "10.48550/arXiv.2301.00942", "date": "2023-01-03", "title": "Deep Learning and Computational Physics (Lecture Notes)", "authors": "Deep Ray, Orazio Pinti, Assad A. Oberai", "abstract": "These notes were compiled as lecture notes for a course developed and taught\nat the University of the Southern California. They should be accessible to a\ntypical engineering graduate student with a strong background in Applied\nMathematics.\n  The main objective of these notes is to introduce a student who is familiar\nwith concepts in linear algebra and partial differential equations to select\ntopics in deep learning. These lecture notes exploit the strong connections\nbetween deep learning algorithms and the more conventional techniques of\ncomputational physics to achieve two goals. First, they use concepts from\ncomputational physics to develop an understanding of deep learning algorithms.\nNot surprisingly, many concepts in deep learning can be connected to similar\nconcepts in computational physics, and one can utilize this connection to\nbetter understand these algorithms. Second, several novel deep learning\nalgorithms can be used to solve challenging problems in computational physics.\nThus, they offer someone who is interested in modeling a physical phenomena\nwith a complementary set of tools.", "journal": ""}
{"doi": "10.48550/arXiv.2307.02679", "date": "2023-07-05", "title": "A Study on the Impact of Face Image Quality on Face Recognition in the Wild", "authors": "Na Zhang", "abstract": "Deep learning has received increasing interests in face recognition recently.\nLarge quantities of deep learning methods have been proposed to handle various\nproblems appeared in face recognition. Quite a lot deep methods claimed that\nthey have gained or even surpassed human-level face verification performance in\ncertain databases. As we know, face image quality poses a great challenge to\ntraditional face recognition methods, e.g. model-driven methods with\nhand-crafted features. However, a little research focus on the impact of face\nimage quality on deep learning methods, and even human performance. Therefore,\nwe raise a question: Is face image quality still one of the challenges for deep\nlearning based face recognition, especially in unconstrained condition. Based\non this, we further investigate this problem on human level. In this paper, we\npartition face images into three different quality sets to evaluate the\nperformance of deep learning methods on cross-quality face images in the wild,\nand then design a human face verification experiment on these cross-quality\ndata. The result indicates that quality issue still needs to be studied\nthoroughly in deep learning, human own better capability in building the\nrelations between different face images with large quality gaps, and saying\ndeep learning method surpasses human-level is too optimistic.", "journal": ""}
{"doi": "10.48550/arXiv.2311.06169", "date": "2023-11-10", "title": "Deep Fast Vision: A Python Library for Accelerated Deep Transfer Learning Vision Prototyping", "authors": "Fabi Prezja", "abstract": "Deep learning-based vision is characterized by intricate frameworks that\noften necessitate a profound understanding, presenting a barrier to newcomers\nand limiting broad adoption. With many researchers grappling with the\nconstraints of smaller datasets, there's a pronounced reliance on pre-trained\nneural networks, especially for tasks such as image classification. This\nreliance is further intensified in niche imaging areas where obtaining vast\ndatasets is challenging. Despite the widespread use of transfer learning as a\nremedy to the small dataset dilemma, a conspicuous absence of tailored auto-ML\nsolutions persists. Addressing these challenges is \"Deep Fast Vision\", a python\nlibrary that streamlines the deep learning process. This tool offers a\nuser-friendly experience, enabling results through a simple nested dictionary\ndefinition, helping to democratize deep learning for non-experts. Designed for\nsimplicity and scalability, Deep Fast Vision appears as a bridge, connecting\nthe complexities of existing deep learning frameworks with the needs of a\ndiverse user base.", "journal": ""}
{"doi": "10.48550/arXiv.2408.06212", "date": "2024-08-12", "title": "Computability of Classification and Deep Learning: From Theoretical Limits to Practical Feasibility through Quantization", "authors": "Holger Boche, Vit Fojtik, Adalbert Fono, Gitta Kutyniok", "abstract": "The unwavering success of deep learning in the past decade led to the\nincreasing prevalence of deep learning methods in various application fields.\nHowever, the downsides of deep learning, most prominently its lack of\ntrustworthiness, may not be compatible with safety-critical or\nhigh-responsibility applications requiring stricter performance guarantees.\nRecently, several instances of deep learning applications have been shown to be\nsubject to theoretical limitations of computability, undermining the\nfeasibility of performance guarantees when employed on real-world computers. We\nextend the findings by studying computability in the deep learning framework\nfrom two perspectives: From an application viewpoint in the context of\nclassification problems and a general limitation viewpoint in the context of\ntraining neural networks. In particular, we show restrictions on the\nalgorithmic solvability of classification problems that also render the\nalgorithmic detection of failure in computations in a general setting\ninfeasible. Subsequently, we prove algorithmic limitations in training deep\nneural networks even in cases where the underlying problem is well-behaved.\nFinally, we end with a positive observation, showing that in quantized versions\nof classification and deep network training, computability restrictions do not\narise or can be overcome to a certain degree.", "journal": ""}
{"doi": "10.48550/arXiv.2502.00833", "date": "2025-02-02", "title": "Cross multiscale vision transformer for deep fake detection", "authors": "Akhshan P, Taneti Sanjay, Chandrakala S", "abstract": "The proliferation of deep fake technology poses significant challenges to\ndigital media authenticity, necessitating robust detection mechanisms. This\nproject evaluates deep fake detection using the SP Cup's 2025 deep fake\ndetection challenge dataset. We focused on exploring various deep learning\nmodels for detecting deep fake content, utilizing traditional deep learning\ntechniques alongside newer architectures. Our approach involved training a\nseries of models and rigorously assessing their performance using metrics such\nas accuracy.", "journal": ""}
{"doi": "10.48550/arXiv.1501.06237", "date": "2015-01-26", "title": "Deep Transductive Semi-supervised Maximum Margin Clustering", "authors": "Gang Chen", "abstract": "Semi-supervised clustering is an very important topic in machine learning and\ncomputer vision. The key challenge of this problem is how to learn a metric,\nsuch that the instances sharing the same label are more likely close to each\nother on the embedded space. However, little attention has been paid to learn\nbetter representations when the data lie on non-linear manifold. Fortunately,\ndeep learning has led to great success on feature learning recently. Inspired\nby the advances of deep learning, we propose a deep transductive\nsemi-supervised maximum margin clustering approach. More specifically, given\npairwise constraints, we exploit both labeled and unlabeled data to learn a\nnon-linear mapping under maximum margin framework for clustering analysis.\nThus, our model unifies transductive learning, feature learning and maximum\nmargin techniques in the semi-supervised clustering framework. We pretrain the\ndeep network structure with restricted Boltzmann machines (RBMs) layer by layer\ngreedily, and optimize our objective function with gradient descent. By\nchecking the most violated constraints, our approach updates the model\nparameters through error backpropagation, in which deep features are learned\nautomatically. The experimental results shows that our model is significantly\nbetter than the state of the art on semi-supervised clustering.", "journal": ""}
{"doi": "10.48550/arXiv.1908.10714", "date": "2019-08-22", "title": "Automated Architecture Design for Deep Neural Networks", "authors": "Steven Abreu", "abstract": "Machine learning has made tremendous progress in recent years and received\nlarge amounts of public attention. Though we are still far from designing a\nfull artificially intelligent agent, machine learning has brought us many\napplications in which computers solve human learning tasks remarkably well.\nMuch of this progress comes from a recent trend within machine learning, called\ndeep learning. Deep learning models are responsible for many state-of-the-art\napplications of machine learning. Despite their success, deep learning models\nare hard to train, very difficult to understand, and often times so complex\nthat training is only possible on very large GPU clusters. Lots of work has\nbeen done on enabling neural networks to learn efficiently. However, the design\nand architecture of such neural networks is often done manually through trial\nand error and expert knowledge. This thesis inspects different approaches,\nexisting and novel, to automate the design of deep feedforward neural networks\nin an attempt to create less complex models with good performance that take\naway the burden of deciding on an architecture and make it more efficient to\ndesign and train such deep networks.", "journal": ""}
{"doi": "10.48550/arXiv.2004.00503", "date": "2020-03-31", "title": "Deep Learning Approach for Enhanced Cyber Threat Indicators in Twitter Stream", "authors": "Simran K, Prathiksha Balakrishna, Vinayakumar R, Soman KP", "abstract": "In recent days, the amount of Cyber Security text data shared via social\nmedia resources mainly Twitter has increased. An accurate analysis of this data\ncan help to develop cyber threat situational awareness framework for a cyber\nthreat. This work proposes a deep learning based approach for tweet data\nanalysis. To convert the tweets into numerical representations, various text\nrepresentations are employed. These features are feed into deep learning\narchitecture for optimal feature extraction as well as classification. Various\nhyperparameter tuning approaches are used for identifying optimal text\nrepresentation method as well as optimal network parameters and network\nstructures for deep learning models. For comparative analysis, the classical\ntext representation method with classical machine learning algorithm is\nemployed. From the detailed analysis of experiments, we found that the deep\nlearning architecture with advanced text representation methods performed\nbetter than the classical text representation and classical machine learning\nalgorithms. The primary reason for this is that the advanced text\nrepresentation methods have the capability to learn sequential properties which\nexist among the textual data and deep learning architectures learns the optimal\nfeatures along with decreasing the feature size.", "journal": ""}
{"doi": "10.48550/arXiv.2010.11560", "date": "2020-10-22", "title": "Deep Learning is Singular, and That's Good", "authors": "Daniel Murfet, Susan Wei, Mingming Gong, Hui Li, Jesse Gell-Redman, Thomas Quella", "abstract": "In singular models, the optimal set of parameters forms an analytic set with\nsingularities and classical statistical inference cannot be applied to such\nmodels. This is significant for deep learning as neural networks are singular\nand thus \"dividing\" by the determinant of the Hessian or employing the Laplace\napproximation are not appropriate. Despite its potential for addressing\nfundamental issues in deep learning, singular learning theory appears to have\nmade little inroads into the developing canon of deep learning theory. Via a\nmix of theory and experiment, we present an invitation to singular learning\ntheory as a vehicle for understanding deep learning and suggest important\nfuture work to make singular learning theory directly applicable to how deep\nlearning is performed in practice.", "journal": "IEEE Transactions on Neural Networks and Learning Systems 34,\n  issue 12, pages 10473-10486, December 2023 (published online on 30 June 2022)"}
{"doi": "10.48550/arXiv.2304.07689", "date": "2023-04-16", "title": "Learning Empirical Bregman Divergence for Uncertain Distance Representation", "authors": "Zhiyuan Li, Ziru Liu, Anna Zou, Anca L. Ralescu", "abstract": "Deep metric learning techniques have been used for visual representation in\nvarious supervised and unsupervised learning tasks through learning embeddings\nof samples with deep networks. However, classic approaches, which employ a\nfixed distance metric as a similarity function between two embeddings, may lead\nto suboptimal performance for capturing the complex data distribution. The\nBregman divergence generalizes measures of various distance metrics and arises\nthroughout many fields of deep metric learning. In this paper, we first show\nhow deep metric learning loss can arise from the Bregman divergence. We then\nintroduce a novel method for learning empirical Bregman divergence directly\nfrom data based on parameterizing the convex function underlying the Bregman\ndivergence with a deep learning setting. We further experimentally show that\nour approach performs effectively on five popular public datasets compared to\nother SOTA deep metric learning methods, particularly for pattern recognition\nproblems.", "journal": ""}
{"doi": "10.48550/arXiv.1907.06572", "date": "2019-07-12", "title": "Deep network as memory space: complexity, generalization, disentangled representation and interpretability", "authors": "X. Dong, L. Zhou", "abstract": "By bridging deep networks and physics, the programme of geometrization of\ndeep networks was proposed as a framework for the interpretability of deep\nlearning systems. Following this programme we can apply two key ideas of\nphysics, the geometrization of physics and the least action principle, on deep\nnetworks and deliver a new picture of deep networks: deep networks as memory\nspace of information, where the capacity, robustness and efficiency of the\nmemory are closely related with the complexity, generalization and\ndisentanglement of deep networks. The key components of this understanding\ninclude:(1) a Fisher metric based formulation of the network complexity; (2)the\nleast action (complexity=action) principle on deep networks and (3)the geometry\nbuilt on deep network configurations. We will show how this picture will bring\nus a new understanding of the interpretability of deep learning systems.", "journal": ""}
{"doi": "10.48550/arXiv.2008.07434", "date": "2020-07-21", "title": "Integrating Deep Reinforcement Learning Networks with Health System Simulations", "authors": "Michael Allen, Thomas Monks", "abstract": "Background and motivation: Combining Deep Reinforcement Learning (Deep RL)\nand Health Systems Simulations has significant potential, for both research\ninto improving Deep RL performance and safety, and in operational practice.\nWhile individual toolkits exist for Deep RL and Health Systems Simulations, no\nframework to integrate the two has been established.\n  Aim: Provide a framework for integrating Deep RL Networks with Health System\nSimulations, and to ensure this framework is compatible with Deep RL agents\nthat have been developed and tested using OpenAI Gym.\n  Methods: We developed our framework based on the OpenAI Gym framework, and\ndemonstrate its use on a simple hospital bed capacity model. We built the Deep\nRL agents using PyTorch, and the Hospital Simulatation using SimPy.\n  Results: We demonstrate example models using a Double Deep Q Network or a\nDuelling Double Deep Q Network as the Deep RL agent.\n  Conclusion: SimPy may be used to create Health System Simulations that are\ncompatible with agents developed and tested on OpenAI Gym environments.\n  GitHub repository of code:\nhttps://github.com/MichaelAllen1966/learninghospital", "journal": ""}
{"doi": "10.48550/arXiv.2002.06262", "date": "2020-02-14", "title": "Why Do Deep Residual Networks Generalize Better than Deep Feedforward Networks? -- A Neural Tangent Kernel Perspective", "authors": "Kaixuan Huang, Yuqing Wang, Molei Tao, Tuo Zhao", "abstract": "Deep residual networks (ResNets) have demonstrated better generalization\nperformance than deep feedforward networks (FFNets). However, the theory behind\nsuch a phenomenon is still largely unknown. This paper studies this fundamental\nproblem in deep learning from a so-called \"neural tangent kernel\" perspective.\nSpecifically, we first show that under proper conditions, as the width goes to\ninfinity, training deep ResNets can be viewed as learning reproducing kernel\nfunctions with some kernel function. We then compare the kernel of deep ResNets\nwith that of deep FFNets and discover that the class of functions induced by\nthe kernel of FFNets is asymptotically not learnable, as the depth goes to\ninfinity. In contrast, the class of functions induced by the kernel of ResNets\ndoes not exhibit such degeneracy. Our discovery partially justifies the\nadvantages of deep ResNets over deep FFNets in generalization abilities.\nNumerical results are provided to support our claim.", "journal": ""}
{"doi": "10.48550/arXiv.1512.06927", "date": "2015-12-22", "title": "A C++ library for Multimodal Deep Learning", "authors": "Jian Jin", "abstract": "MDL, Multimodal Deep Learning Library, is a deep learning framework that\nsupports multiple models, and this document explains its philosophy and\nfunctionality. MDL runs on Linux, Mac, and Unix platforms. It depends on\nOpenCV.", "journal": ""}
{"doi": "10.48550/arXiv.1706.08675", "date": "2017-06-27", "title": "Proceedings of the First International Workshop on Deep Learning and Music", "authors": "Dorien Herremans, Ching-Hua Chuan", "abstract": "Proceedings of the First International Workshop on Deep Learning and Music,\njoint with IJCNN, Anchorage, US, May 17-18, 2017", "journal": "Proceedings of the First International Workshop on Deep Learning\n  and Music, joint with IJCNN, Anchorage, US, May 17-18, 2017"}
{"doi": "10.48550/arXiv.1706.09557", "date": "2017-06-29", "title": "Machine listening intelligence", "authors": "C. E. Cella", "abstract": "This manifesto paper will introduce machine listening intelligence, an\nintegrated research framework for acoustic and musical signals modelling, based\non signal processing, deep learning and computational musicology.", "journal": "Proceedings of the First International Workshop on Deep Learning\n  and Music joint with IJCNN. Anchorage, US. 1(1). pp 50-55 (2017)"}
{"doi": "10.48550/arXiv.1712.04741", "date": "2017-12-13", "title": "Mathematics of Deep Learning", "authors": "Rene Vidal, Joan Bruna, Raja Giryes, Stefano Soatto", "abstract": "Recently there has been a dramatic increase in the performance of recognition\nsystems due to the introduction of deep architectures for representation\nlearning and classification. However, the mathematical reasons for this success\nremain elusive. This tutorial will review recent work that aims to provide a\nmathematical justification for several properties of deep networks, such as\nglobal optimality, geometric stability, and invariance of the learned\nrepresentations.", "journal": ""}
{"doi": "10.48550/arXiv.2205.12752", "date": "2022-05-25", "title": "NECA: Network-Embedded Deep Representation Learning for Categorical Data", "authors": "Xiaonan Gao, Sen Wu, Wenjun Zhou", "abstract": "We propose NECA, a deep representation learning method for categorical data.\nBuilt upon the foundations of network embedding and deep unsupervised\nrepresentation learning, NECA deeply embeds the intrinsic relationship among\nattribute values and explicitly expresses data objects with numeric vector\nrepresentations. Designed specifically for categorical data, NECA can support\nimportant downstream data mining tasks, such as clustering. Extensive\nexperimental analysis demonstrated the effectiveness of NECA.", "journal": ""}
{"doi": "10.48550/arXiv.2310.10250", "date": "2023-10-16", "title": "Leveraging Topological Maps in Deep Reinforcement Learning for Multi-Object Navigation", "authors": "Simon Hakenes, Tobias Glasmachers", "abstract": "This work addresses the challenge of navigating expansive spaces with sparse\nrewards through Reinforcement Learning (RL). Using topological maps, we elevate\nelementary actions to object-oriented macro actions, enabling a simple Deep\nQ-Network (DQN) agent to solve otherwise practically impossible environments.", "journal": ""}
{"doi": "10.48550/arXiv.2005.10247", "date": "2020-05-20", "title": "Model-Based Robust Deep Learning: Generalizing to Natural, Out-of-Distribution Data", "authors": "Alexander Robey, Hamed Hassani, George J. Pappas", "abstract": "While deep learning has resulted in major breakthroughs in many application\ndomains, the frameworks commonly used in deep learning remain fragile to\nartificially-crafted and imperceptible changes in the data. In response to this\nfragility, adversarial training has emerged as a principled approach for\nenhancing the robustness of deep learning with respect to norm-bounded\nperturbations. However, there are other sources of fragility for deep learning\nthat are arguably more common and less thoroughly studied. Indeed, natural\nvariation such as lighting or weather conditions can significantly degrade the\naccuracy of trained neural networks, proving that such natural variation\npresents a significant challenge for deep learning.\n  In this paper, we propose a paradigm shift from perturbation-based\nadversarial robustness toward model-based robust deep learning. Our objective\nis to provide general training algorithms that can be used to train deep neural\nnetworks to be robust against natural variation in data. Critical to our\nparadigm is first obtaining a model of natural variation which can be used to\nvary data over a range of natural conditions. Such models may be either known a\npriori or else learned from data. In the latter case, we show that deep\ngenerative models can be used to learn models of natural variation that are\nconsistent with realistic conditions. We then exploit such models in three\nnovel model-based robust training algorithms in order to enhance the robustness\nof deep learning with respect to the given model. Our extensive experiments\nshow that across a variety of naturally-occurring conditions and across various\ndatasets, deep neural networks trained with our model-based algorithms\nsignificantly outperform both standard deep learning algorithms as well as\nnorm-bounded robust deep learning algorithms.", "journal": ""}
{"doi": "10.48550/arXiv.1607.01354", "date": "2016-03-22", "title": "Learning Discriminative Features using Encoder-Decoder type Deep Neural Nets", "authors": "Vishwajeet Singh, Killamsetti Ravi Kumar, K Eswaran", "abstract": "As machine learning is applied to an increasing variety of complex problems,\nwhich are defined by high dimensional and complex data sets, the necessity for\ntask oriented feature learning grows in importance. With the advancement of\nDeep Learning algorithms, various successful feature learning techniques have\nevolved. In this paper, we present a novel way of learning discriminative\nfeatures by training Deep Neural Nets which have Encoder or Decoder type\narchitecture similar to an Autoencoder. We demonstrate that our approach can\nlearn discriminative features which can perform better at pattern\nclassification tasks when the number of training samples is relatively small in\nsize.", "journal": ""}
{"doi": "10.48550/arXiv.1905.07822", "date": "2019-05-19", "title": "Minimal Achievable Sufficient Statistic Learning", "authors": "Milan Cvitkovic, G\u00fcnther Koliander", "abstract": "We introduce Minimal Achievable Sufficient Statistic (MASS) Learning, a\ntraining method for machine learning models that attempts to produce minimal\nsufficient statistics with respect to a class of functions (e.g. deep networks)\nbeing optimized over. In deriving MASS Learning, we also introduce Conserved\nDifferential Information (CDI), an information-theoretic quantity that - unlike\nstandard mutual information - can be usefully applied to\ndeterministically-dependent continuous random variables like the input and\noutput of a deep network. In a series of experiments, we show that deep\nnetworks trained with MASS Learning achieve competitive performance on\nsupervised learning and uncertainty quantification benchmarks.", "journal": ""}
{"doi": "10.48550/arXiv.2309.13752", "date": "2023-09-24", "title": "Improving Robustness of Deep Convolutional Neural Networks via Multiresolution Learning", "authors": "Hongyan Zhou, Yao Liang", "abstract": "The current learning process of deep learning, regardless of any deep neural\nnetwork (DNN) architecture and/or learning algorithm used, is essentially a\nsingle resolution training. We explore multiresolution learning and show that\nmultiresolution learning can significantly improve robustness of DNN models for\nboth 1D signal and 2D signal (image) prediction problems. We demonstrate this\nimprovement in terms of both noise and adversarial robustness as well as with\nsmall training dataset size. Our results also suggest that it may not be\nnecessary to trade standard accuracy for robustness with multiresolution\nlearning, which is, interestingly, contrary to the observation obtained from\nthe traditional single resolution learning setting.", "journal": ""}
{"doi": "10.48550/arXiv.1707.00116", "date": "2017-07-01", "title": "Image Companding and Inverse Halftoning using Deep Convolutional Neural Networks", "authors": "Xianxu Hou, Guoping Qiu", "abstract": "In this paper, we introduce deep learning technology to tackle two\ntraditional low-level image processing problems, companding and inverse\nhalftoning. We make two main contributions. First, to the best knowledge of the\nauthors, this is the first work that has successfully developed deep learning\nbased solutions to these two traditional low-level image processing problems.\nThis not only introduces new methods to tackle well-known image processing\nproblems but also demonstrates the power of deep learning in solving\ntraditional signal processing problems. Second, we have developed an effective\ndeep learning algorithm based on insights into the properties of visual quality\nof images and the internal representation properties of a deep convolutional\nneural network (CNN). We train a deep CNN as a nonlinear transformation\nfunction to map a low bit depth image to higher bit depth or from a halftone\nimage to a continuous tone image. We also employ another pretrained deep CNN as\na feature extractor to derive visually important features to construct the\nobjective function for the training of the mapping CNN. We present experimental\nresults to demonstrate the effectiveness of the new deep learning based\nsolutions.", "journal": ""}
{"doi": "10.48550/arXiv.2104.10584", "date": "2021-04-21", "title": "Deep Learning for Click-Through Rate Estimation", "authors": "Weinan Zhang, Jiarui Qin, Wei Guo, Ruiming Tang, Xiuqiang He", "abstract": "Click-through rate (CTR) estimation plays as a core function module in\nvarious personalized online services, including online advertising, recommender\nsystems, and web search etc. From 2015, the success of deep learning started to\nbenefit CTR estimation performance and now deep CTR models have been widely\napplied in many industrial platforms. In this survey, we provide a\ncomprehensive review of deep learning models for CTR estimation tasks. First,\nwe take a review of the transfer from shallow to deep CTR models and explain\nwhy going deep is a necessary trend of development. Second, we concentrate on\nexplicit feature interaction learning modules of deep CTR models. Then, as an\nimportant perspective on large platforms with abundant user histories, deep\nbehavior models are discussed. Moreover, the recently emerged automated methods\nfor deep CTR architecture design are presented. Finally, we summarize the\nsurvey and discuss the future prospects of this field.", "journal": ""}
{"doi": "10.48550/arXiv.1810.00123", "date": "2018-09-29", "title": "Generalization and Regularization in DQN", "authors": "Jesse Farebrother, Marlos C. Machado, Michael Bowling", "abstract": "Deep reinforcement learning algorithms have shown an impressive ability to\nlearn complex control policies in high-dimensional tasks. However, despite the\never-increasing performance on popular benchmarks, policies learned by deep\nreinforcement learning algorithms can struggle to generalize when evaluated in\nremarkably similar environments. In this paper we propose a protocol to\nevaluate generalization in reinforcement learning through different modes of\nAtari 2600 games. With that protocol we assess the generalization capabilities\nof DQN, one of the most traditional deep reinforcement learning algorithms, and\nwe provide evidence suggesting that DQN overspecializes to the training\nenvironment. We then comprehensively evaluate the impact of dropout and\n$\\ell_2$ regularization, as well as the impact of reusing learned\nrepresentations to improve the generalization capabilities of DQN. Despite\nregularization being largely underutilized in deep reinforcement learning, we\nshow that it can, in fact, help DQN learn more general features. These features\ncan be reused and fine-tuned on similar tasks, considerably improving DQN's\nsample efficiency.", "journal": ""}
{"doi": "10.48550/arXiv.1805.02686", "date": "2018-05-07", "title": "Holarchic Structures for Decentralized Deep Learning - A Performance Analysis", "authors": "Evangelos Pournaras, Srivatsan Yadhunathan, Ada Diaconescu", "abstract": "Structure plays a key role in learning performance. In centralized\ncomputational systems, hyperparameter optimization and regularization\ntechniques such as dropout are computational means to enhance learning\nperformance by adjusting the deep hierarchical structure. However, in\ndecentralized deep learning by the Internet of Things, the structure is an\nactual network of autonomous interconnected devices such as smart phones that\ninteract via complex network protocols. Self-adaptation of the learning\nstructure is a challenge. Uncertainties such as network latency, node and link\nfailures or even bottlenecks by limited processing capacity and energy\navailability can signif- icantly downgrade learning performance. Network\nself-organization and self-management is complex, while it requires additional\ncomputational and network resources that hinder the feasibility of\ndecentralized deep learning. In contrast, this paper introduces a self-adaptive\nlearning approach based on holarchic learning structures for exploring,\nmitigating and boosting learning performance in distributed environments with\nuncertainties. A large-scale performance analysis with 864000 experiments fed\nwith synthetic and real-world data from smart grid and smart city pilot\nprojects confirm the cost-effectiveness of holarchic structures for\ndecentralized deep learning.", "journal": ""}
{"doi": "10.48550/arXiv.2104.05314", "date": "2021-04-12", "title": "Machine learning and deep learning", "authors": "Christian Janiesch, Patrick Zschech, Kai Heinrich", "abstract": "Today, intelligent systems that offer artificial intelligence capabilities\noften rely on machine learning. Machine learning describes the capacity of\nsystems to learn from problem-specific training data to automate the process of\nanalytical model building and solve associated tasks. Deep learning is a\nmachine learning concept based on artificial neural networks. For many\napplications, deep learning models outperform shallow machine learning models\nand traditional data analysis approaches. In this article, we summarize the\nfundamentals of machine learning and deep learning to generate a broader\nunderstanding of the methodical underpinning of current intelligent systems. In\nparticular, we provide a conceptual distinction between relevant terms and\nconcepts, explain the process of automated analytical model building through\nmachine learning and deep learning, and discuss the challenges that arise when\nimplementing such intelligent systems in the field of electronic markets and\nnetworked business. These naturally go beyond technological aspects and\nhighlight issues in human-machine interaction and artificial intelligence\nservitization.", "journal": ""}
{"doi": "10.48550/arXiv.1603.07846", "date": "2016-03-25", "title": "Deep Learning At Scale and At Ease", "authors": "Wei Wang, Gang Chen, Haibo Chen, Tien Tuan Anh Dinh, Jinyang Gao, Beng Chin Ooi, Kian-Lee Tan, Sheng Wang", "abstract": "Recently, deep learning techniques have enjoyed success in various multimedia\napplications, such as image classification and multi-modal data analysis. Large\ndeep learning models are developed for learning rich representations of complex\ndata. There are two challenges to overcome before deep learning can be widely\nadopted in multimedia and other applications. One is usability, namely the\nimplementation of different models and training algorithms must be done by\nnon-experts without much effort especially when the model is large and complex.\nThe other is scalability, that is the deep learning system must be able to\nprovision for a huge demand of computing resources for training large models\nwith massive datasets. To address these two challenges, in this paper, we\ndesign a distributed deep learning platform called SINGA which has an intuitive\nprogramming model based on the common layer abstraction of deep learning\nmodels. Good scalability is achieved through flexible distributed training\narchitecture and specific optimization techniques. SINGA runs on GPUs as well\nas on CPUs, and we show that it outperforms many other state-of-the-art deep\nlearning systems. Our experience with developing and training deep learning\nmodels for real-life multimedia applications in SINGA shows that the platform\nis both usable and scalable.", "journal": ""}
{"doi": "10.48550/arXiv.1905.09195", "date": "2019-05-22", "title": "On the minimax optimality and superiority of deep neural network learning over sparse parameter spaces", "authors": "Satoshi Hayakawa, Taiji Suzuki", "abstract": "Deep learning has been applied to various tasks in the field of machine\nlearning and has shown superiority to other common procedures such as kernel\nmethods. To provide a better theoretical understanding of the reasons for its\nsuccess, we discuss the performance of deep learning and other methods on a\nnonparametric regression problem with a Gaussian noise. Whereas existing\ntheoretical studies of deep learning have been based mainly on mathematical\ntheories of well-known function classes such as H\\\"{o}lder and Besov classes,\nwe focus on function classes with discontinuity and sparsity, which are those\nnaturally assumed in practice. To highlight the effectiveness of deep learning,\nwe compare deep learning with a class of linear estimators representative of a\nclass of shallow estimators. It is shown that the minimax risk of a linear\nestimator on the convex hull of a target function class does not differ from\nthat of the original target function class. This results in the suboptimality\nof linear methods over a simple but non-convex function class, on which deep\nlearning can attain nearly the minimax-optimal rate. In addition to this\nextreme case, we consider function classes with sparse wavelet coefficients. On\nthese function classes, deep learning also attains the minimax rate up to log\nfactors of the sample size, and linear methods are still suboptimal if the\nassumed sparsity is strong. We also point out that the parameter sharing of\ndeep neural networks can remarkably reduce the complexity of the model in our\nsetting.", "journal": "Neural Networks, 2020"}
{"doi": "10.48550/arXiv.1904.09274", "date": "2019-03-21", "title": "Deep Learning on Mobile Devices - A Review", "authors": "Yunbin Deng", "abstract": "Recent breakthroughs in deep learning and artificial intelligence\ntechnologies have enabled numerous mobile applications. While traditional\ncomputation paradigms rely on mobile sensing and cloud computing, deep learning\nimplemented on mobile devices provides several advantages. These advantages\ninclude low communication bandwidth, small cloud computing resource cost, quick\nresponse time, and improved data privacy. Research and development of deep\nlearning on mobile and embedded devices has recently attracted much attention.\nThis paper provides a timely review of this fast-paced field to give the\nresearcher, engineer, practitioner, and graduate student a quick grasp on the\nrecent advancements of deep learning on mobile devices. In this paper, we\ndiscuss hardware architectures for mobile deep learning, including Field\nProgrammable Gate Arrays, Application Specific Integrated Circuit, and recent\nmobile Graphic Processing Units. We present Size, Weight, Area and Power\nconsiderations and their relation to algorithm optimizations, such as\nquantization, pruning, compression, and approximations that simplify\ncomputation while retaining performance accuracy. We cover existing systems and\ngive a state-of-the-industry review of TensorFlow, MXNet, Mobile AI Compute\nEngine, and Paddle-mobile deep learning platform. We discuss resources for\nmobile deep learning practitioners, including tools, libraries, models, and\nperformance benchmarks. We present applications of various mobile sensing\nmodalities to industries, ranging from robotics, healthcare and multi-media,\nbiometrics to autonomous drive and defense. We address the key deep learning\nchallenges to overcome, including low quality data, and small\ntraining/adaptation data sets. In addition, the review provides numerous\ncitations and links to existing code bases implementing various technologies.", "journal": "SPIE Defense + Commercial Sensing, Invited Paper. April 2019,\n  Baltimore, MD"}
{"doi": "10.48550/arXiv.2012.07976", "date": "2020-12-14", "title": "NeurIPS 2020 Competition: Predicting Generalization in Deep Learning", "authors": "Yiding Jiang, Pierre Foret, Scott Yak, Daniel M. Roy, Hossein Mobahi, Gintare Karolina Dziugaite, Samy Bengio, Suriya Gunasekar, Isabelle Guyon, Behnam Neyshabur", "abstract": "Understanding generalization in deep learning is arguably one of the most\nimportant questions in deep learning. Deep learning has been successfully\nadopted to a large number of problems ranging from pattern recognition to\ncomplex decision making, but many recent researchers have raised many concerns\nabout deep learning, among which the most important is generalization. Despite\nnumerous attempts, conventional statistical learning approaches have yet been\nable to provide a satisfactory explanation on why deep learning works. A recent\nline of works aims to address the problem by trying to predict the\ngeneralization performance through complexity measures. In this competition, we\ninvite the community to propose complexity measures that can accurately predict\ngeneralization of models. A robust and general complexity measure would\npotentially lead to a better understanding of deep learning's underlying\nmechanism and behavior of deep models on unseen data, or shed light on better\ngeneralization bounds. All these outcomes will be important for making deep\nlearning more robust and reliable.", "journal": ""}
{"doi": "10.48550/arXiv.2207.07859", "date": "2022-07-16", "title": "SenseFi: A Library and Benchmark on Deep-Learning-Empowered WiFi Human Sensing", "authors": "Jianfei Yang, Xinyan Chen, Dazhuo Wang, Han Zou, Chris Xiaoxuan Lu, Sumei Sun, Lihua Xie", "abstract": "WiFi sensing has been evolving rapidly in recent years. Empowered by\npropagation models and deep learning methods, many challenging applications are\nrealized such as WiFi-based human activity recognition and gesture recognition.\nHowever, in contrast to deep learning for visual recognition and natural\nlanguage processing, no sufficiently comprehensive public benchmark exists. In\nthis paper, we review the recent progress on deep learning enabled WiFi\nsensing, and then propose a benchmark, SenseFi, to study the effectiveness of\nvarious deep learning models for WiFi sensing. These advanced models are\ncompared in terms of distinct sensing tasks, WiFi platforms, recognition\naccuracy, model size, computational complexity, feature transferability, and\nadaptability of unsupervised learning. It is also regarded as a tutorial for\ndeep learning based WiFi sensing, starting from CSI hardware platform to\nsensing algorithms. The extensive experiments provide us with experiences in\ndeep model design, learning strategy skills and training techniques for\nreal-world applications. To the best of our knowledge, this is the first\nbenchmark with an open-source library for deep learning in WiFi sensing\nresearch. The benchmark codes are available at\nhttps://github.com/xyanchen/WiFi-CSI-Sensing-Benchmark.", "journal": ""}
{"doi": "10.48550/arXiv.2404.08011", "date": "2024-04-10", "title": "An inclusive review on deep learning techniques and their scope in handwriting recognition", "authors": "Sukhdeep Singh, Sudhir Rohilla, Anuj Sharma", "abstract": "Deep learning expresses a category of machine learning algorithms that have\nthe capability to combine raw inputs into intermediate features layers. These\ndeep learning algorithms have demonstrated great results in different fields.\nDeep learning has particularly witnessed for a great achievement of human level\nperformance across a number of domains in computer vision and pattern\nrecognition. For the achievement of state-of-the-art performances in diverse\ndomains, the deep learning used different architectures and these architectures\nused activation functions to perform various computations between hidden and\noutput layers of any architecture. This paper presents a survey on the existing\nstudies of deep learning in handwriting recognition field. Even though the\nrecent progress indicates that the deep learning methods has provided valuable\nmeans for speeding up or proving accurate results in handwriting recognition,\nbut following from the extensive literature survey, the present study finds\nthat the deep learning has yet to revolutionize more and has to resolve many of\nthe most pressing challenges in this field, but promising advances have been\nmade on the prior state of the art. Additionally, an inadequate availability of\nlabelled data to train presents problems in this domain. Nevertheless, the\npresent handwriting recognition survey foresees deep learning enabling changes\nat both bench and bedside with the potential to transform several domains as\nimage processing, speech recognition, computer vision, machine translation,\nrobotics and control, medical imaging, medical information processing,\nbio-informatics, natural language processing, cyber security, and many others.", "journal": ""}
{"doi": "10.48550/arXiv.2302.02406", "date": "2023-02-05", "title": "Pre-screening breast cancer with machine learning and deep learning", "authors": "Rolando Gonzales Martinez, Daan-Max van Dongen", "abstract": "We suggest that deep learning can be used for pre-screening cancer by\nanalyzing demographic and anthropometric information of patients, as well as\nbiological markers obtained from routine blood samples and relative risks\nobtained from meta-analysis and international databases. We applied feature\nselection algorithms to a database of 116 women, including 52 healthy women and\n64 women diagnosed with breast cancer, to identify the best pre-screening\npredictors of cancer. We utilized the best predictors to perform k-fold Monte\nCarlo cross-validation experiments that compare deep learning against\ntraditional machine learning algorithms. Our results indicate that a deep\nlearning model with an input-layer architecture that is fine-tuned using\nfeature selection can effectively distinguish between patients with and without\ncancer. Additionally, compared to machine learning, deep learning has the\nlowest uncertainty in its predictions. These findings suggest that deep\nlearning algorithms applied to cancer pre-screening offer a radiation-free,\nnon-invasive, and affordable complement to screening methods based on imagery.\nThe implementation of deep learning algorithms in cancer pre-screening offer\nopportunities to identify individuals who may require imaging-based screening,\ncan encourage self-examination, and decrease the psychological externalities\nassociated with false positives in cancer screening. The integration of deep\nlearning algorithms for both screening and pre-screening will ultimately lead\nto earlier detection of malignancy, reducing the healthcare and societal burden\nassociated to cancer treatment.", "journal": ""}
{"doi": "10.48550/arXiv.2408.12308", "date": "2024-08-22", "title": "Deep Learning with CNNs: A Compact Holistic Tutorial with Focus on Supervised Regression (Preprint)", "authors": "Yansel Gonzalez Tejeda, Helmut A. Mayer", "abstract": "In this tutorial, we present a compact and holistic discussion of Deep\nLearning with a focus on Convolutional Neural Networks (CNNs) and supervised\nregression. While there are numerous books and articles on the individual\ntopics we cover, comprehensive and detailed tutorials that address Deep\nLearning from a foundational yet rigorous and accessible perspective are rare.\nMost resources on CNNs are either too advanced, focusing on cutting-edge\narchitectures, or too narrow, addressing only specific applications like image\nclassification.This tutorial not only summarizes the most relevant concepts but\nalso provides an in-depth exploration of each, offering a complete yet agile\nset of ideas. Moreover, we highlight the powerful synergy between learning\ntheory, statistic, and machine learning, which together underpin the Deep\nLearning and CNN frameworks. We aim for this tutorial to serve as an optimal\nresource for students, professors, and anyone interested in understanding the\nfoundations of Deep Learning. Upon acceptance we will provide an accompanying\nrepository under\n\\href{https://github.com/neoglez/deep-learning-tutorial}{https://github.com/neoglez/deep-learning-tutorial}\n  Keywords: Tutorial, Deep Learning, Convolutional Neural Networks, Machine\nLearning.", "journal": ""}
{"doi": "10.48550/arXiv.1701.04503", "date": "2017-01-17", "title": "Deep Learning for Computational Chemistry", "authors": "Garrett B. Goh, Nathan O. Hodas, Abhinav Vishnu", "abstract": "The rise and fall of artificial neural networks is well documented in the\nscientific literature of both computer science and computational chemistry. Yet\nalmost two decades later, we are now seeing a resurgence of interest in deep\nlearning, a machine learning algorithm based on multilayer neural networks.\nWithin the last few years, we have seen the transformative impact of deep\nlearning in many domains, particularly in speech recognition and computer\nvision, to the extent that the majority of expert practitioners in those field\nare now regularly eschewing prior established models in favor of deep learning\nmodels. In this review, we provide an introductory overview into the theory of\ndeep neural networks and their unique properties that distinguish them from\ntraditional machine learning algorithms used in cheminformatics. By providing\nan overview of the variety of emerging applications of deep neural networks, we\nhighlight its ubiquity and broad applicability to a wide range of challenges in\nthe field, including QSAR, virtual screening, protein structure prediction,\nquantum chemistry, materials design and property prediction. In reviewing the\nperformance of deep neural networks, we observed a consistent outperformance\nagainst non-neural networks state-of-the-art models across disparate research\ntopics, and deep neural network based models often exceeded the \"glass ceiling\"\nexpectations of their respective tasks. Coupled with the maturity of\nGPU-accelerated computing for training deep neural networks and the exponential\ngrowth of chemical data on which to train these networks on, we anticipate that\ndeep learning algorithms will be a valuable tool for computational chemistry.", "journal": ""}
{"doi": "10.48550/arXiv.1901.02452", "date": "2019-01-08", "title": "Face Recognition System", "authors": "Yang Li, Sangwhan Cha", "abstract": "Deep learning is one of the new and important branches in machine learning.\nDeep learning refers to a set of algorithms that solve various problems such as\nimages and texts by using various machine learning algorithms in multi-layer\nneural networks. Deep learning can be classified as a neural network from the\ngeneral category, but there are many changes in the concrete realization. At\nthe core of deep learning is feature learning, which is designed to obtain\nhierarchical information through hierarchical networks, so as to solve the\nimportant problems that previously required artificial design features. Deep\nLearning is a framework that contains several important algorithms. For\ndifferent applications (images, voice, text), you need to use different network\nmodels to achieve better results. With the development of deep learning and the\nintroduction of deep convolutional neural networks, the accuracy and speed of\nface recognition have made great strides. However, as we said above, the\nresults from different networks and models are very different. In this paper,\nfacial features are extracted by merging and comparing multiple models, and\nthen a deep neural network is constructed to train and construct the combined\nfeatures. In this way, the advantages of multiple models can be combined to\nmention the recognition accuracy. After getting a model with high accuracy, we\nbuild a product model. This article compares the pure-client model with the\nserver-client model, analyzes the pros and cons of the two models, and analyzes\nthe various commercial products that are required for the server-client model.", "journal": ""}
{"doi": "10.48550/arXiv.1809.09060", "date": "2018-09-24", "title": "Deep Confidence: A Computationally Efficient Framework for Calculating Reliable Errors for Deep Neural Networks", "authors": "Isidro Cortes-Ciriano, Andreas Bender", "abstract": "Deep learning architectures have proved versatile in a number of drug\ndiscovery applications, including the modelling of in vitro compound activity.\nWhile controlling for prediction confidence is essential to increase the trust,\ninterpretability and usefulness of virtual screening models in drug discovery,\ntechniques to estimate the reliability of the predictions generated with deep\nlearning networks remain largely underexplored. Here, we present Deep\nConfidence, a framework to compute valid and efficient confidence intervals for\nindividual predictions using the deep learning technique Snapshot Ensembling\nand conformal prediction. Specifically, Deep Confidence generates an ensemble\nof deep neural networks by recording the network parameters throughout the\nlocal minima visited during the optimization phase of a single neural network.\nThis approach serves to derive a set of base learners (i.e., snapshots) with\ncomparable predictive power on average, that will however generate slightly\ndifferent predictions for a given instance. The variability across base\nlearners and the validation residuals are in turn harnessed to compute\nconfidence intervals using the conformal prediction framework. Using a set of\n24 diverse IC50 data sets from ChEMBL 23, we show that Snapshot Ensembles\nperform on par with Random Forest (RF) and ensembles of independently trained\ndeep neural networks. In addition, we find that the confidence regions\npredicted using the Deep Confidence framework span a narrower set of values.\nOverall, Deep Confidence represents a highly versatile error prediction\nframework that can be applied to any deep learning-based application at no\nextra computational cost.", "journal": ""}
{"doi": "10.48550/arXiv.1504.00641", "date": "2015-04-02", "title": "A Probabilistic Theory of Deep Learning", "authors": "Ankit B. Patel, Tan Nguyen, Richard G. Baraniuk", "abstract": "A grand challenge in machine learning is the development of computational\nalgorithms that match or outperform humans in perceptual inference tasks that\nare complicated by nuisance variation. For instance, visual object recognition\ninvolves the unknown object position, orientation, and scale in object\nrecognition while speech recognition involves the unknown voice pronunciation,\npitch, and speed. Recently, a new breed of deep learning algorithms have\nemerged for high-nuisance inference tasks that routinely yield pattern\nrecognition systems with near- or super-human capabilities. But a fundamental\nquestion remains: Why do they work? Intuitions abound, but a coherent framework\nfor understanding, analyzing, and synthesizing deep learning architectures has\nremained elusive. We answer this question by developing a new probabilistic\nframework for deep learning based on the Deep Rendering Model: a generative\nprobabilistic model that explicitly captures latent nuisance variation. By\nrelaxing the generative model to a discriminative one, we can recover two of\nthe current leading deep learning systems, deep convolutional neural networks\nand random decision forests, providing insights into their successes and\nshortcomings, as well as a principled route to their improvement.", "journal": ""}
{"doi": "10.48550/arXiv.1909.04791", "date": "2019-09-10", "title": "A Survey of Techniques All Classifiers Can Learn from Deep Networks: Models, Optimizations, and Regularization", "authors": "Alireza Ghods, Diane J Cook", "abstract": "Deep neural networks have introduced novel and useful tools to the machine\nlearning community. Other types of classifiers can potentially make use of\nthese tools as well to improve their performance and generality. This paper\nreviews the current state of the art for deep learning classifier technologies\nthat are being used outside of deep neural networks. Non-network classifiers\ncan employ many components found in deep neural network architectures. In this\npaper, we review the feature learning, optimization, and regularization methods\nthat form a core of deep network technologies. We then survey non-neural\nnetwork learning algorithms that make innovative use of these methods to\nimprove classification. Because many opportunities and challenges still exist,\nwe discuss directions that can be pursued to expand the area of deep learning\nfor a variety of classification algorithms.", "journal": ""}
{"doi": "10.48550/arXiv.1909.10473", "date": "2019-09-23", "title": "Hydrocephalus verification on brain magnetic resonance images with deep convolutional neural networks and \"transfer learning\" technique", "authors": "Alexey Demyanchuk, Ekaterina Pushkina, Nikolay Russkikh, Dmitry Shtokalo, Sergey Mishinov", "abstract": "The hydrocephalus can be either an independent disease or a concomitant\nsymptom of a number of pathologies, therefore representing an urgent issue in\nthe present-day clinical practice. Deep Learning is an evolving technology and\nthe part of a broader field of Machine Learning. Deep learning is currently\nactively researched in the field of radiology. The aim of this study was to\nevaluate deep learning applicability to the diagnostics of hydrocephalus with\nthe use of MRI images. We retrospectively collected, annotated, and\npreprocessed the brain MRI data of 200 patients with and without radiological\nsigns of hydrocephalus. We applied a state-of-the-art deep convolutional neural\nnetwork in conjunction with transfer learning method to train a hydrocephalus\nclassifier model. Using deep convolutional neural networks, we achieved a high\nquality of machine learning model. Accuracy, sensitivity, and specificity of\nhydrocephalus signs identification was 97%, 98%, and 96% respectively. In this\nstudy, we demonstrated the capacity of deep neural networks to identify\nhydrocephalus syndrome using brain MRI images. Applying transfer learning\ntechnique, the high quality of classification was achieved although trained on\nrather limited data.", "journal": ""}
{"doi": "10.48550/arXiv.1604.01252", "date": "2016-04-05", "title": "Comparative Deep Learning of Hybrid Representations for Image Recommendations", "authors": "Chenyi Lei, Dong Liu, Weiping Li, Zheng-Jun Zha, Houqiang Li", "abstract": "In many image-related tasks, learning expressive and discriminative\nrepresentations of images is essential, and deep learning has been studied for\nautomating the learning of such representations. Some user-centric tasks, such\nas image recommendations, call for effective representations of not only images\nbut also preferences and intents of users over images. Such representations are\ntermed \\emph{hybrid} and addressed via a deep learning approach in this paper.\nWe design a dual-net deep network, in which the two sub-networks map input\nimages and preferences of users into a same latent semantic space, and then the\ndistances between images and users in the latent space are calculated to make\ndecisions. We further propose a comparative deep learning (CDL) method to train\nthe deep network, using a pair of images compared against one user to learn the\npattern of their relative distances. The CDL embraces much more training data\nthan naive deep learning, and thus achieves superior performance than the\nlatter, with no cost of increasing network complexity. Experimental results\nwith real-world data sets for image recommendations have shown the proposed\ndual-net network and CDL greatly outperform other state-of-the-art image\nrecommendation solutions.", "journal": ""}
{"doi": "10.48550/arXiv.2201.02490", "date": "2022-01-07", "title": "Audio representations for deep learning in sound synthesis: A review", "authors": "Anastasia Natsiou, Sean O'Leary", "abstract": "The rise of deep learning algorithms has led many researchers to withdraw\nfrom using classic signal processing methods for sound generation. Deep\nlearning models have achieved expressive voice synthesis, realistic sound\ntextures, and musical notes from virtual instruments. However, the most\nsuitable deep learning architecture is still under investigation. The choice of\narchitecture is tightly coupled to the audio representations. A sound's\noriginal waveform can be too dense and rich for deep learning models to deal\nwith efficiently - and complexity increases training time and computational\ncost. Also, it does not represent sound in the manner in which it is perceived.\nTherefore, in many cases, the raw audio has been transformed into a compressed\nand more meaningful form using upsampling, feature-extraction, or even by\nadopting a higher level illustration of the waveform. Furthermore, conditional\non the form chosen, additional conditioning representations, different model\narchitectures, and numerous metrics for evaluating the reconstructed sound have\nbeen investigated. This paper provides an overview of audio representations\napplied to sound synthesis using deep learning. Additionally, it presents the\nmost significant methods for developing and evaluating a sound synthesis\narchitecture using deep learning models, always depending on the audio\nrepresentation.", "journal": ""}
{"doi": "10.48550/arXiv.1912.07568", "date": "2019-12-11", "title": "Simultaneous Detection of Multiple Appliances from Smart-meter Measurements via Multi-Label Consistent Deep Dictionary Learning and Deep Transform Learning", "authors": "Vanika Singhal, Jyoti Maggu, Angshul Majumdar", "abstract": "Currently there are several well-known approaches to non-intrusive appliance\nload monitoring rule based, stochastic finite state machines, neural networks\nand sparse coding. Recently several studies have proposed a new approach based\non multi label classification. Different appliances are treated as separate\nclasses, and the task is to identify the classes given the aggregate\nsmart-meter reading. Prior studies in this area have used off the shelf\nalgorithms like MLKNN and RAKEL to address this problem. In this work, we\npropose a deep learning based technique. There are hardly any studies in deep\nlearning based multi label classification; two new deep learning techniques to\nsolve the said problem are fundamental contributions of this work. These are\ndeep dictionary learning and deep transform learning. Thorough experimental\nresults on benchmark datasets show marked improvement over existing studies.", "journal": ""}
{"doi": "10.48550/arXiv.2002.04806", "date": "2020-02-12", "title": "The Unreasonable Effectiveness of Deep Learning in Artificial Intelligence", "authors": "Terrence J. Sejnowski", "abstract": "Deep learning networks have been trained to recognize speech, caption\nphotographs and translate text between languages at high levels of performance.\nAlthough applications of deep learning networks to real world problems have\nbecome ubiquitous, our understanding of why they are so effective is lacking.\nThese empirical results should not be possible according to sample complexity\nin statistics and non-convex optimization theory. However, paradoxes in the\ntraining and effectiveness of deep learning networks are being investigated and\ninsights are being found in the geometry of high-dimensional spaces. A\nmathematical theory of deep learning would illuminate how they function, allow\nus to assess the strengths and weaknesses of different network architectures\nand lead to major improvements. Deep learning has provided natural ways for\nhumans to communicate with digital devices and is foundational for building\nartificial general intelligence. Deep learning was inspired by the architecture\nof the cerebral cortex and insights into autonomy and general intelligence may\nbe found in other brain regions that are essential for planning and survival,\nbut major breakthroughs will be needed to achieve these goals.", "journal": "Proceedings of the National Academy of Sciences U.S.A. (2020)\n  https://www.pnas.org/content/early/2020/01/23/1907373117"}
{"doi": "10.48550/arXiv.2011.13974", "date": "2020-11-27", "title": "Trends in deep learning for medical hyperspectral image analysis", "authors": "Uzair Khan, Paheding Sidike, Colin Elkin, Vijay Devabhaktuni", "abstract": "Deep learning algorithms have seen acute growth of interest in their\napplications throughout several fields of interest in the last decade, with\nmedical hyperspectral imaging being a particularly promising domain. So far, to\nthe best of our knowledge, there is no review paper that discusses the\nimplementation of deep learning for medical hyperspectral imaging, which is\nwhat this review paper aims to accomplish by examining publications that\ncurrently utilize deep learning to perform effective analysis of medical\nhyperspectral imagery. This paper discusses deep learning concepts that are\nrelevant and applicable to medical hyperspectral imaging analysis, several of\nwhich have been implemented since the boom in deep learning. This will comprise\nof reviewing the use of deep learning for classification, segmentation, and\ndetection in order to investigate the analysis of medical hyperspectral\nimaging. Lastly, we discuss the current and future challenges pertaining to\nthis discipline and the possible efforts to overcome such trials.", "journal": "in IEEE Access, vol. 9, pp. 79534-79548, 2021"}
{"doi": "10.48550/arXiv.2102.11396", "date": "2021-02-22", "title": "Learning Low-dimensional Manifolds for Scoring of Tissue Microarray Images", "authors": "Donghui Yan, Jian Zou, Zhenpeng Li", "abstract": "Tissue microarray (TMA) images have emerged as an important high-throughput\ntool for cancer study and the validation of biomarkers. Efforts have been\ndedicated to further improve the accuracy of TACOMA, a cutting-edge automatic\nscoring algorithm for TMA images. One major advance is due to deepTacoma, an\nalgorithm that incorporates suitable deep representations of a group nature.\nInspired by the recent advance in semi-supervised learning and deep learning,\nwe propose mfTacoma to learn alternative deep representations in the context of\nTMA image scoring. In particular, mfTacoma learns the low-dimensional\nmanifolds, a common latent structure in high dimensional data. Deep\nrepresentation learning and manifold learning typically requires large data. By\nencoding deep representation of the manifolds as regularizing features,\nmfTacoma effectively leverages the manifold information that is potentially\ncrude due to small data. Our experiments show that deep features by manifolds\noutperforms two alternatives -- deep features by linear manifolds with\nprincipal component analysis or by leveraging the group property.", "journal": ""}
{"doi": "10.48550/arXiv.2103.09750", "date": "2021-03-13", "title": "A Survey of Forex and Stock Price Prediction Using Deep Learning", "authors": "Zexin Hu, Yiqi Zhao, Matloob Khushi", "abstract": "The prediction of stock and foreign exchange (Forex) had always been a hot\nand profitable area of study. Deep learning application had proven to yields\nbetter accuracy and return in the field of financial prediction and\nforecasting. In this survey we selected papers from the DBLP database for\ncomparison and analysis. We classified papers according to different deep\nlearning methods, which included: Convolutional neural network (CNN), Long\nShort-Term Memory (LSTM), Deep neural network (DNN), Recurrent Neural Network\n(RNN), Reinforcement Learning, and other deep learning methods such as HAN,\nNLP, and Wavenet. Furthermore, this paper reviewed the dataset, variable,\nmodel, and results of each article. The survey presented the results through\nthe most used performance metrics: RMSE, MAPE, MAE, MSE, accuracy, Sharpe\nratio, and return rate. We identified that recent models that combined LSTM\nwith other methods, for example, DNN, are widely researched. Reinforcement\nlearning and other deep learning method yielded great returns and performances.\nWe conclude that in recent years the trend of using deep-learning based method\nfor financial modeling is exponentially rising.", "journal": "Appl. Syst. Innov. 2021, 4, 9"}
{"doi": "10.48550/arXiv.2110.00653", "date": "2021-10-01", "title": "Sparse Deep Learning: A New Framework Immune to Local Traps and Miscalibration", "authors": "Yan Sun, Wenjun Xiong, Faming Liang", "abstract": "Deep learning has powered recent successes of artificial intelligence (AI).\nHowever, the deep neural network, as the basic model of deep learning, has\nsuffered from issues such as local traps and miscalibration. In this paper, we\nprovide a new framework for sparse deep learning, which has the above issues\naddressed in a coherent way. In particular, we lay down a theoretical\nfoundation for sparse deep learning and propose prior annealing algorithms for\nlearning sparse neural networks. The former has successfully tamed the sparse\ndeep neural network into the framework of statistical modeling, enabling\nprediction uncertainty correctly quantified. The latter can be asymptotically\nguaranteed to converge to the global optimum, enabling the validity of the\ndown-stream statistical inference. Numerical result indicates the superiority\nof the proposed method compared to the existing ones.", "journal": ""}
{"doi": "10.48550/arXiv.2112.15131", "date": "2021-12-30", "title": "Resource-Efficient Deep Learning: A Survey on Model-, Arithmetic-, and Implementation-Level Techniques", "authors": "JunKyu Lee, Lev Mukhanov, Amir Sabbagh Molahosseini, Umar Minhas, Yang Hua, Jesus Martinez del Rincon, Kiril Dichev, Cheol-Ho Hong, Hans Vandierendonck", "abstract": "Deep learning is pervasive in our daily life, including self-driving cars,\nvirtual assistants, social network services, healthcare services, face\nrecognition, etc. However, deep neural networks demand substantial compute\nresources during training and inference. The machine learning community has\nmainly focused on model-level optimizations such as architectural compression\nof deep learning models, while the system community has focused on\nimplementation-level optimization. In between, various arithmetic-level\noptimization techniques have been proposed in the arithmetic community. This\narticle provides a survey on resource-efficient deep learning techniques in\nterms of model-, arithmetic-, and implementation-level techniques and\nidentifies the research gaps for resource-efficient deep learning techniques\nacross the three different level techniques. Our survey clarifies the influence\nfrom higher to lower-level techniques based on our resource-efficiency metric\ndefinition and discusses the future trend for resource-efficient deep learning\nresearch.", "journal": ""}
{"doi": "10.48550/arXiv.2203.08174", "date": "2022-03-15", "title": "Towards understanding deep learning with the natural clustering prior", "authors": "Simon Carbonnelle", "abstract": "The prior knowledge (a.k.a. priors) integrated into the design of a machine\nlearning system strongly influences its generalization abilities. In the\nspecific context of deep learning, some of these priors are poorly understood\nas they implicitly emerge from the successful heuristics and tentative\napproximations of biological brains involved in deep learning design. Through\nthe lens of supervised image classification problems, this thesis investigates\nthe implicit integration of a natural clustering prior composed of three\nstatements: (i) natural images exhibit a rich clustered structure, (ii) image\nclasses are composed of multiple clusters and (iii) each cluster contains\nexamples from a single class. The decomposition of classes into multiple\nclusters implies that supervised deep learning systems could benefit from\nunsupervised clustering to define appropriate decision boundaries. Hence, this\nthesis attempts to identify implicit clustering abilities, mechanisms and\nhyperparameters in deep learning systems and evaluate their relevance for\nexplaining the generalization abilities of these systems. We do so through an\nextensive empirical study of the training dynamics as well as the neuron- and\nlayer-level representations of deep neural networks. The resulting collection\nof experiments provides preliminary evidence for the relevance of the natural\nclustering prior for understanding deep learning.", "journal": ""}
{"doi": "10.48550/arXiv.2204.01942", "date": "2022-04-05", "title": "Fault-Tolerant Deep Learning: A Hierarchical Perspective", "authors": "Cheng Liu, Zhen Gao, Siting Liu, Xuefei Ning, Huawei Li, Xiaowei Li", "abstract": "With the rapid advancements of deep learning in the past decade, it can be\nforeseen that deep learning will be continuously deployed in more and more\nsafety-critical applications such as autonomous driving and robotics. In this\ncontext, reliability turns out to be critical to the deployment of deep\nlearning in these applications and gradually becomes a first-class citizen\namong the major design metrics like performance and energy efficiency.\nNevertheless, the back-box deep learning models combined with the diverse\nunderlying hardware faults make resilient deep learning extremely challenging.\nIn this special session, we conduct a comprehensive survey of fault-tolerant\ndeep learning design approaches with a hierarchical perspective and investigate\nthese approaches from model layer, architecture layer, circuit layer, and cross\nlayer respectively.", "journal": ""}
{"doi": "10.48550/arXiv.2207.08137", "date": "2022-07-17", "title": "Achieve Optimal Adversarial Accuracy for Adversarial Deep Learning using Stackelberg Game", "authors": "Xiao-Shan Gao, Shuang Liu, Lijia Yu", "abstract": "Adversarial deep learning is to train robust DNNs against adversarial\nattacks, which is one of the major research focuses of deep learning. Game\ntheory has been used to answer some of the basic questions about adversarial\ndeep learning such as the existence of a classifier with optimal robustness and\nthe existence of optimal adversarial samples for a given class of classifiers.\nIn most previous work, adversarial deep learning was formulated as a\nsimultaneous game and the strategy spaces are assumed to be certain probability\ndistributions in order for the Nash equilibrium to exist. But, this assumption\nis not applicable to the practical situation. In this paper, we give answers to\nthese basic questions for the practical case where the classifiers are DNNs\nwith a given structure, by formulating the adversarial deep learning as\nsequential games. The existence of Stackelberg equilibria for these games are\nproved. Furthermore, it is shown that the equilibrium DNN has the largest\nadversarial accuracy among all DNNs with the same structure, when\nCarlini-Wagner's margin loss is used. Trade-off between robustness and accuracy\nin adversarial deep learning is also studied from game theoretical aspect.", "journal": ""}
{"doi": "10.48550/arXiv.2211.15926", "date": "2022-11-29", "title": "Interpretations Cannot Be Trusted: Stealthy and Effective Adversarial Perturbations against Interpretable Deep Learning", "authors": "Eldor Abdukhamidov, Mohammed Abuhamad, Simon S. Woo, Eric Chan-Tin, Tamer Abuhmed", "abstract": "Deep learning methods have gained increased attention in various applications\ndue to their outstanding performance. For exploring how this high performance\nrelates to the proper use of data artifacts and the accurate problem\nformulation of a given task, interpretation models have become a crucial\ncomponent in developing deep learning-based systems. Interpretation models\nenable the understanding of the inner workings of deep learning models and\noffer a sense of security in detecting the misuse of artifacts in the input\ndata. Similar to prediction models, interpretation models are also susceptible\nto adversarial inputs. This work introduces two attacks, AdvEdge and\nAdvEdge$^{+}$, that deceive both the target deep learning model and the coupled\ninterpretation model. We assess the effectiveness of proposed attacks against\ntwo deep learning model architectures coupled with four interpretation models\nthat represent different categories of interpretation models. Our experiments\ninclude the attack implementation using various attack frameworks. We also\nexplore the potential countermeasures against such attacks. Our analysis shows\nthe effectiveness of our attacks in terms of deceiving the deep learning models\nand their interpreters, and highlights insights to improve and circumvent the\nattacks.", "journal": ""}
{"doi": "10.48550/arXiv.2309.15421", "date": "2023-09-27", "title": "Deep Learning in Deterministic Computational Mechanics", "authors": "Leon Herrmann, Stefan Kollmannsberger", "abstract": "The rapid growth of deep learning research, including within the field of\ncomputational mechanics, has resulted in an extensive and diverse body of\nliterature. To help researchers identify key concepts and promising\nmethodologies within this field, we provide an overview of deep learning in\ndeterministic computational mechanics. Five main categories are identified and\nexplored: simulation substitution, simulation enhancement, discretizations as\nneural networks, generative approaches, and deep reinforcement learning. This\nreview focuses on deep learning methods rather than applications for\ncomputational mechanics, thereby enabling researchers to explore this field\nmore effectively. As such, the review is not necessarily aimed at researchers\nwith extensive knowledge of deep learning -- instead, the primary audience is\nresearchers at the verge of entering this field or those who attempt to gain an\noverview of deep learning in computational mechanics. The discussed concepts\nare, therefore, explained as simple as possible.", "journal": ""}
{"doi": "10.48550/arXiv.2310.10550", "date": "2023-10-16", "title": "Deep learning applied to EEG data with different montages using spatial attention", "authors": "Dung Truong, Muhammad Abdullah Khalid, Arnaud Delorme", "abstract": "The ability of Deep Learning to process and extract relevant information in\ncomplex brain dynamics from raw EEG data has been demonstrated in various\nrecent works. Deep learning models, however, have also been shown to perform\nbest on large corpora of data. When processing EEG, a natural approach is to\ncombine EEG datasets from different experiments to train large deep-learning\nmodels. However, most EEG experiments use custom channel montages, requiring\nthe data to be transformed into a common space. Previous methods have used the\nraw EEG signal to extract features of interest and focused on using a common\nfeature space across EEG datasets. While this is a sensible approach, it\nunderexploits the potential richness of EEG raw data. Here, we explore using\nspatial attention applied to EEG electrode coordinates to perform channel\nharmonization of raw EEG data, allowing us to train deep learning on EEG data\nusing different montages. We test this model on a gender classification task.\nWe first show that spatial attention increases model performance. Then, we show\nthat a deep learning model trained on data using different channel montages\nperforms significantly better than deep learning models trained on fixed 23-\nand 128-channel data montages.", "journal": ""}
{"doi": "10.48550/arXiv.2407.07712", "date": "2024-07-10", "title": "Deep-Graph-Sprints: Accelerated Representation Learning in Continuous-Time Dynamic Graphs", "authors": "Ahmad Naser Eddin, Jacopo Bono, David Apar\u00edcio, Hugo Ferreira, Pedro Ribeiro, Pedro Bizarro", "abstract": "Continuous-time dynamic graphs (CTDGs) are essential for modeling\ninterconnected, evolving systems. Traditional methods for extracting knowledge\nfrom these graphs often depend on feature engineering or deep learning. Feature\nengineering is limited by the manual and time-intensive nature of crafting\nfeatures, while deep learning approaches suffer from high inference latency,\nmaking them impractical for real-time applications. This paper introduces\nDeep-Graph-Sprints (DGS), a novel deep learning architecture designed for\nefficient representation learning on CTDGs with low-latency inference\nrequirements. We benchmark DGS against state-of-the-art (SOTA) feature\nengineering and graph neural network methods using five diverse datasets. The\nresults indicate that DGS achieves competitive performance while inference\nspeed improves between 4x and 12x compared to other deep learning approaches on\nour benchmark datasets. Our method effectively bridges the gap between deep\nrepresentation learning and low-latency application requirements for CTDGs.", "journal": ""}
{"doi": "10.48550/arXiv.2408.11720", "date": "2024-08-21", "title": "On Learnable Parameters of Optimal and Suboptimal Deep Learning Models", "authors": "Ziwei Zheng, Huizhi Liang, Vaclav Snasel, Vito Latora, Panos Pardalos, Giuseppe Nicosia, Varun Ojha", "abstract": "We scrutinize the structural and operational aspects of deep learning models,\nparticularly focusing on the nuances of learnable parameters (weight)\nstatistics, distribution, node interaction, and visualization. By establishing\ncorrelations between variance in weight patterns and overall network\nperformance, we investigate the varying (optimal and suboptimal) performances\nof various deep-learning models. Our empirical analysis extends across widely\nrecognized datasets such as MNIST, Fashion-MNIST, and CIFAR-10, and various\ndeep learning models such as deep neural networks (DNNs), convolutional neural\nnetworks (CNNs), and vision transformer (ViT), enabling us to pinpoint\ncharacteristics of learnable parameters that correlate with successful\nnetworks. Through extensive experiments on the diverse architectures of deep\nlearning models, we shed light on the critical factors that influence the\nfunctionality and efficiency of DNNs. Our findings reveal that successful\nnetworks, irrespective of datasets or models, are invariably similar to other\nsuccessful networks in their converged weights statistics and distribution,\nwhile poor-performing networks vary in their weights. In addition, our research\nshows that the learnable parameters of widely varied deep learning models such\nas DNN, CNN, and ViT exhibit similar learning characteristics.", "journal": "31st International Conference on Neural Information Processing\n  (ICONIP) 2024"}
{"doi": "10.48550/arXiv.2505.20235", "date": "2025-05-26", "title": "Variational Deep Learning via Implicit Regularization", "authors": "Jonathan Wenger, Beau Coker, Juraj Marusic, John P. Cunningham", "abstract": "Modern deep learning models generalize remarkably well in-distribution,\ndespite being overparametrized and trained with little to no explicit\nregularization. Instead, current theory credits implicit regularization imposed\nby the choice of architecture, hyperparameters and optimization procedure.\nHowever, deploying deep learning models out-of-distribution, in sequential\ndecision-making tasks, or in safety-critical domains, necessitates reliable\nuncertainty quantification, not just a point estimate. The machinery of modern\napproximate inference -- Bayesian deep learning -- should answer the need for\nuncertainty quantification, but its effectiveness has been challenged by our\ninability to define useful explicit inductive biases through priors, as well as\nthe associated computational burden. Instead, in this work we demonstrate, both\ntheoretically and empirically, how to regularize a variational deep network\nimplicitly via the optimization procedure, just as for standard deep learning.\nWe fully characterize the inductive bias of (stochastic) gradient descent in\nthe case of an overparametrized linear model as generalized variational\ninference and demonstrate the importance of the choice of parametrization.\nFinally, we show empirically that our approach achieves strong in- and\nout-of-distribution performance without tuning of additional hyperparameters\nand with minimal time and memory overhead over standard deep learning.", "journal": ""}
{"doi": "10.48550/arXiv.2107.13614", "date": "2021-07-28", "title": "Clones in Deep Learning Code: What, Where, and Why?", "authors": "Hadhemi Jebnoun, Md Saidur Rahman, Foutse Khomh, Biruk Asmare Muse", "abstract": "Deep Learning applications are becoming increasingly popular. Developers of\ndeep learning systems strive to write more efficient code. Deep learning\nsystems are constantly evolving, imposing tighter development timelines and\nincreasing complexity, which may lead to bad design decisions. A copy-paste\napproach is widely used among deep learning developers because they rely on\ncommon frameworks and duplicate similar tasks. Developers often fail to\nproperly propagate changes to all clones fragments during a maintenance\nactivity. To our knowledge, no study has examined code cloning practices in\ndeep learning development. Given the negative impacts of clones on software\nquality reported in the studies on traditional systems, it is very important to\nunderstand the characteristics and potential impacts of code clones on deep\nlearning systems. To this end, we use the NiCad tool to detect clones from 59\nPython, 14 C# and 6 Java-based deep learning systems and an equal number of\ntraditional software systems. We then analyze the frequency and distribution of\ncode clones in deep learning and traditional systems. We do further analysis of\nthe distribution of code clones using location-based taxonomy. We also study\nthe correlation between bugs and code clones to assess the impacts of clones on\nthe quality of the studied systems. Finally, we introduce a code clone taxonomy\nrelated to deep learning programs and identify the deep learning system\ndevelopment phases in which cloning has the highest risk of faults. Our results\nshow that code cloning is a frequent practice in deep learning systems and that\ndeep learning developers often clone code from files in distant repositories in\nthe system. In addition, we found that code cloning occurs more frequently\nduring DL model construction. And that hyperparameters setting is the phase\nduring which cloning is the riskiest, since it often leads to faults.", "journal": ""}
{"doi": "10.48550/arXiv.1708.00260", "date": "2017-08-01", "title": "Deep Asymmetric Multi-task Feature Learning", "authors": "Hae Beom Lee, Eunho Yang, Sung Ju Hwang", "abstract": "We propose Deep Asymmetric Multitask Feature Learning (Deep-AMTFL) which can\nlearn deep representations shared across multiple tasks while effectively\npreventing negative transfer that may happen in the feature sharing process.\nSpecifically, we introduce an asymmetric autoencoder term that allows reliable\npredictors for the easy tasks to have high contribution to the feature learning\nwhile suppressing the influences of unreliable predictors for more difficult\ntasks. This allows the learning of less noisy representations, and enables\nunreliable predictors to exploit knowledge from the reliable predictors via the\nshared latent features. Such asymmetric knowledge transfer through shared\nfeatures is also more scalable and efficient than inter-task asymmetric\ntransfer. We validate our Deep-AMTFL model on multiple benchmark datasets for\nmultitask learning and image classification, on which it significantly\noutperforms existing symmetric and asymmetric multitask learning models, by\neffectively preventing negative transfer in deep feature learning.", "journal": ""}
{"doi": "10.48550/arXiv.2008.06365", "date": "2020-08-12", "title": "An Overview of Deep Learning Architectures in Few-Shot Learning Domain", "authors": "Shruti Jadon, Aryan Jadon", "abstract": "Since 2012, Deep learning has revolutionized Artificial Intelligence and has\nachieved state-of-the-art outcomes in different domains, ranging from Image\nClassification to Speech Generation. Though it has many potentials, our current\narchitectures come with the pre-requisite of large amounts of data. Few-Shot\nLearning (also known as one-shot learning) is a sub-field of machine learning\nthat aims to create such models that can learn the desired objective with less\ndata, similar to how humans learn. In this paper, we have reviewed some of the\nwell-known deep learning-based approaches towards few-shot learning. We have\ndiscussed the recent achievements, challenges, and possibilities of improvement\nof few-shot learning based deep learning architectures. Our aim for this paper\nis threefold: (i) Give a brief introduction to deep learning architectures for\nfew-shot learning with pointers to core references. (ii) Indicate how deep\nlearning has been applied to the low-data regime, from data preparation to\nmodel training. and, (iii) Provide a starting point for people interested in\nexperimenting and perhaps contributing to the field of few-shot learning by\npointing out some useful resources and open-source code. Our code is available\nat Github: https://github.com/shruti-jadon/Hands-on-One-Shot-Learning.", "journal": ""}
{"doi": "10.48550/arXiv.1404.4661", "date": "2014-04-17", "title": "Learning Fine-grained Image Similarity with Deep Ranking", "authors": "Jiang Wang, Yang song, Thomas Leung, Chuck Rosenberg, Jinbin Wang, James Philbin, Bo Chen, Ying Wu", "abstract": "Learning fine-grained image similarity is a challenging task. It needs to\ncapture between-class and within-class image differences. This paper proposes a\ndeep ranking model that employs deep learning techniques to learn similarity\nmetric directly from images.It has higher learning capability than models based\non hand-crafted features. A novel multiscale network structure has been\ndeveloped to describe the images effectively. An efficient triplet sampling\nalgorithm is proposed to learn the model with distributed asynchronized\nstochastic gradient. Extensive experiments show that the proposed algorithm\noutperforms models based on hand-crafted visual features and deep\nclassification models.", "journal": ""}
{"doi": "10.48550/arXiv.1709.01953", "date": "2017-09-06", "title": "Implicit Regularization in Deep Learning", "authors": "Behnam Neyshabur", "abstract": "In an attempt to better understand generalization in deep learning, we study\nseveral possible explanations. We show that implicit regularization induced by\nthe optimization method is playing a key role in generalization and success of\ndeep learning models. Motivated by this view, we study how different complexity\nmeasures can ensure generalization and explain how optimization algorithms can\nimplicitly regularize complexity measures. We empirically investigate the\nability of these measures to explain different observed phenomena in deep\nlearning. We further study the invariances in neural networks, suggest\ncomplexity measures and optimization algorithms that have similar invariances\nto those in neural networks and evaluate them on a number of learning tasks.", "journal": ""}
{"doi": "10.48550/arXiv.1710.11351", "date": "2017-10-31", "title": "ChainerMN: Scalable Distributed Deep Learning Framework", "authors": "Takuya Akiba, Keisuke Fukuda, Shuji Suzuki", "abstract": "One of the keys for deep learning to have made a breakthrough in various\nfields was to utilize high computing powers centering around GPUs. Enabling the\nuse of further computing abilities by distributed processing is essential not\nonly to make the deep learning bigger and faster but also to tackle unsolved\nchallenges. We present the design, implementation, and evaluation of ChainerMN,\nthe distributed deep learning framework we have developed. We demonstrate that\nChainerMN can scale the learning process of the ResNet-50 model to the ImageNet\ndataset up to 128 GPUs with the parallel efficiency of 90%.", "journal": ""}
{"doi": "10.48550/arXiv.1812.00804", "date": "2018-12-03", "title": "Deep Inverse Optimization", "authors": "Yingcong Tan, Andrew Delong, Daria Terekhov", "abstract": "Given a set of observations generated by an optimization process, the goal of\ninverse optimization is to determine likely parameters of that process. We cast\ninverse optimization as a form of deep learning. Our method, called deep\ninverse optimization, is to unroll an iterative optimization process and then\nuse backpropagation to learn parameters that generate the observations. We\ndemonstrate that by backpropagating through the interior point algorithm we can\nlearn the coefficients determining the cost vector and the constraints,\nindependently or jointly, for both non-parametric and parametric linear\nprograms, starting from one or multiple observations. With this approach,\ninverse optimization can leverage concepts and algorithms from deep learning.", "journal": ""}
{"doi": "10.48550/arXiv.1812.06292", "date": "2018-12-15", "title": "A short review on Applications of Deep learning for Cyber security", "authors": "Mohammed Harun Babu R, Vinayakumar R, Soman KP", "abstract": "Deep learning is an advanced model of traditional machine learning. This has\nthe capability to extract optimal feature representation from raw input\nsamples. This has been applied towards various use cases in cyber security such\nas intrusion detection, malware classification, android malware detection, spam\nand phishing detection and binary analysis. This paper outlines the survey of\nall the works related to deep learning based solutions for various cyber\nsecurity use cases. Keywords: Deep learning, intrusion detection, malware\ndetection, Android malware detection, spam & phishing detection, traffic\nanalysis, binary analysis.", "journal": ""}
{"doi": "10.48550/arXiv.1905.13105", "date": "2019-05-30", "title": "ImJoy: an open-source computational platform for the deep learning era", "authors": "Wei Ouyang, Florian Mueller, Martin Hjelmare, Emma Lundberg, Christophe Zimmer", "abstract": "Deep learning methods have shown extraordinary potential for analyzing very\ndiverse biomedical data, but their dissemination beyond developers is hindered\nby important computational hurdles. We introduce ImJoy (https://imjoy.io/), a\nflexible and open-source browser-based platform designed to facilitate\nwidespread reuse of deep learning solutions in biomedical research. We\nhighlight ImJoy's main features and illustrate its functionalities with deep\nlearning plugins for mobile and interactive image analysis and genomics.", "journal": ""}
{"doi": "10.48550/arXiv.1904.06049", "date": "2019-04-12", "title": "Distributed Layer-Partitioned Training for Privacy-Preserved Deep Learning", "authors": "Chun-Hsien Yu, Chun-Nan Chou, Emily Chang", "abstract": "Deep Learning techniques have achieved remarkable results in many domains.\nOften, training deep learning models requires large datasets, which may require\nsensitive information to be uploaded to the cloud to accelerate training. To\nadequately protect sensitive information, we propose distributed\nlayer-partitioned training with step-wise activation functions for\nprivacy-preserving deep learning. Experimental results attest our method to be\nsimple and effective.", "journal": ""}
{"doi": "10.48550/arXiv.2202.07201", "date": "2022-02-15", "title": "Holistic Adversarial Robustness of Deep Learning Models", "authors": "Pin-Yu Chen, Sijia Liu", "abstract": "Adversarial robustness studies the worst-case performance of a machine\nlearning model to ensure safety and reliability. With the proliferation of\ndeep-learning-based technology, the potential risks associated with model\ndevelopment and deployment can be amplified and become dreadful\nvulnerabilities. This paper provides a comprehensive overview of research\ntopics and foundational principles of research methods for adversarial\nrobustness of deep learning models, including attacks, defenses, verification,\nand novel applications.", "journal": ""}
{"doi": "10.48550/arXiv.2004.13408", "date": "2020-04-28", "title": "Time Series Forecasting With Deep Learning: A Survey", "authors": "Bryan Lim, Stefan Zohren", "abstract": "Numerous deep learning architectures have been developed to accommodate the\ndiversity of time series datasets across different domains. In this article, we\nsurvey common encoder and decoder designs used in both one-step-ahead and\nmulti-horizon time series forecasting -- describing how temporal information is\nincorporated into predictions by each model. Next, we highlight recent\ndevelopments in hybrid deep learning models, which combine well-studied\nstatistical models with neural network components to improve pure methods in\neither category. Lastly, we outline some ways in which deep learning can also\nfacilitate decision support with time series data.", "journal": "Philosophical Transactions of the Royal Society A 2020"}
{"doi": "10.48550/arXiv.2109.04081", "date": "2021-09-09", "title": "DeepEMO: Deep Learning for Speech Emotion Recognition", "authors": "Enkhtogtokh Togootogtokh, Christian Klasen", "abstract": "We proposed the industry level deep learning approach for speech emotion\nrecognition task. In industry, carefully proposed deep transfer learning\ntechnology shows real results due to mostly low amount of training data\navailability, machine training cost, and specialized learning on dedicated AI\ntasks. The proposed speech recognition framework, called DeepEMO, consists of\ntwo main pipelines such that preprocessing to extract efficient main features\nand deep transfer learning model to train and recognize. Main source code is in\nhttps://github.com/enkhtogtokh/deepemo repository", "journal": ""}
{"doi": "10.48550/arXiv.2210.11250", "date": "2022-10-19", "title": "Structure-based drug design with geometric deep learning", "authors": "Clemens Isert, Kenneth Atz, Gisbert Schneider", "abstract": "Structure-based drug design uses three-dimensional geometric information of\nmacromolecules, such as proteins or nucleic acids, to identify suitable\nligands. Geometric deep learning, an emerging concept of neural-network-based\nmachine learning, has been applied to macromolecular structures. This review\nprovides an overview of the recent applications of geometric deep learning in\nbioorganic and medicinal chemistry, highlighting its potential for\nstructure-based drug discovery and design. Emphasis is placed on molecular\nproperty prediction, ligand binding site and pose prediction, and\nstructure-based de novo molecular design. The current challenges and\nopportunities are highlighted, and a forecast of the future of geometric deep\nlearning for drug discovery is presented.", "journal": ""}
{"doi": "10.48550/arXiv.2406.08686", "date": "2024-06-12", "title": "Opportunities in deep learning methods development for computational biology", "authors": "Alex Jihun Lee, Reza Abbasi-Asl", "abstract": "Advances in molecular technologies underlie an enormous growth in the size of\ndata sets pertaining to biology and biomedicine. These advances parallel those\nin the deep learning subfield of machine learning. Components in the\ndifferentiable programming toolbox that makes deep learning possible are\nallowing computer scientists to address an increasingly large array of problems\nwith flexible and effective tools. However many of these tools have not fully\nproliferated into the computational biology and bioinformatics fields. In this\nperspective we survey some of these advances and highlight exemplary examples\nof their utilization in the biosciences, with the goal of increasing awareness\namong practitioners of emerging opportunities to blend expert knowledge with\nnewly emerging deep learning architectural tools.", "journal": ""}
{"doi": "10.48550/arXiv.1611.05244", "date": "2016-11-16", "title": "Deep Transfer Learning for Person Re-identification", "authors": "Mengyue Geng, Yaowei Wang, Tao Xiang, Yonghong Tian", "abstract": "Person re-identification (Re-ID) poses a unique challenge to deep learning:\nhow to learn a deep model with millions of parameters on a small training set\nof few or no labels. In this paper, a number of deep transfer learning models\nare proposed to address the data sparsity problem. First, a deep network\narchitecture is designed which differs from existing deep Re-ID models in that\n(a) it is more suitable for transferring representations learned from large\nimage classification datasets, and (b) classification loss and verification\nloss are combined, each of which adopts a different dropout strategy. Second, a\ntwo-stepped fine-tuning strategy is developed to transfer knowledge from\nauxiliary datasets. Third, given an unlabelled Re-ID dataset, a novel\nunsupervised deep transfer learning model is developed based on co-training.\nThe proposed models outperform the state-of-the-art deep Re-ID models by large\nmargins: we achieve Rank-1 accuracy of 85.4\\%, 83.7\\% and 56.3\\% on CUHK03,\nMarket1501, and VIPeR respectively, whilst on VIPeR, our unsupervised model\n(45.1\\%) beats most supervised models.", "journal": ""}
{"doi": "10.48550/arXiv.1705.07366", "date": "2017-05-20", "title": "Forward Thinking: Building Deep Random Forests", "authors": "Kevin Miller, Chris Hettinger, Jeffrey Humpherys, Tyler Jarvis, David Kartchner", "abstract": "The success of deep neural networks has inspired many to wonder whether other\nlearners could benefit from deep, layered architectures. We present a general\nframework called forward thinking for deep learning that generalizes the\narchitectural flexibility and sophistication of deep neural networks while also\nallowing for (i) different types of learning functions in the network, other\nthan neurons, and (ii) the ability to adaptively deepen the network as needed\nto improve results. This is done by training one layer at a time, and once a\nlayer is trained, the input data are mapped forward through the layer to create\na new learning problem. The process is then repeated, transforming the data\nthrough multiple layers, one at a time, rendering a new dataset, which is\nexpected to be better behaved, and on which a final output layer can achieve\ngood performance. In the case where the neurons of deep neural nets are\nreplaced with decision trees, we call the result a Forward Thinking Deep Random\nForest (FTDRF). We demonstrate a proof of concept by applying FTDRF on the\nMNIST dataset. We also provide a general mathematical formulation that allows\nfor other types of deep learning problems to be considered.", "journal": ""}
{"doi": "10.48550/arXiv.1805.10769", "date": "2018-05-28", "title": "Universality of Deep Convolutional Neural Networks", "authors": "Ding-Xuan Zhou", "abstract": "Deep learning has been widely applied and brought breakthroughs in speech\nrecognition, computer vision, and many other domains. The involved deep neural\nnetwork architectures and computational issues have been well studied in\nmachine learning. But there lacks a theoretical foundation for understanding\nthe approximation or generalization ability of deep learning methods generated\nby the network architectures such as deep convolutional neural networks having\nconvolutional structures. Here we show that a deep convolutional neural network\n(CNN) is universal, meaning that it can be used to approximate any continuous\nfunction to an arbitrary accuracy when the depth of the neural network is large\nenough. This answers an open question in learning theory. Our quantitative\nestimate, given tightly in terms of the number of free parameters to be\ncomputed, verifies the efficiency of deep CNNs in dealing with large\ndimensional data. Our study also demonstrates the role of convolutions in deep\nCNNs.", "journal": ""}
{"doi": "10.48550/arXiv.2012.15754", "date": "2020-12-22", "title": "Limitations of Deep Neural Networks: a discussion of G. Marcus' critical appraisal of deep learning", "authors": "Stefanos Tsimenidis", "abstract": "Deep neural networks have triggered a revolution in artificial intelligence,\nhaving been applied with great results in medical imaging, semi-autonomous\nvehicles, ecommerce, genetics research, speech recognition, particle physics,\nexperimental art, economic forecasting, environmental science, industrial\nmanufacturing, and a wide variety of applications in nearly every field. This\nsudden success, though, may have intoxicated the research community and blinded\nthem to the potential pitfalls of assigning deep learning a higher status than\nwarranted. Also, research directed at alleviating the weaknesses of deep\nlearning may seem less attractive to scientists and engineers, who focus on the\nlow-hanging fruit of finding more and more applications for deep learning\nmodels, thus letting short-term benefits hamper long-term scientific progress.\nGary Marcus wrote a paper entitled Deep Learning: A Critical Appraisal, and\nhere we discuss Marcus' core ideas, as well as attempt a general assessment of\nthe subject. This study examines some of the limitations of deep neural\nnetworks, with the intention of pointing towards potential paths for future\nresearch, and of clearing up some metaphysical misconceptions, held by numerous\nresearchers, that may misdirect them.", "journal": ""}
{"doi": "10.48550/arXiv.2402.17020", "date": "2024-02-26", "title": "Deep Learning Algorithms Used in Intrusion Detection Systems -- A Review", "authors": "Richard Kimanzi, Peter Kimanga, Dedan Cherori, Patrick K. Gikunda", "abstract": "The increase in network attacks has necessitated the development of robust\nand efficient intrusion detection systems (IDS) capable of identifying\nmalicious activities in real-time. In the last five years, deep learning\nalgorithms have emerged as powerful tools in this domain, offering enhanced\ndetection capabilities compared to traditional methods. This review paper\nstudies recent advancements in the application of deep learning techniques,\nincluding Convolutional Neural Networks (CNN), Recurrent Neural Networks (RNN),\nDeep Belief Networks (DBN), Deep Neural Networks (DNN), Long Short-Term Memory\n(LSTM), autoencoders (AE), Multi-Layer Perceptrons (MLP), Self-Normalizing\nNetworks (SNN) and hybrid models, within network intrusion detection systems.\nwe delve into the unique architectures, training models, and classification\nmethodologies tailored for network traffic analysis and anomaly detection.\nFurthermore, we analyze the strengths and limitations of each deep learning\napproach in terms of detection accuracy, computational efficiency, scalability,\nand adaptability to evolving threats. Additionally, this paper highlights\nprominent datasets and benchmarking frameworks commonly utilized for evaluating\nthe performance of deep learning-based IDS. This review will provide\nresearchers and industry practitioners with valuable insights into the\nstate-of-the-art deep learning algorithms for enhancing the security framework\nof network environments through intrusion detection.", "journal": ""}
{"doi": "10.48550/arXiv.1811.07211", "date": "2018-11-17", "title": "Classifiers Based on Deep Sparse Coding Architectures are Robust to Deep Learning Transferable Examples", "authors": "Jacob M. Springer, Charles S. Strauss, Austin M. Thresher, Edward Kim, Garrett T. Kenyon", "abstract": "Although deep learning has shown great success in recent years, researchers\nhave discovered a critical flaw where small, imperceptible changes in the input\nto the system can drastically change the output classification. These attacks\nare exploitable in nearly all of the existing deep learning classification\nframeworks. However, the susceptibility of deep sparse coding models to\nadversarial examples has not been examined. Here, we show that classifiers\nbased on a deep sparse coding model whose classification accuracy is\ncompetitive with a variety of deep neural network models are robust to\nadversarial examples that effectively fool those same deep learning models. We\ndemonstrate both quantitatively and qualitatively that the robustness of deep\nsparse coding models to adversarial examples arises from two key properties.\nFirst, because deep sparse coding models learn general features corresponding\nto generators of the dataset as a whole, rather than highly discriminative\nfeatures for distinguishing specific classes, the resulting classifiers are\nless dependent on idiosyncratic features that might be more easily exploited.\nSecond, because deep sparse coding models utilize fixed point attractor\ndynamics with top-down feedback, it is more difficult to find small changes to\nthe input that drive the resulting representations out of the correct attractor\nbasin.", "journal": ""}
{"doi": "10.48550/arXiv.1706.01983", "date": "2017-06-06", "title": "Deep Learning: Generalization Requires Deep Compositional Feature Space Design", "authors": "Mrinal Haloi", "abstract": "Generalization error defines the discriminability and the representation\npower of a deep model. In this work, we claim that feature space design using\ndeep compositional function plays a significant role in generalization along\nwith explicit and implicit regularizations. Our claims are being established\nwith several image classification experiments. We show that the information\nloss due to convolution and max pooling can be marginalized with the\ncompositional design, improving generalization performance. Also, we will show\nthat learning rate decay acts as an implicit regularizer in deep model\ntraining.", "journal": ""}
{"doi": "10.48550/arXiv.1707.07217", "date": "2017-07-22", "title": "Deep Learning in Robotics: A Review of Recent Research", "authors": "Harry A. Pierson, Michael S. Gashler", "abstract": "Advances in deep learning over the last decade have led to a flurry of\nresearch in the application of deep artificial neural networks to robotic\nsystems, with at least thirty papers published on the subject between 2014 and\nthe present. This review discusses the applications, benefits, and limitations\nof deep learning vis-\\`a-vis physical robotic systems, using contemporary\nresearch as exemplars. It is intended to communicate recent advances to the\nwider robotics community and inspire additional interest in and application of\ndeep learning in robotics.", "journal": ""}
{"doi": "10.48550/arXiv.1912.09261", "date": "2019-12-19", "title": "Practical applicability of deep neural networks for overlapping speaker separation", "authors": "Pieter Appeltans, Jeroen Zegers, Hugo Van hamme", "abstract": "This paper examines the applicability in realistic scenarios of two deep\nlearning based solutions to the overlapping speaker separation problem.\nFirstly, we present experiments that show that these methods are applicable for\na broad range of languages. Further experimentation indicates limited\nperformance loss for untrained languages, when these have common features with\nthe trained language(s). Secondly, it investigates how the methods deal with\nrealistic background noise and proposes some modifications to better cope with\nthese disturbances. The deep learning methods that will be examined are deep\nclustering and deep attractor networks.", "journal": ""}
{"doi": "10.48550/arXiv.2008.01641", "date": "2020-08-04", "title": "Exploring Variational Deep Q Networks", "authors": "A. H. Bell-Thomas", "abstract": "This study provides both analysis and a refined, research-ready\nimplementation of Tang and Kucukelbir's Variational Deep Q Network, a novel\napproach to maximising the efficiency of exploration in complex learning\nenvironments using Variational Bayesian Inference. Alongside reference\nimplementations of both Traditional and Double Deep Q Networks, a small novel\ncontribution is presented - the Double Variational Deep Q Network, which\nincorporates improvements to increase the stability and robustness of\ninference-based learning. Finally, an evaluation and discussion of the\neffectiveness of these approaches is discussed in the wider context of Bayesian\nDeep Learning.", "journal": ""}
{"doi": "10.48550/arXiv.2101.05612", "date": "2021-01-12", "title": "A SOM-based Gradient-Free Deep Learning Method with Convergence Analysis", "authors": "Shaosheng Xu, Jinde Cao, Yichao Cao, Tong Wang", "abstract": "As gradient descent method in deep learning causes a series of questions,\nthis paper proposes a novel gradient-free deep learning structure. By adding a\nnew module into traditional Self-Organizing Map and introducing residual into\nthe map, a Deep Valued Self-Organizing Map network is constructed. And analysis\nabout the convergence performance of such a deep Valued Self-Organizing Map\nnetwork is proved in this paper, which gives an inequality about the designed\nparameters with the dimension of inputs and the loss of prediction.", "journal": ""}
{"doi": "10.48550/arXiv.1804.06458", "date": "2018-04-17", "title": "Deep Probabilistic Programming Languages: A Qualitative Study", "authors": "Guillaume Baudart, Martin Hirzel, Louis Mandel", "abstract": "Deep probabilistic programming languages try to combine the advantages of\ndeep learning with those of probabilistic programming languages. If successful,\nthis would be a big step forward in machine learning and programming languages.\nUnfortunately, as of now, this new crop of languages is hard to use and\nunderstand. This paper addresses this problem directly by explaining deep\nprobabilistic programming languages and indirectly by characterizing their\ncurrent strengths and weaknesses.", "journal": ""}
{"doi": "10.48550/arXiv.2110.14800", "date": "2021-10-27", "title": "Convolutional Deep Exponential Families", "authors": "Chengkuan Hong, Christian R. Shelton", "abstract": "We describe convolutional deep exponential families (CDEFs) in this paper.\nCDEFs are built based on deep exponential families, deep probabilistic models\nthat capture the hierarchical dependence between latent variables. CDEFs\ngreatly reduce the number of free parameters by tying the weights of DEFs. Our\nexperiments show that CDEFs are able to uncover time correlations with a small\namount of data.", "journal": ""}
{"doi": "10.48550/arXiv.2008.01171", "date": "2020-07-31", "title": "Deep Reinforcement Learning using Cyclical Learning Rates", "authors": "Ralf Gulde, Marc Tuscher, Akos Csiszar, Oliver Riedel, Alexander Verl", "abstract": "Deep Reinforcement Learning (DRL) methods often rely on the meticulous tuning\nof hyperparameters to successfully resolve problems. One of the most\ninfluential parameters in optimization procedures based on stochastic gradient\ndescent (SGD) is the learning rate. We investigate cyclical learning and\npropose a method for defining a general cyclical learning rate for various DRL\nproblems. In this paper we present a method for cyclical learning applied to\ncomplex DRL problems. Our experiments show that, utilizing cyclical learning\nachieves similar or even better results than highly tuned fixed learning rates.\nThis paper presents the first application of cyclical learning rates in DRL\nsettings and is a step towards overcoming manual hyperparameter tuning.", "journal": ""}
{"doi": "10.48550/arXiv.1607.07034", "date": "2016-07-24", "title": "Impact of Physical Activity on Sleep:A Deep Learning Based Exploration", "authors": "Aarti Sathyanarayana, Shafiq Joty, Luis Fernandez-Luque, Ferda Ofli, Jaideep Srivastava, Ahmed Elmagarmid, Shahrad Taheri, Teresa Arora", "abstract": "The importance of sleep is paramount for maintaining physical, emotional and\nmental wellbeing. Though the relationship between sleep and physical activity\nis known to be important, it is not yet fully understood. The explosion in\npopularity of actigraphy and wearable devices, provides a unique opportunity to\nunderstand this relationship. Leveraging this information source requires new\ntools to be developed to facilitate data-driven research for sleep and activity\npatient-recommendations.\n  In this paper we explore the use of deep learning to build sleep quality\nprediction models based on actigraphy data. We first use deep learning as a\npure model building device by performing human activity recognition (HAR) on\nraw sensor data, and using deep learning to build sleep prediction models. We\ncompare the deep learning models with those build using classical approaches,\ni.e. logistic regression, support vector machines, random forest and adaboost.\nSecondly, we employ the advantage of deep learning with its ability to handle\nhigh dimensional datasets. We explore several deep learning models on the raw\nwearable sensor output without performing HAR or any other feature extraction.\n  Our results show that using a convolutional neural network on the raw\nwearables output improves the predictive value of sleep quality from physical\nactivity, by an additional 8% compared to state-of-the-art non-deep learning\napproaches, which itself shows a 15% improvement over current practice.\nMoreover, utilizing deep learning on raw data eliminates the need for data\npre-processing and simplifies the overall workflow to analyze actigraphy data\nfor sleep and physical activity research.", "journal": "JMIR Mhealth Uhealth 2016;4(4):e125"}
{"doi": "10.48550/arXiv.2011.11128", "date": "2020-11-22", "title": "Deep Learning in EEG: Advance of the Last Ten-Year Critical Period", "authors": "Shu Gong, Kaibo Xing, Andrzej Cichocki, Junhua Li", "abstract": "Deep learning has achieved excellent performance in a wide range of domains,\nespecially in speech recognition and computer vision. Relatively less work has\nbeen done for EEG, but there is still significant progress attained in the last\ndecade. Due to the lack of a comprehensive and topic widely covered survey for\ndeep learning in EEG, we attempt to summarize recent progress to provide an\noverview, as well as perspectives for future developments. We first briefly\nmention the artifacts removal for EEG signal and then introduce deep learning\nmodels that have been utilized in EEG processing and classification.\nSubsequently, the applications of deep learning in EEG are reviewed by\ncategorizing them into groups such as brain-computer interface, disease\ndetection, and emotion recognition. They are followed by the discussion, in\nwhich the pros and cons of deep learning are presented and future directions\nand challenges for deep learning in EEG are proposed. We hope that this paper\ncould serve as a summary of past work for deep learning in EEG and the\nbeginning of further developments and achievements of EEG studies based on deep\nlearning.", "journal": "IEEE Transactions on Cognitive and Developmental Systems, 2021"}
{"doi": "10.48550/arXiv.2011.14808", "date": "2020-11-25", "title": "Bringing AI To Edge: From Deep Learning's Perspective", "authors": "Di Liu, Hao Kong, Xiangzhong Luo, Weichen Liu, Ravi Subramaniam", "abstract": "Edge computing and artificial intelligence (AI), especially deep learning for\nnowadays, are gradually intersecting to build a novel system, called edge\nintelligence. However, the development of edge intelligence systems encounters\nsome challenges, and one of these challenges is the \\textit{computational gap}\nbetween computation-intensive deep learning algorithms and less-capable edge\nsystems. Due to the computational gap, many edge intelligence systems cannot\nmeet the expected performance requirements. To bridge the gap, a plethora of\ndeep learning techniques and optimization methods are proposed in the past\nyears: light-weight deep learning models, network compression, and efficient\nneural architecture search. Although some reviews or surveys have partially\ncovered this large body of literature, we lack a systematic and comprehensive\nreview to discuss all aspects of these deep learning techniques which are\ncritical for edge intelligence implementation. As various and diverse methods\nwhich are applicable to edge systems are proposed intensively, a holistic\nreview would enable edge computing engineers and community to know the\nstate-of-the-art deep learning techniques which are instrumental for edge\nintelligence and to facilitate the development of edge intelligence systems.\nThis paper surveys the representative and latest deep learning techniques that\nare useful for edge intelligence systems, including hand-crafted models, model\ncompression, hardware-aware neural architecture search and adaptive deep\nlearning models. Finally, based on observations and simple experiments we\nconducted, we discuss some future directions.", "journal": "Neurocomputing2021"}
{"doi": "10.48550/arXiv.2305.00595", "date": "2023-04-30", "title": "Impact of Deep Learning Libraries on Online Adaptive Lightweight Time Series Anomaly Detection", "authors": "Ming-Chang Lee, Jia-Chun Lin", "abstract": "Providing online adaptive lightweight time series anomaly detection without\nhuman intervention and domain knowledge is highly valuable. Several such\nanomaly detection approaches have been introduced in the past years, but all of\nthem were only implemented in one deep learning library. With the development\nof deep learning libraries, it is unclear how different deep learning libraries\nimpact these anomaly detection approaches since there is no such evaluation\navailable. Randomly choosing a deep learning library to implement an anomaly\ndetection approach might not be able to show the true performance of the\napproach. It might also mislead users in believing one approach is better than\nanother. Therefore, in this paper, we investigate the impact of deep learning\nlibraries on online adaptive lightweight time series anomaly detection by\nimplementing two state-of-the-art anomaly detection approaches in three\nwell-known deep learning libraries and evaluating how these two approaches are\nindividually affected by the three deep learning libraries. A series of\nexperiments based on four real-world open-source time series datasets were\nconducted. The results provide a good reference to select an appropriate deep\nlearning library for online adaptive lightweight anomaly detection.", "journal": ""}
{"doi": "10.48550/arXiv.2306.04469", "date": "2023-06-05", "title": "Model-Based Deep Learning", "authors": "Nir Shlezinger, Yonina C. Eldar", "abstract": "Signal processing traditionally relies on classical statistical modeling\ntechniques. Such model-based methods utilize mathematical formulations that\nrepresent the underlying physics, prior information and additional domain\nknowledge. Simple classical models are useful but sensitive to inaccuracies and\nmay lead to poor performance when real systems display complex or dynamic\nbehavior. More recently, deep learning approaches that use deep neural networks\nare becoming increasingly popular. Deep learning systems do not rely on\nmathematical modeling, and learn their mapping from data, which allows them to\noperate in complex environments. However, they lack the interpretability and\nreliability of model-based methods, typically require large training sets to\nobtain good performance, and tend to be computationally complex. Model-based\nsignal processing methods and data-centric deep learning each have their pros\nand cons. These paradigms can be characterized as edges of a continuous\nspectrum varying in specificity and parameterization. The methodologies that\nlie in the middle ground of this spectrum, thus integrating model-based signal\nprocessing with deep learning, are referred to as model-based deep learning,\nand are the focus here. This monograph provides a tutorial style presentation\nof model-based deep learning methodologies. These are families of algorithms\nthat combine principled mathematical models with data-driven systems to benefit\nfrom the advantages of both approaches. Such model-based deep learning methods\nexploit both partial domain knowledge, via mathematical structures designed for\nspecific problems, as well as learning from limited data. We accompany our\npresentation with running examples, in super-resolution, dynamic systems, and\narray processing. We show how they are expressed using the provided\ncharacterization and specialized in each of the detailed methodologies.", "journal": ""}
{"doi": "10.48550/arXiv.2312.13754", "date": "2023-12-21", "title": "Cross-Layer Optimization for Fault-Tolerant Deep Learning", "authors": "Qing Zhang, Cheng Liu, Bo Liu, Haitong Huang, Ying Wang, Huawei Li, Xiaowei Li", "abstract": "Fault-tolerant deep learning accelerator is the basis for highly reliable\ndeep learning processing and critical to deploy deep learning in\nsafety-critical applications such as avionics and robotics. Since deep learning\nis known to be computing- and memory-intensive, traditional fault-tolerant\napproaches based on redundant computing will incur substantial overhead\nincluding power consumption and chip area. To this end, we propose to\ncharacterize deep learning vulnerability difference across both neurons and\nbits of each neuron, and leverage the vulnerability difference to enable\nselective protection of the deep learning processing components from the\nperspective of architecture layer and circuit layer respectively. At the same\ntime, we observe the correlation between model quantization and bit protection\noverhead of the underlying processing elements of deep learning accelerators,\nand propose to reduce the bit protection overhead by adding additional\nquantization constrain without compromising the model accuracy. Finally, we\nemploy Bayesian optimization strategy to co-optimize the correlated cross-layer\ndesign parameters at algorithm layer, architecture layer, and circuit layer to\nminimize the hardware resource consumption while fulfilling multiple user\nconstraints including reliability, accuracy, and performance of the deep\nlearning processing at the same time.", "journal": ""}
{"doi": "10.48550/arXiv.2404.19043", "date": "2024-04-29", "title": "Improving Interpretability of Deep Active Learning for Flood Inundation Mapping Through Class Ambiguity Indices Using Multi-spectral Satellite Imagery", "authors": "Hyunho Lee, Wenwen Li", "abstract": "Flood inundation mapping is a critical task for responding to the increasing\nrisk of flooding linked to global warming. Significant advancements of deep\nlearning in recent years have triggered its extensive applications, including\nflood inundation mapping. To cope with the time-consuming and labor-intensive\ndata labeling process in supervised learning, deep active learning strategies\nare one of the feasible approaches. However, there remains limited exploration\ninto the interpretability of how deep active learning strategies operate, with\na specific focus on flood inundation mapping in the field of remote sensing. In\nthis study, we introduce a novel framework of Interpretable Deep Active\nLearning for Flood inundation Mapping (IDAL-FIM), specifically in terms of\nclass ambiguity of multi-spectral satellite images. In the experiments, we\nutilize Sen1Floods11 dataset, and adopt U-Net with MC-dropout. In addition, we\nemploy five acquisition functions, which are the random, K-means, BALD,\nentropy, and margin acquisition functions. Based on the experimental results,\nwe demonstrate that two proposed class ambiguity indices are effective\nvariables to interpret the deep active learning by establishing statistically\nsignificant correlation with the predictive uncertainty of the deep learning\nmodel at the tile level. Then, we illustrate the behaviors of deep active\nlearning through visualizing two-dimensional density plots and providing\ninterpretations regarding the operation of deep active learning, in flood\ninundation mapping.", "journal": "Remote Sensing of Environment, 309, 114213"}
{"doi": "10.48550/arXiv.2411.15674", "date": "2024-11-24", "title": "Quantile deep learning models for multi-step ahead time series prediction", "authors": "Jimmy Cheung, Smruthi Rangarajan, Amelia Maddocks, Xizhe Chen, Rohitash Chandra", "abstract": "Uncertainty quantification is crucial in time series prediction, and quantile\nregression offers a valuable mechanism for uncertainty quantification which is\nuseful for extreme value forecasting. Although deep learning models have been\nprominent in multi-step ahead prediction, the development and evaluation of\nquantile deep learning models have been limited. We present a novel quantile\nregression deep learning framework for multi-step time series prediction. In\nthis way, we elevate the capabilities of deep learning models by incorporating\nquantile regression, thus providing a more nuanced understanding of predictive\nvalues. We provide an implementation of prominent deep learning models for\nmulti-step ahead time series prediction and evaluate their performance under\nhigh volatility and extreme conditions. We include multivariate and univariate\nmodelling, strategies and provide a comparison with conventional deep learning\nmodels from the literature. Our models are tested on two cryptocurrencies:\nBitcoin and Ethereum, using daily close-price data and selected benchmark time\nseries datasets. The results show that integrating a quantile loss function\nwith deep learning provides additional predictions for selected quantiles\nwithout a loss in the prediction accuracy when compared to the literature. Our\nquantile model has the ability to handle volatility more effectively and\nprovides additional information for decision-making and uncertainty\nquantification through the use of quantiles when compared to conventional deep\nlearning models.", "journal": ""}
{"doi": "10.48550/arXiv.1707.00797", "date": "2017-07-04", "title": "Learning Deep Energy Models: Contrastive Divergence vs. Amortized MLE", "authors": "Qiang Liu, Dilin Wang", "abstract": "We propose a number of new algorithms for learning deep energy models and\ndemonstrate their properties. We show that our SteinCD performs well in term of\ntest likelihood, while SteinGAN performs well in terms of generating realistic\nlooking images. Our results suggest promising directions for learning better\nmodels by combining GAN-style methods with traditional energy-based learning.", "journal": ""}
{"doi": "10.48550/arXiv.1711.03705", "date": "2017-11-10", "title": "Online Deep Learning: Learning Deep Neural Networks on the Fly", "authors": "Doyen Sahoo, Quang Pham, Jing Lu, Steven C. H. Hoi", "abstract": "Deep Neural Networks (DNNs) are typically trained by backpropagation in a\nbatch learning setting, which requires the entire training data to be made\navailable prior to the learning task. This is not scalable for many real-world\nscenarios where new data arrives sequentially in a stream form. We aim to\naddress an open challenge of \"Online Deep Learning\" (ODL) for learning DNNs on\nthe fly in an online setting. Unlike traditional online learning that often\noptimizes some convex objective function with respect to a shallow model (e.g.,\na linear/kernel-based hypothesis), ODL is significantly more challenging since\nthe optimization of the DNN objective function is non-convex, and regular\nbackpropagation does not work well in practice, especially for online learning\nsettings. In this paper, we present a new online deep learning framework that\nattempts to tackle the challenges by learning DNN models of adaptive depth from\na sequence of training data in an online learning setting. In particular, we\npropose a novel Hedge Backpropagation (HBP) method for online updating the\nparameters of DNN effectively, and validate the efficacy of our method on\nlarge-scale data sets, including both stationary and concept drifting\nscenarios.", "journal": ""}
{"doi": "10.48550/arXiv.2005.09428", "date": "2020-05-15", "title": "Quantum-Classical Machine learning by Hybrid Tensor Networks", "authors": "Ding Liu, Jiaqi Yao, Zekun Yao, Quan Zhang", "abstract": "Tensor networks (TN) have found a wide use in machine learning, and in\nparticular, TN and deep learning bear striking similarities. In this work, we\npropose the quantum-classical hybrid tensor networks (HTN) which combine tensor\nnetworks with classical neural networks in a uniform deep learning framework to\novercome the limitations of regular tensor networks in machine learning. We\nfirst analyze the limitations of regular tensor networks in the applications of\nmachine learning involving the representation power and architecture\nscalability. We conclude that in fact the regular tensor networks are not\ncompetent to be the basic building blocks of deep learning. Then, we discuss\nthe performance of HTN which overcome all the deficiency of regular tensor\nnetworks for machine learning. In this sense, we are able to train HTN in the\ndeep learning way which is the standard combination of algorithms such as Back\nPropagation and Stochastic Gradient Descent. We finally provide two applicable\ncases to show the potential applications of HTN, including quantum states\nclassification and quantum-classical autoencoder. These cases also demonstrate\nthe great potentiality to design various HTN in deep learning way.", "journal": ""}
{"doi": "10.48550/arXiv.2203.11196", "date": "2022-03-18", "title": "Performance of Deep Learning models with transfer learning for multiple-step-ahead forecasts in monthly time series", "authors": "Mart\u00edn Sol\u00eds, Luis-Alexander Calvo-Valverde", "abstract": "Deep Learning and transfer learning models are being used to generate time\nseries forecasts; however, there is scarce evidence about their performance\nprediction that it is more evident for monthly time series. The purpose of this\npaper is to compare Deep Learning models with transfer learning and without\ntransfer learning and other traditional methods used for monthly forecasts to\nanswer three questions about the suitability of Deep Learning and Transfer\nLearning to generate predictions of time series. Time series of M4 and M3\ncompetitions were used for the experiments. The results suggest that deep\nlearning models based on TCN, LSTM, and CNN with transfer learning tend to\nsurpass the performance prediction of other traditional methods. On the other\nhand, TCN and LSTM, trained directly on the target time series, got similar or\nbetter performance than traditional methods for some forecast horizons.", "journal": ""}
{"doi": "10.48550/arXiv.1810.07862", "date": "2018-10-18", "title": "Applications of Deep Reinforcement Learning in Communications and Networking: A Survey", "authors": "Nguyen Cong Luong, Dinh Thai Hoang, Shimin Gong, Dusit Niyato, Ping Wang, Ying-Chang Liang, Dong In Kim", "abstract": "This paper presents a comprehensive literature review on applications of deep\nreinforcement learning in communications and networking. Modern networks, e.g.,\nInternet of Things (IoT) and Unmanned Aerial Vehicle (UAV) networks, become\nmore decentralized and autonomous. In such networks, network entities need to\nmake decisions locally to maximize the network performance under uncertainty of\nnetwork environment. Reinforcement learning has been efficiently used to enable\nthe network entities to obtain the optimal policy including, e.g., decisions or\nactions, given their states when the state and action spaces are small.\nHowever, in complex and large-scale networks, the state and action spaces are\nusually large, and the reinforcement learning may not be able to find the\noptimal policy in reasonable time. Therefore, deep reinforcement learning, a\ncombination of reinforcement learning with deep learning, has been developed to\novercome the shortcomings. In this survey, we first give a tutorial of deep\nreinforcement learning from fundamental concepts to advanced models. Then, we\nreview deep reinforcement learning approaches proposed to address emerging\nissues in communications and networking. The issues include dynamic network\naccess, data rate control, wireless caching, data offloading, network security,\nand connectivity preservation which are all important to next generation\nnetworks such as 5G and beyond. Furthermore, we present applications of deep\nreinforcement learning for traffic routing, resource sharing, and data\ncollection. Finally, we highlight important challenges, open issues, and future\nresearch directions of applying deep reinforcement learning.", "journal": ""}
{"doi": "10.48550/arXiv.1812.06175", "date": "2018-12-14", "title": "Can Deep Learning Predict Risky Retail Investors? A Case Study in Financial Risk Behavior Forecasting", "authors": "Yaodong Yang, Alisa Kolesnikova, Stefan Lessmann, Tiejun Ma, Ming-Chien Sung, Johnnie E. V. Johnson", "abstract": "The paper examines the potential of deep learning to support decisions in\nfinancial risk management. We develop a deep learning model for predicting\nwhether individual spread traders secure profits from future trades. This task\nembodies typical modeling challenges faced in risk and behavior forecasting.\nConventional machine learning requires data that is representative of the\nfeature-target relationship and relies on the often costly development,\nmaintenance, and revision of handcrafted features. Consequently, modeling\nhighly variable, heterogeneous patterns such as trader behavior is challenging.\nDeep learning promises a remedy. Learning hierarchical distributed\nrepresentations of the data in an automatic manner (e.g. risk taking behavior),\nit uncovers generative features that determine the target (e.g., trader's\nprofitability), avoids manual feature engineering, and is more robust toward\nchange (e.g. dynamic market conditions). The results of employing a deep\nnetwork for operational risk forecasting confirm the feature learning\ncapability of deep learning, provide guidance on designing a suitable network\narchitecture and demonstrate the superiority of deep learning over machine\nlearning and rule-based benchmarks.", "journal": ""}
{"doi": "10.48550/arXiv.1907.08674", "date": "2019-07-17", "title": "Deep Learning to Address Candidate Generation and Cold Start Challenges in Recommender Systems: A Research Survey", "authors": "Kiran Rama, Pradeep Kumar, Bharat Bhasker", "abstract": "Among the machine learning applications to business, recommender systems\nwould take one of the top places when it comes to success and adoption. They\nhelp the user in accelerating the process of search while helping businesses\nmaximize sales. Post phenomenal success in computer vision and speech\nrecognition, deep learning methods are beginning to get applied to recommender\nsystems. Current survey papers on deep learning in recommender systems provide\na historical overview and taxonomy of recommender systems based on type. Our\npaper addresses the gaps of providing a taxonomy of deep learning approaches to\naddress recommender systems problems in the areas of cold start and candidate\ngeneration in recommender systems. We outline different challenges in\nrecommender systems into those related to the recommendations themselves\n(include relevance, speed, accuracy and scalability), those related to the\nnature of the data (cold start problem, imbalance and sparsity) and candidate\ngeneration. We then provide a taxonomy of deep learning techniques to address\nthese challenges. Deep learning techniques are mapped to the different\nchallenges in recommender systems providing an overview of how deep learning\ntechniques can be used to address them. We contribute a taxonomy of deep\nlearning techniques to address the cold start and candidate generation problems\nin recommender systems. Cold Start is addressed through additional features\n(for audio, images, text) and by learning hidden user and item representations.\nCandidate generation has been addressed by separate networks, RNNs,\nautoencoders and hybrid methods. We also summarize the advantages and\nlimitations of these techniques while outlining areas for future research.", "journal": ""}
{"doi": "10.48550/arXiv.2304.12602", "date": "2023-04-25", "title": "Is deep learning a useful tool for the pure mathematician?", "authors": "Geordie Williamson", "abstract": "A personal and informal account of what a pure mathematician might expect\nwhen using tools from deep learning in their research.", "journal": ""}
{"doi": "10.48550/arXiv.2305.16808", "date": "2023-05-26", "title": "Geometric deep learning approach to knot theory", "authors": "Lennart Jaretzki", "abstract": "In this paper, we introduce a novel way to use geometric deep learning for\nknot data by constructing a functor that takes knots to graphs and using graph\nneural networks. We will attempt to predict several knot invariants with this\napproach. This approach demonstrates high generalization capabilities.", "journal": ""}
{"doi": "10.48550/arXiv.2505.03677", "date": "2025-05-06", "title": "Neural Integral Operators for Inverse problems in Spectroscopy", "authors": "Emanuele Zappala, Alice Giola, Andreas Kramer, Enrico Greco", "abstract": "Deep learning has shown high performance on spectroscopic inverse problems\nwhen sufficient data is available. However, it is often the case that data in\nspectroscopy is scarce, and this usually causes severe overfitting problems\nwith deep learning methods. Traditional machine learning methods are viable\nwhen datasets are smaller, but the accuracy and applicability of these methods\nis generally more limited. We introduce a deep learning method for\nclassification of molecular spectra based on learning integral operators via\nintegral equations of the first kind, which results in an algorithm that is\nless affected by overfitting issues on small datasets, compared to other deep\nlearning models. The problem formulation of the deep learning approach is based\non inverse problems, which have traditionally found important applications in\nspectroscopy. We perform experiments on real world data to showcase our\nalgorithm. It is seen that the model outperforms traditional machine learning\napproaches such as decision tree and support vector machine, and for small\ndatasets it outperforms other deep learning models. Therefore, our methodology\nleverages the power of deep learning, still maintaining the performance when\nthe available data is very limited, which is one of the main issues that deep\nlearning faces in spectroscopy, where datasets are often times of small size.", "journal": ""}
{"doi": "10.48550/arXiv.2004.08410", "date": "2020-04-17", "title": "Deep Reinforcement Learning for Adaptive Learning Systems", "authors": "Xiao Li, Hanchen Xu, Jinming Zhang, Hua-hua Chang", "abstract": "In this paper, we formulate the adaptive learning problem---the problem of\nhow to find an individualized learning plan (called policy) that chooses the\nmost appropriate learning materials based on learner's latent traits---faced in\nadaptive learning systems as a Markov decision process (MDP). We assume latent\ntraits to be continuous with an unknown transition model. We apply a model-free\ndeep reinforcement learning algorithm---the deep Q-learning algorithm---that\ncan effectively find the optimal learning policy from data on learners'\nlearning process without knowing the actual transition model of the learners'\ncontinuous latent traits. To efficiently utilize available data, we also\ndevelop a transition model estimator that emulates the learner's learning\nprocess using neural networks. The transition model estimator can be used in\nthe deep Q-learning algorithm so that it can more efficiently discover the\noptimal learning policy for a learner. Numerical simulation studies verify that\nthe proposed algorithm is very efficient in finding a good learning policy,\nespecially with the aid of a transition model estimator, it can find the\noptimal learning policy after training using a small number of learners.", "journal": ""}
{"doi": "10.48550/arXiv.2012.01141", "date": "2020-12-02", "title": "Algebraically-Informed Deep Networks (AIDN): A Deep Learning Approach to Represent Algebraic Structures", "authors": "Mustafa Hajij, Ghada Zamzmi, Matthew Dawson, Greg Muller", "abstract": "One of the central problems in the interface of deep learning and mathematics\nis that of building learning systems that can automatically uncover underlying\nmathematical laws from observed data. In this work, we make one step towards\nbuilding a bridge between algebraic structures and deep learning, and introduce\n\\textbf{AIDN}, \\textit{Algebraically-Informed Deep Networks}. \\textbf{AIDN} is\na deep learning algorithm to represent any finitely-presented algebraic object\nwith a set of deep neural networks. The deep networks obtained via\n\\textbf{AIDN} are \\textit{algebraically-informed} in the sense that they\nsatisfy the algebraic relations of the presentation of the algebraic structure\nthat serves as the input to the algorithm. Our proposed network can robustly\ncompute linear and non-linear representations of most finitely-presented\nalgebraic structures such as groups, associative algebras, and Lie algebras. We\nevaluate our proposed approach and demonstrate its applicability to algebraic\nand geometric objects that are significant in low-dimensional topology. In\nparticular, we study solutions for the Yang-Baxter equations and their\napplications on braid groups. Further, we study the representations of the\nTemperley-Lieb algebra. Finally, we show, using the Reshetikhin-Turaev\nconstruction, how our proposed deep learning approach can be utilized to\nconstruct new link invariants. We believe the proposed approach would tread a\npath toward a promising future research in deep learning applied to algebraic\nand geometric structures.", "journal": ""}
{"doi": "10.48550/arXiv.2310.16154", "date": "2023-10-24", "title": "Breaking the Curse of Dimensionality in Deep Neural Networks by Learning Invariant Representations", "authors": "Leonardo Petrini", "abstract": "Artificial intelligence, particularly the subfield of machine learning, has\nseen a paradigm shift towards data-driven models that learn from and adapt to\ndata. This has resulted in unprecedented advancements in various domains such\nas natural language processing and computer vision, largely attributed to deep\nlearning, a special class of machine learning models. Deep learning arguably\nsurpasses traditional approaches by learning the relevant features from raw\ndata through a series of computational layers.\n  This thesis explores the theoretical foundations of deep learning by studying\nthe relationship between the architecture of these models and the inherent\nstructures found within the data they process. In particular, we ask What\ndrives the efficacy of deep learning algorithms and allows them to beat the\nso-called curse of dimensionality-i.e. the difficulty of generally learning\nfunctions in high dimensions due to the exponentially increasing need for data\npoints with increased dimensionality? Is it their ability to learn relevant\nrepresentations of the data by exploiting their structure? How do different\narchitectures exploit different data structures? In order to address these\nquestions, we push forward the idea that the structure of the data can be\neffectively characterized by its invariances-i.e. aspects that are irrelevant\nfor the task at hand.\n  Our methodology takes an empirical approach to deep learning, combining\nexperimental studies with physics-inspired toy models. These simplified models\nallow us to investigate and interpret the complex behaviors we observe in deep\nlearning systems, offering insights into their inner workings, with the\nfar-reaching goal of bridging the gap between theory and practice.", "journal": ""}
{"doi": "10.48550/arXiv.1312.5602", "date": "2013-12-19", "title": "Playing Atari with Deep Reinforcement Learning", "authors": "Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, Martin Riedmiller", "abstract": "We present the first deep learning model to successfully learn control\npolicies directly from high-dimensional sensory input using reinforcement\nlearning. The model is a convolutional neural network, trained with a variant\nof Q-learning, whose input is raw pixels and whose output is a value function\nestimating future rewards. We apply our method to seven Atari 2600 games from\nthe Arcade Learning Environment, with no adjustment of the architecture or\nlearning algorithm. We find that it outperforms all previous approaches on six\nof the games and surpasses a human expert on three of them.", "journal": ""}
{"doi": "10.48550/arXiv.1402.4884", "date": "2014-02-20", "title": "Le Cam meets LeCun: Deficiency and Generic Feature Learning", "authors": "Brendan van Rooyen, Robert C. Williamson", "abstract": "\"Deep Learning\" methods attempt to learn generic features in an unsupervised\nfashion from a large unlabelled data set. These generic features should perform\nas well as the best hand crafted features for any learning problem that makes\nuse of this data. We provide a definition of generic features, characterize\nwhen it is possible to learn them and provide methods closely related to the\nautoencoder and deep belief network of deep learning. In order to do so we use\nthe notion of deficiency and illustrate its value in studying certain general\nlearning problems.", "journal": ""}
{"doi": "10.48550/arXiv.2012.00152", "date": "2020-11-30", "title": "Every Model Learned by Gradient Descent Is Approximately a Kernel Machine", "authors": "Pedro Domingos", "abstract": "Deep learning's successes are often attributed to its ability to\nautomatically discover new representations of the data, rather than relying on\nhandcrafted features like other learning methods. We show, however, that deep\nnetworks learned by the standard gradient descent algorithm are in fact\nmathematically approximately equivalent to kernel machines, a learning method\nthat simply memorizes the data and uses it directly for prediction via a\nsimilarity function (the kernel). This greatly enhances the interpretability of\ndeep network weights, by elucidating that they are effectively a superposition\nof the training examples. The network architecture incorporates knowledge of\nthe target function into the kernel. This improved understanding should lead to\nbetter learning algorithms.", "journal": ""}
{"doi": "10.48550/arXiv.1707.07700", "date": "2017-07-24", "title": "A Deep Investigation of Deep IR Models", "authors": "Liang Pang, Yanyan Lan, Jiafeng Guo, Jun Xu, Xueqi Cheng", "abstract": "The effective of information retrieval (IR) systems have become more\nimportant than ever. Deep IR models have gained increasing attention for its\nability to automatically learning features from raw text; thus, many deep IR\nmodels have been proposed recently. However, the learning process of these deep\nIR models resemble a black box. Therefore, it is necessary to identify the\ndifference between automatically learned features by deep IR models and\nhand-crafted features used in traditional learning to rank approaches.\nFurthermore, it is valuable to investigate the differences between these deep\nIR models. This paper aims to conduct a deep investigation on deep IR models.\nSpecifically, we conduct an extensive empirical study on two different\ndatasets, including Robust and LETOR4.0. We first compared the automatically\nlearned features and hand-crafted features on the respects of query term\ncoverage, document length, embeddings and robustness. It reveals a number of\ndisadvantages compared with hand-crafted features. Therefore, we establish\nguidelines for improving existing deep IR models. Furthermore, we compare two\ndifferent categories of deep IR models, i.e. representation-focused models and\ninteraction-focused models. It is shown that two types of deep IR models focus\non different categories of words, including topic-related words and\nquery-related words.", "journal": ""}
{"doi": "10.48550/arXiv.1912.06444", "date": "2019-12-13", "title": "Deep Self-representative Concept Factorization Network for Representation Learning", "authors": "Yan Zhang, Zhao Zhang, Zheng Zhang, Mingbo Zhao, Li Zhang, Zhengjun Zha, Meng Wang", "abstract": "In this paper, we investigate the unsupervised deep representation learning\nissue and technically propose a novel framework called Deep Self-representative\nConcept Factorization Network (DSCF-Net), for clustering deep features. To\nimprove the representation and clustering abilities, DSCF-Net explicitly\nconsiders discovering hidden deep semantic features, enhancing the robustness\nproper-ties of the deep factorization to noise and preserving the local\nman-ifold structures of deep features. Specifically, DSCF-Net seamlessly\nintegrates the robust deep concept factorization, deep self-expressive\nrepresentation and adaptive locality preserving feature learning into a unified\nframework. To discover hidden deep repre-sentations, DSCF-Net designs a\nhierarchical factorization architec-ture using multiple layers of linear\ntransformations, where the hierarchical representation is performed by\nformulating the prob-lem as optimizing the basis concepts in each layer to\nimprove the representation indirectly. DSCF-Net also improves the robustness by\nsubspace recovery for sparse error correction firstly and then performs the\ndeep factorization in the recovered visual subspace. To obtain\nlocality-preserving representations, we also present an adaptive deep\nself-representative weighting strategy by using the coefficient matrix as the\nadaptive reconstruction weights to keep the locality of representations.\nExtensive comparison results with several other related models show that\nDSCF-Net delivers state-of-the-art performance on several public databases.", "journal": ""}
{"doi": "10.48550/arXiv.1504.01716", "date": "2015-04-07", "title": "An Empirical Evaluation of Deep Learning on Highway Driving", "authors": "Brody Huval, Tao Wang, Sameep Tandon, Jeff Kiske, Will Song, Joel Pazhayampallil, Mykhaylo Andriluka, Pranav Rajpurkar, Toki Migimatsu, Royce Cheng-Yue, Fernando Mujica, Adam Coates, Andrew Y. Ng", "abstract": "Numerous groups have applied a variety of deep learning techniques to\ncomputer vision problems in highway perception scenarios. In this paper, we\npresented a number of empirical evaluations of recent deep learning advances.\nComputer vision, combined with deep learning, has the potential to bring about\na relatively inexpensive, robust solution to autonomous driving. To prepare\ndeep learning for industry uptake and practical applications, neural networks\nwill require large data sets that represent all possible driving environments\nand scenarios. We collect a large data set of highway data and apply deep\nlearning and computer vision algorithms to problems such as car and lane\ndetection. We show how existing convolutional neural networks (CNNs) can be\nused to perform lane and vehicle detection while running at frame rates\nrequired for a real-time system. Our results lend credence to the hypothesis\nthat deep learning holds promise for autonomous driving.", "journal": ""}
{"doi": "10.48550/arXiv.1509.06061", "date": "2015-09-20", "title": "A Statistical Theory of Deep Learning via Proximal Splitting", "authors": "Nicholas G. Polson, Brandon T. Willard, Massoud Heidari", "abstract": "In this paper we develop a statistical theory and an implementation of deep\nlearning models. We show that an elegant variable splitting scheme for the\nalternating direction method of multipliers optimises a deep learning\nobjective. We allow for non-smooth non-convex regularisation penalties to\ninduce sparsity in parameter weights. We provide a link between traditional\nshallow layer statistical models such as principal component and sliced inverse\nregression and deep layer models. We also define the degrees of freedom of a\ndeep learning predictor and a predictive MSE criteria to perform model\nselection for comparing architecture designs. We focus on deep multiclass\nlogistic learning although our methods apply more generally. Our results\nsuggest an interesting and previously under-exploited relationship between deep\nlearning and proximal splitting techniques. To illustrate our methodology, we\nprovide a multi-class logit classification analysis of Fisher's Iris data where\nwe illustrate the convergence of our algorithm. Finally, we conclude with\ndirections for future research.", "journal": ""}
{"doi": "10.48550/arXiv.1612.07454", "date": "2016-12-22", "title": "How to Train Your Deep Neural Network with Dictionary Learning", "authors": "Vanika Singhal, Shikha Singh, Angshul Majumdar", "abstract": "Currently there are two predominant ways to train deep neural networks. The\nfirst one uses restricted Boltzmann machine (RBM) and the second one\nautoencoders. RBMs are stacked in layers to form deep belief network (DBN); the\nfinal representation layer is attached to the target to complete the deep\nneural network. Autoencoders are nested one inside the other to form stacked\nautoencoders; once the stcaked autoencoder is learnt the decoder portion is\ndetached and the target attached to the deepest layer of the encoder to form\nthe deep neural network. This work proposes a new approach to train deep neural\nnetworks using dictionary learning as the basic building block; the idea is to\nuse the features from the shallower layer as inputs for training the next\ndeeper layer. One can use any type of dictionary learning (unsupervised,\nsupervised, discriminative etc.) as basic units till the pre-final layer. In\nthe final layer one needs to use the label consistent dictionary learning\nformulation for classification. We compare our proposed framework with existing\nstate-of-the-art deep learning techniques on benchmark problems; we are always\nwithin the top 10 results. In actual problems of age and gender classification,\nwe are better than the best known techniques.", "journal": ""}
{"doi": "10.48550/arXiv.1612.09057", "date": "2016-12-29", "title": "Deep Learning and Hierarchal Generative Models", "authors": "Elchanan Mossel", "abstract": "It is argued that deep learning is efficient for data that is generated from\nhierarchal generative models. Examples of such generative models include\nwavelet scattering networks, functions of compositional structure, and deep\nrendering models. Unfortunately so far, for all such models, it is either not\nrigorously known that they can be learned efficiently, or it is not known that\n\"deep algorithms\" are required in order to learn them.\n  We propose a simple family of \"generative hierarchal models\" which can be\nefficiently learned and where \"deep\" algorithm are necessary for learning. Our\ndefinition of \"deep\" algorithms is based on the empirical observation that deep\nnets necessarily use correlations between features. More formally, we show that\nin a semi-supervised setting, given access to low-order moments of the labeled\ndata and all of the unlabeled data, it is information theoretically impossible\nto perform classification while at the same time there is an efficient\nalgorithm, that given all labelled and unlabeled data, perfectly labels all\nunlabelled data with high probability.\n  For the proof, we use and strengthen the fact that Belief Propagation does\nnot admit a good approximation in terms of linear functions.", "journal": ""}
{"doi": "10.48550/arXiv.1808.02394", "date": "2018-08-07", "title": "Application of End-to-End Deep Learning in Wireless Communications Systems", "authors": "Woongsup Lee, Ohyun Jo, Minhoe Kim", "abstract": "Deep learning is a potential paradigm changer for the design of wireless\ncommunications systems (WCS), from conventional handcrafted schemes based on\nsophisticated mathematical models with assumptions to autonomous schemes based\non the end-to-end deep learning using a large number of data. In this article,\nwe present a basic concept of the deep learning and its application to WCS by\ninvestigating the resource allocation (RA) scheme based on a deep neural\nnetwork (DNN) where multiple goals with various constraints can be satisfied\nthrough the end-to-end deep learning. Especially, the optimality and\nfeasibility of the DNN based RA are verified through simulation. Then, we\ndiscuss the technical challenges regarding the application of deep learning in\nWCS.", "journal": ""}
{"doi": "10.48550/arXiv.2003.01291", "date": "2020-03-03", "title": "Overall error analysis for the training of deep neural networks via stochastic gradient descent with random initialisation", "authors": "Arnulf Jentzen, Timo Welti", "abstract": "In spite of the accomplishments of deep learning based algorithms in numerous\napplications and very broad corresponding research interest, at the moment\nthere is still no rigorous understanding of the reasons why such algorithms\nproduce useful results in certain situations. A thorough mathematical analysis\nof deep learning based algorithms seems to be crucial in order to improve our\nunderstanding and to make their implementation more effective and efficient. In\nthis article we provide a mathematically rigorous full error analysis of deep\nlearning based empirical risk minimisation with quadratic loss function in the\nprobabilistically strong sense, where the underlying deep neural networks are\ntrained using stochastic gradient descent with random initialisation. The\nconvergence speed we obtain is presumably far from optimal and suffers under\nthe curse of dimensionality. To the best of our knowledge, we establish,\nhowever, the first full error analysis in the scientific literature for a deep\nlearning based algorithm in the probabilistically strong sense and, moreover,\nthe first full error analysis in the scientific literature for a deep learning\nbased algorithm where stochastic gradient descent with random initialisation is\nthe employed optimisation method.", "journal": ""}
{"doi": "10.48550/arXiv.1707.03750", "date": "2017-07-12", "title": "DeepProf: Performance Analysis for Deep Learning Applications via Mining GPU Execution Patterns", "authors": "Jiazhen Gu, Huan Liu, Yangfan Zhou, Xin Wang", "abstract": "Deep learning applications are computation-intensive and often employ GPU as\nthe underlying computing devices. Deep learning frameworks provide powerful\nprogramming interfaces, but the gap between source codes and practical GPU\noperations make it difficult to analyze the performance of deep learning\napplications. In this paper, through examing the features of GPU traces and\ndeep learning applications, we use the suffix tree structure to extract the\nrepeated patten in GPU traces. Performance analysis graphs can be generated\nfrom the preprocessed GPU traces. We further present \\texttt{DeepProf}, a novel\ntool to automatically process GPU traces and generate performance analysis\nreports for deep learning applications. Empirical study verifies the\neffectiveness of \\texttt{DeepProf} in performance analysis and diagnosis. We\nalso find out some interesting properties of Tensorflow, which can be used to\nguide the deep learning system setup.", "journal": ""}
{"doi": "10.48550/arXiv.1911.00353", "date": "2019-10-31", "title": "Does deep learning always outperform simple linear regression in optical imaging?", "authors": "Shuming Jiao, Yang Gao, Jun Feng, Ting Lei, Xiaocong Yuan", "abstract": "Deep learning has been extensively applied in many optical imaging\napplications in recent years. Despite the success, the limitations and\ndrawbacks of deep learning in optical imaging have been seldom investigated. In\nthis work, we show that conventional linear-regression-based methods can\noutperform the previously proposed deep learning approaches for two black-box\noptical imaging problems in some extent. Deep learning demonstrates its\nweakness especially when the number of training samples is small. The\nadvantages and disadvantages of linear-regression-based methods and deep\nlearning are analyzed and compared. Since many optical systems are essentially\nlinear, a deep learning network containing many nonlinearity functions\nsometimes may not be the most suitable option.", "journal": ""}
{"doi": "10.48550/arXiv.1912.13156", "date": "2019-12-31", "title": "Hiding Information in Big Data based on Deep Learning", "authors": "Dingju Zhu", "abstract": "The current approach of information hiding based on deep learning model can\nnot directly use the original data as carriers, which means the approach can\nnot make use of the existing data in big data to hiding information. We\nproposed a novel method of information hiding in big data based on deep\nlearning. Our method uses the existing data in big data as carriers and uses\ndeep learning models to hide and extract secret messages in big data. The data\namount of big data is unlimited and thus the data amount of secret messages\nhided in big data can also be unlimited. Before opponents want to extract\nsecret messages from carriers, they need to find the carriers, however finding\nout the carriers from big data is just like finding out a box from the sea.\nDeep learning models are well known as deep black boxes in which the process\nfrom the input to the output is very complex, and thus the deep learning model\nfor information hiding is almost impossible for opponents to reconstruct. The\nresults also show that our method can hide secret messages safely,\nconveniently, quickly and with no limitation on the data amount.", "journal": ""}
{"doi": "10.48550/arXiv.2004.00245", "date": "2020-04-01", "title": "Depth Selection for Deep ReLU Nets in Feature Extraction and Generalization", "authors": "Zhi Han, Siquan Yu, Shao-Bo Lin, Ding-Xuan Zhou", "abstract": "Deep learning is recognized to be capable of discovering deep features for\nrepresentation learning and pattern recognition without requiring elegant\nfeature engineering techniques by taking advantage of human ingenuity and prior\nknowledge. Thus it has triggered enormous research activities in machine\nlearning and pattern recognition. One of the most important challenge of deep\nlearning is to figure out relations between a feature and the depth of deep\nneural networks (deep nets for short) to reflect the necessity of depth. Our\npurpose is to quantify this feature-depth correspondence in feature extraction\nand generalization. We present the adaptivity of features to depths and\nvice-verse via showing a depth-parameter trade-off in extracting both single\nfeature and composite features. Based on these results, we prove that\nimplementing the classical empirical risk minimization on deep nets can achieve\nthe optimal generalization performance for numerous learning tasks. Our\ntheoretical results are verified by a series of numerical experiments including\ntoy simulations and a real application of earthquake seismic intensity\nprediction.", "journal": ""}
{"doi": "10.48550/arXiv.2005.09687", "date": "2020-05-19", "title": "Deep learning approaches for neural decoding: from CNNs to LSTMs and spikes to fMRI", "authors": "Jesse A. Livezey, Joshua I. Glaser", "abstract": "Decoding behavior, perception, or cognitive state directly from neural\nsignals has applications in brain-computer interface research as well as\nimplications for systems neuroscience. In the last decade, deep learning has\nbecome the state-of-the-art method in many machine learning tasks ranging from\nspeech recognition to image segmentation. The success of deep networks in other\ndomains has led to a new wave of applications in neuroscience. In this article,\nwe review deep learning approaches to neural decoding. We describe the\narchitectures used for extracting useful features from neural recording\nmodalities ranging from spikes to EEG. Furthermore, we explore how deep\nlearning has been leveraged to predict common outputs including movement,\nspeech, and vision, with a focus on how pretrained deep networks can be\nincorporated as priors for complex decoding targets like acoustic speech or\nimages. Deep learning has been shown to be a useful tool for improving the\naccuracy and flexibility of neural decoding across a wide range of tasks, and\nwe point out areas for future scientific development.", "journal": ""}
{"doi": "10.48550/arXiv.2107.12732", "date": "2021-07-27", "title": "Towards Black-box Attacks on Deep Learning Apps", "authors": "Hongchen Cao, Shuai Li, Yuming Zhou, Ming Fan, Xuejiao Zhao, Yutian Tang", "abstract": "Deep learning is a powerful weapon to boost application performance in many\nfields, including face recognition, object detection, image classification,\nnatural language understanding, and recommendation system. With the rapid\nincrease in the computing power of mobile devices, developers can embed deep\nlearning models into their apps for building more competitive products with\nmore accurate and faster responses. Although there are several works about\nadversarial attacks against deep learning models in mobile apps, they all need\ninformation about the models' internals (i.e., structures, weights) or need to\nmodify the models. In this paper, we propose an effective black-box approach by\ntraining a substitute model to spoof the deep learning system inside the apps.\nTo evaluate our approach, we select 10 real-world deep-learning apps with high\npopularity from Google Play to perform black-box adversarial attacks. Through\nthe study, we find three factors that can influence the performance of attacks.\nOur approach can reach a relatively high attack success rate of 66.60% on\naverage. Compared with other adversarial attacks on mobile deep learning\nmodels, in terms of the average attack success rates, our approach outperforms\ncounterparts by 27.63%.", "journal": ""}
{"doi": "10.48550/arXiv.2110.04442", "date": "2021-10-09", "title": "A Primer on Deep Learning for Causal Inference", "authors": "Bernard Koch, Tim Sainburg, Pablo Geraldo, Song Jiang, Yizhou Sun, Jacob Gates Foster", "abstract": "This review systematizes the emerging literature for causal inference using\ndeep neural networks under the potential outcomes framework. It provides an\nintuitive introduction on how deep learning can be used to estimate/predict\nheterogeneous treatment effects and extend causal inference to settings where\nconfounding is non-linear, time varying, or encoded in text, networks, and\nimages. To maximize accessibility, we also introduce prerequisite concepts from\ncausal inference and deep learning. The survey differs from other treatments of\ndeep learning and causal inference in its sharp focus on observational causal\nestimation, its extended exposition of key algorithms, and its detailed\ntutorials for implementing, training, and selecting among deep estimators in\nTensorflow 2 available at github.com/kochbj/Deep-Learning-for-Causal-Inference.", "journal": ""}
{"doi": "10.48550/arXiv.2302.00842", "date": "2023-02-02", "title": "Effective Random Test Generation for Deep Learning Compilers", "authors": "Luyao Ren, ZiHeng Wang, Yingfei Xiong, Li Zhang, Guoyue Jiang, Tao Xie", "abstract": "Deep learning compilers help address the difficulties of deploying deep\nlearning models on diverse types of hardware. Testing deep learning compilers\nis highly crucial, because they are impacting countless AI applications that\nuse them for model optimization and deployment. To test deep learning\ncompilers, random testing, the testing method popularly used for compiler\ntesting practices, faces the challenge of generating semantically valid test\ninputs, i.e., deep learning models that satisfy the semantic model\nspecifications (in short as semantic specifications). To tackle this challenge,\nin this paper, we propose a novel approach named Isra, including a\ndomain-specific constraint solver that resolves the constraints from the\nsemantic specifications without backtracking. We implement and apply our\napproach to three popular real-world deep learning compilers including TVM,\nGlow, and a commercial compiler named SophGo. The evaluation results show that\nIsra is more effective than the state-of-the-art approaches and the baseline\napproaches on constructing valid test inputs for compiler-bug detection, and\nIsra successfully finds 24 previously unknown bugs in released versions of the\nthree compilers. These results indicate Isra's effectiveness and practical\nvalue.", "journal": ""}
{"doi": "10.48550/arXiv.2312.04289", "date": "2023-12-07", "title": "Fast simulation of airfoil flow field via deep neural network", "authors": "Kuijun Zuo, Zhengyin Ye, Shuhui Bu, Xianxu Yuan, Weiwei Zhang", "abstract": "Computational Fluid Dynamics (CFD) has become an indispensable tool in the\noptimization design, and evaluation of aircraft aerodynamics. However, solving\nthe Navier-Stokes (NS) equations is a time-consuming, memory demanding and\ncomputationally expensive task. Artificial intelligence offers a promising\navenue for flow field solving. In this work, we propose a novel deep learning\nframework for rapidly reconstructing airfoil flow fields. Channel attention and\nspatial attention modules are utilized in the downsampling stage of the UNet to\nenhance the feature learning capabilities of the deep learning model.\nAdditionally, integrating the predicted flow field values generated by the deep\nlearning model into the NS equation solver validates the credibility of the\nflow field prediction results. The NACA series airfoils were used to validate\nthe prediction accuracy and generalization of the deep learning model. The\nexperimental results represent the deep learning model achieving flow field\nprediction speeds three orders of magnitude faster than CFD solver.\nFurthermore, the CFD solver integrated with deep learning model demonstrates a\nthreefold acceleration compared to CFD solver. By extensively mining historical\nflow field data, an efficient solution is derived for the rapid simulation of\naircraft flow fields.", "journal": ""}
{"doi": "10.48550/arXiv.2405.09559", "date": "2024-05-02", "title": "KID-PPG: Knowledge Informed Deep Learning for Extracting Heart Rate from a Smartwatch", "authors": "Christodoulos Kechris, Jonathan Dan, Jose Miranda, David Atienza", "abstract": "Accurate extraction of heart rate from photoplethysmography (PPG) signals\nremains challenging due to motion artifacts and signal degradation. Although\ndeep learning methods trained as a data-driven inference problem offer\npromising solutions, they often underutilize existing knowledge from the\nmedical and signal processing community. In this paper, we address three\nshortcomings of deep learning models: motion artifact removal, degradation\nassessment, and physiologically plausible analysis of the PPG signal. We\npropose KID-PPG, a knowledge-informed deep learning model that integrates\nexpert knowledge through adaptive linear filtering, deep probabilistic\ninference, and data augmentation. We evaluate KID-PPG on the PPGDalia dataset,\nachieving an average mean absolute error of 2.85 beats per minute, surpassing\nexisting reproducible methods. Our results demonstrate a significant\nperformance improvement in heart rate tracking through the incorporation of\nprior knowledge into deep learning models. This approach shows promise in\nenhancing various biomedical applications by incorporating existing expert\nknowledge in deep learning models.", "journal": ""}
{"doi": "10.48550/arXiv.2407.00707", "date": "2024-06-30", "title": "Deep learning quantum Monte Carlo for solids", "authors": "Yubing Qian, Xiang Li, Zhe Li, Weiluo Ren, Ji Chen", "abstract": "Deep learning has deeply changed the paradigms of many research fields. At\nthe heart of chemical and physical sciences is the accurate ab initio\ncalculation of many-body wavefunction, which has become one of the most notable\nexamples to demonstrate the power of deep learning in science. In particular,\nthe introduction of deep learning into quantum Monte Carlo (QMC) has\nsignificantly advanced the frontier of ab initio calculation, offering a\nuniversal tool to solve the electronic structure of materials and molecules.\nDeep learning QMC architectures were initial designed and tested on small\nmolecules, focusing on comparisons with other state-of-the-art ab initio\nmethods. Methodological developments, including extensions to real solids and\nperiodic models, have been rapidly progressing and reported applications are\nfast expanding. This review covers the theoretical foundation of deep learning\nQMC for solids, the neural network wavefunction ansatz, and various of other\nmethodological developments. Applications on computing energy, electron\ndensity, electric polarization, force and stress of real solids are also\nreviewed. The methods have also been extended to other periodic systems and\nfinite temperature calculations. The review highlights the potentials and\nexisting challenges of deep learning QMC in materials chemistry and condensed\nmatter physics.", "journal": ""}
{"doi": "10.48550/arXiv.2409.07975", "date": "2024-09-12", "title": "Deep Learning for Personalized Electrocardiogram Diagnosis: A Review", "authors": "Cheng Ding, Tianliang Yao, Chenwei Wu, Jianyuan Ni", "abstract": "The electrocardiogram (ECG) remains a fundamental tool in cardiac\ndiagnostics, yet its interpretation traditionally reliant on the expertise of\ncardiologists. The emergence of deep learning has heralded a revolutionary era\nin medical data analysis, particularly in the domain of ECG diagnostics.\nHowever, inter-patient variability prohibit the generalibility of ECG-AI model\ntrained on a population dataset, hence degrade the performance of ECG-AI on\nspecific patient or patient group. Many studies have address this challenge\nusing different deep learning technologies. This comprehensive review\nsystematically synthesizes research from a wide range of studies to provide an\nin-depth examination of cutting-edge deep-learning techniques in personalized\nECG diagnosis. The review outlines a rigorous methodology for the selection of\npertinent scholarly articles and offers a comprehensive overview of deep\nlearning approaches applied to personalized ECG diagnostics. Moreover, the\nchallenges these methods encounter are investigated, along with future research\ndirections, culminating in insights into how the integration of deep learning\ncan transform personalized ECG diagnosis and enhance cardiac care. By\nemphasizing both the strengths and limitations of current methodologies, this\nreview underscores the immense potential of deep learning to refine and\nredefine ECG analysis in clinical practice, paving the way for more accurate,\nefficient, and personalized cardiac diagnostics.", "journal": ""}
{"doi": "10.48550/arXiv.2411.02893", "date": "2024-11-05", "title": "Generalization vs. Hallucination", "authors": "Xuyu Zhang, Haofan Huang, Dawei Zhang, Songlin Zhuang, Shensheng Han, Puxiang Lai, Honglin Liu", "abstract": "With fast developments in computational power and algorithms, deep learning\nhas made breakthroughs and been applied in many fields. However, generalization\nremains to be a critical challenge, and the limited generalization capability\nseverely constrains its practical applications. Hallucination issue is another\nunresolved conundrum haunting deep learning and large models. By leveraging a\nphysical model of imaging through scattering media, we studied the lack of\ngeneralization to system response functions in deep learning, identified its\ncause, and proposed a universal solution. The research also elucidates the\ncreation process of a hallucination in image prediction and reveals its cause,\nand the common relationship between generalization and hallucination is\ndiscovered and clarified. Generally speaking, it enhances the interpretability\nof deep learning from a physics-based perspective, and builds a universal\nphysical framework for deep learning in various fields. It may pave a way for\ndirect interaction between deep learning and the real world, facilitating the\ntransition of deep learning from a demo model to a practical tool in diverse\napplications.", "journal": ""}
{"doi": "10.48550/arXiv.2412.17349", "date": "2024-12-23", "title": "Deep Learning in Proteomics Informatics: Applications, Challenges, and Future Directions", "authors": "Yindan Luo, Jiaxin Cai", "abstract": "Deep learning is an advanced technology that relies on large-scale data and\ncomplex models for feature extraction and pattern recognition. It has been\nwidely applied across various fields, including computer vision, natural\nlanguage processing, and speech recognition. In recent years, deep learning has\ndemonstrated significant potential in the realm of proteomics informatics,\nparticularly in deciphering complex biological information. The introduction of\nthis technology not only accelerates the processing speed of protein data but\nalso enhances the accuracy of predictions regarding protein structure and\nfunction. This provides robust support for both fundamental biology research\nand applied biotechnological studies. Currently, deep learning is primarily\nfocused on applications such as protein sequence analysis, three-dimensional\nstructure prediction, functional annotation, and the construction of protein\ninteraction networks. These applications offer numerous advantages to proteomic\nresearch. Despite its growing prevalence in this field, deep learning faces\nseveral challenges including data scarcity, insufficient model\ninterpretability, and computational complexity; these factors hinder its\nfurther advancement within proteomics. This paper comprehensively reviews the\napplications of deep learning in proteomics along with the challenges it\nencounters. The aim is to provide a systematic theoretical discussion and\npractical basis for research in this domain to facilitate ongoing development\nand innovation of deep learning technologies within proteomics.", "journal": ""}
{"doi": "10.48550/arXiv.1501.03084", "date": "2015-01-13", "title": "Deep Learning with Nonparametric Clustering", "authors": "Gang Chen", "abstract": "Clustering is an essential problem in machine learning and data mining. One\nvital factor that impacts clustering performance is how to learn or design the\ndata representation (or features). Fortunately, recent advances in deep\nlearning can learn unsupervised features effectively, and have yielded state of\nthe art performance in many classification problems, such as character\nrecognition, object recognition and document categorization. However, little\nattention has been paid to the potential of deep learning for unsupervised\nclustering problems. In this paper, we propose a deep belief network with\nnonparametric clustering. As an unsupervised method, our model first leverages\nthe advantages of deep learning for feature representation and dimension\nreduction. Then, it performs nonparametric clustering under a maximum margin\nframework -- a discriminative clustering model and can be trained online\nefficiently in the code space. Lastly model parameters are refined in the deep\nbelief network. Thus, this model can learn features for clustering and infer\nmodel complexity in an unified framework. The experimental results show the\nadvantage of our approach over competitive baselines.", "journal": ""}
{"doi": "10.48550/arXiv.1710.08531", "date": "2017-10-23", "title": "Benchmark of Deep Learning Models on Large Healthcare MIMIC Datasets", "authors": "Sanjay Purushotham, Chuizheng Meng, Zhengping Che, Yan Liu", "abstract": "Deep learning models (aka Deep Neural Networks) have revolutionized many\nfields including computer vision, natural language processing, speech\nrecognition, and is being increasingly used in clinical healthcare\napplications. However, few works exist which have benchmarked the performance\nof the deep learning models with respect to the state-of-the-art machine\nlearning models and prognostic scoring systems on publicly available healthcare\ndatasets. In this paper, we present the benchmarking results for several\nclinical prediction tasks such as mortality prediction, length of stay\nprediction, and ICD-9 code group prediction using Deep Learning models,\nensemble of machine learning models (Super Learner algorithm), SAPS II and SOFA\nscores. We used the Medical Information Mart for Intensive Care III (MIMIC-III)\n(v1.4) publicly available dataset, which includes all patients admitted to an\nICU at the Beth Israel Deaconess Medical Center from 2001 to 2012, for the\nbenchmarking tasks. Our results show that deep learning models consistently\noutperform all the other approaches especially when the `raw' clinical time\nseries data is used as input features to the models.", "journal": ""}
{"doi": "10.48550/arXiv.1810.00368", "date": "2018-09-30", "title": "Deep Quality-Value (DQV) Learning", "authors": "Matthia Sabatelli, Gilles Louppe, Pierre Geurts, Marco A. Wiering", "abstract": "We introduce a novel Deep Reinforcement Learning (DRL) algorithm called Deep\nQuality-Value (DQV) Learning. DQV uses temporal-difference learning to train a\nValue neural network and uses this network for training a second Quality-value\nnetwork that learns to estimate state-action values. We first test DQV's update\nrules with Multilayer Perceptrons as function approximators on two classic RL\nproblems, and then extend DQV with the use of Deep Convolutional Neural\nNetworks, `Experience Replay' and `Target Neural Networks' for tackling four\ngames of the Atari Arcade Learning environment. Our results show that DQV\nlearns significantly faster and better than Deep Q-Learning and Double Deep\nQ-Learning, suggesting that our algorithm can potentially be a better\nperforming synchronous temporal difference algorithm than what is currently\npresent in DRL.", "journal": ""}
{"doi": "10.48550/arXiv.1805.01890", "date": "2018-05-03", "title": "RMDL: Random Multimodel Deep Learning for Classification", "authors": "Kamran Kowsari, Mojtaba Heidarysafa, Donald E. Brown, Kiana Jafari Meimandi, Laura E. Barnes", "abstract": "The continually increasing number of complex datasets each year necessitates\never improving machine learning methods for robust and accurate categorization\nof these data. This paper introduces Random Multimodel Deep Learning (RMDL): a\nnew ensemble, deep learning approach for classification. Deep learning models\nhave achieved state-of-the-art results across many domains. RMDL solves the\nproblem of finding the best deep learning structure and architecture while\nsimultaneously improving robustness and accuracy through ensembles of deep\nlearning architectures. RDML can accept as input a variety data to include\ntext, video, images, and symbolic. This paper describes RMDL and shows test\nresults for image and text data including MNIST, CIFAR-10, WOS, Reuters, IMDB,\nand 20newsgroup. These test results show that RDML produces consistently better\nperformance than standard methods over a broad range of data types and\nclassification problems.", "journal": ""}
{"doi": "10.48550/arXiv.1809.02069", "date": "2018-09-06", "title": "Deep learning for in vitro prediction of pharmaceutical formulations", "authors": "Yilong Yang, Zhuyifan Ye, Yan Su, Qianqian Zhao, Xiaoshan Li, Defang Ouyang", "abstract": "Current pharmaceutical formulation development still strongly relies on the\ntraditional trial-and-error approach by individual experiences of\npharmaceutical scientists, which is laborious, time-consuming and costly.\nRecently, deep learning has been widely applied in many challenging domains\nbecause of its important capability of automatic feature extraction. The aim of\nthis research is to use deep learning to predict pharmaceutical formulations.\nIn this paper, two different types of dosage forms were chosen as model\nsystems. Evaluation criteria suitable for pharmaceutics were applied to\nassessing the performance of the models. Moreover, an automatic dataset\nselection algorithm was developed for selecting the representative data as\nvalidation and test datasets. Six machine learning methods were compared with\ndeep learning. The result shows the accuracies of both two deep neural networks\nwere above 80% and higher than other machine learning models, which showed good\nprediction in pharmaceutical formulations. In summary, deep learning with the\nautomatic data splitting algorithm and the evaluation criteria suitable for\npharmaceutical formulation data was firstly developed for the prediction of\npharmaceutical formulations. The cross-disciplinary integration of\npharmaceutics and artificial intelligence may shift the paradigm of\npharmaceutical researches from experience-dependent studies to data-driven\nmethodologies.", "journal": ""}
{"doi": "10.48550/arXiv.1910.12417", "date": "2019-10-28", "title": "Deep causal representation learning for unsupervised domain adaptation", "authors": "Raha Moraffah, Kai Shu, Adrienne Raglin, Huan Liu", "abstract": "Studies show that the representations learned by deep neural networks can be\ntransferred to similar prediction tasks in other domains for which we do not\nhave enough labeled data. However, as we transition to higher layers in the\nmodel, the representations become more task-specific and less generalizable.\nRecent research on deep domain adaptation proposed to mitigate this problem by\nforcing the deep model to learn more transferable feature representations\nacross domains. This is achieved by incorporating domain adaptation methods\ninto deep learning pipeline. The majority of existing models learn the\ntransferable feature representations which are highly correlated with the\noutcome. However, correlations are not always transferable. In this paper, we\npropose a novel deep causal representation learning framework for unsupervised\ndomain adaptation, in which we propose to learn domain-invariant causal\nrepresentations of the input from the source domain. We simulate a virtual\ntarget domain using reweighted samples from the source domain and estimate the\ncausal effect of features on the outcomes. The extensive comparative study\ndemonstrates the strengths of the proposed model for unsupervised domain\nadaptation via causal representations.", "journal": ""}
{"doi": "10.48550/arXiv.2105.04026", "date": "2021-05-09", "title": "The Modern Mathematics of Deep Learning", "authors": "Julius Berner, Philipp Grohs, Gitta Kutyniok, Philipp Petersen", "abstract": "We describe the new field of mathematical analysis of deep learning. This\nfield emerged around a list of research questions that were not answered within\nthe classical framework of learning theory. These questions concern: the\noutstanding generalization power of overparametrized neural networks, the role\nof depth in deep architectures, the apparent absence of the curse of\ndimensionality, the surprisingly successful optimization performance despite\nthe non-convexity of the problem, understanding what features are learned, why\ndeep architectures perform exceptionally well in physical problems, and which\nfine aspects of an architecture affect the behavior of a learning task in which\nway. We present an overview of modern approaches that yield partial answers to\nthese questions. For selected approaches, we describe the main ideas in more\ndetail.", "journal": "Mathematical Aspects of Deep Learning, pp. 1-111. Cambridge\n  University Press, 2022"}
{"doi": "10.48550/arXiv.1811.12560", "date": "2018-11-30", "title": "An Introduction to Deep Reinforcement Learning", "authors": "Vincent Francois-Lavet, Peter Henderson, Riashat Islam, Marc G. Bellemare, Joelle Pineau", "abstract": "Deep reinforcement learning is the combination of reinforcement learning (RL)\nand deep learning. This field of research has been able to solve a wide range\nof complex decision-making tasks that were previously out of reach for a\nmachine. Thus, deep RL opens up many new applications in domains such as\nhealthcare, robotics, smart grids, finance, and many more. This manuscript\nprovides an introduction to deep reinforcement learning models, algorithms and\ntechniques. Particular focus is on the aspects related to generalization and\nhow deep RL can be used for practical applications. We assume the reader is\nfamiliar with basic machine learning concepts.", "journal": "Foundations and Trends in Machine Learning: Vol. 11, No. 3-4, 2018"}
{"doi": "10.48550/arXiv.2001.01432", "date": "2020-01-06", "title": "Deep Learning-Based Solvability of Underdetermined Inverse Problems in Medical Imaging", "authors": "Chang Min Hyun, Seong Hyeon Baek, Mingyu Lee, Sung Min Lee, Jin Keun Seo", "abstract": "Recently, with the significant developments in deep learning techniques,\nsolving underdetermined inverse problems has become one of the major concerns\nin the medical imaging domain. Typical examples include undersampled magnetic\nresonance imaging, interior tomography, and sparse-view computed tomography,\nwhere deep learning techniques have achieved excellent performances. Although\ndeep learning methods appear to overcome the limitations of existing\nmathematical methods when handling various underdetermined problems, there is a\nlack of rigorous mathematical foundations that would allow us to elucidate the\nreasons for the remarkable performance of deep learning methods. This study\nfocuses on learning the causal relationship regarding the structure of the\ntraining data suitable for deep learning, to solve highly underdetermined\ninverse problems. We observe that a majority of the problems of solving\nunderdetermined linear systems in medical imaging are highly non-linear.\nFurthermore, we analyze if a desired reconstruction map can be learnable from\nthe training data and underdetermined system.", "journal": ""}
{"doi": "10.48550/arXiv.2001.10362", "date": "2020-01-27", "title": "The Final Frontier: Deep Learning in Space", "authors": "Vivek Kothari, Edgar Liberis, Nicholas D. Lane", "abstract": "Machine learning, particularly deep learning, is being increasing utilised in\nspace applications, mirroring the groundbreaking success in many earthbound\nproblems. Deploying a space device, e.g. a satellite, is becoming more\naccessible to small actors due to the development of modular satellites and\ncommercial space launches, which fuels further growth of this area. Deep\nlearning's ability to deliver sophisticated computational intelligence makes it\nan attractive option to facilitate various tasks on space devices and reduce\noperational costs. In this work, we identify deep learning in space as one of\ndevelopment directions for mobile and embedded machine learning. We collate\nvarious applications of machine learning to space data, such as satellite\nimaging, and describe how on-device deep learning can meaningfully improve the\noperation of a spacecraft, such as by reducing communication costs or\nfacilitating navigation. We detail and contextualise compute platform of\nsatellites and draw parallels with embedded systems and current research in\ndeep learning for resource-constrained environments.", "journal": ""}
{"doi": "10.48550/arXiv.2004.10780", "date": "2020-04-22", "title": "Diagram Image Retrieval using Sketch-Based Deep Learning and Transfer Learning", "authors": "Manish Bhattarai, Diane Oyen, Juan Castorena, Liping Yang, Brendt Wohlberg", "abstract": "Resolution of the complex problem of image retrieval for diagram images has\nyet to be reached. Deep learning methods continue to excel in the fields of\nobject detection and image classification applied to natural imagery. However,\nthe application of such methodologies applied to binary imagery remains limited\ndue to lack of crucial features such as textures,color and intensity\ninformation. This paper presents a deep learning based method for image-based\nsearch for binary patent images by taking advantage of existing large natural\nimage repositories for image search and sketch-based methods (Sketches are not\nidentical to diagrams, but they do share some characteristics; for example,\nboth imagery types are gray scale (binary), composed of contours, and are\nlacking in texture).\n  We begin by using deep learning to generate sketches from natural images for\nimage retrieval and then train a second deep learning model on the sketches. We\nthen use our small set of manually labeled patent diagram images via transfer\nlearning to adapt the image search from sketches of natural images to diagrams.\nOur experiment results show the effectiveness of deep learning with transfer\nlearning for detecting near-identical copies in patent images and querying\nsimilar images based on content.", "journal": ""}
{"doi": "10.48550/arXiv.2009.06681", "date": "2020-09-14", "title": "Deep Actor-Critic Learning for Distributed Power Control in Wireless Mobile Networks", "authors": "Yasar Sinan Nasir, Dongning Guo", "abstract": "Deep reinforcement learning offers a model-free alternative to supervised\ndeep learning and classical optimization for solving the transmit power control\nproblem in wireless networks. The multi-agent deep reinforcement learning\napproach considers each transmitter as an individual learning agent that\ndetermines its transmit power level by observing the local wireless\nenvironment. Following a certain policy, these agents learn to collaboratively\nmaximize a global objective, e.g., a sum-rate utility function. This\nmulti-agent scheme is easily scalable and practically applicable to large-scale\ncellular networks. In this work, we present a distributively executed\ncontinuous power control algorithm with the help of deep actor-critic learning,\nand more specifically, by adapting deep deterministic policy gradient.\nFurthermore, we integrate the proposed power control algorithm to a\ntime-slotted system where devices are mobile and channel conditions change\nrapidly. We demonstrate the functionality of the proposed algorithm using\nsimulation results.", "journal": ""}
{"doi": "10.48550/arXiv.2106.09461", "date": "2021-06-17", "title": "Modelling resource allocation in uncertain system environment through deep reinforcement learning", "authors": "Neel Gandhi, Shakti Mishra", "abstract": "Reinforcement Learning has applications in field of mechatronics, robotics,\nand other resource-constrained control system. Problem of resource allocation\nis primarily solved using traditional predefined techniques and modern deep\nlearning methods. The drawback of predefined and most deep learning methods for\nresource allocation is failing to meet the requirements in cases of uncertain\nsystem environment. We can approach problem of resource allocation in uncertain\nsystem environment alongside following certain criteria using deep\nreinforcement learning. Also, reinforcement learning has ability for adapting\nto new uncertain environment for prolonged period of time. The paper provides a\ndetailed comparative analysis on various deep reinforcement learning methods by\napplying different components to modify architecture of reinforcement learning\nwith use of noisy layers, prioritized replay, bagging, duelling networks, and\nother related combination to obtain improvement in terms of performance and\nreduction of computational cost. The paper identifies problem of resource\nallocation in uncertain environment could be effectively solved using Noisy\nBagging duelling double deep Q network achieving efficiency of 97.7% by\nmaximizing reward with significant exploration in given simulated environment\nfor resource allocation.", "journal": ""}
{"doi": "10.48550/arXiv.2302.01440", "date": "2023-02-02", "title": "Generalized Uncertainty of Deep Neural Networks: Taxonomy and Applications", "authors": "Chengyu Dong", "abstract": "Deep neural networks have seen enormous success in various real-world\napplications. Beyond their predictions as point estimates, increasing attention\nhas been focused on quantifying the uncertainty of their predictions. In this\nreview, we show that the uncertainty of deep neural networks is not only\nimportant in a sense of interpretability and transparency, but also crucial in\nfurther advancing their performance, particularly in learning systems seeking\nrobustness and efficiency. We will generalize the definition of the uncertainty\nof deep neural networks to any number or vector that is associated with an\ninput or an input-label pair, and catalog existing methods on ``mining'' such\nuncertainty from a deep model. We will include those methods from the classic\nfield of uncertainty quantification as well as those methods that are specific\nto deep neural networks. We then show a wide spectrum of applications of such\ngeneralized uncertainty in realistic learning tasks including robust learning\nsuch as noisy learning, adversarially robust learning; data-efficient learning\nsuch as semi-supervised and weakly-supervised learning; and model-efficient\nlearning such as model compression and knowledge distillation.", "journal": ""}
{"doi": "10.48550/arXiv.2302.11075", "date": "2023-02-22", "title": "Deep Active Learning in the Presence of Label Noise: A Survey", "authors": "Moseli Mots'oehli, Kyungim Baek", "abstract": "Deep active learning has emerged as a powerful tool for training deep\nlearning models within a predefined labeling budget. These models have achieved\nperformances comparable to those trained in an offline setting. However, deep\nactive learning faces substantial issues when dealing with classification\ndatasets containing noisy labels. In this literature review, we discuss the\ncurrent state of deep active learning in the presence of label noise,\nhighlighting unique approaches, their strengths, and weaknesses. With the\nrecent success of vision transformers in image classification tasks, we provide\na brief overview and consider how the transformer layers and attention\nmechanisms can be used to enhance diversity, importance, and uncertainty-based\nselection in queries sent to an oracle for labeling. We further propose\nexploring contrastive learning methods to derive good image representations\nthat can aid in selecting high-value samples for labeling in an active learning\nsetting. We also highlight the need for creating unified benchmarks and\nstandardized datasets for deep active learning in the presence of label noise\nfor image classification to promote the reproducibility of research. The review\nconcludes by suggesting avenues for future research in this area.", "journal": ""}
{"doi": "10.48550/arXiv.1903.04959", "date": "2019-03-12", "title": "Deep Multi-Agent Reinforcement Learning with Discrete-Continuous Hybrid Action Spaces", "authors": "Haotian Fu, Hongyao Tang, Jianye Hao, Zihan Lei, Yingfeng Chen, Changjie Fan", "abstract": "Deep Reinforcement Learning (DRL) has been applied to address a variety of\ncooperative multi-agent problems with either discrete action spaces or\ncontinuous action spaces. However, to the best of our knowledge, no previous\nwork has ever succeeded in applying DRL to multi-agent problems with\ndiscrete-continuous hybrid (or parameterized) action spaces which is very\ncommon in practice. Our work fills this gap by proposing two novel algorithms:\nDeep Multi-Agent Parameterized Q-Networks (Deep MAPQN) and Deep Multi-Agent\nHierarchical Hybrid Q-Networks (Deep MAHHQN). We follow the centralized\ntraining but decentralized execution paradigm: different levels of\ncommunication between different agents are used to facilitate the training\nprocess, while each agent executes its policy independently based on local\nobservations during execution. Our empirical results on several challenging\ntasks (simulated RoboCup Soccer and game Ghost Story) show that both Deep MAPQN\nand Deep MAHHQN are effective and significantly outperform existing independent\ndeep parameterized Q-learning method.", "journal": "IJCAI 2019"}
{"doi": "10.48550/arXiv.2412.08933", "date": "2024-12-12", "title": "Deep clustering using adversarial net based clustering loss", "authors": "Kart-Leong Lim", "abstract": "Deep clustering is a recent deep learning technique which combines deep\nlearning with traditional unsupervised clustering. At the heart of deep\nclustering is a loss function which penalizes samples for being an outlier from\ntheir ground truth cluster centers in the latent space. The probabilistic\nvariant of deep clustering reformulates the loss using KL divergence. Often,\nthe main constraint of deep clustering is the necessity of a closed form loss\nfunction to make backpropagation tractable. Inspired by deep clustering and\nadversarial net, we reformulate deep clustering as an adversarial net over\ntraditional closed form KL divergence. Training deep clustering becomes a task\nof minimizing the encoder and maximizing the discriminator. At optimality, this\nmethod theoretically approaches the JS divergence between the distribution\nassumption of the encoder and the discriminator. We demonstrated the\nperformance of our proposed method on several well cited datasets such as\nMNIST, REUTERS10K and CIFAR10, achieving on-par or better performance with some\nof the state-of-the-art deep clustering methods.", "journal": ""}
{"doi": "10.48550/arXiv.1904.12462", "date": "2019-04-29", "title": "Deep Learning-Based Video Coding: A Review and A Case Study", "authors": "Dong Liu, Yue Li, Jianping Lin, Houqiang Li, Feng Wu", "abstract": "The past decade has witnessed great success of deep learning technology in\nmany disciplines, especially in computer vision and image processing. However,\ndeep learning-based video coding remains in its infancy. This paper reviews the\nrepresentative works about using deep learning for image/video coding, which\nhas been an actively developing research area since the year of 2015. We divide\nthe related works into two categories: new coding schemes that are built\nprimarily upon deep networks (deep schemes), and deep network-based coding\ntools (deep tools) that shall be used within traditional coding schemes or\ntogether with traditional coding tools. For deep schemes, pixel probability\nmodeling and auto-encoder are the two approaches, that can be viewed as\npredictive coding scheme and transform coding scheme, respectively. For deep\ntools, there have been several proposed techniques using deep learning to\nperform intra-picture prediction, inter-picture prediction, cross-channel\nprediction, probability distribution prediction, transform, post- or in-loop\nfiltering, down- and up-sampling, as well as encoding optimizations. In the\nhope of advocating the research of deep learning-based video coding, we present\na case study of our developed prototype video codec, namely Deep Learning Video\nCoding (DLVC). DLVC features two deep tools that are both based on\nconvolutional neural network (CNN), namely CNN-based in-loop filter (CNN-ILF)\nand CNN-based block adaptive resolution coding (CNN-BARC). Both tools help\nimprove the compression efficiency by a significant margin. With the two deep\ntools as well as other non-deep coding tools, DLVC is able to achieve on\naverage 39.6\\% and 33.0\\% bits saving than HEVC, under random-access and\nlow-delay configurations, respectively. The source code of DLVC has been\nreleased for future researches.", "journal": "ACM Computing Surveys, vol 53, no 1, article no 11, Feb 2020"}
{"doi": "10.48550/arXiv.2401.03069", "date": "2024-01-05", "title": "Towards Enhancing the Reproducibility of Deep Learning Bugs: An Empirical Study", "authors": "Mehil B. Shah, Mohammad Masudur Rahman, Foutse Khomh", "abstract": "Context: Deep learning has achieved remarkable progress in various domains.\nHowever, like any software system, deep learning systems contain bugs, some of\nwhich can have severe impacts, as evidenced by crashes involving autonomous\nvehicles. Despite substantial advancements in deep learning techniques, little\nresearch has focused on reproducing deep learning bugs, which is an essential\nstep for their resolution. Existing literature suggests that only 3% of deep\nlearning bugs are reproducible, underscoring the need for further research.\n  Objective: This paper examines the reproducibility of deep learning bugs. We\nidentify edit actions and useful information that could improve the\nreproducibility of deep learning bugs.\n  Method: First, we construct a dataset of 668 deep-learning bugs from Stack\nOverflow and GitHub across three frameworks and 22 architectures. Second, out\nof the 668 bugs, we select 165 bugs using stratified sampling and attempt to\ndetermine their reproducibility. While reproducing these bugs, we identify edit\nactions and useful information for their reproduction. Third, we used the\nApriori algorithm to identify useful information and edit actions required to\nreproduce specific types of bugs. Finally, we conducted a user study involving\n22 developers to assess the effectiveness of our findings in real-life\nsettings.\n  Results: We successfully reproduced 148 out of 165 bugs attempted. We\nidentified ten edit actions and five useful types of component information that\ncan help us reproduce the deep learning bugs. With the help of our findings,\nthe developers were able to reproduce 22.92% more bugs and reduce their\nreproduction time by 24.35%.\n  Conclusions: Our research addresses the critical issue of deep learning bug\nreproducibility. Practitioners and researchers can leverage our findings to\nimprove deep learning bug reproducibility.", "journal": ""}
{"doi": "10.48550/arXiv.1807.00051", "date": "2018-06-29", "title": "Adversarial Examples in Deep Learning: Characterization and Divergence", "authors": "Wenqi Wei, Ling Liu, Margaret Loper, Stacey Truex, Lei Yu, Mehmet Emre Gursoy, Yanzhao Wu", "abstract": "The burgeoning success of deep learning has raised the security and privacy\nconcerns as more and more tasks are accompanied with sensitive data.\nAdversarial attacks in deep learning have emerged as one of the dominating\nsecurity threat to a range of mission-critical deep learning systems and\napplications. This paper takes a holistic and principled approach to perform\nstatistical characterization of adversarial examples in deep learning. We\nprovide a general formulation of adversarial examples and elaborate on the\nbasic principle for adversarial attack algorithm design. We introduce easy and\nhard categorization of adversarial attacks to analyze the effectiveness of\nadversarial examples in terms of attack success rate, degree of change in\nadversarial perturbation, average entropy of prediction qualities, and fraction\nof adversarial examples that lead to successful attacks. We conduct extensive\nexperimental study on adversarial behavior in easy and hard attacks under deep\nlearning models with different hyperparameters and different deep learning\nframeworks. We show that the same adversarial attack behaves differently under\ndifferent hyperparameters and across different frameworks due to the different\nfeatures learned under different deep learning model training process. Our\nstatistical characterization with strong empirical evidence provides a\ntransformative enlightenment on mitigation strategies towards effective\ncountermeasures against present and future adversarial attacks.", "journal": ""}
{"doi": "10.48550/arXiv.1906.04278", "date": "2019-06-10", "title": "Performance Analysis and Characterization of Training Deep Learning Models on Mobile Devices", "authors": "Jie Liu, Jiawen Liu, Wan Du, Dong Li", "abstract": "Training deep learning models on mobile devices recently becomes possible,\nbecause of increasing computation power on mobile hardware and the advantages\nof enabling high user experiences. Most of the existing work on machine\nlearning at mobile devices is focused on the inference of deep learning models\n(particularly convolutional neural network and recurrent neural network), but\nnot training. The performance characterization of training deep learning models\non mobile devices is largely unexplored, although understanding the performance\ncharacterization is critical for designing and implementing deep learning\nmodels on mobile devices.\n  In this paper, we perform a variety of experiments on a representative mobile\ndevice (the NVIDIA TX2) to study the performance of training deep learning\nmodels. We introduce a benchmark suite and tools to study performance of\ntraining deep learning models on mobile devices, from the perspectives of\nmemory consumption, hardware utilization, and power consumption. The tools can\ncorrelate performance results with fine-grained operations in deep learning\nmodels, providing capabilities to capture performance variance and problems at\na fine granularity. We reveal interesting performance problems and\nopportunities, including under-utilization of heterogeneous hardware, large\nenergy consumption of the memory, and high predictability of workload\ncharacterization. Based on the performance analysis, we suggest interesting\nresearch directions.", "journal": ""}
{"doi": "10.48550/arXiv.2001.03359", "date": "2020-01-10", "title": "Deep Interactive Reinforcement Learning for Path Following of Autonomous Underwater Vehicle", "authors": "Qilei Zhang, Jinying Lin, Qixin Sha, Bo He, Guangliang Li", "abstract": "Autonomous underwater vehicle (AUV) plays an increasingly important role in\nocean exploration. Existing AUVs are usually not fully autonomous and generally\nlimited to pre-planning or pre-programming tasks. Reinforcement learning (RL)\nand deep reinforcement learning have been introduced into the AUV design and\nresearch to improve its autonomy. However, these methods are still difficult to\napply directly to the actual AUV system because of the sparse rewards and low\nlearning efficiency. In this paper, we proposed a deep interactive\nreinforcement learning method for path following of AUV by combining the\nadvantages of deep reinforcement learning and interactive RL. In addition,\nsince the human trainer cannot provide human rewards for AUV when it is running\nin the ocean and AUV needs to adapt to a changing environment, we further\npropose a deep reinforcement learning method that learns from both human\nrewards and environmental rewards at the same time. We test our methods in two\npath following tasks---straight line and sinusoids curve following of AUV by\nsimulating in the Gazebo platform. Our experimental results show that with our\nproposed deep interactive RL method, AUV can converge faster than a DQN learner\nfrom only environmental reward. Moreover, AUV learning with our deep RL from\nboth human and environmental rewards can also achieve a similar or even better\nperformance than that with the deep interactive RL method and can adapt to the\nactual environment by further learning from environmental rewards.", "journal": ""}
{"doi": "10.48550/arXiv.2012.13321", "date": "2020-12-24", "title": "Unsupervised deep clustering and reinforcement learning can accurately segment MRI brain tumors with very small training sets", "authors": "Joseph Stember, Hrithwik Shalu", "abstract": "Purpose: Lesion segmentation in medical imaging is key to evaluating\ntreatment response. We have recently shown that reinforcement learning can be\napplied to radiological images for lesion localization. Furthermore, we\ndemonstrated that reinforcement learning addresses important limitations of\nsupervised deep learning; namely, it can eliminate the requirement for large\namounts of annotated training data and can provide valuable intuition lacking\nin supervised approaches. However, we did not address the fundamental task of\nlesion/structure-of-interest segmentation. Here we introduce a method combining\nunsupervised deep learning clustering with reinforcement learning to segment\nbrain lesions on MRI.\n  Materials and Methods: We initially clustered images using unsupervised deep\nlearning clustering to generate candidate lesion masks for each MRI image. The\nuser then selected the best mask for each of 10 training images. We then\ntrained a reinforcement learning algorithm to select the masks. We tested the\ncorresponding trained deep Q network on a separate testing set of 10 images.\nFor comparison, we also trained and tested a U-net supervised deep learning\nnetwork on the same set of training/testing images.\n  Results: Whereas the supervised approach quickly overfit the training data\nand predictably performed poorly on the testing set (16% average Dice score),\nthe unsupervised deep clustering and reinforcement learning achieved an average\nDice score of 83%.\n  Conclusion: We have demonstrated a proof-of-principle application of\nunsupervised deep clustering and reinforcement learning to segment brain\ntumors. The approach represents human-allied AI that requires minimal input\nfrom the radiologist without the need for hand-traced annotation.", "journal": ""}
{"doi": "10.48550/arXiv.2403.05314", "date": "2024-03-08", "title": "Advances of Deep Learning in Protein Science: A Comprehensive Survey", "authors": "Bozhen Hu, Cheng Tan, Lirong Wu, Jiangbin Zheng, Jun Xia, Zhangyang Gao, Zicheng Liu, Fandi Wu, Guijun Zhang, Stan Z. Li", "abstract": "Protein representation learning plays a crucial role in understanding the\nstructure and function of proteins, which are essential biomolecules involved\nin various biological processes. In recent years, deep learning has emerged as\na powerful tool for protein modeling due to its ability to learn complex\npatterns and representations from large-scale protein data. This comprehensive\nsurvey aims to provide an overview of the recent advances in deep learning\ntechniques applied to protein science. The survey begins by introducing the\ndevelopments of deep learning based protein models and emphasizes the\nimportance of protein representation learning in drug discovery, protein\nengineering, and function annotation. It then delves into the fundamentals of\ndeep learning, including convolutional neural networks, recurrent neural\nnetworks, attention models, and graph neural networks in modeling protein\nsequences, structures, and functions, and explores how these techniques can be\nused to extract meaningful features and capture intricate relationships within\nprotein data. Next, the survey presents various applications of deep learning\nin the field of proteins, including protein structure prediction,\nprotein-protein interaction prediction, protein function prediction, etc.\nFurthermore, it highlights the challenges and limitations of these deep\nlearning techniques and also discusses potential solutions and future\ndirections for overcoming these challenges. This comprehensive survey provides\na valuable resource for researchers and practitioners in the field of proteins\nwho are interested in harnessing the power of deep learning techniques. By\nconsolidating the latest advancements and discussing potential avenues for\nimprovement, this review contributes to the ongoing progress in protein\nresearch and paves the way for future breakthroughs in the field.", "journal": ""}
{"doi": "10.48550/arXiv.1709.01574", "date": "2017-09-05", "title": "Opening the Black Box of Financial AI with CLEAR-Trade: A CLass-Enhanced Attentive Response Approach for Explaining and Visualizing Deep Learning-Driven Stock Market Prediction", "authors": "Devinder Kumar, Graham W Taylor, Alexander Wong", "abstract": "Deep learning has been shown to outperform traditional machine learning\nalgorithms across a wide range of problem domains. However, current deep\nlearning algorithms have been criticized as uninterpretable \"black-boxes\" which\ncannot explain their decision making processes. This is a major shortcoming\nthat prevents the widespread application of deep learning to domains with\nregulatory processes such as finance. As such, industries such as finance have\nto rely on traditional models like decision trees that are much more\ninterpretable but less effective than deep learning for complex problems. In\nthis paper, we propose CLEAR-Trade, a novel financial AI visualization\nframework for deep learning-driven stock market prediction that mitigates the\ninterpretability issue of deep learning methods. In particular, CLEAR-Trade\nprovides a effective way to visualize and explain decisions made by deep stock\nmarket prediction models. We show the efficacy of CLEAR-Trade in enhancing the\ninterpretability of stock market prediction by conducting experiments based on\nS&P 500 stock index prediction. The results demonstrate that CLEAR-Trade can\nprovide significant insight into the decision-making process of deep\nlearning-driven financial models, particularly for regulatory processes, thus\nimproving their potential uptake in the financial industry.", "journal": ""}
{"doi": "10.48550/arXiv.1910.00121", "date": "2019-09-30", "title": "Full error analysis for the training of deep neural networks", "authors": "Christan Beck, Arnulf Jentzen, Benno Kuckuck", "abstract": "Deep learning algorithms have been applied very successfully in recent years\nto a range of problems out of reach for classical solution paradigms.\nNevertheless, there is no completely rigorous mathematical error and\nconvergence analysis which explains the success of deep learning algorithms.\nThe error of a deep learning algorithm can in many situations be decomposed\ninto three parts, the approximation error, the generalization error, and the\noptimization error. In this work we estimate for a certain deep learning\nalgorithm each of these three errors and combine these three error estimates to\nobtain an overall error analysis for the deep learning algorithm under\nconsideration. In particular, we thereby establish convergence with a suitable\nconvergence speed for the overall error of the deep learning algorithm under\nconsideration. Our convergence speed analysis is far from optimal and the\nconvergence speed that we establish is rather slow, increases exponentially in\nthe dimensions, and, in particular, suffers from the curse of dimensionality.\nThe main contribution of this work is, instead, to provide a full error\nanalysis (i) which covers each of the three different sources of errors usually\nemerging in deep learning algorithms and (ii) which merges these three sources\nof errors into one overall error estimate for the considered deep learning\nalgorithm.", "journal": "Infin. Dimens. Anal. Quantum Probab. Relat. Top. 25 (2022), no. 2,\n  2150020, 77 pp"}
{"doi": "10.48550/arXiv.2001.06280", "date": "2020-01-17", "title": "Review: deep learning on 3D point clouds", "authors": "Saifullahi Aminu Bello, Shangshu Yu, Cheng Wang", "abstract": "Point cloud is point sets defined in 3D metric space. Point cloud has become\none of the most significant data format for 3D representation. Its gaining\nincreased popularity as a result of increased availability of acquisition\ndevices, such as LiDAR, as well as increased application in areas such as\nrobotics, autonomous driving, augmented and virtual reality. Deep learning is\nnow the most powerful tool for data processing in computer vision, becoming the\nmost preferred technique for tasks such as classification, segmentation, and\ndetection. While deep learning techniques are mainly applied to data with a\nstructured grid, point cloud, on the other hand, is unstructured. The\nunstructuredness of point clouds makes use of deep learning for its processing\ndirectly very challenging. Earlier approaches overcome this challenge by\npreprocessing the point cloud into a structured grid format at the cost of\nincreased computational cost or lost of depth information. Recently, however,\nmany state-of-the-arts deep learning techniques that directly operate on point\ncloud are being developed. This paper contains a survey of the recent\nstate-of-the-art deep learning techniques that mainly focused on point cloud\ndata. We first briefly discussed the major challenges faced when using deep\nlearning directly on point cloud, we also briefly discussed earlier approaches\nwhich overcome the challenges by preprocessing the point cloud into a\nstructured grid. We then give the review of the various state-of-the-art deep\nlearning approaches that directly process point cloud in its unstructured form.\nWe introduced the popular 3D point cloud benchmark datasets. And we also\nfurther discussed the application of deep learning in popular 3D vision tasks\nincluding classification, segmentation and detection.", "journal": ""}
{"doi": "10.48550/arXiv.2011.14597", "date": "2020-11-30", "title": "A Survey on Deep Learning for Software Engineering", "authors": "Yanming Yang, Xin Xia, David Lo, John Grundy", "abstract": "In 2006, Geoffrey Hinton proposed the concept of training ''Deep Neural\nNetworks (DNNs)'' and an improved model training method to break the bottleneck\nof neural network development. More recently, the introduction of AlphaGo in\n2016 demonstrated the powerful learning ability of deep learning and its\nenormous potential. Deep learning has been increasingly used to develop\nstate-of-the-art software engineering (SE) research tools due to its ability to\nboost performance for various SE tasks. There are many factors, e.g., deep\nlearning model selection, internal structure differences, and model\noptimization techniques, that may have an impact on the performance of DNNs\napplied in SE. Few works to date focus on summarizing, classifying, and\nanalyzing the application of deep learning techniques in SE. To fill this gap,\nwe performed a survey to analyse the relevant studies published since 2006. We\nfirst provide an example to illustrate how deep learning techniques are used in\nSE. We then summarize and classify different deep learning techniques used in\nSE. We analyzed key optimization technologies used in these deep learning\nmodels, and finally describe a range of key research topics using DNNs in SE.\nBased on our findings, we present a set of current challenges remaining to be\ninvestigated and outline a proposed research road map highlighting key\nopportunities for future work.", "journal": ""}
{"doi": "10.48550/arXiv.2203.15076", "date": "2022-03-28", "title": "Neurosymbolic hybrid approach to driver collision warning", "authors": "Kyongsik Yun, Thomas Lu, Alexander Huyen, Patrick Hammer, Pei Wang", "abstract": "There are two main algorithmic approaches to autonomous driving systems: (1)\nAn end-to-end system in which a single deep neural network learns to map\nsensory input directly into appropriate warning and driving responses. (2) A\nmediated hybrid recognition system in which a system is created by combining\nindependent modules that detect each semantic feature. While some researchers\nbelieve that deep learning can solve any problem, others believe that a more\nengineered and symbolic approach is needed to cope with complex environments\nwith less data. Deep learning alone has achieved state-of-the-art results in\nmany areas, from complex gameplay to predicting protein structures. In\nparticular, in image classification and recognition, deep learning models have\nachieved accuracies as high as humans. But sometimes it can be very difficult\nto debug if the deep learning model doesn't work. Deep learning models can be\nvulnerable and are very sensitive to changes in data distribution.\nGeneralization can be problematic. It's usually hard to prove why it works or\ndoesn't. Deep learning models can also be vulnerable to adversarial attacks.\nHere, we combine deep learning-based object recognition and tracking with an\nadaptive neurosymbolic network agent, called the Non-Axiomatic Reasoning System\n(NARS), that can adapt to its environment by building concepts based on\nperceptual sequences. We achieved an improved intersection-over-union (IOU)\nobject recognition performance of 0.65 in the adaptive retraining model\ncompared to IOU 0.31 in the COCO data pre-trained model. We improved the object\ndetection limits using RADAR sensors in a simulated environment, and\ndemonstrated the weaving car detection capability by combining deep\nlearning-based object detection and tracking with a neurosymbolic model.", "journal": ""}
{"doi": "10.48550/arXiv.2209.13233", "date": "2022-09-27", "title": "Genetic Programming-Based Evolutionary Deep Learning for Data-Efficient Image Classification", "authors": "Ying Bi, Bing Xue, Mengjie Zhang", "abstract": "Data-efficient image classification is a challenging task that aims to solve\nimage classification using small training data. Neural network-based deep\nlearning methods are effective for image classification, but they typically\nrequire large-scale training data and have major limitations such as requiring\nexpertise to design network architectures and having poor interpretability.\nEvolutionary deep learning is a recent hot topic that combines evolutionary\ncomputation with deep learning. However, most evolutionary deep learning\nmethods focus on evolving architectures of neural networks, which still suffer\nfrom limitations such as poor interpretability. To address this, this paper\nproposes a new genetic programming-based evolutionary deep learning approach to\ndata-efficient image classification. The new approach can automatically evolve\nvariable-length models using many important operators from both image and\nclassification domains. It can learn different types of image features from\ncolour or gray-scale images, and construct effective and diverse ensembles for\nimage classification. A flexible multi-layer representation enables the new\napproach to automatically construct shallow or deep models/trees for different\ntasks and perform effective transformations on the input data via multiple\ninternal nodes. The new approach is applied to solve five image classification\ntasks with different training set sizes. The results show that it achieves\nbetter performance in most cases than deep learning methods for data-efficient\nimage classification. A deep analysis shows that the new approach has good\nconvergence and evolves models with high interpretability, different\nlengths/sizes/shapes, and good transferability.", "journal": "IEEE Transactions on Evolutionary Computation, 2022,\n  https://ieeexplore.ieee.org/document/9919314"}
{"doi": "10.48550/arXiv.2311.13744", "date": "2023-11-23", "title": "Security and Privacy Challenges in Deep Learning Models", "authors": "Gopichandh Golla", "abstract": "These days, deep learning models have achieved great success in multiple\nfields, from autonomous driving to medical diagnosis. These models have\nexpanded the abilities of artificial intelligence by offering great solutions\nto complex problems that were very difficult to solve earlier. In spite of\ntheir unseen success in various, it has been identified, through research\nconducted, that deep learning models can be subjected to various attacks that\ncompromise model security and data privacy of the Deep Neural Network models.\nDeep learning models can be subjected to various attacks at different stages of\ntheir lifecycle. During the testing phase, attackers can exploit\nvulnerabilities through different kinds of attacks such as Model Extraction\nAttacks, Model Inversion attacks, and Adversarial attacks. Model Extraction\nAttacks are aimed at reverse-engineering a trained deep learning model, with\nthe primary objective of revealing its architecture and parameters. Model\ninversion attacks aim to compromise the privacy of the data used in the Deep\nlearning model. These attacks are done to compromise the confidentiality of the\nmodel by going through the sensitive training data from the model's\npredictions. By analyzing the model's responses, attackers aim to reconstruct\nsensitive information. In this way, the model's data privacy is compromised.\nAdversarial attacks, mainly employed on computer vision models, are made to\ncorrupt models into confidently making incorrect predictions through malicious\ntesting data. These attacks subtly alter the input data, making it look normal\nbut misleading deep learning models to make incorrect decisions. Such attacks\ncan happen during both the model's evaluation and training phases. Data\nPoisoning Attacks add harmful data to the training set, disrupting the learning\nprocess and reducing the reliability of the deep learning mode.", "journal": ""}
{"doi": "10.48550/arXiv.2405.04861", "date": "2024-05-08", "title": "Insights into Deep Learning Refactoring: Bridging the Gap Between Practices and Expectations", "authors": "SiQi Wang, Xing Hu, Bei Wang, WenXin Yao, Xin Xia, XingYu Wang", "abstract": "With the rapid development of deep learning, the implementation of intricate\nalgorithms and substantial data processing have become standard elements of\ndeep learning projects. As a result, the code has become progressively complex\nas the software evolves, which is difficult to maintain and understand.\nExisting studies have investigated the impact of refactoring on software\nquality within traditional software. However, the insight of code refactoring\nin the context of deep learning is still unclear. This study endeavors to fill\nthis knowledge gap by empirically examining the current state of code\nrefactoring in deep learning realm, and practitioners' views on refactoring. We\nfirst manually analyzed the commit history of five popular and well-maintained\ndeep learning projects (e.g., PyTorch). We mined 4,921 refactoring practices in\nhistorical commits and measured how different types and elements of refactoring\noperations are distributed and found that refactoring operation types'\ndistribution in deep learning projects is different from it in traditional Java\nsoftware. We then surveyed 159 practitioners about their views of code\nrefactoring in deep learning projects and their expectations of current\nrefactoring tools. The result of the survey showed that refactoring research\nand the development of related tools in the field of deep learning are crucial\nfor improving project maintainability and code quality, and that current\nrefactoring tools do not adequately meet the needs of practitioners. Lastly, we\nprovided our perspective on the future advancement of refactoring tools and\noffered suggestions for developers' development practices.", "journal": ""}
{"doi": "10.48550/arXiv.1611.04687", "date": "2016-11-15", "title": "Intrinsic Geometric Information Transfer Learning on Multiple Graph-Structured Datasets", "authors": "Jaekoo Lee, Hyunjae Kim, Jongsun Lee, Sungroh Yoon", "abstract": "Graphs provide a powerful means for representing complex interactions between\nentities. Recently, deep learning approaches are emerging for representing and\nmodeling graph-structured data, although the conventional deep learning methods\n(such as convolutional neural networks and recurrent neural networks) have\nmainly focused on grid-structured inputs (image and audio). Leveraged by the\ncapability of representation learning, deep learning based techniques are\nreporting promising results for graph applications by detecting structural\ncharacteristics of graphs in an automated fashion. In this paper, we attempt to\nadvance deep learning for graph-structured data by incorporating another\ncomponent, transfer learning. By transferring the intrinsic geometric\ninformation learned in the source domain, our approach can help us to construct\na model for a new but related task in the target domain without collecting new\ndata and without training a new model from scratch. We thoroughly test our\napproach with large-scale real corpora and confirm the effectiveness of the\nproposed transfer learning framework for deep learning on graphs. According to\nour experiments, transfer learning is most effective when the source and target\ndomains bear a high level of structural similarity in their graph\nrepresentations.", "journal": ""}
{"doi": "10.48550/arXiv.2006.05082", "date": "2020-06-09", "title": "Learning to Stop While Learning to Predict", "authors": "Xinshi Chen, Hanjun Dai, Yu Li, Xin Gao, Le Song", "abstract": "There is a recent surge of interest in designing deep architectures based on\nthe update steps in traditional algorithms, or learning neural networks to\nimprove and replace traditional algorithms. While traditional algorithms have\ncertain stopping criteria for outputting results at different iterations, many\nalgorithm-inspired deep models are restricted to a ``fixed-depth'' for all\ninputs. Similar to algorithms, the optimal depth of a deep architecture may be\ndifferent for different input instances, either to avoid ``over-thinking'', or\nbecause we want to compute less for operations converged already. In this\npaper, we tackle this varying depth problem using a steerable architecture,\nwhere a feed-forward deep model and a variational stopping policy are learned\ntogether to sequentially determine the optimal number of layers for each input\ninstance. Training such architecture is very challenging. We provide a\nvariational Bayes perspective and design a novel and effective training\nprocedure which decomposes the task into an oracle model learning stage and an\nimitation stage. Experimentally, we show that the learned deep model along with\nthe stopping policy improves the performances on a diverse set of tasks,\nincluding learning sparse recovery, few-shot meta learning, and computer vision\ntasks.", "journal": ""}
{"doi": "10.48550/arXiv.1804.08597", "date": "2018-04-23", "title": "Towards Symbolic Reinforcement Learning with Common Sense", "authors": "Artur d'Avila Garcez, Aimore Resende Riquetti Dutra, Eduardo Alonso", "abstract": "Deep Reinforcement Learning (deep RL) has made several breakthroughs in\nrecent years in applications ranging from complex control tasks in unmanned\nvehicles to game playing. Despite their success, deep RL still lacks several\nimportant capacities of human intelligence, such as transfer learning,\nabstraction and interpretability. Deep Symbolic Reinforcement Learning (DSRL)\nseeks to incorporate such capacities to deep Q-networks (DQN) by learning a\nrelevant symbolic representation prior to using Q-learning. In this paper, we\npropose a novel extension of DSRL, which we call Symbolic Reinforcement\nLearning with Common Sense (SRL+CS), offering a better balance between\ngeneralization and specialization, inspired by principles of common sense when\nassigning rewards and aggregating Q-values. Experiments reported in this paper\nshow that SRL+CS learns consistently faster than Q-learning and DSRL, achieving\nalso a higher accuracy. In the hardest case, where agents were trained in a\ndeterministic environment and tested in a random environment, SRL+CS achieves\nnearly 100% average accuracy compared to DSRL's 70% and DQN's 50% accuracy. To\nthe best of our knowledge, this is the first case of near perfect zero-shot\ntransfer learning using Reinforcement Learning.", "journal": ""}
{"doi": "10.48550/arXiv.2103.04339", "date": "2021-03-07", "title": "Network Representation Learning: From Traditional Feature Learning to Deep Learning", "authors": "Ke Sun, Lei Wang, Bo Xu, Wenhong Zhao, Shyh Wei Teng, Feng Xia", "abstract": "Network representation learning (NRL) is an effective graph analytics\ntechnique and promotes users to deeply understand the hidden characteristics of\ngraph data. It has been successfully applied in many real-world tasks related\nto network science, such as social network data processing, biological\ninformation processing, and recommender systems. Deep Learning is a powerful\ntool to learn data features. However, it is non-trivial to generalize deep\nlearning to graph-structured data since it is different from the regular data\nsuch as pictures having spatial information and sounds having temporal\ninformation. Recently, researchers proposed many deep learning-based methods in\nthe area of NRL. In this survey, we investigate classical NRL from traditional\nfeature learning method to the deep learning-based model, analyze relationships\nbetween them, and summarize the latest progress. Finally, we discuss open\nissues considering NRL and point out the future directions in this field.", "journal": ""}
{"doi": "10.48550/arXiv.2303.16310", "date": "2023-03-28", "title": "Crime Prediction Using Machine Learning and Deep Learning: A Systematic Review and Future Directions", "authors": "Varun Mandalapu, Lavanya Elluri, Piyush Vyas, Nirmalya Roy", "abstract": "Predicting crime using machine learning and deep learning techniques has\ngained considerable attention from researchers in recent years, focusing on\nidentifying patterns and trends in crime occurrences. This review paper\nexamines over 150 articles to explore the various machine learning and deep\nlearning algorithms applied to predict crime. The study provides access to the\ndatasets used for crime prediction by researchers and analyzes prominent\napproaches applied in machine learning and deep learning algorithms to predict\ncrime, offering insights into different trends and factors related to criminal\nactivities. Additionally, the paper highlights potential gaps and future\ndirections that can enhance the accuracy of crime prediction. Finally, the\ncomprehensive overview of research discussed in this paper on crime prediction\nusing machine learning and deep learning approaches serves as a valuable\nreference for researchers in this field. By gaining a deeper understanding of\ncrime prediction techniques, law enforcement agencies can develop strategies to\nprevent and respond to criminal activities more effectively.", "journal": ""}
{"doi": "10.48550/arXiv.2308.11027", "date": "2023-08-21", "title": "Split Learning for Distributed Collaborative Training of Deep Learning Models in Health Informatics", "authors": "Zhuohang Li, Chao Yan, Xinmeng Zhang, Gharib Gharibi, Zhijun Yin, Xiaoqian Jiang, Bradley A. Malin", "abstract": "Deep learning continues to rapidly evolve and is now demonstrating remarkable\npotential for numerous medical prediction tasks. However, realizing deep\nlearning models that generalize across healthcare organizations is challenging.\nThis is due, in part, to the inherent siloed nature of these organizations and\npatient privacy requirements. To address this problem, we illustrate how split\nlearning can enable collaborative training of deep learning models across\ndisparate and privately maintained health datasets, while keeping the original\nrecords and model parameters private. We introduce a new privacy-preserving\ndistributed learning framework that offers a higher level of privacy compared\nto conventional federated learning. We use several biomedical imaging and\nelectronic health record (EHR) datasets to show that deep learning models\ntrained via split learning can achieve highly similar performance to their\ncentralized and federated counterparts while greatly improving computational\nefficiency and reducing privacy risks.", "journal": ""}
{"doi": "10.48550/arXiv.2003.02218", "date": "2020-03-04", "title": "The large learning rate phase of deep learning: the catapult mechanism", "authors": "Aitor Lewkowycz, Yasaman Bahri, Ethan Dyer, Jascha Sohl-Dickstein, Guy Gur-Ari", "abstract": "The choice of initial learning rate can have a profound effect on the\nperformance of deep networks. We present a class of neural networks with\nsolvable training dynamics, and confirm their predictions empirically in\npractical deep learning settings. The networks exhibit sharply distinct\nbehaviors at small and large learning rates. The two regimes are separated by a\nphase transition. In the small learning rate phase, training can be understood\nusing the existing theory of infinitely wide neural networks. At large learning\nrates the model captures qualitatively distinct phenomena, including the\nconvergence of gradient descent dynamics to flatter minima. One key prediction\nof our model is a narrow range of large, stable learning rates. We find good\nagreement between our model's predictions and training dynamics in realistic\ndeep learning settings. Furthermore, we find that the optimal performance in\nsuch settings is often found in the large learning rate phase. We believe our\nresults shed light on characteristics of models trained at different learning\nrates. In particular, they fill a gap between existing wide neural network\ntheory, and the nonlinear, large learning rate, training dynamics relevant to\npractice.", "journal": ""}
{"doi": "10.48550/arXiv.2410.12598", "date": "2024-10-16", "title": "Dynamic Learning Rate for Deep Reinforcement Learning: A Bandit Approach", "authors": "Henrique Don\u00e2ncio, Antoine Barrier, Leah F. South, Florence Forbes", "abstract": "In deep Reinforcement Learning (RL) models trained using gradient-based\ntechniques, the choice of optimizer and its learning rate are crucial to\nachieving good performance: higher learning rates can prevent the model from\nlearning effectively, while lower ones might slow convergence. Additionally,\ndue to the non-stationarity of the objective function, the best-performing\nlearning rate can change over the training steps. To adapt the learning rate, a\nstandard technique consists of using decay schedulers. However, these\nschedulers assume that the model is progressively approaching convergence,\nwhich may not always be true, leading to delayed or premature adjustments. In\nthis work, we propose dynamic Learning Rate for deep Reinforcement Learning\n(LRRL), a meta-learning approach that selects the learning rate based on the\nagent's performance during training. LRRL is based on a multi-armed bandit\nalgorithm, where each arm represents a different learning rate, and the bandit\nfeedback is provided by the cumulative returns of the RL policy to update the\narms' probability distribution. Our empirical results demonstrate that LRRL can\nsubstantially improve the performance of deep RL algorithms for some tasks.", "journal": ""}
{"doi": "10.48550/arXiv.2201.09267", "date": "2022-01-23", "title": "Spectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey", "authors": "Benyamin Ghojogh, Ali Ghodsi, Fakhri Karray, Mark Crowley", "abstract": "This is a tutorial and survey paper on metric learning. Algorithms are\ndivided into spectral, probabilistic, and deep metric learning. We first start\nwith the definition of distance metric, Mahalanobis distance, and generalized\nMahalanobis distance. In spectral methods, we start with methods using scatters\nof data, including the first spectral metric learning, relevant methods to\nFisher discriminant analysis, Relevant Component Analysis (RCA), Discriminant\nComponent Analysis (DCA), and the Fisher-HSIC method. Then, large-margin metric\nlearning, imbalanced metric learning, locally linear metric adaptation, and\nadversarial metric learning are covered. We also explain several kernel\nspectral methods for metric learning in the feature space. We also introduce\ngeometric metric learning methods on the Riemannian manifolds. In probabilistic\nmethods, we start with collapsing classes in both input and feature spaces and\nthen explain the neighborhood component analysis methods, Bayesian metric\nlearning, information theoretic methods, and empirical risk minimization in\nmetric learning. In deep learning methods, we first introduce reconstruction\nautoencoders and supervised loss functions for metric learning. Then, Siamese\nnetworks and its various loss functions, triplet mining, and triplet sampling\nare explained. Deep discriminant analysis methods, based on Fisher discriminant\nanalysis, are also reviewed. Finally, we introduce multi-modal deep metric\nlearning, geometric metric learning by neural networks, and few-shot metric\nlearning.", "journal": ""}
{"doi": "10.48550/arXiv.1603.05691", "date": "2016-03-17", "title": "Do Deep Convolutional Nets Really Need to be Deep and Convolutional?", "authors": "Gregor Urban, Krzysztof J. Geras, Samira Ebrahimi Kahou, Ozlem Aslan, Shengjie Wang, Rich Caruana, Abdelrahman Mohamed, Matthai Philipose, Matt Richardson", "abstract": "Yes, they do. This paper provides the first empirical demonstration that deep\nconvolutional models really need to be both deep and convolutional, even when\ntrained with methods such as distillation that allow small or shallow models of\nhigh accuracy to be trained. Although previous research showed that shallow\nfeed-forward nets sometimes can learn the complex functions previously learned\nby deep nets while using the same number of parameters as the deep models they\nmimic, in this paper we demonstrate that the same methods cannot be used to\ntrain accurate models on CIFAR-10 unless the student models contain multiple\nlayers of convolution. Although the student models do not have to be as deep as\nthe teacher model they mimic, the students need multiple convolutional layers\nto learn functions of comparable accuracy as the deep convolutional teacher.", "journal": ""}
{"doi": "10.48550/arXiv.1905.09680", "date": "2019-05-23", "title": "DEEP-BO for Hyperparameter Optimization of Deep Networks", "authors": "Hyunghun Cho, Yongjin Kim, Eunjung Lee, Daeyoung Choi, Yongjae Lee, Wonjong Rhee", "abstract": "The performance of deep neural networks (DNN) is very sensitive to the\nparticular choice of hyper-parameters. To make it worse, the shape of the\nlearning curve can be significantly affected when a technique like batchnorm is\nused. As a result, hyperparameter optimization of deep networks can be much\nmore challenging than traditional machine learning models. In this work, we\nstart from well known Bayesian Optimization solutions and provide enhancement\nstrategies specifically designed for hyperparameter optimization of deep\nnetworks. The resulting algorithm is named as DEEP-BO (Diversified,\nEarly-termination-Enabled, and Parallel Bayesian Optimization). When evaluated\nover six DNN benchmarks, DEEP-BO easily outperforms or shows comparable\nperformance with some of the well-known solutions including GP-Hedge,\nHyperband, BOHB, Median Stopping Rule, and Learning Curve Extrapolation. The\ncode used is made publicly available at https://github.com/snu-adsl/DEEP-BO.", "journal": ""}
{"doi": "10.48550/arXiv.1805.07008", "date": "2018-05-18", "title": "Hierarchical Reinforcement Learning with Deep Nested Agents", "authors": "Marc Brittain, Peng Wei", "abstract": "Deep hierarchical reinforcement learning has gained a lot of attention in\nrecent years due to its ability to produce state-of-the-art results in\nchallenging environments where non-hierarchical frameworks fail to learn useful\npolicies. However, as problem domains become more complex, deep hierarchical\nreinforcement learning can become inefficient, leading to longer convergence\ntimes and poor performance. We introduce the Deep Nested Agent framework, which\nis a variant of deep hierarchical reinforcement learning where information from\nthe main agent is propagated to the low level $nested$ agent by incorporating\nthis information into the nested agent's state. We demonstrate the\neffectiveness and performance of the Deep Nested Agent framework by applying it\nto three scenarios in Minecraft with comparisons to a deep non-hierarchical\nsingle agent framework, as well as, a deep hierarchical framework.", "journal": ""}
{"doi": "10.48550/arXiv.2003.14162", "date": "2020-03-31", "title": "Deep State Space Models for Nonlinear System Identification", "authors": "Daniel Gedon, Niklas Wahlstr\u00f6m, Thomas B. Sch\u00f6n, Lennart Ljung", "abstract": "Deep state space models (SSMs) are an actively researched model class for\ntemporal models developed in the deep learning community which have a close\nconnection to classic SSMs. The use of deep SSMs as a black-box identification\nmodel can describe a wide range of dynamics due to the flexibility of deep\nneural networks. Additionally, the probabilistic nature of the model class\nallows the uncertainty of the system to be modelled. In this work a deep SSM\nclass and its parameter learning algorithm are explained in an effort to extend\nthe toolbox of nonlinear identification methods with a deep learning based\nmethod. Six recent deep SSMs are evaluated in a first unified implementation on\nnonlinear system identification benchmarks.", "journal": ""}
{"doi": "10.48550/arXiv.1902.05731", "date": "2019-02-15", "title": "SVM-based Deep Stacking Networks", "authors": "Jingyuan Wang, Kai Feng, Junjie Wu", "abstract": "The deep network model, with the majority built on neural networks, has been\nproved to be a powerful framework to represent complex data for high\nperformance machine learning. In recent years, more and more studies turn to\nnonneural network approaches to build diverse deep structures, and the Deep\nStacking Network (DSN) model is one of such approaches that uses stacked\neasy-to-learn blocks to build a parameter-training-parallelizable deep network.\nIn this paper, we propose a novel SVM-based Deep Stacking Network (SVM-DSN),\nwhich uses the DSN architecture to organize linear SVM classifiers for deep\nlearning. A BP-like layer tuning scheme is also proposed to ensure holistic and\nlocal optimizations of stacked SVMs simultaneously. Some good math properties\nof SVM, such as the convex optimization, is introduced into the DSN framework\nby our model. From a global view, SVM-DSN can iteratively extract data\nrepresentations layer by layer as a deep neural network but with\nparallelizability, and from a local view, each stacked SVM can converge to its\noptimal solution and obtain the support vectors, which compared with neural\nnetworks could lead to interesting improvements in anti-saturation and\ninterpretability. Experimental results on both image and text data sets\ndemonstrate the excellent performances of SVM-DSN compared with some\ncompetitive benchmark models.", "journal": ""}
{"doi": "10.48550/arXiv.1907.08310", "date": "2019-07-18", "title": "Deep Perceptual Compression", "authors": "Yash Patel, Srikar Appalaraju, R. Manmatha", "abstract": "Several deep learned lossy compression techniques have been proposed in the\nrecent literature. Most of these are optimized by using either MS-SSIM\n(multi-scale structural similarity) or MSE (mean squared error) as a loss\nfunction. Unfortunately, neither of these correlate well with human perception\nand this is clearly visible from the resulting compressed images. In several\ncases, the MS-SSIM for deep learned techniques is higher than say a\nconventional, non-deep learned codec such as JPEG-2000 or BPG. However, the\nimages produced by these deep learned techniques are in many cases clearly\nworse to human eyes than those produced by JPEG-2000 or BPG.\n  We propose the use of an alternative, deep perceptual metric, which has been\nshown to align better with human perceptual similarity. We then propose Deep\nPerceptual Compression (DPC) which makes use of an encoder-decoder based image\ncompression model to jointly optimize on the deep perceptual metric and\nMS-SSIM. Via extensive human evaluations, we show that the proposed method\ngenerates visually better results than previous learning based compression\nmethods and JPEG-2000, and is comparable to BPG. Furthermore, we demonstrate\nthat for tasks like object-detection, images compressed with DPC give better\naccuracy.", "journal": ""}
{"doi": "10.48550/arXiv.2106.06097", "date": "2021-06-11", "title": "Neural Optimization Kernel: Towards Robust Deep Learning", "authors": "Yueming Lyu, Ivor Tsang", "abstract": "Deep neural networks (NN) have achieved great success in many applications.\nHowever, why do deep neural networks obtain good generalization at an\nover-parameterization regime is still unclear. To better understand deep NN, we\nestablish the connection between deep NN and a novel kernel family, i.e.,\nNeural Optimization Kernel (NOK). The architecture of structured approximation\nof NOK performs monotonic descent updates of implicit regularization problems.\nWe can implicitly choose the regularization problems by employing different\nactivation functions, e.g., ReLU, max pooling, and soft-thresholding. We\nfurther establish a new generalization bound of our deep structured\napproximated NOK architecture. Our unsupervised structured approximated NOK\nblock can serve as a simple plug-in of popular backbones for a good\ngeneralization against input noise.", "journal": ""}
{"doi": "10.48550/arXiv.2307.13217", "date": "2023-07-25", "title": "Adversarial Deep Hedging: Learning to Hedge without Price Process Modeling", "authors": "Masanori Hirano, Kentaro Minami, Kentaro Imajo", "abstract": "Deep hedging is a deep-learning-based framework for derivative hedging in\nincomplete markets. The advantage of deep hedging lies in its ability to handle\nvarious realistic market conditions, such as market frictions, which are\nchallenging to address within the traditional mathematical finance framework.\nSince deep hedging relies on market simulation, the underlying asset price\nprocess model is crucial. However, existing literature on deep hedging often\nrelies on traditional mathematical finance models, e.g., Brownian motion and\nstochastic volatility models, and discovering effective underlying asset models\nfor deep hedging learning has been a challenge. In this study, we propose a new\nframework called adversarial deep hedging, inspired by adversarial learning. In\nthis framework, a hedger and a generator, which respectively model the\nunderlying asset process and the underlying asset process, are trained in an\nadversarial manner. The proposed method enables to learn a robust hedger\nwithout explicitly modeling the underlying asset process. Through numerical\nexperiments, we demonstrate that our proposed method achieves competitive\nperformance to models that assume explicit underlying asset processes across\nvarious real market data.", "journal": ""}
{"doi": "10.48550/arXiv.1803.08631", "date": "2018-03-23", "title": "SEGEN: Sample-Ensemble Genetic Evolutional Network Model", "authors": "Jiawei Zhang, Limeng Cui, Fisher B. Gouza", "abstract": "Deep learning, a rebranding of deep neural network research works, has\nachieved a remarkable success in recent years. With multiple hidden layers,\ndeep learning models aim at computing the hierarchical feature representations\nof the observational data. Meanwhile, due to its severe disadvantages in data\nconsumption, computational resources, parameter tuning costs and the lack of\nresult explainability, deep learning has also suffered from lots of criticism.\nIn this paper, we will introduce a new representation learning model, namely\n\"Sample-Ensemble Genetic Evolutionary Network\" (SEGEN), which can serve as an\nalternative approach to deep learning models. Instead of building one single\ndeep model, based on a set of sampled sub-instances, SEGEN adopts a\ngenetic-evolutionary learning strategy to build a group of unit models\ngenerations by generations. The unit models incorporated in SEGEN can be either\ntraditional machine learning models or the recent deep learning models with a\nmuch \"narrower\" and \"shallower\" architecture. The learning results of each\ninstance at the final generation will be effectively combined from each unit\nmodel via diffusive propagation and ensemble learning strategies. From the\ncomputational perspective, SEGEN requires far less data, fewer computational\nresources and parameter tuning efforts, but has sound theoretic\ninterpretability of the learning process and results. Extensive experiments\nhave been done on several different real-world benchmark datasets, and the\nexperimental results obtained by SEGEN have demonstrated its advantages over\nthe state-of-the-art representation learning models.", "journal": ""}
{"doi": "10.48550/arXiv.2007.03363", "date": "2020-07-07", "title": "Deep Reinforcement Learning with Interactive Feedback in a Human-Robot Environment", "authors": "Ithan Moreira, Javier Rivas, Francisco Cruz, Richard Dazeley, Angel Ayala, Bruno Fernandes", "abstract": "Robots are extending their presence in domestic environments every day, being\nmore common to see them carrying out tasks in home scenarios. In the future,\nrobots are expected to increasingly perform more complex tasks and, therefore,\nbe able to acquire experience from different sources as quickly as possible. A\nplausible approach to address this issue is interactive feedback, where a\ntrainer advises a learner on which actions should be taken from specific states\nto speed up the learning process. Moreover, deep reinforcement learning has\nbeen recently widely utilized in robotics to learn the environment and acquire\nnew skills autonomously. However, an open issue when using deep reinforcement\nlearning is the excessive time needed to learn a task from raw input images. In\nthis work, we propose a deep reinforcement learning approach with interactive\nfeedback to learn a domestic task in a human-robot scenario. We compare three\ndifferent learning methods using a simulated robotic arm for the task of\norganizing different objects; the proposed methods are (i) deep reinforcement\nlearning (DeepRL); (ii) interactive deep reinforcement learning using a\npreviously trained artificial agent as an advisor (agent-IDeepRL); and (iii)\ninteractive deep reinforcement learning using a human advisor (human-IDeepRL).\nWe demonstrate that interactive approaches provide advantages for the learning\nprocess. The obtained results show that a learner agent, using either\nagent-IDeepRL or human-IDeepRL, completes the given task earlier and has fewer\nmistakes compared to the autonomous DeepRL approach.", "journal": ""}
{"doi": "10.48550/arXiv.2009.07988", "date": "2020-09-17", "title": "Deep Collective Learning: Learning Optimal Inputs and Weights Jointly in Deep Neural Networks", "authors": "Xiang Deng, Zhongfei, Zhang", "abstract": "It is well observed that in deep learning and computer vision literature,\nvisual data are always represented in a manually designed coding scheme (eg.,\nRGB images are represented as integers ranging from 0 to 255 for each channel)\nwhen they are input to an end-to-end deep neural network (DNN) for any learning\ntask. We boldly question whether the manually designed inputs are good for DNN\ntraining for different tasks and study whether the input to a DNN can be\noptimally learned end-to-end together with learning the weights of the DNN. In\nthis paper, we propose the paradigm of {\\em deep collective learning} which\naims to learn the weights of DNNs and the inputs to DNNs simultaneously for\ngiven tasks. We note that collective learning has been implicitly but widely\nused in natural language processing while it has almost never been studied in\ncomputer vision. Consequently, we propose the lookup vision networks\n(Lookup-VNets) as a solution to deep collective learning in computer vision.\nThis is achieved by associating each color in each channel with a vector in\nlookup tables. As learning inputs in computer vision has almost never been\nstudied in the existing literature, we explore several aspects of this question\nthrough varieties of experiments on image classification tasks. Experimental\nresults on four benchmark datasets, i.e., CIFAR-10, CIFAR-100, Tiny ImageNet,\nand ImageNet (ILSVRC2012) have shown several surprising characteristics of\nLookup-VNets and have demonstrated the advantages and promise of Lookup-VNets\nand deep collective learning.", "journal": ""}
{"doi": "10.48550/arXiv.2112.01423", "date": "2021-12-02", "title": "Training Efficiency and Robustness in Deep Learning", "authors": "Fartash Faghri", "abstract": "Deep Learning has revolutionized machine learning and artificial\nintelligence, achieving superhuman performance in several standard benchmarks.\nIt is well-known that deep learning models are inefficient to train; they learn\nby processing millions of training data multiple times and require powerful\ncomputational resources to process large batches of data in parallel at the\nsame time rather than sequentially. Deep learning models also have unexpected\nfailure modes; they can be fooled into misbehaviour, producing unexpectedly\nincorrect predictions.\n  In this thesis, we study approaches to improve the training efficiency and\nrobustness of deep learning models. In the context of learning visual-semantic\nembeddings, we find that prioritizing learning on more informative training\ndata increases convergence speed and improves generalization performance on\ntest data. We formalize a simple trick called hard negative mining as a\nmodification to the learning objective function with no computational overhead.\nNext, we seek improvements to optimization speed in general-purpose\noptimization methods in deep learning. We show that a redundancy-aware\nmodification to the sampling of training data improves the training speed and\ndevelops an efficient method for detecting the diversity of training signal,\nnamely, gradient clustering. Finally, we study adversarial robustness in deep\nlearning and approaches to achieve maximal adversarial robustness without\ntraining with additional data. For linear models, we prove guaranteed maximal\nrobustness achieved only by appropriate choice of the optimizer,\nregularization, or architecture.", "journal": ""}
{"doi": "10.48550/arXiv.2211.06034", "date": "2022-11-11", "title": "Does Deep Learning REALLY Outperform Non-deep Machine Learning for Clinical Prediction on Physiological Time Series?", "authors": "Ke Liao, Wei Wang, Armagan Elibol, Lingzhong Meng, Xu Zhao, Nak Young Chong", "abstract": "Machine learning has been widely used in healthcare applications to\napproximate complex models, for clinical diagnosis, prognosis, and treatment.\nAs deep learning has the outstanding ability to extract information from time\nseries, its true capabilities on sparse, irregularly sampled, multivariate, and\nimbalanced physiological data are not yet fully explored. In this paper, we\nsystematically examine the performance of machine learning models for the\nclinical prediction task based on the EHR, especially physiological time\nseries. We choose Physionet 2019 challenge public dataset to predict Sepsis\noutcomes in ICU units. Ten baseline machine learning models are compared,\nincluding 3 deep learning methods and 7 non-deep learning methods, commonly\nused in the clinical prediction domain. Nine evaluation metrics with specific\nclinical implications are used to assess the performance of models. Besides, we\nsub-sample training dataset sizes and use learning curve fit to investigate the\nimpact of the training dataset size on the performance of the machine learning\nmodels. We also propose the general pre-processing method for the physiology\ntime-series data and use Dice Loss to deal with the dataset imbalanced problem.\nThe results show that deep learning indeed outperforms non-deep learning, but\nwith certain conditions: firstly, evaluating with some particular evaluation\nmetrics (AUROC, AUPRC, Sensitivity, and FNR), but not others; secondly, the\ntraining dataset size is large enough (with an estimation of a magnitude of\nthousands).", "journal": ""}
{"doi": "10.48550/arXiv.2310.14036", "date": "2023-10-21", "title": "On discretisation drift and smoothness regularisation in neural network training", "authors": "Mihaela Claudia Rosca", "abstract": "The deep learning recipe of casting real-world problems as mathematical\noptimisation and tackling the optimisation by training deep neural networks\nusing gradient-based optimisation has undoubtedly proven to be a fruitful one.\nThe understanding behind why deep learning works, however, has lagged behind\nits practical significance. We aim to make steps towards an improved\nunderstanding of deep learning with a focus on optimisation and model\nregularisation. We start by investigating gradient descent (GD), a\ndiscrete-time algorithm at the basis of most popular deep learning optimisation\nalgorithms. Understanding the dynamics of GD has been hindered by the presence\nof discretisation drift, the numerical integration error between GD and its\noften studied continuous-time counterpart, the negative gradient flow (NGF). To\nadd to the toolkit available to study GD, we derive novel continuous-time flows\nthat account for discretisation drift. Unlike the NGF, these new flows can be\nused to describe learning rate specific behaviours of GD, such as training\ninstabilities observed in supervised learning and two-player games. We then\ntranslate insights from continuous time into mitigation strategies for unstable\nGD dynamics, by constructing novel learning rate schedules and regularisers\nthat do not require additional hyperparameters. Like optimisation, smoothness\nregularisation is another pillar of deep learning's success with wide use in\nsupervised learning and generative modelling. Despite their individual\nsignificance, the interactions between smoothness regularisation and\noptimisation have yet to be explored. We find that smoothness regularisation\naffects optimisation across multiple deep learning domains, and that\nincorporating smoothness regularisation in reinforcement learning leads to a\nperformance boost that can be recovered using adaptions to optimisation\nmethods.", "journal": ""}
{"doi": "10.48550/arXiv.1306.2759", "date": "2013-06-12", "title": "Horizontal and Vertical Ensemble with Deep Representation for Classification", "authors": "Jingjing Xie, Bing Xu, Zhang Chuang", "abstract": "Representation learning, especially which by using deep learning, has been\nwidely applied in classification. However, how to use limited size of labeled\ndata to achieve good classification performance with deep neural network, and\nhow can the learned features further improve classification remain indefinite.\nIn this paper, we propose Horizontal Voting Vertical Voting and Horizontal\nStacked Ensemble methods to improve the classification performance of deep\nneural networks. In the ICML 2013 Black Box Challenge, via using these methods\nindependently, Bing Xu achieved 3rd in public leaderboard, and 7th in private\nleaderboard; Jingjing Xie achieved 4th in public leaderboard, and 5th in\nprivate leaderboard.", "journal": ""}
{"doi": "10.48550/arXiv.1505.06800", "date": "2015-05-26", "title": "Boosting-like Deep Learning For Pedestrian Detection", "authors": "Lei Wang, Baochang Zhang", "abstract": "This paper proposes boosting-like deep learning (BDL) framework for\npedestrian detection. Due to overtraining on the limited training samples,\noverfitting is a major problem of deep learning. We incorporate a boosting-like\ntechnique into deep learning to weigh the training samples, and thus prevent\novertraining in the iterative process. We theoretically give the details of\nderivation of our algorithm, and report the experimental results on open data\nsets showing that BDL achieves a better stable performance than the\nstate-of-the-arts. Our approach achieves 15.85% and 3.81% reduction in the\naverage miss rate compared with ACF and JointDeep on the largest Caltech\nbenchmark dataset, respectively.", "journal": ""}
{"doi": "10.48550/arXiv.1602.06561", "date": "2016-02-21", "title": "Deep Learning in Finance", "authors": "J. B. Heaton, N. G. Polson, J. H. Witte", "abstract": "We explore the use of deep learning hierarchical models for problems in\nfinancial prediction and classification. Financial prediction problems -- such\nas those presented in designing and pricing securities, constructing\nportfolios, and risk management -- often involve large data sets with complex\ndata interactions that currently are difficult or impossible to specify in a\nfull economic model. Applying deep learning methods to these problems can\nproduce more useful results than standard methods in finance. In particular,\ndeep learning can detect and exploit interactions in the data that are, at\nleast currently, invisible to any existing financial economic theory.", "journal": ""}
{"doi": "10.48550/arXiv.1610.01178", "date": "2016-10-01", "title": "A Tour of TensorFlow", "authors": "Peter Goldsborough", "abstract": "Deep learning is a branch of artificial intelligence employing deep neural\nnetwork architectures that has significantly advanced the state-of-the-art in\ncomputer vision, speech recognition, natural language processing and other\ndomains. In November 2015, Google released $\\textit{TensorFlow}$, an open\nsource deep learning software library for defining, training and deploying\nmachine learning models. In this paper, we review TensorFlow and put it in\ncontext of modern deep learning concepts and software. We discuss its basic\ncomputational paradigms and distributed execution model, its programming\ninterface as well as accompanying visualization toolkits. We then compare\nTensorFlow to alternative libraries such as Theano, Torch or Caffe on a\nqualitative as well as quantitative basis and finally comment on observed\nuse-cases of TensorFlow in academia and industry.", "journal": ""}
{"doi": "10.48550/arXiv.1610.02707", "date": "2016-10-09", "title": "Multi-Objective Deep Reinforcement Learning", "authors": "Hossam Mossalam, Yannis M. Assael, Diederik M. Roijers, Shimon Whiteson", "abstract": "We propose Deep Optimistic Linear Support Learning (DOL) to solve\nhigh-dimensional multi-objective decision problems where the relative\nimportances of the objectives are not known a priori. Using features from the\nhigh-dimensional inputs, DOL computes the convex coverage set containing all\npotential optimal solutions of the convex combinations of the objectives. To\nour knowledge, this is the first time that deep reinforcement learning has\nsucceeded in learning multi-objective policies. In addition, we provide a\ntestbed with two experiments to be used as a benchmark for deep multi-objective\nreinforcement learning.", "journal": ""}
{"doi": "10.48550/arXiv.1611.00201", "date": "2016-11-01", "title": "Towards Lifelong Self-Supervision: A Deep Learning Direction for Robotics", "authors": "Jay M. Wong", "abstract": "Despite outstanding success in vision amongst other domains, many of the\nrecent deep learning approaches have evident drawbacks for robots. This\nmanuscript surveys recent work in the literature that pertain to applying deep\nlearning systems to the robotics domain, either as means of estimation or as a\ntool to resolve motor commands directly from raw percepts. These recent\nadvances are only a piece to the puzzle. We suggest that deep learning as a\ntool alone is insufficient in building a unified framework to acquire general\nintelligence. For this reason, we complement our survey with insights from\ncognitive development and refer to ideas from classical control theory,\nproducing an integrated direction for a lifelong learning architecture.", "journal": ""}
{"doi": "10.48550/arXiv.1611.08675", "date": "2016-11-26", "title": "Deep Reinforcement Learning for Multi-Domain Dialogue Systems", "authors": "Heriberto Cuay\u00e1huitl, Seunghak Yu, Ashley Williamson, Jacob Carse", "abstract": "Standard deep reinforcement learning methods such as Deep Q-Networks (DQN)\nfor multiple tasks (domains) face scalability problems. We propose a method for\nmulti-domain dialogue policy learning---termed NDQN, and apply it to an\ninformation-seeking spoken dialogue system in the domains of restaurants and\nhotels. Experimental results comparing DQN (baseline) versus NDQN (proposed)\nusing simulations report that our proposed method exhibits better scalability\nand is promising for optimising the behaviour of multi-domain dialogue systems.", "journal": ""}
{"doi": "10.48550/arXiv.1702.08039", "date": "2017-02-26", "title": "Criticality & Deep Learning I: Generally Weighted Nets", "authors": "Dan Oprisa, Peter Toth", "abstract": "Motivated by the idea that criticality and universality of phase transitions\nmight play a crucial role in achieving and sustaining learning and intelligent\nbehaviour in biological and artificial networks, we analyse a theoretical and a\npragmatic experimental set up for critical phenomena in deep learning. On the\ntheoretical side, we use results from statistical physics to carry out critical\npoint calculations in feed-forward/fully connected networks, while on the\nexperimental side we set out to find traces of criticality in deep neural\nnetworks. This is our first step in a series of upcoming investigations to map\nout the relationship between criticality and learning in deep networks.", "journal": ""}
{"doi": "10.48550/arXiv.1705.03562", "date": "2017-05-09", "title": "Deep Episodic Value Iteration for Model-based Meta-Reinforcement Learning", "authors": "Steven Stenberg Hansen", "abstract": "We present a new deep meta reinforcement learner, which we call Deep Episodic\nValue Iteration (DEVI). DEVI uses a deep neural network to learn a similarity\nmetric for a non-parametric model-based reinforcement learning algorithm. Our\nmodel is trained end-to-end via back-propagation. Despite being trained using\nthe model-free Q-learning objective, we show that DEVI's model-based internal\nstructure provides `one-shot' transfer to changes in reward and transition\nstructure, even for tasks with very high-dimensional state spaces.", "journal": ""}
{"doi": "10.48550/arXiv.1705.06452", "date": "2017-05-18", "title": "Delving into adversarial attacks on deep policies", "authors": "Jernej Kos, Dawn Song", "abstract": "Adversarial examples have been shown to exist for a variety of deep learning\narchitectures. Deep reinforcement learning has shown promising results on\ntraining agent policies directly on raw inputs such as image pixels. In this\npaper we present a novel study into adversarial attacks on deep reinforcement\nlearning polices. We compare the effectiveness of the attacks using adversarial\nexamples vs. random noise. We present a novel method for reducing the number of\ntimes adversarial examples need to be injected for a successful attack, based\non the value function. We further explore how re-training on random noise and\nFGSM perturbations affects the resilience against adversarial examples.", "journal": ""}
{"doi": "10.48550/arXiv.1705.11023", "date": "2017-05-31", "title": "Criticality & Deep Learning II: Momentum Renormalisation Group", "authors": "Dan Oprisa, Peter Toth", "abstract": "Guided by critical systems found in nature we develop a novel mechanism\nconsisting of inhomogeneous polynomial regularisation via which we can induce\nscale invariance in deep learning systems. Technically, we map our deep\nlearning (DL) setup to a genuine field theory, on which we act with the\nRenormalisation Group (RG) in momentum space and produce the flow equations of\nthe couplings; those are translated to constraints and consequently interpreted\nas \"critical regularisation\" conditions in the optimiser; the resulting\nequations hence prove to be sufficient conditions for - and serve as an elegant\nand simple mechanism to induce scale invariance in any deep learning setup.", "journal": ""}
{"doi": "10.48550/arXiv.1706.06302", "date": "2017-06-20", "title": "Deep Learning in (and of) Agent-Based Models: A Prospectus", "authors": "Sander van der Hoog", "abstract": "A very timely issue for economic agent-based models (ABMs) is their empirical\nestimation. This paper describes a line of research that could resolve the\nissue by using machine learning techniques, using multi-layer artificial neural\nnetworks (ANNs), or so called Deep Nets. The seminal contribution by Hinton et\nal. (2006) introduced a fast and efficient training algorithm called Deep\nLearning, and there have been major breakthroughs in machine learning ever\nsince. Economics has not yet benefited from these developments, and therefore\nwe believe that now is the right time to apply Deep Learning and multi-layered\nneural networks to agent-based models in economics.", "journal": ""}
{"doi": "10.48550/arXiv.1802.08726", "date": "2018-02-23", "title": "Longitudinal Face Aging in the Wild - Recent Deep Learning Approaches", "authors": "Chi Nhan Duong, Khoa Luu, Kha Gia Quach, Tien D. Bui", "abstract": "Face Aging has raised considerable attentions and interest from the computer\nvision community in recent years. Numerous approaches ranging from purely image\nprocessing techniques to deep learning structures have been proposed in\nliterature. In this paper, we aim to give a review of recent developments of\nmodern deep learning based approaches, i.e. Deep Generative Models, for Face\nAging task. Their structures, formulation, learning algorithms as well as\nsynthesized results are also provided with systematic discussions. Moreover,\nthe aging databases used in most methods to learn the aging process are also\nreviewed.", "journal": ""}
{"doi": "10.48550/arXiv.1810.01622", "date": "2018-10-03", "title": "Theory of Generative Deep Learning : Probe Landscape of Empirical Error via Norm Based Capacity Control", "authors": "Wendi Xu, Ming Zhang", "abstract": "Despite its remarkable empirical success as a highly competitive branch of\nartificial intelligence, deep learning is often blamed for its widely known low\ninterpretation and lack of firm and rigorous mathematical foundation. However,\nmost theoretical endeavor is devoted in discriminative deep learning case,\nwhose complementary part is generative deep learning. To the best of our\nknowledge, we firstly highlight landscape of empirical error in generative case\nto complete the full picture through exquisite design of image super resolution\nunder norm based capacity control. Our theoretical advance in interpretation of\nthe training dynamic is achieved from both mathematical and biological sides.", "journal": ""}
{"doi": "10.48550/arXiv.1810.04066", "date": "2018-10-09", "title": "Deep learning with differential Gaussian process flows", "authors": "Pashupati Hegde, Markus Heinonen, Harri L\u00e4hdesm\u00e4ki, Samuel Kaski", "abstract": "We propose a novel deep learning paradigm of differential flows that learn a\nstochastic differential equation transformations of inputs prior to a standard\nclassification or regression function. The key property of differential\nGaussian processes is the warping of inputs through infinitely deep, but\ninfinitesimal, differential fields, that generalise discrete layers into a\ndynamical system. We demonstrate state-of-the-art results that exceed the\nperformance of deep Gaussian processes and neural networks", "journal": ""}
{"doi": "10.48550/arXiv.1810.05052", "date": "2018-10-11", "title": "Deep Learning for Image Denoising: A Survey", "authors": "Chunwei Tian, Yong Xu, Lunke Fei, Ke Yan", "abstract": "Since the proposal of big data analysis and Graphic Processing Unit (GPU),\nthe deep learning technology has received a great deal of attention and has\nbeen widely applied in the field of imaging processing. In this paper, we have\nan aim to completely review and summarize the deep learning technologies for\nimage denoising proposed in recent years. Morever, we systematically analyze\nthe conventional machine learning methods for image denoising. Finally, we\npoint out some research directions for the deep learning technologies in image\ndenoising.", "journal": ""}
{"doi": "10.48550/arXiv.1905.02825", "date": "2019-05-07", "title": "Toybox: A Suite of Environments for Experimental Evaluation of Deep Reinforcement Learning", "authors": "Emma Tosch, Kaleigh Clary, John Foley, David Jensen", "abstract": "Evaluation of deep reinforcement learning (RL) is inherently challenging. In\nparticular, learned policies are largely opaque, and hypotheses about the\nbehavior of deep RL agents are difficult to test in black-box environments.\nConsiderable effort has gone into addressing opacity, but almost no effort has\nbeen devoted to producing high quality environments for experimental evaluation\nof agent behavior. We present TOYBOX, a new high-performance, open-source*\nsubset of Atari environments re-designed for the experimental evaluation of\ndeep RL. We show that TOYBOX enables a wide range of experiments and analyses\nthat are impossible in other environments.\n  *https://kdl-umass.github.io/Toybox/", "journal": ""}
{"doi": "10.48550/arXiv.1801.05407", "date": "2018-01-16", "title": "Deep Canonically Correlated LSTMs", "authors": "Neil Mallinar, Corbin Rosset", "abstract": "We examine Deep Canonically Correlated LSTMs as a way to learn nonlinear\ntransformations of variable length sequences and embed them into a correlated,\nfixed dimensional space. We use LSTMs to transform multi-view time-series data\nnon-linearly while learning temporal relationships within the data. We then\nperform correlation analysis on the outputs of these neural networks to find a\ncorrelated subspace through which we get our final representation via\nprojection. This work follows from previous work done on Deep Canonical\nCorrelation (DCCA), in which deep feed-forward neural networks were used to\nlearn nonlinear transformations of data while maximizing correlation.", "journal": ""}
{"doi": "10.48550/arXiv.1805.01891", "date": "2018-05-04", "title": "Power Law in Sparsified Deep Neural Networks", "authors": "Lu Hou, James T. Kwok", "abstract": "The power law has been observed in the degree distributions of many\nbiological neural networks. Sparse deep neural networks, which learn an\neconomical representation from the data, resemble biological neural networks in\nmany ways. In this paper, we study if these artificial networks also exhibit\nproperties of the power law. Experimental results on two popular deep learning\nmodels, namely, multilayer perceptrons and convolutional neural networks, are\naffirmative. The power law is also naturally related to preferential\nattachment. To study the dynamical properties of deep networks in continual\nlearning, we propose an internal preferential attachment model to explain how\nthe network topology evolves. Experimental results show that with the arrival\nof a new task, the new connections made follow this preferential attachment\nprocess.", "journal": ""}
{"doi": "10.48550/arXiv.1811.02640", "date": "2018-11-06", "title": "Deep Probabilistic Ensembles: Approximate Variational Inference through KL Regularization", "authors": "Kashyap Chitta, Jose M. Alvarez, Adam Lesnikowski", "abstract": "In this paper, we introduce Deep Probabilistic Ensembles (DPEs), a scalable\ntechnique that uses a regularized ensemble to approximate a deep Bayesian\nNeural Network (BNN). We do so by incorporating a KL divergence penalty term\ninto the training objective of an ensemble, derived from the evidence lower\nbound used in variational inference. We evaluate the uncertainty estimates\nobtained from our models for active learning on visual classification. Our\napproach steadily improves upon active learning baselines as the annotation\nbudget is increased.", "journal": ""}
{"doi": "10.48550/arXiv.1902.04704", "date": "2019-02-13", "title": "Neural network models and deep learning - a primer for biologists", "authors": "Nikolaus Kriegeskorte, Tal Golan", "abstract": "Originally inspired by neurobiology, deep neural network models have become a\npowerful tool of machine learning and artificial intelligence, where they are\nused to approximate functions and dynamics by learning from examples. Here we\ngive a brief introduction to neural network models and deep learning for\nbiologists. We introduce feedforward and recurrent networks and explain the\nexpressive power of this modeling framework and the backpropagation algorithm\nfor setting the parameters. Finally, we consider how deep neural networks might\nhelp us understand the brain's computations.", "journal": "Current Biology 29(7) (2019) R231-R236"}
{"doi": "10.48550/arXiv.1902.07068", "date": "2019-02-18", "title": "Classifying textual data: shallow, deep and ensemble methods", "authors": "Laura Anderlucci, Lucia Guastadisegni, Cinzia Viroli", "abstract": "This paper focuses on a comparative evaluation of the most common and modern\nmethods for text classification, including the recent deep learning strategies\nand ensemble methods. The study is motivated by a challenging real data\nproblem, characterized by high-dimensional and extremely sparse data, deriving\nfrom incoming calls to the customer care of an Italian phone company. We will\nshow that deep learning outperforms many classical (shallow) strategies but the\ncombination of shallow and deep learning methods in a unique ensemble\nclassifier may improve the robustness and the accuracy of \"single\"\nclassification methods.", "journal": ""}
{"doi": "10.48550/arXiv.2009.03394", "date": "2020-09-07", "title": "Deep Learning, Predictability, and Optimal Portfolio Returns", "authors": "Mykola Babiak, Jozef Barunik", "abstract": "We study dynamic portfolio choice of a long-horizon investor who uses deep\nlearning methods to predict equity returns when forming optimal portfolios. Our\nresults show statistically and economically significant benefits from using\ndeep learning to form optimal portfolios through certainty equivalent returns\nand Sharpe ratios. We demonstrate that a long-short-term-memory recurrent\nneural network, which excels in learning complex time-series dependencies,\ngenerates a superior performance among a variety of networks considered. Return\npredictability via deep learning generates substantially improved portfolio\nperformance across different subsamples, particularly during recessionary\nperiods. These gains are robust to including transaction costs, short-selling\nand borrowing constraints.", "journal": ""}
{"doi": "10.48550/arXiv.2011.01787", "date": "2020-10-28", "title": "Predicting intubation support requirement of patients using Chest X-ray with Deep Representation Learning", "authors": "Aniket Maurya", "abstract": "Recent developments in medical imaging with Deep Learning presents evidence\nof automated diagnosis and prognosis. It can also be a complement to currently\navailable diagnosis methods. Deep Learning can be leveraged for diagnosis,\nseverity prediction, intubation support prediction and many similar tasks. We\npresent prediction of intubation support requirement for patients from the\nChest X-ray using Deep representation learning. We release our source code\npublicly at https://github.com/aniketmaurya/covid-research.", "journal": ""}
{"doi": "10.48550/arXiv.2012.06969", "date": "2020-12-13", "title": "Predicting Generalization in Deep Learning via Local Measures of Distortion", "authors": "Abhejit Rajagopal, Vamshi C. Madala, Shivkumar Chandrasekaran, Peder E. Z. Larson", "abstract": "We study generalization in deep learning by appealing to complexity measures\noriginally developed in approximation and information theory. While these\nconcepts are challenged by the high-dimensional and data-defined nature of deep\nlearning, we show that simple vector quantization approaches such as PCA, GMMs,\nand SVMs capture their spirit when applied layer-wise to deep extracted\nfeatures giving rise to relatively inexpensive complexity measures that\ncorrelate well with generalization performance. We discuss our results in 2020\nNeurIPS PGDL challenge.", "journal": ""}
{"doi": "10.48550/arXiv.2101.03197", "date": "2021-01-08", "title": "Deep Diffusion Processes for Active Learning of Hyperspectral Images", "authors": "Abiy Tasissa, Duc Nguyen, James Murphy", "abstract": "A method for active learning of hyperspectral images (HSI) is proposed, which\ncombines deep learning with diffusion processes on graphs. A deep variational\nautoencoder extracts smoothed, denoised features from a high-dimensional HSI,\nwhich are then used to make labeling queries based on graph diffusion\nprocesses. The proposed method combines the robust representations of deep\nlearning with the mathematical tractability of diffusion geometry, and leads to\nstrong performance on real HSI.", "journal": ""}
{"doi": "10.48550/arXiv.2103.12982", "date": "2021-03-24", "title": "From Semantic Retrieval to Pairwise Ranking: Applying Deep Learning in E-commerce Search", "authors": "Rui Li, Yunjiang Jiang, Wenyun Yang, Guoyu Tang, Songlin Wang, Chaoyi Ma, Wei He, Xi Xiong, Yun Xiao, Eric Yihong Zhao", "abstract": "We introduce deep learning models to the two most important stages in product\nsearch at JD.com, one of the largest e-commerce platforms in the world.\nSpecifically, we outline the design of a deep learning system that retrieves\nsemantically relevant items to a query within milliseconds, and a pairwise deep\nre-ranking system, which learns subtle user preferences. Compared to\ntraditional search systems, the proposed approaches are better at semantic\nretrieval and personalized ranking, achieving significant improvements.", "journal": ""}
{"doi": "10.48550/arXiv.2112.01675", "date": "2021-12-03", "title": "Challenges and Opportunities in Approximate Bayesian Deep Learning for Intelligent IoT Systems", "authors": "Meet P. Vadera, Benjamin M. Marlin", "abstract": "Approximate Bayesian deep learning methods hold significant promise for\naddressing several issues that occur when deploying deep learning components in\nintelligent systems, including mitigating the occurrence of over-confident\nerrors and providing enhanced robustness to out of distribution examples.\nHowever, the computational requirements of existing approximate Bayesian\ninference methods can make them ill-suited for deployment in intelligent IoT\nsystems that include lower-powered edge devices. In this paper, we present a\nrange of approximate Bayesian inference methods for supervised deep learning\nand highlight the challenges and opportunities when applying these methods on\ncurrent edge hardware. We highlight several potential solutions to decreasing\nmodel storage requirements and improving computational scalability, including\nmodel pruning and distillation methods.", "journal": ""}
{"doi": "10.48550/arXiv.2112.09420", "date": "2021-12-17", "title": "A random energy approach to deep learning", "authors": "Rongrong Xie, Matteo Marsili", "abstract": "We study a generic ensemble of deep belief networks which is parametrized by\nthe distribution of energy levels of the hidden states of each layer. We show\nthat, within a random energy approach, statistical dependence can propagate\nfrom the visible to deep layers only if each layer is tuned close to the\ncritical point during learning. As a consequence, efficiently trained learning\nmachines are characterised by a broad distribution of energy levels. The\nanalysis of Deep Belief Networks and Restricted Boltzmann Machines on different\ndatasets confirms these conclusions.", "journal": ""}
{"doi": "10.48550/arXiv.2305.05601", "date": "2023-05-09", "title": "Deep Learning and Geometric Deep Learning: an introduction for mathematicians and physicists", "authors": "R. Fioresi, F. Zanchetta", "abstract": "In this expository paper we want to give a brief introduction, with few key\nreferences for further reading, to the inner functioning of the new and\nsuccessfull algorithms of Deep Learning and Geometric Deep Learning with a\nfocus on Graph Neural Networks. We go over the key ingredients for these\nalgorithms: the score and loss function and we explain the main steps for the\ntraining of a model. We do not aim to give a complete and exhaustive treatment,\nbut we isolate few concepts to give a fast introduction to the subject. We\nprovide some appendices to complement our treatment discussing Kullback-Leibler\ndivergence, regression, Multi-layer Perceptrons and the Universal Approximation\nTheorem.", "journal": ""}
{"doi": "10.48550/arXiv.2306.15337", "date": "2023-06-27", "title": "Homological Neural Networks: A Sparse Architecture for Multivariate Complexity", "authors": "Yuanrong Wang, Antonio Briola, Tomaso Aste", "abstract": "The rapid progress of Artificial Intelligence research came with the\ndevelopment of increasingly complex deep learning models, leading to growing\nchallenges in terms of computational complexity, energy efficiency and\ninterpretability. In this study, we apply advanced network-based information\nfiltering techniques to design a novel deep neural network unit characterized\nby a sparse higher-order graphical architecture built over the homological\nstructure of underlying data. We demonstrate its effectiveness in two\napplication domains which are traditionally challenging for deep learning:\ntabular data and time series regression problems. Results demonstrate the\nadvantages of this novel design which can tie or overcome the results of\nstate-of-the-art machine learning and deep learning models using only a\nfraction of parameters.", "journal": ""}
{"doi": "10.48550/arXiv.2401.10857", "date": "2024-01-19", "title": "Motion Consistency Loss for Monocular Visual Odometry with Attention-Based Deep Learning", "authors": "Andr\u00e9 O. Fran\u00e7ani, Marcos R. O. A. Maximo", "abstract": "Deep learning algorithms have driven expressive progress in many complex\ntasks. The loss function is a core component of deep learning techniques,\nguiding the learning process of neural networks. This paper contributes by\nintroducing a consistency loss for visual odometry with deep learning-based\napproaches. The motion consistency loss explores repeated motions that appear\nin consecutive overlapped video clips. Experimental results show that our\napproach increased the performance of a model on the KITTI odometry benchmark.", "journal": ""}
{"doi": "10.48550/arXiv.2407.18384", "date": "2024-07-25", "title": "Mathematical theory of deep learning", "authors": "Philipp Petersen, Jakob Zech", "abstract": "This book provides an introduction to the mathematical analysis of deep\nlearning. It covers fundamental results in approximation theory, optimization\ntheory, and statistical learning theory, which are the three main pillars of\ndeep neural network theory. Serving as a guide for students and researchers in\nmathematics and related fields, the book aims to equip readers with\nfoundational knowledge on the topic. It prioritizes simplicity over generality,\nand presents rigorous yet accessible results to help build an understanding of\nthe essential mathematical concepts underpinning deep learning.", "journal": ""}
{"doi": "10.48550/arXiv.2504.01212", "date": "2025-04-01", "title": "Cooper: A Library for Constrained Optimization in Deep Learning", "authors": "Jose Gallego-Posada, Juan Ramirez, Meraj Hashemizadeh, Simon Lacoste-Julien", "abstract": "Cooper is an open-source package for solving constrained optimization\nproblems involving deep learning models. Cooper implements several\nLagrangian-based first-order update schemes, making it easy to combine\nconstrained optimization algorithms with high-level features of PyTorch such as\nautomatic differentiation, and specialized deep learning architectures and\noptimizers. Although Cooper is specifically designed for deep learning\napplications where gradients are estimated based on mini-batches, it is\nsuitable for general non-convex continuous constrained optimization. Cooper's\nsource code is available at https://github.com/cooper-org/cooper.", "journal": ""}
{"doi": "10.48550/arXiv.2505.03789", "date": "2025-05-01", "title": "A new architecture of high-order deep neural networks that learn martingales", "authors": "Syoiti Ninomiya, Yuming Ma", "abstract": "A new deep-learning neural network architecture based on high-order weak\napproximation algorithms for stochastic differential equations (SDEs) is\nproposed. The architecture enables the efficient learning of martingales by\ndeep learning models. The behaviour of deep neural networks based on this\narchitecture, when applied to the problem of pricing financial derivatives, is\nalso examined. The core of this new architecture lies in the high-order weak\napproximation algorithms of the explicit Runge--Kutta type, wherein the\napproximation is realised solely through iterative compositions and linear\ncombinations of vector fields of the target SDEs.", "journal": ""}
{"doi": "10.48550/arXiv.2505.24353", "date": "2025-05-30", "title": "Cartan Networks: Group theoretical Hyperbolic Deep Learning", "authors": "Federico Milanesio, Matteo Santoro, Pietro G. Fr\u00e9, Guido Sanguinetti", "abstract": "Hyperbolic deep learning leverages the metric properties of hyperbolic spaces\nto develop efficient and informative embeddings of hierarchical data. Here, we\nfocus on the solvable group structure of hyperbolic spaces, which follows\nnaturally from their construction as symmetric spaces. This dual nature of Lie\ngroup and Riemannian manifold allows us to propose a new class of hyperbolic\ndeep learning algorithms where group homomorphisms are interleaved with\nmetric-preserving diffeomorphisms. The resulting algorithms, which we call\nCartan networks, show promising results on various benchmark data sets and open\nthe way to a novel class of hyperbolic deep learning architectures.", "journal": ""}
{"doi": "10.48550/arXiv.1902.03793", "date": "2019-02-11", "title": "Understanding over-parameterized deep networks by geometrization", "authors": "Xiao Dong, Ling Zhou", "abstract": "A complete understanding of the widely used over-parameterized deep networks\nis a key step for AI. In this work we try to give a geometric picture of\nover-parameterized deep networks using our geometrization scheme. We show that\nthe Riemannian geometry of network complexity plays a key role in understanding\nthe basic properties of over-parameterizaed deep networks, including the\ngeneralization, convergence and parameter sensitivity. We also point out deep\nnetworks share lots of similarities with quantum computation systems. This can\nbe regarded as a strong support of our proposal that geometrization is not only\nthe bible for physics, it is also the key idea to understand deep learning\nsystems.", "journal": ""}
{"doi": "10.48550/arXiv.1902.06320", "date": "2019-02-17", "title": "Towards Improved Testing For Deep Learning", "authors": "Jasmine Sekhon, Cody Fleming", "abstract": "The growing use of deep neural networks in safety-critical applications makes\nit necessary to carry out adequate testing to detect and correct any incorrect\nbehavior for corner case inputs before they can be actually used. Deep neural\nnetworks lack an explicit control-flow structure, making it impossible to apply\nto them traditional software testing criteria such as code coverage. In this\npaper, we examine existing testing methods for deep neural networks, the\nopportunities for improvement and the need for a fast, scalable, generalizable\nend-to-end testing method. We also propose a coverage criterion for deep neural\nnetworks that tries to capture all possible parts of the deep neural network's\nlogic.", "journal": ""}
{"doi": "10.48550/arXiv.2001.04114", "date": "2020-01-13", "title": "Approximation smooth and sparse functions by deep neural networks without saturation", "authors": "Xia Liu", "abstract": "Constructing neural networks for function approximation is a classical and\nlongstanding topic in approximation theory. In this paper, we aim at\nconstructing deep neural networks (deep nets for short) with three hidden\nlayers to approximate smooth and sparse functions. In particular, we prove that\nthe constructed deep nets can reach the optimal approximation rate in\napproximating both smooth and sparse functions with controllable magnitude of\nfree parameters. Since the saturation that describes the bottleneck of\napproximate is an insurmountable problem of constructive neural networks, we\nalso prove that deepening the neural network with only one more hidden layer\ncan avoid the saturation. The obtained results underlie advantages of deep nets\nand provide theoretical explanations for deep learning.", "journal": ""}
{"doi": "10.48550/arXiv.2004.02355", "date": "2020-04-06", "title": "Deep Multilayer Perceptrons for Dimensional Speech Emotion Recognition", "authors": "Bagus Tris Atmaja, Masato Akagi", "abstract": "Modern deep learning architectures are ordinarily performed on\nhigh-performance computing facilities due to the large size of the input\nfeatures and complexity of its model. This paper proposes traditional\nmultilayer perceptrons (MLP) with deep layers and small input size to tackle\nthat computation requirement limitation. The result shows that our proposed\ndeep MLP outperformed modern deep learning architectures, i.e., LSTM and CNN,\non the same number of layers and value of parameters. The deep MLP exhibited\nthe highest performance on both speaker-dependent and speaker-independent\nscenarios on IEMOCAP and MSP-IMPROV corpus.", "journal": "Asia-Pacific Signal and Information Processing Association Annual\n  Summit and Conference, APSIPA ASC 2020"}
{"doi": "10.48550/arXiv.2109.10317", "date": "2021-09-21", "title": "Introduction to Neural Network Verification", "authors": "Aws Albarghouthi", "abstract": "Deep learning has transformed the way we think of software and what it can\ndo. But deep neural networks are fragile and their behaviors are often\nsurprising. In many settings, we need to provide formal guarantees on the\nsafety, security, correctness, or robustness of neural networks. This book\ncovers foundational ideas from formal verification and their adaptation to\nreasoning about neural networks and deep learning.", "journal": ""}
{"doi": "10.48550/arXiv.2410.13110", "date": "2024-10-17", "title": "Deep Learning-based Software Engineering: Progress, Challenges, and Opportunities", "authors": "Xiangping Chen, Xing Hu, Yuan Huang, He Jiang, Weixing Ji, Yanjie Jiang, Yanyan Jiang, Bo Liu, Hui Liu, Xiaochen Li, Xiaoli Lian, Guozhu Meng, Xin Peng, Hailong Sun, Lin Shi, Bo Wang, Chong Wang, Jiayi Wang, Tiantian Wang, Jifeng Xuan, Xin Xia, Yibiao Yang, Yixin Yang, Li Zhang, Yuming Zhou, Lu Zhang", "abstract": "Researchers have recently achieved significant advances in deep learning\ntechniques, which in turn has substantially advanced other research\ndisciplines, such as natural language processing, image processing, speech\nrecognition, and software engineering. Various deep learning techniques have\nbeen successfully employed to facilitate software engineering tasks, including\ncode generation, software refactoring, and fault localization. Many papers have\nalso been presented in top conferences and journals, demonstrating the\napplications of deep learning techniques in resolving various software\nengineering tasks. However, although several surveys have provided overall\npictures of the application of deep learning techniques in software\nengineering, they focus more on learning techniques, that is, what kind of deep\nlearning techniques are employed and how deep models are trained or fine-tuned\nfor software engineering tasks. We still lack surveys explaining the advances\nof subareas in software engineering driven by deep learning techniques, as well\nas challenges and opportunities in each subarea. To this end, in this paper, we\npresent the first task-oriented survey on deep learning-based software\nengineering. It covers twelve major software engineering subareas significantly\nimpacted by deep learning techniques. Such subareas spread out the through the\nwhole lifecycle of software development and maintenance, including requirements\nengineering, software development, testing, maintenance, and developer\ncollaboration. As we believe that deep learning may provide an opportunity to\nrevolutionize the whole discipline of software engineering, providing one\nsurvey covering as many subareas as possible in software engineering can help\nfuture research push forward the frontier of deep learning-based software\nengineering more systematically.", "journal": ""}
{"doi": "10.48550/arXiv.2201.13380", "date": "2022-01-31", "title": "Deep Learning Macroeconomics", "authors": "Rafael R. S. Guimaraes", "abstract": "Limited datasets and complex nonlinear relationships are among the challenges\nthat may emerge when applying econometrics to macroeconomic problems. This\nresearch proposes deep learning as an approach to transfer learning in the\nformer case and to map relationships between variables in the latter case.\nAlthough macroeconomists already apply transfer learning when assuming a given\na priori distribution in a Bayesian context, estimating a structural VAR with\nsignal restriction and calibrating parameters based on results observed in\nother models, to name a few examples, advance in a more systematic transfer\nlearning strategy in applied macroeconomics is the innovation we are\nintroducing. We explore the proposed strategy empirically, showing that data\nfrom different but related domains, a type of transfer learning, helps identify\nthe business cycle phases when there is no business cycle dating committee and\nto quick estimate a economic-based output gap. Next, since deep learning\nmethods are a way of learning representations, those that are formed by the\ncomposition of multiple non-linear transformations, to yield more abstract\nrepresentations, we apply deep learning for mapping low-frequency from\nhigh-frequency variables. The results obtained show the suitability of deep\nlearning models applied to macroeconomic problems. First, models learned to\nclassify United States business cycles correctly. Then, applying transfer\nlearning, they were able to identify the business cycles of out-of-sample\nBrazilian and European data. Along the same lines, the models learned to\nestimate the output gap based on the U.S. data and obtained good performance\nwhen faced with Brazilian data. Additionally, deep learning proved adequate for\nmapping low-frequency variables from high-frequency data to interpolate,\ndistribute, and extrapolate time series by related series.", "journal": ""}
{"doi": "10.48550/arXiv.2102.06285", "date": "2021-02-11", "title": "COVID-19 detection from scarce chest x-ray image data using few-shot deep learning approach", "authors": "Shruti Jadon", "abstract": "In the current COVID-19 pandemic situation, there is an urgent need to screen\ninfected patients quickly and accurately. Using deep learning models trained on\nchest X-ray images can become an efficient method for screening COVID-19\npatients in these situations. Deep learning approaches are already widely used\nin the medical community. However, they require a large amount of data to be\naccurate. The open-source community collectively has made efforts to collect\nand annotate the data, but it is not enough to train an accurate deep learning\nmodel. Few-shot learning is a sub-field of machine learning that aims to learn\nthe objective with less amount of data. In this work, we have experimented with\nwell-known solutions for data scarcity in deep learning to detect COVID-19.\nThese include data augmentation, transfer learning, and few-shot learning, and\nunsupervised learning. We have also proposed a custom few-shot learning\napproach to detect COVID-19 using siamese networks. Our experimental results\nshowcased that we can implement an efficient and accurate deep learning model\nfor COVID-19 detection by adopting the few-shot learning approaches even with\nless amount of data. Using our proposed approach we were able to achieve 96.4%\naccuracy an improvement from 83% using baseline models.", "journal": ""}
{"doi": "10.48550/arXiv.1511.03855", "date": "2015-11-12", "title": "Feature Learning based Deep Supervised Hashing with Pairwise Labels", "authors": "Wu-Jun Li, Sheng Wang, Wang-Cheng Kang", "abstract": "Recent years have witnessed wide application of hashing for large-scale image\nretrieval. However, most existing hashing methods are based on hand-crafted\nfeatures which might not be optimally compatible with the hashing procedure.\nRecently, deep hashing methods have been proposed to perform simultaneous\nfeature learning and hash-code learning with deep neural networks, which have\nshown better performance than traditional hashing methods with hand-crafted\nfeatures. Most of these deep hashing methods are supervised whose supervised\ninformation is given with triplet labels. For another common application\nscenario with pairwise labels, there have not existed methods for simultaneous\nfeature learning and hash-code learning. In this paper, we propose a novel deep\nhashing method, called deep pairwise-supervised hashing(DPSH), to perform\nsimultaneous feature learning and hash-code learning for applications with\npairwise labels. Experiments on real datasets show that our DPSH method can\noutperform other methods to achieve the state-of-the-art performance in image\nretrieval applications.", "journal": ""}
{"doi": "10.48550/arXiv.1511.09337", "date": "2015-11-30", "title": "Cost-aware Pre-training for Multiclass Cost-sensitive Deep Learning", "authors": "Yu-An Chung, Hsuan-Tien Lin, Shao-Wen Yang", "abstract": "Deep learning has been one of the most prominent machine learning techniques\nnowadays, being the state-of-the-art on a broad range of applications where\nautomatic feature extraction is needed. Many such applications also demand\nvarying costs for different types of mis-classification errors, but it is not\nclear whether or how such cost information can be incorporated into deep\nlearning to improve performance. In this work, we propose a novel cost-aware\nalgorithm that takes into account the cost information into not only the\ntraining stage but also the pre-training stage of deep learning. The approach\nallows deep learning to conduct automatic feature extraction with the cost\ninformation effectively. Extensive experimental results demonstrate that the\nproposed approach outperforms other deep learning models that do not digest the\ncost information in the pre-training stage.", "journal": ""}
{"doi": "10.48550/arXiv.1602.07031", "date": "2016-02-23", "title": "Mobile Big Data Analytics Using Deep Learning and Apache Spark", "authors": "Mohammad Abu Alsheikh, Dusit Niyato, Shaowei Lin, Hwee-Pink Tan, Zhu Han", "abstract": "The proliferation of mobile devices, such as smartphones and Internet of\nThings (IoT) gadgets, results in the recent mobile big data (MBD) era.\nCollecting MBD is unprofitable unless suitable analytics and learning methods\nare utilized for extracting meaningful information and hidden patterns from\ndata. This article presents an overview and brief tutorial of deep learning in\nMBD analytics and discusses a scalable learning framework over Apache Spark.\nSpecifically, a distributed deep learning is executed as an iterative MapReduce\ncomputing on many Spark workers. Each Spark worker learns a partial deep model\non a partition of the overall MBD, and a master deep model is then built by\naveraging the parameters of all partial models. This Spark-based framework\nspeeds up the learning of deep models consisting of many hidden layers and\nmillions of parameters. We use a context-aware activity recognition application\nwith a real-world dataset containing millions of samples to validate our\nframework and assess its speedup effectiveness.", "journal": "IEEE Network, vol. 30, no. 3, pp. 22-29, June 2016"}
{"doi": "10.48550/arXiv.1710.10784", "date": "2017-10-30", "title": "How deep learning works --The geometry of deep learning", "authors": "Xiao Dong, Jiasong Wu, Ling Zhou", "abstract": "Why and how that deep learning works well on different tasks remains a\nmystery from a theoretical perspective. In this paper we draw a geometric\npicture of the deep learning system by finding its analogies with two existing\ngeometric structures, the geometry of quantum computations and the geometry of\nthe diffeomorphic template matching. In this framework, we give the geometric\nstructures of different deep learning systems including convolutional neural\nnetworks, residual networks, recursive neural networks, recurrent neural\nnetworks and the equilibrium prapagation framework. We can also analysis the\nrelationship between the geometrical structures and their performance of\ndifferent networks in an algorithmic level so that the geometric framework may\nguide the design of the structures and algorithms of deep learning systems.", "journal": ""}
{"doi": "10.48550/arXiv.1807.01083", "date": "2018-07-03", "title": "A Mean-Field Optimal Control Formulation of Deep Learning", "authors": "Weinan E, Jiequn Han, Qianxiao Li", "abstract": "Recent work linking deep neural networks and dynamical systems opened up new\navenues to analyze deep learning. In particular, it is observed that new\ninsights can be obtained by recasting deep learning as an optimal control\nproblem on difference or differential equations. However, the mathematical\naspects of such a formulation have not been systematically explored. This paper\nintroduces the mathematical formulation of the population risk minimization\nproblem in deep learning as a mean-field optimal control problem. Mirroring the\ndevelopment of classical optimal control, we state and prove optimality\nconditions of both the Hamilton-Jacobi-Bellman type and the Pontryagin type.\nThese mean-field results reflect the probabilistic nature of the learning\nproblem. In addition, by appealing to the mean-field Pontryagin's maximum\nprinciple, we establish some quantitative relationships between population and\nempirical learning problems. This serves to establish a mathematical foundation\nfor investigating the algorithmic and theoretical connections between optimal\ncontrol and deep learning.", "journal": "Research in the Mathematical Sciences, 6:10 (2019)"}
{"doi": "10.48550/arXiv.1807.11809", "date": "2018-07-31", "title": "Deep learning in agriculture: A survey", "authors": "Andreas Kamilaris, Francesc X. Prenafeta-Boldu", "abstract": "Deep learning constitutes a recent, modern technique for image processing and\ndata analysis, with promising results and large potential. As deep learning has\nbeen successfully applied in various domains, it has recently entered also the\ndomain of agriculture. In this paper, we perform a survey of 40 research\nefforts that employ deep learning techniques, applied to various agricultural\nand food production challenges. We examine the particular agricultural problems\nunder study, the specific models and frameworks employed, the sources, nature\nand pre-processing of data used, and the overall performance achieved according\nto the metrics used at each work under study. Moreover, we study comparisons of\ndeep learning with other existing popular techniques, in respect to differences\nin classification or regression performance. Our findings indicate that deep\nlearning provides high accuracy, outperforming existing commonly used image\nprocessing techniques.", "journal": "Computers and Electronics in Agriculture International Journal,\n  2018"}
{"doi": "10.48550/arXiv.1810.06665", "date": "2018-10-15", "title": "Stop Illegal Comments: A Multi-Task Deep Learning Approach", "authors": "Ahmed Elnaggar, Bernhard Waltl, Ingo Glaser, J\u00f6rg Landthaler, Elena Scepankova, Florian Matthes", "abstract": "Deep learning methods are often difficult to apply in the legal domain due to\nthe large amount of labeled data required by deep learning methods. A recent\nnew trend in the deep learning community is the application of multi-task\nmodels that enable single deep neural networks to perform more than one task at\nthe same time, for example classification and translation tasks. These powerful\nnovel models are capable of transferring knowledge among different tasks or\ntraining sets and therefore could open up the legal domain for many deep\nlearning applications. In this paper, we investigate the transfer learning\ncapabilities of such a multi-task model on a classification task on the\npublicly available Kaggle toxic comment dataset for classifying illegal\ncomments and we can report promising results.", "journal": ""}
{"doi": "10.48550/arXiv.2006.10027", "date": "2020-06-17", "title": "Deep Learning Meets SAR", "authors": "Xiao Xiang Zhu, Sina Montazeri, Mohsin Ali, Yuansheng Hua, Yuanyuan Wang, Lichao Mou, Yilei Shi, Feng Xu, Richard Bamler", "abstract": "Deep learning in remote sensing has become an international hype, but it is\nmostly limited to the evaluation of optical data. Although deep learning has\nbeen introduced in Synthetic Aperture Radar (SAR) data processing, despite\nsuccessful first attempts, its huge potential remains locked. In this paper, we\nprovide an introduction to the most relevant deep learning models and concepts,\npoint out possible pitfalls by analyzing special characteristics of SAR data,\nreview the state-of-the-art of deep learning applied to SAR in depth, summarize\navailable benchmarks, and recommend some important future research directions.\nWith this effort, we hope to stimulate more research in this interesting yet\nunder-exploited research field and to pave the way for use of deep learning in\nbig SAR data processing workflows.", "journal": ""}
{"doi": "10.48550/arXiv.1809.06064", "date": "2018-09-17", "title": "Object-sensitive Deep Reinforcement Learning", "authors": "Yuezhang Li, Katia Sycara, Rahul Iyer", "abstract": "Deep reinforcement learning has become popular over recent years, showing\nsuperiority on different visual-input tasks such as playing Atari games and\nrobot navigation. Although objects are important image elements, few work\nconsiders enhancing deep reinforcement learning with object characteristics. In\nthis paper, we propose a novel method that can incorporate object recognition\nprocessing to deep reinforcement learning models. This approach can be adapted\nto any existing deep reinforcement learning frameworks. State-of-the-art\nresults are shown in experiments on Atari games. We also propose a new approach\ncalled \"object saliency maps\" to visually explain the actions made by deep\nreinforcement learning agents.", "journal": ""}
{"doi": "10.48550/arXiv.1904.07320", "date": "2019-04-12", "title": "Low-Rank Deep Convolutional Neural Network for Multi-Task Learning", "authors": "Fang Su, Hai-Yang Shang, Jing-Yan Wang", "abstract": "In this paper, we propose a novel multi-task learning method based on the\ndeep convolutional network. The proposed deep network has four convolutional\nlayers, three max-pooling layers, and two parallel fully connected layers. To\nadjust the deep network to multi-task learning problem, we propose to learn a\nlow-rank deep network so that the relation among different tasks can be\nexplored. We proposed to minimize the number of independent parameter rows of\none fully connected layer to explore the relations among different tasks, which\nis measured by the nuclear norm of the parameter of one fully connected layer,\nand seek a low-rank parameter matrix. Meanwhile, we also propose to regularize\nanother fully connected layer by sparsity penalty, so that the useful features\nlearned by the lower layers can be selected. The learning problem is solved by\nan iterative algorithm based on gradient descent and back-propagation\nalgorithms. The proposed algorithm is evaluated over benchmark data sets of\nmultiple face attribute prediction, multi-task natural language processing, and\njoint economics index predictions. The evaluation results show the advantage of\nthe low-rank deep CNN model over multi-task problems.", "journal": ""}
{"doi": "10.48550/arXiv.1910.12799", "date": "2019-10-28", "title": "Deep learning is adaptive to intrinsic dimensionality of model smoothness in anisotropic Besov space", "authors": "Taiji Suzuki, Atsushi Nitanda", "abstract": "Deep learning has exhibited superior performance for various tasks,\nespecially for high-dimensional datasets, such as images. To understand this\nproperty, we investigate the approximation and estimation ability of deep\nlearning on anisotropic Besov spaces. The anisotropic Besov space is\ncharacterized by direction-dependent smoothness and includes several function\nclasses that have been investigated thus far. We demonstrate that the\napproximation error and estimation error of deep learning only depend on the\naverage value of the smoothness parameters in all directions. Consequently, the\ncurse of dimensionality can be avoided if the smoothness of the target function\nis highly anisotropic. Unlike existing studies, our analysis does not require a\nlow-dimensional structure of the input data. We also investigate the minimax\noptimality of deep learning and compare its performance with that of the kernel\nmethod (more generally, linear estimators). The results show that deep learning\nhas better dependence on the input dimensionality if the target function\npossesses anisotropic smoothness, and it achieves an adaptive rate for\nfunctions with spatially inhomogeneous smoothness.", "journal": ""}
{"doi": "10.48550/arXiv.2003.13541", "date": "2020-03-30", "title": "A Privacy-Preserving Distributed Architecture for Deep-Learning-as-a-Service", "authors": "Simone Disabato, Alessandro Falcetta, Alessio Mongelluzzo, Manuel Roveri", "abstract": "Deep-learning-as-a-service is a novel and promising computing paradigm aiming\nat providing machine/deep learning solutions and mechanisms through Cloud-based\ncomputing infrastructures. Thanks to its ability to remotely execute and train\ndeep learning models (that typically require high computational loads and\nmemory occupation), such an approach guarantees high performance, scalability,\nand availability. Unfortunately, such an approach requires to send information\nto be processed (e.g., signals, images, positions, sounds, videos) to the\nCloud, hence having potentially catastrophic-impacts on the privacy of users.\nThis paper introduces a novel distributed architecture for\ndeep-learning-as-a-service that is able to preserve the user sensitive data\nwhile providing Cloud-based machine and deep learning services. The proposed\narchitecture, which relies on Homomorphic Encryption that is able to perform\noperations on encrypted data, has been tailored for Convolutional Neural\nNetworks (CNNs) in the domain of image analysis and implemented through a\nclient-server REST-based approach. Experimental results show the effectiveness\nof the proposed architecture.", "journal": ""}
{"doi": "10.48550/arXiv.1708.01867", "date": "2017-08-06", "title": "An Information-Theoretic Optimality Principle for Deep Reinforcement Learning", "authors": "Felix Leibfried, Jordi Grau-Moya, Haitham Bou-Ammar", "abstract": "We methodologically address the problem of Q-value overestimation in deep\nreinforcement learning to handle high-dimensional state spaces efficiently. By\nadapting concepts from information theory, we introduce an intrinsic penalty\nsignal encouraging reduced Q-value estimates. The resultant algorithm\nencompasses a wide range of learning outcomes containing deep Q-networks as a\nspecial case. Different learning outcomes can be demonstrated by tuning a\nLagrange multiplier accordingly. We furthermore propose a novel scheduling\nscheme for this Lagrange multiplier to ensure efficient and robust learning. In\nexperiments on Atari, our algorithm outperforms other algorithms (e.g. deep and\ndouble deep Q-networks) in terms of both game-play performance and sample\ncomplexity. These results remain valid under the recently proposed dueling\narchitecture.", "journal": ""}
{"doi": "10.48550/arXiv.1804.01653", "date": "2018-04-05", "title": "Review of Deep Learning", "authors": "Rong Zhang, Weiping Li, Tong Mo", "abstract": "In recent years, China, the United States and other countries, Google and\nother high-tech companies have increased investment in artificial intelligence.\nDeep learning is one of the current artificial intelligence research's key\nareas. This paper analyzes and summarizes the latest progress and future\nresearch directions of deep learning. Firstly, three basic models of deep\nlearning are outlined, including multilayer perceptrons, convolutional neural\nnetworks, and recurrent neural networks. On this basis, we further analyze the\nemerging new models of convolution neural networks and recurrent neural\nnetworks. This paper then summarizes deep learning's applications in many areas\nof artificial intelligence, including speech processing, computer vision,\nnatural language processing and so on. Finally, this paper discusses the\nexisting problems of deep learning and gives the corresponding possible\nsolutions.", "journal": ""}
{"doi": "10.48550/arXiv.1804.06309", "date": "2018-04-17", "title": "On Improving Deep Reinforcement Learning for POMDPs", "authors": "Pengfei Zhu, Xin Li, Pascal Poupart, Guanghui Miao", "abstract": "Deep Reinforcement Learning (RL) recently emerged as one of the most\ncompetitive approaches for learning in sequential decision making problems with\nfully observable environments, e.g., computer Go. However, very little work has\nbeen done in deep RL to handle partially observable environments. We propose a\nnew architecture called Action-specific Deep Recurrent Q-Network (ADRQN) to\nenhance learning performance in partially observable domains. Actions are\nencoded by a fully connected layer and coupled with a convolutional observation\nto form an action-observation pair. The time series of action-observation pairs\nare then integrated by an LSTM layer that learns latent states based on which a\nfully connected layer computes Q-values as in conventional Deep Q-Networks\n(DQNs). We demonstrate the effectiveness of our new architecture in several\npartially observable domains, including flickering Atari games.", "journal": ""}
{"doi": "10.48550/arXiv.1902.00566", "date": "2019-02-01", "title": "Visual Rationalizations in Deep Reinforcement Learning for Atari Games", "authors": "Laurens Weitkamp, Elise van der Pol, Zeynep Akata", "abstract": "Due to the capability of deep learning to perform well in high dimensional\nproblems, deep reinforcement learning agents perform well in challenging tasks\nsuch as Atari 2600 games. However, clearly explaining why a certain action is\ntaken by the agent can be as important as the decision itself. Deep\nreinforcement learning models, as other deep learning models, tend to be opaque\nin their decision-making process. In this work, we propose to make deep\nreinforcement learning more transparent by visualizing the evidence on which\nthe agent bases its decision. In this work, we emphasize the importance of\nproducing a justification for an observed action, which could be applied to a\nblack-box decision agent.", "journal": ""}
{"doi": "10.48550/arXiv.1907.03626", "date": "2019-07-05", "title": "Benchmarking Contemporary Deep Learning Hardware and Frameworks:A Survey of Qualitative Metrics", "authors": "Wei Dai, Daniel Berleant", "abstract": "This paper surveys benchmarking principles, machine learning devices\nincluding GPUs, FPGAs, and ASICs, and deep learning software frameworks. It\nalso reviews these technologies with respect to benchmarking from the\nperspectives of a 6-metric approach to frameworks and an 11-metric approach to\nhardware platforms. Because MLPerf is a benchmark organization working with\nindustry and academia, and offering deep learning benchmarks that evaluate\ntraining and inference on deep learning hardware devices, the survey also\nmentions MLPerf benchmark results, benchmark metrics, datasets, deep learning\nframeworks and algorithms. We summarize seven benchmarking principles,\ndifferential characteristics of mainstream AI devices, and qualitative\ncomparison of deep learning hardware and frameworks.", "journal": ""}
{"doi": "10.48550/arXiv.2009.01989", "date": "2020-09-04", "title": "A Comprehensive Analysis of Information Leakage in Deep Transfer Learning", "authors": "Cen Chen, Bingzhe Wu, Minghui Qiu, Li Wang, Jun Zhou", "abstract": "Transfer learning is widely used for transferring knowledge from a source\ndomain to the target domain where the labeled data is scarce. Recently, deep\ntransfer learning has achieved remarkable progress in various applications.\nHowever, the source and target datasets usually belong to two different\norganizations in many real-world scenarios, potential privacy issues in deep\ntransfer learning are posed. In this study, to thoroughly analyze the potential\nprivacy leakage in deep transfer learning, we first divide previous methods\ninto three categories. Based on that, we demonstrate specific threats that lead\nto unintentional privacy leakage in each category. Additionally, we also\nprovide some solutions to prevent these threats. To the best of our knowledge,\nour study is the first to provide a thorough analysis of the information\nleakage issues in deep transfer learning methods and provide potential\nsolutions to the issue. Extensive experiments on two public datasets and an\nindustry dataset are conducted to show the privacy leakage under different deep\ntransfer learning settings and defense solution effectiveness.", "journal": ""}
{"doi": "10.48550/arXiv.2009.11112", "date": "2020-09-23", "title": "ANNdotNET -- deep learning tool on .NET Platform", "authors": "Bahrudin Hrnjica", "abstract": "ANNdotNET is an open source project for deep learning written in C# with\nability to create, train, evaluate and export deep learning models. The project\nconsists of the Graphical User Interface module capable to visually prepare\ndata, fine tune hyper-parameters, design network architecture, evaluate and\ntest trained models. The ANNdotNET introduces the Visual Network Designer,\n(VND) for visually design almost any sequential deep learning network. Beside\nVND, ANNdotNET implements Machine Learning Engine, (MLE) based on CNTK - deep\nlearning framework, with ability to train and evaluate models on GPU. For model\nevaluation ANNdotNET contains rich set of visual and descriptive performance\nparameters, history of the training process and set of export/deployment\noptions. The advantage of using ANNdotNET over the classic code based ML\napproach is more focus on deep learning network design and training process\ninstead of focusing on coding and debugging. It is ideal for engineers not\nfamiliar with supported programming languages. The project is hosted at\ngithub.com/bhrnjica/anndotnet.", "journal": ""}
{"doi": "10.48550/arXiv.2012.02825", "date": "2020-12-04", "title": "A Survey on Deep Learning for Human Mobility", "authors": "Massimiliano Luca, Gianni Barlacchi, Bruno Lepri, Luca Pappalardo", "abstract": "The study of human mobility is crucial due to its impact on several aspects\nof our society, such as disease spreading, urban planning, well-being,\npollution, and more. The proliferation of digital mobility data, such as phone\nrecords, GPS traces, and social media posts, combined with the predictive power\nof artificial intelligence, triggered the application of deep learning to human\nmobility. Existing surveys focus on single tasks, data sources, mechanistic or\ntraditional machine learning approaches, while a comprehensive description of\ndeep learning solutions is missing. This survey provides a taxonomy of mobility\ntasks, a discussion on the challenges related to each task and how deep\nlearning may overcome the limitations of traditional models, a description of\nthe most relevant solutions to the mobility tasks described above and the\nrelevant challenges for the future. Our survey is a guide to the leading deep\nlearning solutions to next-location prediction, crowd flow prediction,\ntrajectory generation, and flow generation. At the same time, it helps deep\nlearning scientists and practitioners understand the fundamental concepts and\nthe open challenges of the study of human mobility.", "journal": ""}
{"doi": "10.48550/arXiv.2103.15213", "date": "2021-03-28", "title": "A Temporal Kernel Approach for Deep Learning with Continuous-time Information", "authors": "Da Xu, Chuanwei Ruan, Evren Korpeoglu, Sushant Kumar, Kannan Achan", "abstract": "Sequential deep learning models such as RNN, causal CNN and attention\nmechanism do not readily consume continuous-time information. Discretizing the\ntemporal data, as we show, causes inconsistency even for simple continuous-time\nprocesses. Current approaches often handle time in a heuristic manner to be\nconsistent with the existing deep learning architectures and implementations.\nIn this paper, we provide a principled way to characterize continuous-time\nsystems using deep learning tools. Notably, the proposed approach applies to\nall the major deep learning architectures and requires little modifications to\nthe implementation. The critical insight is to represent the continuous-time\nsystem by composing neural networks with a temporal kernel, where we gain our\nintuition from the recent advancements in understanding deep learning with\nGaussian process and neural tangent kernel. To represent the temporal kernel,\nwe introduce the random feature approach and convert the kernel learning\nproblem to spectral density estimation under reparameterization. We further\nprove the convergence and consistency results even when the temporal kernel is\nnon-stationary, and the spectral density is misspecified. The simulations and\nreal-data experiments demonstrate the empirical effectiveness of our temporal\nkernel approach in a broad range of settings.", "journal": ""}
{"doi": "10.48550/arXiv.2110.14597", "date": "2021-10-03", "title": "Evaluating Deep Learning Models and Adversarial Attacks on Accelerometer-Based Gesture Authentication", "authors": "Elliu Huang, Fabio Di Troia, Mark Stamp", "abstract": "Gesture-based authentication has emerged as a non-intrusive, effective means\nof authenticating users on mobile devices. Typically, such authentication\ntechniques have relied on classical machine learning techniques, but recently,\ndeep learning techniques have been applied this problem. Although prior\nresearch has shown that deep learning models are vulnerable to adversarial\nattacks, relatively little research has been done in the adversarial domain for\nbehavioral biometrics. In this research, we collect tri-axial accelerometer\ngesture data (TAGD) from 46 users and perform classification experiments with\nboth classical machine learning and deep learning models. Specifically, we\ntrain and test support vector machines (SVM) and convolutional neural networks\n(CNN). We then consider a realistic adversarial attack, where we assume the\nattacker has access to real users' TAGD data, but not the authentication model.\nWe use a deep convolutional generative adversarial network (DC-GAN) to create\nadversarial samples, and we show that our deep learning model is surprisingly\nrobust to such an attack scenario.", "journal": ""}
{"doi": "10.48550/arXiv.2112.09741", "date": "2021-12-17", "title": "Envisioning Future Deep Learning Theories: Some Basic Concepts and Characteristics", "authors": "Weijie J. Su", "abstract": "To advance deep learning methodologies in the next decade, a theoretical\nframework for reasoning about modern neural networks is needed. While efforts\nare increasing toward demystifying why deep learning is so effective, a\ncomprehensive picture remains lacking, suggesting that a better theory is\npossible. We argue that a future deep learning theory should inherit three\ncharacteristics: a \\textit{hierarchically} structured network architecture,\nparameters \\textit{iteratively} optimized using stochastic gradient-based\nmethods, and information from the data that evolves \\textit{compressively}. As\nan instantiation, we integrate these characteristics into a graphical model\ncalled \\textit{neurashed}. This model effectively explains some common\nempirical patterns in deep learning. In particular, neurashed enables insights\ninto implicit regularization, information bottleneck, and local elasticity.\nFinally, we discuss how neurashed can guide the development of deep learning\ntheories.", "journal": ""}
{"doi": "10.48550/arXiv.2203.06347", "date": "2022-03-12", "title": "Combining Deep Learning with Physics Based Features in Explosion-Earthquake Discrimination", "authors": "Qingkai Kong, Ruijia Wang, William R. Walter, Moira Pyle, Keith Koper, Brandon Schmandt", "abstract": "This paper combines the power of deep-learning with the generalizability of\nphysics-based features, to present an advanced method for seismic\ndiscrimination between earthquakes and explosions. The proposed method contains\ntwo branches: a deep learning branch operating directly on seismic waveforms or\nspectrograms, and a second branch operating on physics-based parametric\nfeatures. These features are high-frequency P/S amplitude ratios and the\ndifference between local magnitude (ML) and coda duration magnitude (MC). The\ncombination achieves better generalization performance when applied to new\nregions than models that are developed solely with deep learning. We also\nexamined which parts of the waveform data dominate deep learning decisions\n(i.e., via Grad-CAM). Such visualization provides a window into the black-box\nnature of the machine-learning models and offers new insight into how the deep\nlearning derived models use data to make the decisions.", "journal": ""}
{"doi": "10.48550/arXiv.2210.00973", "date": "2022-10-03", "title": "NCVX: A General-Purpose Optimization Solver for Constrained Machine and Deep Learning", "authors": "Buyun Liang, Tim Mitchell, Ju Sun", "abstract": "Imposing explicit constraints is relatively new but increasingly pressing in\ndeep learning, stimulated by, e.g., trustworthy AI that performs robust\noptimization over complicated perturbation sets and scientific applications\nthat need to respect physical laws and constraints. However, it can be hard to\nreliably solve constrained deep learning problems without optimization\nexpertise. The existing deep learning frameworks do not admit constraints.\nGeneral-purpose optimization packages can handle constraints but do not perform\nauto-differentiation and have trouble dealing with nonsmoothness. In this\npaper, we introduce a new software package called NCVX, whose initial release\ncontains the solver PyGRANSO, a PyTorch-enabled general-purpose optimization\npackage for constrained machine/deep learning problems, the first of its kind.\nNCVX inherits auto-differentiation, GPU acceleration, and tensor variables from\nPyTorch, and is built on freely available and widely used open-source\nframeworks. NCVX is available at https://ncvx.org, with detailed documentation\nand numerous examples from machine/deep learning and other fields.", "journal": ""}
{"doi": "10.48550/arXiv.2211.09705", "date": "2022-10-27", "title": "A Review of Deep Learning Techniques for Protein Function Prediction", "authors": "Divyanshu Aggarwal, Yasha Hasija", "abstract": "Deep Learning and big data have shown tremendous success in bioinformatics\nand computational biology in recent years; artificial intelligence methods have\nalso significantly contributed in the task of protein function classification.\nThis review paper analyzes the recent developments in approaches for the task\nof predicting protein function using deep learning. We explain the importance\nof determining the protein function and why automating the following task is\ncrucial. Then, after reviewing the widely used deep learning techniques for\nthis task, we continue our review and highlight the emergence of the modern\nState of The Art (SOTA) deep learning models which have achieved groundbreaking\nresults in the field of computer vision, natural language processing and\nmulti-modal learning in the last few years. We hope that this review will\nprovide a broad view of the current role and advances of deep learning in\nbiological sciences, especially in predicting protein function tasks and\nencourage new researchers to contribute to this area.", "journal": "2021 2nd International Conference for Emerging Technology (INCET)\n  Belgaum, India. May 21-23, 2021"}
{"doi": "10.48550/arXiv.2301.07487", "date": "2023-01-17", "title": "Adversarial Robust Deep Reinforcement Learning Requires Redefining Robustness", "authors": "Ezgi Korkmaz", "abstract": "Learning from raw high dimensional data via interaction with a given\nenvironment has been effectively achieved through the utilization of deep\nneural networks. Yet the observed degradation in policy performance caused by\nimperceptible worst-case policy dependent translations along high sensitivity\ndirections (i.e. adversarial perturbations) raises concerns on the robustness\nof deep reinforcement learning policies. In our paper, we show that these high\nsensitivity directions do not lie only along particular worst-case directions,\nbut rather are more abundant in the deep neural policy landscape and can be\nfound via more natural means in a black-box setting. Furthermore, we show that\nvanilla training techniques intriguingly result in learning more robust\npolicies compared to the policies learnt via the state-of-the-art adversarial\ntraining techniques. We believe our work lays out intriguing properties of the\ndeep reinforcement learning policy manifold and our results can help to build\nrobust and generalizable deep reinforcement learning policies.", "journal": ""}
{"doi": "10.48550/arXiv.2302.06370", "date": "2023-02-13", "title": "Review of Deep Reinforcement Learning for Autonomous Driving", "authors": "B. Udugama", "abstract": "Since deep neural networks' resurgence, reinforcement learning has gradually\nstrengthened and surpassed humans in many conventional games. However, it is\nnot easy to copy these accomplishments to autonomous driving because state\nspaces are immensely complicated in the real world and action spaces are\ncontinuous and fine control is necessary. Besides, autonomous driving systems\nmust also maintain their functionality regardless of the environment's\ncomplexity. The deep reinforcement learning domain (DRL) has become a robust\nlearning framework to handle complex policies in high dimensional surroundings\nwith deep representation learning. This research outlines deep, reinforcement\nlearning algorithms (DRL). It presents a nomenclature of autonomous driving in\nwhich DRL techniques have been used, thus discussing important computational\nissues in evaluating autonomous driving agents in the real environment.\nInstead, it involves similar but not standard RL techniques, adjoining fields\nsuch as emulation of actions, modelling imitation, inverse reinforcement\nlearning. The simulators' role in training agents is addressed, as are the\nmethods for validating, checking and robustness of existing RL solutions.", "journal": ""}
{"doi": "10.48550/arXiv.2310.00727", "date": "2023-10-01", "title": "Review of deep learning in healthcare", "authors": "Hasan Hejbari Zargar, Saha Hejbari Zargar, Raziye Mehri", "abstract": "Given the growing complexity of healthcare data over the last several years,\nusing machine learning techniques like Deep Neural Network (DNN) models has\ngained increased appeal. In order to extract hidden patterns and other valuable\ninformation from the huge quantity of health data, which traditional analytics\nare unable to do in a reasonable length of time, machine learning (ML)\ntechniques are used. Deep Learning (DL) algorithms in particular have been\nshown as potential approaches to pattern identification in healthcare systems.\nThis thought has led to the contribution of this research, which examines deep\nlearning methods used in healthcare systems via an examination of cutting-edge\nnetwork designs, applications, and market trends. To connect deep learning\nmethodologies and human healthcare interpretability, the initial objective is\nto provide in-depth insight into the deployment of deep learning models in\nhealthcare solutions. And last, to outline the current unresolved issues and\npotential directions.", "journal": ""}
{"doi": "10.48550/arXiv.2405.01304", "date": "2024-05-02", "title": "Misclassification bounds for PAC-Bayesian sparse deep learning", "authors": "The Tien Mai", "abstract": "Recently, there has been a significant focus on exploring the theoretical\naspects of deep learning, especially regarding its performance in\nclassification tasks. Bayesian deep learning has emerged as a unified\nprobabilistic framework, seeking to integrate deep learning with Bayesian\nmethodologies seamlessly. However, there exists a gap in the theoretical\nunderstanding of Bayesian approaches in deep learning for classification. This\nstudy presents an attempt to bridge that gap. By leveraging PAC-Bayes bounds\ntechniques, we present theoretical results on the prediction or\nmisclassification error of a probabilistic approach utilizing Spike-and-Slab\npriors for sparse deep learning in classification. We establish non-asymptotic\nresults for the prediction error. Additionally, we demonstrate that, by\nconsidering different architectures, our results can achieve minimax optimal\nrates in both low and high-dimensional settings, up to a logarithmic factor.\nMoreover, our additional logarithmic term yields slight improvements over\nprevious works. Additionally, we propose and analyze an automated model\nselection approach aimed at optimally choosing a network architecture with\nguaranteed optimality.", "journal": "Machine Learning 2025"}
{"doi": "10.48550/arXiv.2406.17001", "date": "2024-06-24", "title": "Deep Learning for Prediction and Classifying the Dynamical behaviour of Piecewise Smooth Maps", "authors": "Vismaya V S, Bharath V Nair, Sishu Shankar Muni", "abstract": "This paper explores the prediction of the dynamics of piecewise smooth maps\nusing various deep learning models. We have shown various novel ways of\npredicting the dynamics of piecewise smooth maps using deep learning models.\nMoreover, we have used machine learning models such as Decision Tree\nClassifier, Logistic Regression, K-Nearest Neighbor, Random Forest, and Support\nVector Machine for predicting the border collision bifurcation in the 1D normal\nform map and the 1D tent map. Further, we classified the regular and chaotic\nbehaviour of the 1D tent map and the 2D Lozi map using deep learning models\nlike Convolutional Neural Network (CNN), ResNet50, and ConvLSTM via cobweb\ndiagram and phase portraits. We also classified the chaotic and hyperchaotic\nbehaviour of the 3D piecewise smooth map using deep learning models such as the\nFeed Forward Neural Network (FNN), Long Short-Term Memory (LSTM), and Recurrent\nNeural Network (RNN). Finally, deep learning models such as Long Short-Term\nMemory (LSTM) and Recurrent Neural Network (RNN) are used for reconstructing\nthe two parametric charts of 2D border collision bifurcation normal form map.", "journal": ""}
{"doi": "10.48550/arXiv.2409.08356", "date": "2024-09-12", "title": "COMEX Copper Futures Volatility Forecasting: Econometric Models and Deep Learning", "authors": "Zian Wang, Xinyi Lu", "abstract": "This paper investigates the forecasting performance of COMEX copper futures\nrealized volatility across various high-frequency intervals using both\neconometric volatility models and deep learning recurrent neural network\nmodels. The econometric models considered are GARCH and HAR, while the deep\nlearning models include RNN (Recurrent Neural Network), LSTM (Long Short-Term\nMemory), and GRU (Gated Recurrent Unit). In forecasting daily realized\nvolatility for COMEX copper futures with a rolling window approach, the\neconometric models, particularly HAR, outperform recurrent neural networks\noverall, with HAR achieving the lowest QLIKE loss function value. However, when\nthe data is replaced with hourly high-frequency realized volatility, the deep\nlearning models outperform the GARCH model, and HAR attains a comparable QLIKE\nloss function value. Despite the black-box nature of machine learning models,\nthe deep learning models demonstrate superior forecasting performance,\nsurpassing the fixed QLIKE value of HAR in the experiment. Moreover, as the\nforecast horizon extends for daily realized volatility, deep learning models\ngradually close the performance gap with the GARCH model in certain loss\nfunction metrics. Nonetheless, HAR remains the most effective model overall for\ndaily realized volatility forecasting in copper futures.", "journal": ""}
{"doi": "10.48550/arXiv.2009.07888", "date": "2020-09-16", "title": "Transfer Learning in Deep Reinforcement Learning: A Survey", "authors": "Zhuangdi Zhu, Kaixiang Lin, Anil K. Jain, Jiayu Zhou", "abstract": "Reinforcement learning is a learning paradigm for solving sequential\ndecision-making problems. Recent years have witnessed remarkable progress in\nreinforcement learning upon the fast development of deep neural networks. Along\nwith the promising prospects of reinforcement learning in numerous domains such\nas robotics and game-playing, transfer learning has arisen to tackle various\nchallenges faced by reinforcement learning, by transferring knowledge from\nexternal expertise to facilitate the efficiency and effectiveness of the\nlearning process. In this survey, we systematically investigate the recent\nprogress of transfer learning approaches in the context of deep reinforcement\nlearning. Specifically, we provide a framework for categorizing the\nstate-of-the-art transfer learning approaches, under which we analyze their\ngoals, methodologies, compatible reinforcement learning backbones, and\npractical applications. We also draw connections between transfer learning and\nother relevant topics from the reinforcement learning perspective and explore\ntheir potential challenges that await future research progress.", "journal": ""}
{"doi": "10.48550/arXiv.2302.00150", "date": "2023-02-01", "title": "Multi-Grade Deep Learning", "authors": "Yuesheng Xu", "abstract": "The current deep learning model is of a single-grade, that is, it learns a\ndeep neural network by solving a single nonconvex optimization problem. When\nthe layer number of the neural network is large, it is computationally\nchallenging to carry out such a task efficiently. Inspired by the human\neducation process which arranges learning in grades, we propose a multi-grade\nlearning model: We successively solve a number of optimization problems of\nsmall sizes, which are organized in grades, to learn a shallow neural network\nfor each grade. Specifically, the current grade is to learn the leftover from\nthe previous grade. In each of the grades, we learn a shallow neural network\nstacked on the top of the neural network, learned in the previous grades, which\nremains unchanged in training of the current and future grades. By dividing the\ntask of learning a deep neural network into learning several shallow neural\nnetworks, one can alleviate the severity of the nonconvexity of the original\noptimization problem of a large size. When all grades of the learning are\ncompleted, the final neural network learned is a stair-shape neural network,\nwhich is the superposition of networks learned from all grades. Such a model\nenables us to learn a deep neural network much more effectively and\nefficiently. Moreover, multi-grade learning naturally leads to adaptive\nlearning. We prove that in the context of function approximation if the neural\nnetwork generated by a new grade is nontrivial, the optimal error of the grade\nis strictly reduced from the optimal error of the previous grade. Furthermore,\nwe provide several proof-of-concept numerical examples which demonstrate that\nthe proposed multi-grade model outperforms significantly the traditional\nsingle-grade model and is much more robust than the traditional model.", "journal": ""}
{"doi": "10.48550/arXiv.1711.08976", "date": "2017-11-24", "title": "Deep Cross-Modal Correlation Learning for Audio and Lyrics in Music Retrieval", "authors": "Yi Yu, Suhua Tang, Francisco Raposo, Lei Chen", "abstract": "Little research focuses on cross-modal correlation learning where temporal\nstructures of different data modalities such as audio and lyrics are taken into\naccount. Stemming from the characteristic of temporal structures of music in\nnature, we are motivated to learn the deep sequential correlation between audio\nand lyrics. In this work, we propose a deep cross-modal correlation learning\narchitecture involving two-branch deep neural networks for audio modality and\ntext modality (lyrics). Different modality data are converted to the same\ncanonical space where inter modal canonical correlation analysis is utilized as\nan objective function to calculate the similarity of temporal structures. This\nis the first study on understanding the correlation between language and music\naudio through deep architectures for learning the paired temporal correlation\nof audio and lyrics. Pre-trained Doc2vec model followed by fully-connected\nlayers (fully-connected deep neural network) is used to represent lyrics. Two\nsignificant contributions are made in the audio branch, as follows: i)\npre-trained CNN followed by fully-connected layers is investigated for\nrepresenting music audio. ii) We further suggest an end-to-end architecture\nthat simultaneously trains convolutional layers and fully-connected layers to\nbetter learn temporal structures of music audio. Particularly, our end-to-end\ndeep architecture contains two properties: simultaneously implementing feature\nlearning and cross-modal correlation learning, and learning joint\nrepresentation by considering temporal structures. Experimental results, using\naudio to retrieve lyrics or using lyrics to retrieve audio, verify the\neffectiveness of the proposed deep correlation learning architectures in\ncross-modal music retrieval.", "journal": ""}
{"doi": "10.48550/arXiv.1712.04371", "date": "2017-12-09", "title": "Music Generation by Deep Learning - Challenges and Directions", "authors": "Jean-Pierre Briot, Fran\u00e7ois Pachet", "abstract": "In addition to traditional tasks such as prediction, classification and\ntranslation, deep learning is receiving growing attention as an approach for\nmusic generation, as witnessed by recent research groups such as Magenta at\nGoogle and CTRL (Creator Technology Research Lab) at Spotify. The motivation is\nin using the capacity of deep learning architectures and training techniques to\nautomatically learn musical styles from arbitrary musical corpora and then to\ngenerate samples from the estimated distribution. However, a direct application\nof deep learning to generate content rapidly reaches limits as the generated\ncontent tends to mimic the training set without exhibiting true creativity.\nMoreover, deep learning architectures do not offer direct ways for controlling\ngeneration (e.g., imposing some tonality or other arbitrary constraints).\nFurthermore, deep learning architectures alone are autistic automata which\ngenerate music autonomously without human user interaction, far from the\nobjective of interactively assisting musicians to compose and refine music.\nIssues such as: control, structure, creativity and interactivity are the focus\nof our analysis. In this paper, we select some limitations of a direct\napplication of deep learning to music generation, analyze why the issues are\nnot fulfilled and how to address them by possible approaches. Various examples\nof recent systems are cited as examples of promising directions.", "journal": ""}
{"doi": "10.48550/arXiv.1802.02904", "date": "2018-02-07", "title": "Deep Reinforcement Learning for Image Hashing", "authors": "Yuxin Peng, Jian Zhang, Zhaoda Ye", "abstract": "Deep hashing methods have received much attention recently, which achieve\npromising results by taking advantage of the strong representation power of\ndeep networks. However, most existing deep hashing methods learn a whole set of\nhashing functions independently, while ignore the correlations between\ndifferent hashing functions that can promote the retrieval accuracy greatly.\nInspired by the sequential decision ability of deep reinforcement learning, we\npropose a new Deep Reinforcement Learning approach for Image Hashing (DRLIH).\nOur proposed DRLIH approach models the hashing learning problem as a sequential\ndecision process, which learns each hashing function by correcting the errors\nimposed by previous ones and promotes retrieval accuracy. To the best of our\nknowledge, this is the first work to address hashing problem from deep\nreinforcement learning perspective. The main contributions of our proposed\nDRLIH approach can be summarized as follows: (1) We propose a deep\nreinforcement learning hashing network. In the proposed network, we utilize\nrecurrent neural network (RNN) as agents to model the hashing functions, which\ntake actions of projecting images into binary codes sequentially, so that the\ncurrent hashing function learning can take previous hashing functions' error\ninto account. (2) We propose a sequential learning strategy based on proposed\nDRLIH. We define the state as a tuple of internal features of RNN's hidden\nlayers and image features, which can reflect history decisions made by the\nagents. We also propose an action group method to enhance the correlation of\nhash functions in the same group. Experiments on three widely-used datasets\ndemonstrate the effectiveness of our proposed DRLIH approach.", "journal": ""}
{"doi": "10.48550/arXiv.1801.06889", "date": "2018-01-21", "title": "Visual Analytics in Deep Learning: An Interrogative Survey for the Next Frontiers", "authors": "Fred Hohman, Minsuk Kahng, Robert Pienta, Duen Horng Chau", "abstract": "Deep learning has recently seen rapid development and received significant\nattention due to its state-of-the-art performance on previously-thought hard\nproblems. However, because of the internal complexity and nonlinear structure\nof deep neural networks, the underlying decision making processes for why these\nmodels are achieving such performance are challenging and sometimes mystifying\nto interpret. As deep learning spreads across domains, it is of paramount\nimportance that we equip users of deep learning with tools for understanding\nwhen a model works correctly, when it fails, and ultimately how to improve its\nperformance. Standardized toolkits for building neural networks have helped\ndemocratize deep learning; visual analytics systems have now been developed to\nsupport model explanation, interpretation, debugging, and improvement. We\npresent a survey of the role of visual analytics in deep learning research,\nwhich highlights its short yet impactful history and thoroughly summarizes the\nstate-of-the-art using a human-centered interrogative framework, focusing on\nthe Five W's and How (Why, Who, What, How, When, and Where). We conclude by\nhighlighting research directions and open research problems. This survey helps\nresearchers and practitioners in both visual analytics and deep learning to\nquickly learn key aspects of this young and rapidly growing body of research,\nwhose impact spans a diverse range of domains.", "journal": ""}
{"doi": "10.48550/arXiv.1809.03559", "date": "2018-09-10", "title": "Deep Learning Towards Mobile Applications", "authors": "Ji Wang, Bokai Cao, Philip S. Yu, Lichao Sun, Weidong Bao, Xiaomin Zhu", "abstract": "Recent years have witnessed an explosive growth of mobile devices. Mobile\ndevices are permeating every aspect of our daily lives. With the increasing\nusage of mobile devices and intelligent applications, there is a soaring demand\nfor mobile applications with machine learning services. Inspired by the\ntremendous success achieved by deep learning in many machine learning tasks, it\nbecomes a natural trend to push deep learning towards mobile applications.\nHowever, there exist many challenges to realize deep learning in mobile\napplications, including the contradiction between the miniature nature of\nmobile devices and the resource requirement of deep neural networks, the\nprivacy and security concerns about individuals' data, and so on. To resolve\nthese challenges, during the past few years, great leaps have been made in this\narea. In this paper, we provide an overview of the current challenges and\nrepresentative achievements about pushing deep learning on mobile devices from\nthree aspects: training with mobile data, efficient inference on mobile\ndevices, and applications of mobile deep learning. The former two aspects cover\nthe primary tasks of deep learning. Then, we go through our two recent\napplications that apply the data collected by mobile devices to inferring mood\ndisturbance and user identification. Finally, we conclude this paper with the\ndiscussion of the future of this area.", "journal": ""}
{"doi": "10.48550/arXiv.1904.07404", "date": "2019-04-16", "title": "swTVM: Towards Optimized Tensor Code Generation for Deep Learning on Sunway Many-Core Processor", "authors": "Mingzhen Li, Changxi Liu, Jianjin Liao, Xuegui Zheng, Hailong Yang, Rujun Sun, Jun Xu, Lin Gan, Guangwen Yang, Zhongzhi Luan, Depei Qian", "abstract": "The flourish of deep learning frameworks and hardware platforms has been\ndemanding an efficient compiler that can shield the diversity in both software\nand hardware in order to provide application portability. Among the existing\ndeep learning compilers, TVM is well known for its efficiency in code\ngeneration and optimization across diverse hardware devices. In the meanwhile,\nthe Sunway many-core processor renders itself as a competitive candidate for\nits attractive computational power in both scientific computing and deep\nlearning workloads. This paper combines the trends in these two directions.\nSpecifically, we propose swTVM that extends the original TVM to support\nahead-of-time compilation for architecture requiring cross-compilation such as\nSunway. In addition, we leverage the architecture features during the\ncompilation such as core group for massive parallelism, DMA for high bandwidth\nmemory transfer and local device memory for data locality, in order to generate\nefficient codes for deep learning workloads on Sunway. The experiment results\nshow that the codes generated by swTVM achieves 1.79x on average compared to\nthe state-of-the-art deep learning framework on Sunway, across six\nrepresentative benchmarks. This work is the first attempt from the compiler\nperspective to bridge the gap of deep learning and Sunway processor\nparticularly with productivity and efficiency in mind. We believe this work\nwill encourage more people to embrace the power of deep learning and Sunway\nmany-core processor.", "journal": ""}
{"doi": "10.48550/arXiv.2003.11755", "date": "2020-03-26", "title": "A Survey of Deep Learning for Scientific Discovery", "authors": "Maithra Raghu, Eric Schmidt", "abstract": "Over the past few years, we have seen fundamental breakthroughs in core\nproblems in machine learning, largely driven by advances in deep neural\nnetworks. At the same time, the amount of data collected in a wide array of\nscientific domains is dramatically increasing in both size and complexity.\nTaken together, this suggests many exciting opportunities for deep learning\napplications in scientific settings. But a significant challenge to this is\nsimply knowing where to start. The sheer breadth and diversity of different\ndeep learning techniques makes it difficult to determine what scientific\nproblems might be most amenable to these methods, or which specific combination\nof methods might offer the most promising first approach. In this survey, we\nfocus on addressing this central issue, providing an overview of many widely\nused deep learning models, spanning visual, sequential and graph structured\ndata, associated tasks and different training methods, along with techniques to\nuse deep learning with less data and better interpret these complex models ---\ntwo central considerations for many scientific use cases. We also include\noverviews of the full design process, implementation tips, and links to a\nplethora of tutorials, research summaries and open-sourced deep learning\npipelines and pretrained models, developed by the community. We hope that this\nsurvey will help accelerate the use of deep learning across different\nscientific domains.", "journal": ""}
{"doi": "10.48550/arXiv.1911.12562", "date": "2019-11-28", "title": "Towards Security Threats of Deep Learning Systems: A Survey", "authors": "Yingzhe He, Guozhu Meng, Kai Chen, Xingbo Hu, Jinwen He", "abstract": "Deep learning has gained tremendous success and great popularity in the past\nfew years. However, deep learning systems are suffering several inherent\nweaknesses, which can threaten the security of learning models. Deep learning's\nwide use further magnifies the impact and consequences. To this end, lots of\nresearch has been conducted with the purpose of exhaustively identifying\nintrinsic weaknesses and subsequently proposing feasible mitigation. Yet few\nare clear about how these weaknesses are incurred and how effective these\nattack approaches are in assaulting deep learning. In order to unveil the\nsecurity weaknesses and aid in the development of a robust deep learning\nsystem, we undertake an investigation on attacks towards deep learning, and\nanalyze these attacks to conclude some findings in multiple views. In\nparticular, we focus on four types of attacks associated with security threats\nof deep learning: model extraction attack, model inversion attack, poisoning\nattack and adversarial attack. For each type of attack, we construct its\nessential workflow as well as adversary capabilities and attack goals. Pivot\nmetrics are devised for comparing the attack approaches, by which we perform\nquantitative and qualitative analyses. From the analysis, we have identified\nsignificant and indispensable factors in an attack vector, e.g., how to reduce\nqueries to target models, what distance should be used for measuring\nperturbation. We shed light on 18 findings covering these approaches' merits\nand demerits, success probability, deployment complexity and prospects.\nMoreover, we discuss other potential security weaknesses and possible\nmitigation which can inspire relevant research in this area.", "journal": "IEEE Transactions on Software Engineering 2020"}
{"doi": "10.48550/arXiv.2004.14545", "date": "2020-04-30", "title": "Explainable Deep Learning: A Field Guide for the Uninitiated", "authors": "Gabrielle Ras, Ning Xie, Marcel van Gerven, Derek Doran", "abstract": "Deep neural networks (DNNs) have become a proven and indispensable machine\nlearning tool. As a black-box model, it remains difficult to diagnose what\naspects of the model's input drive the decisions of a DNN. In countless\nreal-world domains, from legislation and law enforcement to healthcare, such\ndiagnosis is essential to ensure that DNN decisions are driven by aspects\nappropriate in the context of its use. The development of methods and studies\nenabling the explanation of a DNN's decisions has thus blossomed into an\nactive, broad area of research. A practitioner wanting to study explainable\ndeep learning may be intimidated by the plethora of orthogonal directions the\nfield has taken. This complexity is further exacerbated by competing\ndefinitions of what it means ``to explain'' the actions of a DNN and to\nevaluate an approach's ``ability to explain''. This article offers a field\nguide to explore the space of explainable deep learning aimed at those\nuninitiated in the field. The field guide: i) Introduces three simple\ndimensions defining the space of foundational methods that contribute to\nexplainable deep learning, ii) discusses the evaluations for model\nexplanations, iii) places explainability in the context of other related deep\nlearning research areas, and iv) finally elaborates on user-oriented\nexplanation designing and potential future directions on explainable deep\nlearning. We hope the guide is used as an easy-to-digest starting point for\nthose just embarking on research in this field.", "journal": ""}
{"doi": "10.48550/arXiv.2007.15580", "date": "2020-07-28", "title": "Deep-Learning based Inverse Modeling Approaches: A Subsurface Flow Example", "authors": "Nanzhe Wang, Haibin Chang, Dongxiao Zhang", "abstract": "Deep-learning has achieved good performance and shown great potential for\nsolving forward and inverse problems. In this work, two categories of\ninnovative deep-learning based inverse modeling methods are proposed and\ncompared. The first category is deep-learning surrogate-based inversion\nmethods, in which the Theory-guided Neural Network (TgNN) is constructed as a\ndeep-learning surrogate for problems with uncertain model parameters. By\nincorporating physical laws and other constraints, the TgNN surrogate can be\nconstructed with limited simulation runs and accelerate the inversion process\nsignificantly. Three TgNN surrogate-based inversion methods are proposed,\nincluding the gradient method, the iterative ensemble smoother (IES), and the\ntraining method. The second category is direct-deep-learning-inversion methods,\nin which TgNN constrained with geostatistical information, named TgNN-geo, is\nproposed for direct inverse modeling. In TgNN-geo, two neural networks are\nintroduced to approximate the respective random model parameters and the\nsolution. Since the prior geostatistical information can be incorporated, the\ndirect-inversion method based on TgNN-geo works well, even in cases with sparse\nspatial measurements or imprecise prior statistics. Although the proposed\ndeep-learning based inverse modeling methods are general in nature, and thus\napplicable to a wide variety of problems, they are tested with several\nsubsurface flow problems. It is found that satisfactory results are obtained\nwith a high efficiency. Moreover, both the advantages and disadvantages are\nfurther analyzed for the proposed two categories of deep-learning based\ninversion methods.", "journal": "Journal of Geophysical Research: Solid Earth, e2020JB020549, 2020"}
{"doi": "10.48550/arXiv.2010.02343", "date": "2020-10-05", "title": "Multi-level Feature Learning on Embedding Layer of Convolutional Autoencoders and Deep Inverse Feature Learning for Image Clustering", "authors": "Behzad Ghazanfari, Fatemeh Afghah", "abstract": "This paper introduces Multi-Level feature learning alongside the Embedding\nlayer of Convolutional Autoencoder (CAE-MLE) as a novel approach in deep\nclustering. We use agglomerative clustering as the multi-level feature learning\nthat provides a hierarchical structure on the latent feature space. It is shown\nthat applying multi-level feature learning considerably improves the basic deep\nconvolutional embedding clustering (DCEC). CAE-MLE considers the clustering\nloss of agglomerative clustering simultaneously alongside the learning latent\nfeature of CAE. In the following of the previous works in inverse feature\nlearning, we show that the representation of learning of error as a general\nstrategy can be applied on different deep clustering approaches and it leads to\npromising results. We develop deep inverse feature learning (deep IFL) on\nCAE-MLE as a novel approach that leads to the state-of-the-art results among\nthe same category methods. The experimental results show that the CAE-MLE\nimproves the results of the basic method, DCEC, around 7% -14% on two\nwell-known datasets of MNIST and USPS. Also, it is shown that the proposed deep\nIFL improves the primary results about 9%-17%. Therefore, both proposed\napproaches of CAE-MLE and deep IFL based on CAE-MLE can lead to notable\nperformance improvement in comparison to the majority of existing techniques.\nThe proposed approaches while are based on a basic convolutional autoencoder\nlead to outstanding results even in comparison to variational autoencoders or\ngenerative adversarial networks.", "journal": ""}
{"doi": "10.48550/arXiv.2204.01782", "date": "2022-04-04", "title": "The First Principles of Deep Learning and Compression", "authors": "Max Ehrlich", "abstract": "The deep learning revolution incited by the 2012 Alexnet paper has been\ntransformative for the field of computer vision. Many problems which were\nseverely limited using classical solutions are now seeing unprecedented\nsuccess. The rapid proliferation of deep learning methods has led to a sharp\nincrease in their use in consumer and embedded applications. One consequence of\nconsumer and embedded applications is lossy multimedia compression which is\nrequired to engineer the efficient storage and transmission of data in these\nreal-world scenarios. As such, there has been increased interest in a deep\nlearning solution for multimedia compression which would allow for higher\ncompression ratios and increased visual quality.\n  The deep learning approach to multimedia compression, so called Learned\nMultimedia Compression, involves computing a compressed representation of an\nimage or video using a deep network for the encoder and the decoder. While\nthese techniques have enjoyed impressive academic success, their industry\nadoption has been essentially non-existent. Classical compression techniques\nlike JPEG and MPEG are too entrenched in modern computing to be easily\nreplaced. This dissertation takes an orthogonal approach and leverages deep\nlearning to improve the compression fidelity of these classical algorithms.\nThis allows the incredible advances in deep learning to be used for multimedia\ncompression without threatening the ubiquity of the classical methods.\n  The key insight of this work is that methods which are motivated by first\nprinciples, i.e., the underlying engineering decisions that were made when the\ncompression algorithms were developed, are more effective than general methods.\nBy encoding prior knowledge into the design of the algorithm, the flexibility,\nperformance, and/or accuracy are improved at the cost of generality...", "journal": ""}
{"doi": "10.48550/arXiv.2305.17473", "date": "2023-05-27", "title": "A Comprehensive Overview and Comparative Analysis on Deep Learning Models: CNN, RNN, LSTM, GRU", "authors": "Farhad Mortezapour Shiri, Thinagaran Perumal, Norwati Mustapha, Raihani Mohamed", "abstract": "Deep learning (DL) has emerged as a powerful subset of machine learning (ML)\nand artificial intelligence (AI), outperforming traditional ML methods,\nespecially in handling unstructured and large datasets. Its impact spans across\nvarious domains, including speech recognition, healthcare, autonomous vehicles,\ncybersecurity, predictive analytics, and more. However, the complexity and\ndynamic nature of real-world problems present challenges in designing effective\ndeep learning models. Consequently, several deep learning models have been\ndeveloped to address different problems and applications. In this article, we\nconduct a comprehensive survey of various deep learning models, including\nConvolutional Neural Network (CNN), Recurrent Neural Network (RNN), Temporal\nConvolutional Networks (TCN), Transformer, Kolmogorov-Arnold networks (KAN),\nGenerative Models, Deep Reinforcement Learning (DRL), and Deep Transfer\nLearning. We examine the structure, applications, benefits, and limitations of\neach model. Furthermore, we perform an analysis using three publicly available\ndatasets: IMDB, ARAS, and Fruit-360. We compared the performance of six\nrenowned deep learning models: CNN, RNN, Long Short-Term Memory (LSTM),\nBidirectional LSTM, Gated Recurrent Unit (GRU), and Bidirectional GRU alongside\ntwo newer models, TCN and Transformer, using the IMDB and ARAS datasets.\nAdditionally, we evaluated the performance of eight CNN-based models, including\nVGG (Visual Geometry Group), Inception, ResNet (Residual Network),\nInceptionResNet, Xception (Extreme Inception), MobileNet, DenseNet (Dense\nConvolutional Network), and NASNet (Neural Architecture Search Network), for\nimage classification tasks using the Fruit-360 dataset.", "journal": "Journal on Artificial Intelligence 2024 Vol. 6 Issue 1 Pages\n  301-360"}
{"doi": "10.48550/arXiv.2312.12693", "date": "2023-12-20", "title": "Anderson Accelerated Gauss-Newton-guided deep learning for nonlinear inverse problems with Application to Electrical Impedance Tomography", "authors": "Qingping Zhou, Guixian Xu, Zhexin Wen, Hongqiao Wang", "abstract": "Physics-guided deep learning is an important prevalent research topic in\nscientific machine learning, which has tremendous potential in various complex\napplications including science and engineering. In these applications, data is\nexpensive to acquire and high accuracy is required for making decisions. In\nthis work, we introduce an efficient physics-guided deep learning framework for\nthe variational modeling of nonlinear inverse problems, which is then applied\nto solve an electrical impedance tomography (EIT) inverse problem. The\nframework is achieved by unrolling the proposed Anderson accelerated\nGauss-Newton (GNAA) algorithm into an end-to-end deep learning method. Firstly,\nwe show the convergence of the GNAA algorithm in both cases: Anderson depth is\nequal to one and Anderson depth is greater than one. Then, we propose three\ntypes of strategies by combining the complementary strengths of GNAA and deep\nlearning: GNAA of learned regularization (GNAA-LRNet), where the singular\nvalues of the regularization matrix are learned by a deep neural network; GNAA\nof learned proximity (GNAA-LPNet), where the regularization proximal operator\nis learned by using a deep neural network; GNAA of plug-and-play method\n(GNAA-PnPNet) where the regularization proximal operator is replaced by a\npre-trained deep denoisers. Lastly, we present some numerical experiments to\nillustrate that the proposed approaches greatly improve the convergence rate\nand the quality of inverse solutions.", "journal": ""}
{"doi": "10.48550/arXiv.2401.13912", "date": "2024-01-25", "title": "A Survey of Deep Learning and Foundation Models for Time Series Forecasting", "authors": "John A. Miller, Mohammed Aldosari, Farah Saeed, Nasid Habib Barna, Subas Rana, I. Budak Arpinar, Ninghao Liu", "abstract": "Deep Learning has been successfully applied to many application domains, yet\nits advantages have been slow to emerge for time series forecasting. For\nexample, in the well-known Makridakis (M) Competitions, hybrids of traditional\nstatistical or machine learning techniques have only recently become the top\nperformers. With the recent architectural advances in deep learning being\napplied to time series forecasting (e.g., encoder-decoders with attention,\ntransformers, and graph neural networks), deep learning has begun to show\nsignificant advantages. Still, in the area of pandemic prediction, there remain\nchallenges for deep learning models: the time series is not long enough for\neffective training, unawareness of accumulated scientific knowledge, and\ninterpretability of the model. To this end, the development of foundation\nmodels (large deep learning models with extensive pre-training) allows models\nto understand patterns and acquire knowledge that can be applied to new related\nproblems before extensive training data becomes available. Furthermore, there\nis a vast amount of knowledge available that deep learning models can tap into,\nincluding Knowledge Graphs and Large Language Models fine-tuned with scientific\ndomain knowledge. There is ongoing research examining how to utilize or inject\nsuch knowledge into deep learning models. In this survey, several\nstate-of-the-art modeling techniques are reviewed, and suggestions for further\nwork are provided.", "journal": ""}
{"doi": "10.48550/arXiv.2410.10505", "date": "2024-10-14", "title": "Comparison of deep learning and conventional methods for disease onset prediction", "authors": "Luis H. John, Chungsoo Kim, Jan A. Kors, Junhyuk Chang, Hannah Morgan-Cooper, Priya Desai, Chao Pang, Peter R. Rijnbeek, Jenna M. Reps, Egill A. Fridgeirsson", "abstract": "Background: Conventional prediction methods such as logistic regression and\ngradient boosting have been widely utilized for disease onset prediction for\ntheir reliability and interpretability. Deep learning methods promise enhanced\nprediction performance by extracting complex patterns from clinical data, but\nface challenges like data sparsity and high dimensionality.\n  Methods: This study compares conventional and deep learning approaches to\npredict lung cancer, dementia, and bipolar disorder using observational data\nfrom eleven databases from North America, Europe, and Asia. Models were\ndeveloped using logistic regression, gradient boosting, ResNet, and\nTransformer, and validated both internally and externally across the data\nsources. Discrimination performance was assessed using AUROC, and calibration\nwas evaluated using Eavg.\n  Findings: Across 11 datasets, conventional methods generally outperformed\ndeep learning methods in terms of discrimination performance, particularly\nduring external validation, highlighting their better transportability.\nLearning curves suggest that deep learning models require substantially larger\ndatasets to reach the same performance levels as conventional methods.\nCalibration performance was also better for conventional methods, with ResNet\nshowing the poorest calibration.\n  Interpretation: Despite the potential of deep learning models to capture\ncomplex patterns in structured observational healthcare data, conventional\nmodels remain highly competitive for disease onset prediction, especially in\nscenarios involving smaller datasets and if lengthy training times need to be\navoided. The study underscores the need for future research focused on\noptimizing deep learning models to handle the sparsity, high dimensionality,\nand heterogeneity inherent in healthcare datasets, and find new strategies to\nexploit the full capabilities of deep learning methods.", "journal": ""}
{"doi": "10.48550/arXiv.1612.01072", "date": "2016-12-04", "title": "Word Recognition with Deep Conditional Random Fields", "authors": "Gang Chen, Yawei Li, Sargur N. Srihari", "abstract": "Recognition of handwritten words continues to be an important problem in\ndocument analysis and recognition. Existing approaches extract hand-engineered\nfeatures from word images--which can perform poorly with new data sets.\nRecently, deep learning has attracted great attention because of the ability to\nlearn features from raw data. Moreover they have yielded state-of-the-art\nresults in classification tasks including character recognition and scene\nrecognition. On the other hand, word recognition is a sequential problem where\nwe need to model the correlation between characters. In this paper, we propose\nusing deep Conditional Random Fields (deep CRFs) for word recognition.\nBasically, we combine CRFs with deep learning, in which deep features are\nlearned and sequences are labeled in a unified framework. We pre-train the deep\nstructure with stacked restricted Boltzmann machines (RBMs) for feature\nlearning and optimize the entire network with an online learning algorithm. The\nproposed model was evaluated on two datasets, and seen to perform significantly\nbetter than competitive baseline models. The source code is available at\nhttps://github.com/ganggit/deepCRFs.", "journal": ""}
{"doi": "10.48550/arXiv.1709.05870", "date": "2017-09-18", "title": "ZhuSuan: A Library for Bayesian Deep Learning", "authors": "Jiaxin Shi, Jianfei Chen, Jun Zhu, Shengyang Sun, Yucen Luo, Yihong Gu, Yuhao Zhou", "abstract": "In this paper we introduce ZhuSuan, a python probabilistic programming\nlibrary for Bayesian deep learning, which conjoins the complimentary advantages\nof Bayesian methods and deep learning. ZhuSuan is built upon Tensorflow. Unlike\nexisting deep learning libraries, which are mainly designed for deterministic\nneural networks and supervised tasks, ZhuSuan is featured for its deep root\ninto Bayesian inference, thus supporting various kinds of probabilistic models,\nincluding both the traditional hierarchical Bayesian models and recent deep\ngenerative models. We use running examples to illustrate the probabilistic\nprogramming on ZhuSuan, including Bayesian logistic regression, variational\nauto-encoders, deep sigmoid belief networks and Bayesian recurrent neural\nnetworks.", "journal": ""}
{"doi": "10.48550/arXiv.1909.01500", "date": "2019-09-03", "title": "rlpyt: A Research Code Base for Deep Reinforcement Learning in PyTorch", "authors": "Adam Stooke, Pieter Abbeel", "abstract": "Since the recent advent of deep reinforcement learning for game play and\nsimulated robotic control, a multitude of new algorithms have flourished. Most\nare model-free algorithms which can be categorized into three families: deep\nQ-learning, policy gradients, and Q-value policy gradients. These have\ndeveloped along separate lines of research, such that few, if any, code bases\nincorporate all three kinds. Yet these algorithms share a great depth of common\ndeep reinforcement learning machinery. We are pleased to share rlpyt, which\nimplements all three algorithm families on top of a shared, optimized\ninfrastructure, in a single repository. It contains modular implementations of\nmany common deep RL algorithms in Python using PyTorch, a leading deep learning\nlibrary. rlpyt is designed as a high-throughput code base for small- to\nmedium-scale research in deep RL. This white paper summarizes its features,\nalgorithms implemented, and relation to prior work, and concludes with detailed\nimplementation and usage notes. rlpyt is available at\nhttps://github.com/astooke/rlpyt.", "journal": ""}
{"doi": "10.48550/arXiv.1801.08360", "date": "2018-01-25", "title": "Dual Asymmetric Deep Hashing Learning", "authors": "Jinxing Li, Bob Zhang, Guangming Lu, David Zhang", "abstract": "Due to the impressive learning power, deep learning has achieved a remarkable\nperformance in supervised hash function learning. In this paper, we propose a\nnovel asymmetric supervised deep hashing method to preserve the semantic\nstructure among different categories and generate the binary codes\nsimultaneously. Specifically, two asymmetric deep networks are constructed to\nreveal the similarity between each pair of images according to their semantic\nlabels. The deep hash functions are then learned through two networks by\nminimizing the gap between the learned features and discrete codes.\nFurthermore, since the binary codes in the Hamming space also should keep the\nsemantic affinity existing in the original space, another asymmetric pairwise\nloss is introduced to capture the similarity between the binary codes and\nreal-value features. This asymmetric loss not only improves the retrieval\nperformance, but also contributes to a quick convergence at the training phase.\nBy taking advantage of the two-stream deep structures and two types of\nasymmetric pairwise functions, an alternating algorithm is designed to optimize\nthe deep features and high-quality binary codes efficiently. Experimental\nresults on three real-world datasets substantiate the effectiveness and\nsuperiority of our approach as compared with state-of-the-art.", "journal": ""}
{"doi": "10.48550/arXiv.1912.01933", "date": "2019-12-04", "title": "Deep Distributional Sequence Embeddings Based on a Wasserstein Loss", "authors": "Ahmed Abdelwahab, Niels Landwehr", "abstract": "Deep metric learning employs deep neural networks to embed instances into a\nmetric space such that distances between instances of the same class are small\nand distances between instances from different classes are large. In most\nexisting deep metric learning techniques, the embedding of an instance is given\nby a feature vector produced by a deep neural network and Euclidean distance or\ncosine similarity defines distances between these vectors. In this paper, we\nstudy deep distributional embeddings of sequences, where the embedding of a\nsequence is given by the distribution of learned deep features across the\nsequence. This has the advantage of capturing statistical information about the\ndistribution of patterns within the sequence in the embedding. When embeddings\nare distributions rather than vectors, measuring distances between embeddings\ninvolves comparing their respective distributions. We propose a distance metric\nbased on Wasserstein distances between the distributions and a corresponding\nloss function for metric learning, which leads to a novel end-to-end trainable\nembedding model. We empirically observe that distributional embeddings\noutperform standard vector embeddings and that training with the proposed\nWasserstein metric outperforms training with other distance functions.", "journal": ""}
{"doi": "10.48550/arXiv.2011.00177", "date": "2020-10-31", "title": "Evaluation of Inference Attack Models for Deep Learning on Medical Data", "authors": "Maoqiang Wu, Xinyue Zhang, Jiahao Ding, Hien Nguyen, Rong Yu, Miao Pan, Stephen T. Wong", "abstract": "Deep learning has attracted broad interest in healthcare and medical\ncommunities. However, there has been little research into the privacy issues\ncreated by deep networks trained for medical applications. Recently developed\ninference attack algorithms indicate that images and text records can be\nreconstructed by malicious parties that have the ability to query deep\nnetworks. This gives rise to the concern that medical images and electronic\nhealth records containing sensitive patient information are vulnerable to these\nattacks. This paper aims to attract interest from researchers in the medical\ndeep learning community to this important problem. We evaluate two prominent\ninference attack models, namely, attribute inference attack and model inversion\nattack. We show that they can reconstruct real-world medical images and\nclinical reports with high fidelity. We then investigate how to protect\npatients' privacy using defense mechanisms, such as label perturbation and\nmodel perturbation. We provide a comparison of attack results between the\noriginal and the medical deep learning models with defenses. The experimental\nevaluations show that our proposed defense approaches can effectively reduce\nthe potential privacy leakage of medical deep learning from the inference\nattacks.", "journal": ""}
{"doi": "10.48550/arXiv.2101.06749", "date": "2021-01-17", "title": "A Layer-Wise Information Reinforcement Approach to Improve Learning in Deep Belief Networks", "authors": "Mateus Roder, Leandro A. Passos, Luiz Carlos Felix Ribeiro, Clayton Pereira, Jo\u00e3o Paulo Papa", "abstract": "With the advent of deep learning, the number of works proposing new methods\nor improving existent ones has grown exponentially in the last years. In this\nscenario, \"very deep\" models were emerging, once they were expected to extract\nmore intrinsic and abstract features while supporting a better performance.\nHowever, such models suffer from the gradient vanishing problem, i.e.,\nbackpropagation values become too close to zero in their shallower layers,\nultimately causing learning to stagnate. Such an issue was overcome in the\ncontext of convolution neural networks by creating \"shortcut connections\"\nbetween layers, in a so-called deep residual learning framework. Nonetheless, a\nvery popular deep learning technique called Deep Belief Network still suffers\nfrom gradient vanishing when dealing with discriminative tasks. Therefore, this\npaper proposes the Residual Deep Belief Network, which considers the\ninformation reinforcement layer-by-layer to improve the feature extraction and\nknowledge retaining, that support better discriminative performance.\nExperiments conducted over three public datasets demonstrate its robustness\nconcerning the task of binary image classification.", "journal": ""}
{"doi": "10.48550/arXiv.2103.02552", "date": "2021-03-03", "title": "Multi-Channel and Multi-Microphone Acoustic Echo Cancellation Using A Deep Learning Based Approach", "authors": "Hao Zhang, DeLiang Wang", "abstract": "Building on the deep learning based acoustic echo cancellation (AEC) in the\nsingle-loudspeaker (single-channel) and single-microphone setup, this paper\ninvestigates multi-channel AEC (MCAEC) and multi-microphone AEC (MMAEC). We\ntrain a deep neural network (DNN) to predict the near-end speech from\nmicrophone signals with far-end signals used as additional information. We find\nthat the deep learning approach avoids the non-uniqueness problem in\ntraditional MCAEC algorithms. For the AEC setup with multiple microphones,\nrather than employing AEC for each microphone, a single DNN is trained to\nachieve echo removal for all microphones. Also, combining deep learning based\nAEC with deep learning based beamforming further improves the system\nperformance. Experimental results show the effectiveness of both bidirectional\nlong short-term memory (BLSTM) and convolutional recurrent network (CRN) based\nmethods for MCAEC and MMAEC. Furthermore, deep learning based methods are\ncapable of removing echo and noise simultaneously and work well in the presence\nof nonlinear distortions.", "journal": ""}
{"doi": "10.48550/arXiv.2205.08358", "date": "2022-05-17", "title": "Perturbation of Deep Autoencoder Weights for Model Compression and Classification of Tabular Data", "authors": "Manar Samad, Sakib Abrar", "abstract": "Fully connected deep neural networks (DNN) often include redundant weights\nleading to overfitting and high memory requirements. Additionally, the\nperformance of DNN is often challenged by traditional machine learning models\nin tabular data classification. In this paper, we propose periodical\nperturbations (prune and regrow) of DNN weights, especially at the\nself-supervised pre-training stage of deep autoencoders. The proposed weight\nperturbation strategy outperforms dropout learning in four out of six tabular\ndata sets in downstream classification tasks. The L1 or L2 regularization of\nweights at the same pretraining stage results in inferior classification\nperformance compared to dropout or our weight perturbation routine. Unlike\ndropout learning, the proposed weight perturbation routine additionally\nachieves 15% to 40% sparsity across six tabular data sets for the compression\nof deep pretrained models. Our experiments reveal that a pretrained deep\nautoencoder with weight perturbation or dropout can outperform traditional\nmachine learning in tabular data classification when fully connected DNN fails\nmiserably. However, traditional machine learning models appear superior to any\ndeep models when a tabular data set contains uncorrelated variables. Therefore,\nthe success of deep models can be attributed to the inevitable presence of\ncorrelated variables in real-world data sets.", "journal": ""}
{"doi": "10.48550/arXiv.2207.09228", "date": "2022-07-19", "title": "Image Super-Resolution with Deep Dictionary", "authors": "Shunta Maeda", "abstract": "Since the first success of Dong et al., the deep-learning-based approach has\nbecome dominant in the field of single-image super-resolution. This replaces\nall the handcrafted image processing steps of traditional sparse-coding-based\nmethods with a deep neural network. In contrast to sparse-coding-based methods,\nwhich explicitly create high/low-resolution dictionaries, the dictionaries in\ndeep-learning-based methods are implicitly acquired as a nonlinear combination\nof multiple convolutions. One disadvantage of deep-learning-based methods is\nthat their performance is degraded for images created differently from the\ntraining dataset (out-of-domain images). We propose an end-to-end\nsuper-resolution network with a deep dictionary (SRDD), where a high-resolution\ndictionary is explicitly learned without sacrificing the advantages of deep\nlearning. Extensive experiments show that explicit learning of high-resolution\ndictionary makes the network more robust for out-of-domain test images while\nmaintaining the performance of the in-domain test images.", "journal": ""}
{"doi": "10.48550/arXiv.2208.00953", "date": "2022-08-01", "title": "Visual Interpretable and Explainable Deep Learning Models for Brain Tumor MRI and COVID-19 Chest X-ray Images", "authors": "Yusuf Brima, Marcellin Atemkeng", "abstract": "Deep learning shows promise for medical image analysis but lacks\ninterpretability, hindering adoption in healthcare. Attribution techniques that\nexplain model reasoning may increase trust in deep learning among clinical\nstakeholders. This paper aimed to evaluate attribution methods for illuminating\nhow deep neural networks analyze medical images. Using adaptive path-based\ngradient integration, we attributed predictions from brain tumor MRI and\nCOVID-19 chest X-ray datasets made by recent deep convolutional neural network\nmodels. The technique highlighted possible biomarkers, exposed model biases,\nand offered insights into the links between input and prediction. Our analysis\ndemonstrates the method's ability to elucidate model reasoning on these\ndatasets. The resulting attributions show promise for improving deep learning\ntransparency for domain experts by revealing the rationale behind predictions.\nThis study advances model interpretability to increase trust in deep learning\namong healthcare stakeholders.", "journal": ""}
{"doi": "10.48550/arXiv.2404.06526", "date": "2024-04-09", "title": "Onboard Processing of Hyperspectral Imagery: Deep Learning Advancements, Methodologies, Challenges, and Emerging Trends", "authors": "Nafiseh Ghasemi, Jon Alvarez Justo, Marco Celesti, Laurent Despoisse, Jens Nieke", "abstract": "Recent advancements in deep learning techniques have spurred considerable\ninterest in their application to hyperspectral imagery processing. This paper\nprovides a comprehensive review of the latest developments in this field,\nfocusing on methodologies, challenges, and emerging trends. Deep learning\narchitectures such as Convolutional Neural Networks (CNNs), Autoencoders, Deep\nBelief Networks (DBNs), Generative Adversarial Networks (GANs), and Recurrent\nNeural Networks (RNNs) are examined for their suitability in processing\nhyperspectral data. Key challenges, including limited training data and\ncomputational constraints, are identified, along with strategies such as data\naugmentation and noise reduction using GANs. The paper discusses the efficacy\nof different network architectures, highlighting the advantages of lightweight\nCNN models and 1D CNNs for onboard processing. Moreover, the potential of\nhardware accelerators, particularly Field Programmable Gate Arrays (FPGAs), for\nenhancing processing efficiency is explored. The review concludes with insights\ninto ongoing research trends, including the integration of deep learning\ntechniques into Earth observation missions such as the CHIME mission, and\nemphasizes the need for further exploration and refinement of deep learning\nmethodologies to address the evolving demands of hyperspectral image\nprocessing.", "journal": ""}
{"doi": "10.48550/arXiv.2412.18563", "date": "2024-12-24", "title": "A Deep Reinforcement Learning Framework for Dynamic Portfolio Optimization: Evidence from China's Stock Market", "authors": "Gang Huang, Xiaohua Zhou, Qingyang Song", "abstract": "Artificial intelligence is transforming financial investment decision-making\nframeworks, with deep reinforcement learning demonstrating substantial\npotential in robo-advisory applications. This paper addresses the limitations\nof traditional portfolio optimization methods in dynamic asset weight\nadjustment through the development of a deep reinforcement learning-based\ndynamic optimization model grounded in practical trading processes. The\nresearch advances two key innovations: first, the introduction of a novel\nSharpe ratio reward function engineered for Actor-Critic deep reinforcement\nlearning algorithms, which ensures stable convergence during training while\nconsistently achieving positive average Sharpe ratios; second, the development\nof an innovative comprehensive approach to portfolio optimization utilizing\ndeep reinforcement learning, which significantly enhances model optimization\ncapability through the integration of random sampling strategies during\ntraining with image-based deep neural network architectures for\nmulti-dimensional financial time series data processing, average Sharpe ratio\nreward functions, and deep reinforcement learning algorithms. The empirical\nanalysis validates the model using randomly selected constituent stocks from\nthe CSI 300 Index, benchmarking against established financial econometric\noptimization models. Backtesting results demonstrate the model's efficacy in\noptimizing portfolio allocation and mitigating investment risk, yielding\nsuperior comprehensive performance metrics.", "journal": ""}
{"doi": "10.48550/arXiv.2505.18401", "date": "2025-05-23", "title": "Recent Deep Learning in Crowd Behaviour Analysis: A Brief Review", "authors": "Jiangbei Yue, He Wang", "abstract": "Crowd behaviour analysis is essential to numerous real-world applications,\nsuch as public safety and urban planning, and therefore has been studied for\ndecades. In the last decade or so, the development of deep learning has\nsignificantly propelled the research on crowd behaviours. This chapter reviews\nrecent advances in crowd behaviour analysis using deep learning. We mainly\nreview the research in two core tasks in this field, crowd behaviour prediction\nand recognition. We broadly cover how different deep neural networks, after\nfirst being proposed in machine learning, are applied to analysing crowd\nbehaviours. This includes pure deep neural network models as well as recent\ndevelopment of methodologies combining physics with deep learning. In addition,\nrepresentative studies are discussed and compared in detail. Finally, we\ndiscuss the effectiveness of existing methods and future research directions in\nthis rapidly evolving field. This chapter aims to provide a high-level summary\nof the ongoing deep learning research in crowd behaviour analysis. It intends\nto help new researchers who just entered this field to obtain an overall\nunderstanding of the ongoing research, as well as to provide a retrospective\nanalysis for existing researchers to identify possible future directions", "journal": ""}
{"doi": "10.48550/arXiv.2003.03369", "date": "2020-03-04", "title": "A Survey on Deep Hashing Methods", "authors": "Xiao Luo, Haixin Wang, Daqing Wu, Chong Chen, Minghua Deng, Jianqiang Huang, Xian-Sheng Hua", "abstract": "Nearest neighbor search aims to obtain the samples in the database with the\nsmallest distances from them to the queries, which is a basic task in a range\nof fields, including computer vision and data mining. Hashing is one of the\nmost widely used methods for its computational and storage efficiency. With the\ndevelopment of deep learning, deep hashing methods show more advantages than\ntraditional methods. In this survey, we detailedly investigate current deep\nhashing algorithms including deep supervised hashing and deep unsupervised\nhashing. Specifically, we categorize deep supervised hashing methods into\npairwise methods, ranking-based methods, pointwise methods as well as\nquantization according to how measuring the similarities of the learned hash\ncodes. Moreover, deep unsupervised hashing is categorized into similarity\nreconstruction-based methods, pseudo-label-based methods and prediction-free\nself-supervised learning-based methods based on their semantic learning\nmanners. We also introduce three related important topics including\nsemi-supervised deep hashing, domain adaption deep hashing and multi-modal deep\nhashing. Meanwhile, we present some commonly used public datasets and the\nscheme to measure the performance of deep hashing algorithms. Finally, we\ndiscuss some potential research directions in conclusion.", "journal": ""}
{"doi": "10.48550/arXiv.1803.02956", "date": "2018-03-08", "title": "Some Approximation Bounds for Deep Networks", "authors": "Brendan McCane, Lech Szymanski", "abstract": "In this paper we introduce new bounds on the approximation of functions in\ndeep networks and in doing so introduce some new deep network architectures for\nfunction approximation. These results give some theoretical insight into the\nsuccess of autoencoders and ResNets.", "journal": ""}
{"doi": "10.48550/arXiv.2008.12650", "date": "2020-08-25", "title": "Are Deep Neural Networks \"Robust\"?", "authors": "Peter Meer", "abstract": "Separating outliers from inliers is the definition of robustness in computer\nvision. This essay delineates how deep neural networks are different than\ntypical robust estimators. Deep neural networks not robust by this traditional\ndefinition.", "journal": ""}
{"doi": "10.48550/arXiv.1607.05966", "date": "2016-07-20", "title": "Onsager-corrected deep learning for sparse linear inverse problems", "authors": "Mark Borgerding, Philip Schniter", "abstract": "Deep learning has gained great popularity due to its widespread success on\nmany inference problems. We consider the application of deep learning to the\nsparse linear inverse problem encountered in compressive sensing, where one\nseeks to recover a sparse signal from a small number of noisy linear\nmeasurements. In this paper, we propose a novel neural-network architecture\nthat decouples prediction errors across layers in the same way that the\napproximate message passing (AMP) algorithm decouples them across iterations:\nthrough Onsager correction. Numerical experiments suggest that our \"learned\nAMP\" network significantly improves upon Gregor and LeCun's \"learned ISTA\"\nnetwork in both accuracy and complexity.", "journal": ""}
{"doi": "10.48550/arXiv.1611.01606", "date": "2016-11-05", "title": "Learning to Play in a Day: Faster Deep Reinforcement Learning by Optimality Tightening", "authors": "Frank S. He, Yang Liu, Alexander G. Schwing, Jian Peng", "abstract": "We propose a novel training algorithm for reinforcement learning which\ncombines the strength of deep Q-learning with a constrained optimization\napproach to tighten optimality and encourage faster reward propagation. Our\nnovel technique makes deep reinforcement learning more practical by drastically\nreducing the training time. We evaluate the performance of our approach on the\n49 games of the challenging Arcade Learning Environment, and report significant\nimprovements in both training time and accuracy.", "journal": ""}
{"doi": "10.48550/arXiv.1712.04101", "date": "2017-12-12", "title": "Deep Reinforcement Learning Boosted by External Knowledge", "authors": "Nicolas Bougie, Ryutaro Ichise", "abstract": "Recent improvements in deep reinforcement learning have allowed to solve\nproblems in many 2D domains such as Atari games. However, in complex 3D\nenvironments, numerous learning episodes are required which may be too time\nconsuming or even impossible especially in real-world scenarios. We present a\nnew architecture to combine external knowledge and deep reinforcement learning\nusing only visual input. A key concept of our system is augmenting image input\nby adding environment feature information and combining two sources of\ndecision. We evaluate the performances of our method in a 3D\npartially-observable environment from the Microsoft Malmo platform.\nExperimental evaluation exhibits higher performance and faster learning\ncompared to a single reinforcement learning model.", "journal": ""}
{"doi": "10.48550/arXiv.1712.05016", "date": "2017-12-13", "title": "Deep Prior", "authors": "Alexandre Lacoste, Thomas Boquet, Negar Rostamzadeh, Boris Oreshkin, Wonchang Chung, David Krueger", "abstract": "The recent literature on deep learning offers new tools to learn a rich\nprobability distribution over high dimensional data such as images or sounds.\nIn this work we investigate the possibility of learning the prior distribution\nover neural network parameters using such tools. Our resulting variational\nBayes algorithm generalizes well to new tasks, even when very few training\nexamples are provided. Furthermore, this learned prior allows the model to\nextrapolate correctly far from a given task's training data on a meta-dataset\nof periodic signals.", "journal": ""}
{"doi": "10.48550/arXiv.1812.03288", "date": "2018-12-08", "title": "No Peek: A Survey of private distributed deep learning", "authors": "Praneeth Vepakomma, Tristan Swedish, Ramesh Raskar, Otkrist Gupta, Abhimanyu Dubey", "abstract": "We survey distributed deep learning models for training or inference without\naccessing raw data from clients. These methods aim to protect confidential\npatterns in data while still allowing servers to train models. The distributed\ndeep learning methods of federated learning, split learning and large batch\nstochastic gradient descent are compared in addition to private and secure\napproaches of differential privacy, homomorphic encryption, oblivious transfer\nand garbled circuits in the context of neural networks. We study their\nbenefits, limitations and trade-offs with regards to computational resources,\ndata leakage and communication efficiency and also share our anticipated future\ntrends.", "journal": ""}
{"doi": "10.48550/arXiv.2404.01794", "date": "2024-04-02", "title": "Imitation Game: A Model-based and Imitation Learning Deep Reinforcement Learning Hybrid", "authors": "Eric MSP Veith, Torben Logemann, Aleksandr Berezin, Arlena Well\u00dfow, Stephan Balduin", "abstract": "Autonomous and learning systems based on Deep Reinforcement Learning have\nfirmly established themselves as a foundation for approaches to creating\nresilient and efficient Cyber-Physical Energy Systems. However, most current\napproaches suffer from two distinct problems: Modern model-free algorithms such\nas Soft Actor Critic need a high number of samples to learn a meaningful\npolicy, as well as a fallback to ward against concept drifts (e. g.,\ncatastrophic forgetting). In this paper, we present the work in progress\ntowards a hybrid agent architecture that combines model-based Deep\nReinforcement Learning with imitation learning to overcome both problems.", "journal": ""}
{"doi": "10.48550/arXiv.2407.19353", "date": "2024-07-28", "title": "A spring-block theory of feature learning in deep neural networks", "authors": "Cheng Shi, Liming Pan, Ivan Dokmani\u0107", "abstract": "Feature-learning deep nets progressively collapse data to a regular\nlow-dimensional geometry. How this emerges from the collective action of\nnonlinearity, noise, learning rate, and other factors, has eluded\nfirst-principles theories built from microscopic neuronal dynamics. We exhibit\na noise-nonlinearity phase diagram that identifies regimes where shallow or\ndeep layers learn more effectively and propose a macroscopic mechanical theory\nthat reproduces the diagram and links feature learning across layers to\ngeneralization.", "journal": ""}
{"doi": "10.48550/arXiv.2002.01633", "date": "2020-02-05", "title": "Structural Deep Clustering Network", "authors": "Deyu Bo, Xiao Wang, Chuan Shi, Meiqi Zhu, Emiao Lu, Peng Cui", "abstract": "Clustering is a fundamental task in data analysis. Recently, deep clustering,\nwhich derives inspiration primarily from deep learning approaches, achieves\nstate-of-the-art performance and has attracted considerable attention. Current\ndeep clustering methods usually boost the clustering results by means of the\npowerful representation ability of deep learning, e.g., autoencoder, suggesting\nthat learning an effective representation for clustering is a crucial\nrequirement. The strength of deep clustering methods is to extract the useful\nrepresentations from the data itself, rather than the structure of data, which\nreceives scarce attention in representation learning. Motivated by the great\nsuccess of Graph Convolutional Network (GCN) in encoding the graph structure,\nwe propose a Structural Deep Clustering Network (SDCN) to integrate the\nstructural information into deep clustering. Specifically, we design a delivery\noperator to transfer the representations learned by autoencoder to the\ncorresponding GCN layer, and a dual self-supervised mechanism to unify these\ntwo different deep neural architectures and guide the update of the whole\nmodel. In this way, the multiple structures of data, from low-order to\nhigh-order, are naturally combined with the multiple representations learned by\nautoencoder. Furthermore, we theoretically analyze the delivery operator, i.e.,\nwith the delivery operator, GCN improves the autoencoder-specific\nrepresentation as a high-order graph regularization constraint and autoencoder\nhelps alleviate the over-smoothing problem in GCN. Through comprehensive\nexperiments, we demonstrate that our propose model can consistently perform\nbetter over the state-of-the-art techniques.", "journal": ""}
{"doi": "10.48550/arXiv.2002.06761", "date": "2020-02-17", "title": "Hybrid Embedded Deep Stacked Sparse Autoencoder with w_LPPD SVM Ensemble", "authors": "Yongming Li, Yan Lei, Pin Wang, Yuchuan Liu", "abstract": "Deep learning is a kind of feature learning method with strong nonliear\nfeature transformation and becomes more and more important in many fields of\nartificial intelligence. Deep autoencoder is one representative method of the\ndeep learning methods, and can effectively extract abstract the information of\ndatasets. However, it does not consider the complementarity between the deep\nfeatures and original features during deep feature transformation. Besides, it\nsuffers from small sample problem. In order to solve these problems, a novel\ndeep autoencoder - hybrid feature embedded stacked sparse autoencoder(HESSAE)\nhas been proposed in this paper. HFESAE is capable to learn discriminant deep\nfeatures with the help of embedding original features to filter weak\nhidden-layer outputs during training. For the issue that class representation\nability of abstract information is limited by small sample problem, a feature\nfusion strategy has been designed aiming to combining abstract information\nlearned by HFESAE with original feature and obtain hybrid features for feature\nreduction. The strategy is hybrid feature selection strategy based on L1\nregularization followed by an support vector machine(SVM) ensemble model, in\nwhich weighted local discriminant preservation projection (w_LPPD), is designed\nand employed on each base classifier. At the end of this paper, several\nrepresentative public datasets are used to verify the effectiveness of the\nproposed algorithm. The experimental results demonstrated that, the proposed\nfeature learning method yields superior performance compared to other existing\nand state of art feature learning algorithms including some representative deep\nautoencoder methods.", "journal": ""}
{"doi": "10.48550/arXiv.2110.04596", "date": "2021-10-09", "title": "Deep Long-Tailed Learning: A Survey", "authors": "Yifan Zhang, Bingyi Kang, Bryan Hooi, Shuicheng Yan, Jiashi Feng", "abstract": "Deep long-tailed learning, one of the most challenging problems in visual\nrecognition, aims to train well-performing deep models from a large number of\nimages that follow a long-tailed class distribution. In the last decade, deep\nlearning has emerged as a powerful recognition model for learning high-quality\nimage representations and has led to remarkable breakthroughs in generic visual\nrecognition. However, long-tailed class imbalance, a common problem in\npractical visual recognition tasks, often limits the practicality of deep\nnetwork based recognition models in real-world applications, since they can be\neasily biased towards dominant classes and perform poorly on tail classes. To\naddress this problem, a large number of studies have been conducted in recent\nyears, making promising progress in the field of deep long-tailed learning.\nConsidering the rapid evolution of this field, this paper aims to provide a\ncomprehensive survey on recent advances in deep long-tailed learning. To be\nspecific, we group existing deep long-tailed learning studies into three main\ncategories (i.e., class re-balancing, information augmentation and module\nimprovement), and review these methods following this taxonomy in detail.\nAfterward, we empirically analyze several state-of-the-art methods by\nevaluating to what extent they address the issue of class imbalance via a newly\nproposed evaluation metric, i.e., relative accuracy. We conclude the survey by\nhighlighting important applications of deep long-tailed learning and\nidentifying several promising directions for future research.", "journal": ""}
{"doi": "10.48550/arXiv.2209.05627", "date": "2022-09-12", "title": "SENDER: SEmi-Nonlinear Deep Efficient Reconstructor for Extraction Canonical, Meta, and Sub Functional Connectivity in the Human Brain", "authors": "Wei Zhang, Yu Bao", "abstract": "Deep Linear and Nonlinear learning methods have already been vital machine\nlearning methods for investigating the hierarchical features such as functional\nconnectivity in the human brain via functional Magnetic Resonance signals;\nhowever, there are three major shortcomings: 1). For deep linear learning\nmethods, although the identified hierarchy of functional connectivity is easily\nexplainable, it is challenging to reveal more hierarchical functional\nconnectivity; 2). For deep nonlinear learning methods, although non-fully\nconnected architecture reduces the complexity of neural network structures that\nare easy to optimize and not vulnerable to overfitting, the functional\nconnectivity hierarchy is difficult to explain; 3). Importantly, it is\nchallenging for Deep Linear/Nonlinear methods to detect meta and sub-functional\nconnectivity even in the shallow layers; 4). Like most conventional Deep\nNonlinear Methods, such as Deep Neural Networks, the hyperparameters must be\ntuned manually, which is time-consuming. Thus, in this work, we propose a novel\ndeep hybrid learning method named SEmi-Nonlinear Deep Efficient Reconstruction\n(SENDER), to overcome the aforementioned shortcomings: 1). SENDER utilizes a\nmultiple-layer stacked structure for the linear learning methods to detect the\ncanonical functional connectivity; 2). SENDER implements a non-fully connected\narchitecture conducted for the nonlinear learning methods to reveal the\nmeta-functional connectivity through shallow and deeper layers; 3). SENDER\nincorporates the proposed background components to extract the sub-functional\nconnectivity; 4). SENDER adopts a novel rank reduction operator to implement\nthe hyperparameters tuning automatically. To further validate the\neffectiveness, we compared SENDER with four peer methodologies using real\nfunctional Magnetic Resonance Imaging data for the human brain.", "journal": ""}
{"doi": "10.48550/arXiv.2501.05093", "date": "2025-01-09", "title": "Hierarchical Decomposed Dual-domain Deep Learning for Sparse-View CT Reconstruction", "authors": "Yoseob Han", "abstract": "Objective: X-ray computed tomography employing sparse projection views has\nemerged as a contemporary technique to mitigate radiation dose. However, due to\nthe inadequate number of projection views, an analytic reconstruction method\nutilizing filtered backprojection results in severe streaking artifacts.\nRecently, deep learning strategies employing image-domain networks have\ndemonstrated remarkable performance in eliminating the streaking artifact\ncaused by analytic reconstruction methods with sparse projection views.\nNevertheless, it is difficult to clarify the theoretical justification for\napplying deep learning to sparse view CT reconstruction, and it has been\nunderstood as restoration by removing image artifacts, not reconstruction.\n  Approach: By leveraging the theory of deep convolutional framelets and the\nhierarchical decomposition of measurement, this research reveals the\nconstraints of conventional image- and projection-domain deep learning\nmethodologies, subsequently, the research proposes a novel dual-domain deep\nlearning framework utilizing hierarchical decomposed measurements.\nSpecifically, the research elucidates how the performance of the\nprojection-domain network can be enhanced through a low-rank property of deep\nconvolutional framelets and a bowtie support of hierarchical decomposed\nmeasurement in the Fourier domain.\n  Main Results: This study demonstrated performance improvement of the proposed\nframework based on the low-rank property, resulting in superior reconstruction\nperformance compared to conventional analytic and deep learning methods.\n  Significance: By providing a theoretically justified deep learning approach\nfor sparse-view CT reconstruction, this study not only offers a superior\nalternative to existing methods but also opens new avenues for research in\nmedical imaging.", "journal": ""}
{"doi": "10.48550/arXiv.1605.01133", "date": "2016-05-04", "title": "Deep Motif: Visualizing Genomic Sequence Classifications", "authors": "Jack Lanchantin, Ritambhara Singh, Zeming Lin, Yanjun Qi", "abstract": "This paper applies a deep convolutional/highway MLP framework to classify\ngenomic sequences on the transcription factor binding site task. To make the\nmodel understandable, we propose an optimization driven strategy to extract\n\"motifs\", or symbolic patterns which visualize the positive class learned by\nthe network. We show that our system, Deep Motif (DeMo), extracts motifs that\nare similar to, and in some cases outperform the current well known motifs. In\naddition, we find that a deeper model consisting of multiple convolutional and\nhighway layers can outperform a single convolutional and fully connected layer\nin the previous state-of-the-art.", "journal": ""}
{"doi": "10.48550/arXiv.1811.01171", "date": "2018-11-03", "title": "Radius-margin bounds for deep neural networks", "authors": "Mayank Sharma, Jayadeva, Sumit Soman", "abstract": "Explaining the unreasonable effectiveness of deep learning has eluded\nresearchers around the globe. Various authors have described multiple metrics\nto evaluate the capacity of deep architectures. In this paper, we allude to the\nradius margin bounds described for a support vector machine (SVM) with hinge\nloss, apply the same to the deep feed-forward architectures and derive the\nVapnik-Chervonenkis (VC) bounds which are different from the earlier bounds\nproposed in terms of number of weights of the network. In doing so, we also\nrelate the effectiveness of techniques like Dropout and Dropconnect in bringing\ndown the capacity of the network. Finally, we describe the effect of maximizing\nthe input as well as the output margin to achieve an input noise-robust deep\narchitecture.", "journal": ""}
{"doi": "10.48550/arXiv.2406.18007", "date": "2024-04-09", "title": "Deep Mamba Multi-modal Learning", "authors": "Jian Zhu, Xin Zou, Yu Cui, Zhangmin Huang, Chenshu Hu, Bo Lyu", "abstract": "Inspired by the excellent performance of Mamba networks, we propose a novel\nDeep Mamba Multi-modal Learning (DMML). It can be used to achieve the fusion of\nmulti-modal features. We apply DMML to the field of multimedia retrieval and\npropose an innovative Deep Mamba Multi-modal Hashing (DMMH) method. It combines\nthe advantages of algorithm accuracy and inference speed. We validated the\neffectiveness of DMMH on three public datasets and achieved state-of-the-art\nresults.", "journal": ""}
