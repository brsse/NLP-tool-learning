{"doi": "10.48550/arXiv.2111.02188", "date": "2021-11-03", "title": "BERT-DRE: BERT with Deep Recursive Encoder for Natural Language Sentence Matching", "authors": "Ehsan Tavan, Ali Rahmati, Maryam Najafi, Saeed Bibak, Zahed Rahmati", "abstract": "This paper presents a deep neural architecture, for Natural Language Sentence\nMatching (NLSM) by adding a deep recursive encoder to BERT so called BERT with\nDeep Recursive Encoder (BERT-DRE). Our analysis of model behavior shows that\nBERT still does not capture the full complexity of text, so a deep recursive\nencoder is applied on top of BERT. Three Bi-LSTM layers with residual\nconnection are used to design a recursive encoder and an attention module is\nused on top of this encoder. To obtain the final vector, a pooling layer\nconsisting of average and maximum pooling is used. We experiment our model on\nfour benchmarks, SNLI, FarsTail, MultiNLI, SciTail, and a novel Persian\nreligious questions dataset. This paper focuses on improving the BERT results\nin the NLSM task. In this regard, comparisons between BERT-DRE and BERT are\nconducted, and it is shown that in all cases, BERT-DRE outperforms BERT. The\nBERT algorithm on the religious dataset achieved an accuracy of 89.70%, and\nBERT-DRE architectures improved to 90.29% using the same dataset.", "journal": ""}
{"doi": "10.48550/arXiv.2011.04266", "date": "2020-11-09", "title": "BERT-JAM: Boosting BERT-Enhanced Neural Machine Translation with Joint Attention", "authors": "Zhebin Zhang, Sai Wu, Dawei Jiang, Gang Chen", "abstract": "BERT-enhanced neural machine translation (NMT) aims at leveraging\nBERT-encoded representations for translation tasks. A recently proposed\napproach uses attention mechanisms to fuse Transformer's encoder and decoder\nlayers with BERT's last-layer representation and shows enhanced performance.\nHowever, their method doesn't allow for the flexible distribution of attention\nbetween the BERT representation and the encoder/decoder representation. In this\nwork, we propose a novel BERT-enhanced NMT model called BERT-JAM which improves\nupon existing models from two aspects: 1) BERT-JAM uses joint-attention modules\nto allow the encoder/decoder layers to dynamically allocate attention between\ndifferent representations, and 2) BERT-JAM allows the encoder/decoder layers to\nmake use of BERT's intermediate representations by composing them using a gated\nlinear unit (GLU). We train BERT-JAM with a novel three-phase optimization\nstrategy that progressively unfreezes different components of BERT-JAM. Our\nexperiments show that BERT-JAM achieves SOTA BLEU scores on multiple\ntranslation tasks.", "journal": ""}
{"doi": "10.48550/arXiv.2211.17201", "date": "2022-11-30", "title": "ExtremeBERT: A Toolkit for Accelerating Pretraining of Customized BERT", "authors": "Rui Pan, Shizhe Diao, Jianlin Chen, Tong Zhang", "abstract": "In this paper, we present ExtremeBERT, a toolkit for accelerating and\ncustomizing BERT pretraining. Our goal is to provide an easy-to-use BERT\npretraining toolkit for the research community and industry. Thus, the\npretraining of popular language models on customized datasets is affordable\nwith limited resources. Experiments show that, to achieve the same or better\nGLUE scores, the time cost of our toolkit is over $6\\times$ times less for BERT\nBase and $9\\times$ times less for BERT Large when compared with the original\nBERT paper. The documentation and code are released at\nhttps://github.com/extreme-bert/extreme-bert under the Apache-2.0 license.", "journal": ""}
{"doi": "10.48550/arXiv.1910.14296", "date": "2019-10-31", "title": "LIMIT-BERT : Linguistic Informed Multi-Task BERT", "authors": "Junru Zhou, Zhuosheng Zhang, Hai Zhao, Shuailiang Zhang", "abstract": "In this paper, we present a Linguistic Informed Multi-Task BERT (LIMIT-BERT)\nfor learning language representations across multiple linguistic tasks by\nMulti-Task Learning (MTL). LIMIT-BERT includes five key linguistic syntax and\nsemantics tasks: Part-Of-Speech (POS) tags, constituent and dependency\nsyntactic parsing, span and dependency semantic role labeling (SRL). Besides,\nLIMIT-BERT adopts linguistics mask strategy: Syntactic and Semantic Phrase\nMasking which mask all of the tokens corresponding to a syntactic/semantic\nphrase. Different from recent Multi-Task Deep Neural Networks (MT-DNN) (Liu et\nal., 2019), our LIMIT-BERT is linguistically motivated and learning in a\nsemi-supervised method which provides large amounts of linguistic-task data as\nsame as BERT learning corpus. As a result, LIMIT-BERT not only improves\nlinguistic tasks performance but also benefits from a regularization effect and\nlinguistic information that leads to more general representations to help adapt\nto new tasks and domains. LIMIT-BERT obtains new state-of-the-art or\ncompetitive results on both span and dependency semantic parsing on Propbank\nbenchmarks and both dependency and constituent syntactic parsing on Penn\nTreebank.", "journal": ""}
{"doi": "10.48550/arXiv.2002.03283", "date": "2020-02-09", "title": "Segmented Graph-Bert for Graph Instance Modeling", "authors": "Jiawei Zhang", "abstract": "In graph instance representation learning, both the diverse graph instance\nsizes and the graph node orderless property have been the major obstacles that\nrender existing representation learning models fail to work. In this paper, we\nwill examine the effectiveness of GRAPH-BERT on graph instance representation\nlearning, which was designed for node representation learning tasks originally.\nTo adapt GRAPH-BERT to the new problem settings, we re-design it with a\nsegmented architecture instead, which is also named as SEG-BERT (Segmented\nGRAPH-BERT) for reference simplicity in this paper. SEG-BERT involves no\nnode-order-variant inputs or functional components anymore, and it can handle\nthe graph node orderless property naturally. What's more, SEG-BERT has a\nsegmented architecture and introduces three different strategies to unify the\ngraph instance sizes, i.e., full-input, padding/pruning and segment shifting,\nrespectively. SEG-BERT is pre-trainable in an unsupervised manner, which can be\nfurther transferred to new tasks directly or with necessary fine-tuning. We\nhave tested the effectiveness of SEG-BERT with experiments on seven graph\ninstance benchmark datasets, and SEG-BERT can out-perform the comparison\nmethods on six out of them with significant performance advantages.", "journal": ""}
{"doi": "10.48550/arXiv.2002.06823", "date": "2020-02-17", "title": "Incorporating BERT into Neural Machine Translation", "authors": "Jinhua Zhu, Yingce Xia, Lijun Wu, Di He, Tao Qin, Wengang Zhou, Houqiang Li, Tie-Yan Liu", "abstract": "The recently proposed BERT has shown great power on a variety of natural\nlanguage understanding tasks, such as text classification, reading\ncomprehension, etc. However, how to effectively apply BERT to neural machine\ntranslation (NMT) lacks enough exploration. While BERT is more commonly used as\nfine-tuning instead of contextual embedding for downstream language\nunderstanding tasks, in NMT, our preliminary exploration of using BERT as\ncontextual embedding is better than using for fine-tuning. This motivates us to\nthink how to better leverage BERT for NMT along this direction. We propose a\nnew algorithm named BERT-fused model, in which we first use BERT to extract\nrepresentations for an input sequence, and then the representations are fused\nwith each layer of the encoder and decoder of the NMT model through attention\nmechanisms. We conduct experiments on supervised (including sentence-level and\ndocument-level translations), semi-supervised and unsupervised machine\ntranslation, and achieve state-of-the-art results on seven benchmark datasets.\nOur code is available at \\url{https://github.com/bert-nmt/bert-nmt}.", "journal": ""}
{"doi": "10.48550/arXiv.2101.10642", "date": "2021-01-26", "title": "Evaluation of BERT and ALBERT Sentence Embedding Performance on Downstream NLP Tasks", "authors": "Hyunjin Choi, Judong Kim, Seongho Joe, Youngjune Gwon", "abstract": "Contextualized representations from a pre-trained language model are central\nto achieve a high performance on downstream NLP task. The pre-trained BERT and\nA Lite BERT (ALBERT) models can be fine-tuned to give state-ofthe-art results\nin sentence-pair regressions such as semantic textual similarity (STS) and\nnatural language inference (NLI). Although BERT-based models yield the [CLS]\ntoken vector as a reasonable sentence embedding, the search for an optimal\nsentence embedding scheme remains an active research area in computational\nlinguistics. This paper explores on sentence embedding models for BERT and\nALBERT. In particular, we take a modified BERT network with siamese and triplet\nnetwork structures called Sentence-BERT (SBERT) and replace BERT with ALBERT to\ncreate Sentence-ALBERT (SALBERT). We also experiment with an outer CNN\nsentence-embedding network for SBERT and SALBERT. We evaluate performances of\nall sentence-embedding models considered using the STS and NLI datasets. The\nempirical results indicate that our CNN architecture improves ALBERT models\nsubstantially more than BERT models for STS benchmark. Despite significantly\nfewer model parameters, ALBERT sentence embedding is highly competitive to BERT\nin downstream NLP evaluations.", "journal": ""}
{"doi": "10.48550/arXiv.2109.11308", "date": "2021-09-23", "title": "Breaking BERT: Understanding its Vulnerabilities for Named Entity Recognition through Adversarial Attack", "authors": "Anne Dirkson, Suzan Verberne, Wessel Kraaij", "abstract": "Both generic and domain-specific BERT models are widely used for natural\nlanguage processing (NLP) tasks. In this paper we investigate the vulnerability\nof BERT models to variation in input data for Named Entity Recognition (NER)\nthrough adversarial attack. Experimental results show that BERT models are\nvulnerable to variation in the entity context with 20.2 to 45.0% of entities\npredicted completely wrong and another 29.3 to 53.3% of entities predicted\nwrong partially. BERT models seem most vulnerable to changes in the local\ncontext of entities and often a single change is sufficient to fool the model.\nThe domain-specific BERT model trained from scratch (SciBERT) is more\nvulnerable than the original BERT model or the domain-specific model that\nretains the BERT vocabulary (BioBERT). We also find that BERT models are\nparticularly vulnerable to emergent entities. Our results chart the\nvulnerabilities of BERT models for NER and emphasize the importance of further\nresearch into uncovering and reducing these weaknesses.", "journal": ""}
{"doi": "10.48550/arXiv.1902.04094", "date": "2019-02-11", "title": "BERT has a Mouth, and It Must Speak: BERT as a Markov Random Field Language Model", "authors": "Alex Wang, Kyunghyun Cho", "abstract": "We show that BERT (Devlin et al., 2018) is a Markov random field language\nmodel. This formulation gives way to a natural procedure to sample sentences\nfrom BERT. We generate from BERT and find that it can produce high-quality,\nfluent generations. Compared to the generations of a traditional left-to-right\nlanguage model, BERT generates sentences that are more diverse but of slightly\nworse quality.", "journal": ""}
{"doi": "10.48550/arXiv.2204.04477", "date": "2022-04-09", "title": "FoundationLayerNorm: Scaling BERT and GPT to 1,000 Layers", "authors": "Dezhou Shen", "abstract": "The mainstream BERT/GPT model contains only 10 to 20 layers, and there is\nlittle literature to discuss the training of deep BERT/GPT. This paper proposes\na simple yet effective method to stabilize BERT and GPT training. We\nsuccessfully scale up BERT and GPT to 1,000 layers, which is an order of\nmagnitude deeper than previous BERT and GPT. The proposed method\nFoundationLayerNormalization enables efficient training of deep neural networks\nand is validated at the 1000-layer scale.", "journal": ""}
{"doi": "10.48550/arXiv.2210.05043", "date": "2022-10-10", "title": "Multi-CLS BERT: An Efficient Alternative to Traditional Ensembling", "authors": "Haw-Shiuan Chang, Ruei-Yao Sun, Kathryn Ricci, Andrew McCallum", "abstract": "Ensembling BERT models often significantly improves accuracy, but at the cost\nof significantly more computation and memory footprint. In this work, we\npropose Multi-CLS BERT, a novel ensembling method for CLS-based prediction\ntasks that is almost as efficient as a single BERT model. Multi-CLS BERT uses\nmultiple CLS tokens with a parameterization and objective that encourages their\ndiversity. Thus instead of fine-tuning each BERT model in an ensemble (and\nrunning them all at test time), we need only fine-tune our single Multi-CLS\nBERT model (and run the one model at test time, ensembling just the multiple\nfinal CLS embeddings). To test its effectiveness, we build Multi-CLS BERT on\ntop of a state-of-the-art pretraining method for BERT (Aroca-Ouellette and\nRudzicz, 2020). In experiments on GLUE and SuperGLUE we show that our Multi-CLS\nBERT reliably improves both overall accuracy and confidence estimation. When\nonly 100 training samples are available in GLUE, the Multi-CLS BERT_Base model\ncan even outperform the corresponding BERT_Large model. We analyze the behavior\nof our Multi-CLS BERT, showing that it has many of the same characteristics and\nbehavior as a typical BERT 5-way ensemble, but with nearly 4-times less\ncomputation and memory.", "journal": ""}
{"doi": "10.48550/arXiv.1908.08167", "date": "2019-08-22", "title": "Multi-passage BERT: A Globally Normalized BERT Model for Open-domain Question Answering", "authors": "Zhiguo Wang, Patrick Ng, Xiaofei Ma, Ramesh Nallapati, Bing Xiang", "abstract": "BERT model has been successfully applied to open-domain QA tasks. However,\nprevious work trains BERT by viewing passages corresponding to the same\nquestion as independent training instances, which may cause incomparable scores\nfor answers from different passages. To tackle this issue, we propose a\nmulti-passage BERT model to globally normalize answer scores across all\npassages of the same question, and this change enables our QA model find better\nanswers by utilizing more passages. In addition, we find that splitting\narticles into passages with the length of 100 words by sliding window improves\nperformance by 4%. By leveraging a passage ranker to select high-quality\npassages, multi-passage BERT gains additional 2%. Experiments on four standard\nbenchmarks showed that our multi-passage BERT outperforms all state-of-the-art\nmodels on all benchmarks. In particular, on the OpenSQuAD dataset, our model\ngains 21.4% EM and 21.5% $F_1$ over all non-BERT models, and 5.8% EM and 6.5%\n$F_1$ over BERT-based models.", "journal": ""}
{"doi": "10.48550/arXiv.2001.09309", "date": "2020-01-25", "title": "BERT's output layer recognizes all hidden layers? Some Intriguing Phenomena and a simple way to boost BERT", "authors": "Wei-Tsung Kao, Tsung-Han Wu, Po-Han Chi, Chun-Cheng Hsieh, Hung-Yi Lee", "abstract": "Although Bidirectional Encoder Representations from Transformers (BERT) have\nachieved tremendous success in many natural language processing (NLP) tasks, it\nremains a black box. A variety of previous works have tried to lift the veil of\nBERT and understand each layer's functionality. In this paper, we found that\nsurprisingly the output layer of BERT can reconstruct the input sentence by\ndirectly taking each layer of BERT as input, even though the output layer has\nnever seen the input other than the final hidden layer. This fact remains true\nacross a wide variety of BERT-based models, even when some layers are\nduplicated. Based on this observation, we propose a quite simple method to\nboost the performance of BERT. By duplicating some layers in the BERT-based\nmodels to make it deeper (no extra training required in this step), they obtain\nbetter performance in the downstream tasks after fine-tuning.", "journal": ""}
{"doi": "10.48550/arXiv.2103.11792", "date": "2021-03-12", "title": "Comparing the Performance of NLP Toolkits and Evaluation measures in Legal Tech", "authors": "Muhammad Zohaib Khan", "abstract": "Recent developments in Natural Language Processing have led to the\nintroduction of state-of-the-art Neural Language Models, enabled with\nunsupervised transferable learning, using different pretraining objectives.\nWhile these models achieve excellent results on the downstream NLP tasks,\nvarious domain adaptation techniques can improve their performance on\ndomain-specific tasks. We compare and analyze the pretrained Neural Language\nModels, XLNet (autoregressive), and BERT (autoencoder) on the Legal Tasks.\nResults show that XLNet Model performs better on our Sequence Classification\ntask of Legal Opinions Classification, whereas BERT produces better results on\nthe NER task. We use domain-specific pretraining and additional legal\nvocabulary to adapt BERT Model further to the Legal Domain. We prepared\nmultiple variants of the BERT Model, using both methods and their combination.\nComparing our variants of the BERT Model, specializing in the Legal Domain, we\nconclude that both additional pretraining and vocabulary techniques enhance the\nBERT model's performance on the Legal Opinions Classification task. Additional\nlegal vocabulary improves BERT's performance on the NER task. Combining the\npretraining and vocabulary techniques further improves the final results. Our\nLegal-Vocab-BERT Model gives the best results on the Legal Opinions Task,\noutperforming the larger pretrained general Language Models, i.e., BERT-Base\nand XLNet-Base.", "journal": ""}
{"doi": "10.48550/arXiv.2505.06889", "date": "2025-05-11", "title": "IM-BERT: Enhancing Robustness of BERT through the Implicit Euler Method", "authors": "Mihyeon Kim, Juhyoung Park, Youngbin Kim", "abstract": "Pre-trained Language Models (PLMs) have achieved remarkable performance on\ndiverse NLP tasks through pre-training and fine-tuning. However, fine-tuning\nthe model with a large number of parameters on limited downstream datasets\noften leads to vulnerability to adversarial attacks, causing overfitting of the\nmodel on standard datasets.\n  To address these issues, we propose IM-BERT from the perspective of a dynamic\nsystem by conceptualizing a layer of BERT as a solution of Ordinary\nDifferential Equations (ODEs). Under the situation of initial value\nperturbation, we analyze the numerical stability of two main numerical ODE\nsolvers: the explicit and implicit Euler approaches.\n  Based on these analyses, we introduce a numerically robust IM-connection\nincorporating BERT's layers. This strategy enhances the robustness of PLMs\nagainst adversarial attacks, even in low-resource scenarios, without\nintroducing additional parameters or adversarial training strategies.\n  Experimental results on the adversarial GLUE (AdvGLUE) dataset validate the\nrobustness of IM-BERT under various conditions. Compared to the original BERT,\nIM-BERT exhibits a performance improvement of approximately 8.3\\%p on the\nAdvGLUE dataset. Furthermore, in low-resource scenarios, IM-BERT outperforms\nBERT by achieving 5.9\\%p higher accuracy.", "journal": ""}
{"doi": "10.48550/arXiv.2204.00989", "date": "2022-04-03", "title": "POS-BERT: Point Cloud One-Stage BERT Pre-Training", "authors": "Kexue Fu, Peng Gao, ShaoLei Liu, Renrui Zhang, Yu Qiao, Manning Wang", "abstract": "Recently, the pre-training paradigm combining Transformer and masked language\nmodeling has achieved tremendous success in NLP, images, and point clouds, such\nas BERT. However, directly extending BERT from NLP to point clouds requires\ntraining a fixed discrete Variational AutoEncoder (dVAE) before pre-training,\nwhich results in a complex two-stage method called Point-BERT. Inspired by BERT\nand MoCo, we propose POS-BERT, a one-stage BERT pre-training method for point\nclouds. Specifically, we use the mask patch modeling (MPM) task to perform\npoint cloud pre-training, which aims to recover masked patches information\nunder the supervision of the corresponding tokenizer output. Unlike Point-BERT,\nits tokenizer is extra-trained and frozen. We propose to use the dynamically\nupdated momentum encoder as the tokenizer, which is updated and outputs the\ndynamic supervision signal along with the training process. Further, in order\nto learn high-level semantic representation, we combine contrastive learning to\nmaximize the class token consistency between different transformation point\nclouds. Extensive experiments have demonstrated that POS-BERT can extract\nhigh-quality pre-training features and promote downstream tasks to improve\nperformance. Using the pre-training model without any fine-tuning to extract\nfeatures and train linear SVM on ModelNet40, POS-BERT achieves the\nstate-of-the-art classification accuracy, which exceeds Point-BERT by 3.5\\%. In\naddition, our approach has significantly improved many downstream tasks, such\nas fine-tuned classification, few-shot classification, part segmentation. The\ncode and trained-models will be available at:\n\\url{https://github.com/fukexue/POS-BERT}.", "journal": ""}
{"doi": "10.48550/arXiv.9707230", "date": "1997-07-28", "title": "A Course on: \"An Algebraic Approach to Nonperturbative Quantum Field Theory\"", "authors": "Bert Schroer", "abstract": "The content of this paper is incorporated into hep-th/9805093", "journal": ""}
{"doi": "10.48550/arXiv.9802046", "date": "1998-02-07", "title": "Coincidences between M(atrix) Theory and Algebraic QFT?", "authors": "Bert Schroer", "abstract": "The content of this paper is incorporated into hep-th/9805093", "journal": ""}
{"doi": "10.48550/arXiv.1908.10084", "date": "2019-08-27", "title": "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks", "authors": "Nils Reimers, Iryna Gurevych", "abstract": "BERT (Devlin et al., 2018) and RoBERTa (Liu et al., 2019) has set a new\nstate-of-the-art performance on sentence-pair regression tasks like semantic\ntextual similarity (STS). However, it requires that both sentences are fed into\nthe network, which causes a massive computational overhead: Finding the most\nsimilar pair in a collection of 10,000 sentences requires about 50 million\ninference computations (~65 hours) with BERT. The construction of BERT makes it\nunsuitable for semantic similarity search as well as for unsupervised tasks\nlike clustering.\n  In this publication, we present Sentence-BERT (SBERT), a modification of the\npretrained BERT network that use siamese and triplet network structures to\nderive semantically meaningful sentence embeddings that can be compared using\ncosine-similarity. This reduces the effort for finding the most similar pair\nfrom 65 hours with BERT / RoBERTa to about 5 seconds with SBERT, while\nmaintaining the accuracy from BERT.\n  We evaluate SBERT and SRoBERTa on common STS tasks and transfer learning\ntasks, where it outperforms other state-of-the-art sentence embeddings methods.", "journal": ""}
{"doi": "10.48550/arXiv.1904.08398", "date": "2019-04-17", "title": "DocBERT: BERT for Document Classification", "authors": "Ashutosh Adhikari, Achyudh Ram, Raphael Tang, Jimmy Lin", "abstract": "We present, to our knowledge, the first application of BERT to document\nclassification. A few characteristics of the task might lead one to think that\nBERT is not the most appropriate model: syntactic structures matter less for\ncontent categories, documents can often be longer than typical BERT input, and\ndocuments often have multiple labels. Nevertheless, we show that a\nstraightforward classification model using BERT is able to achieve the state of\nthe art across four popular datasets. To address the computational expense\nassociated with BERT inference, we distill knowledge from BERT-large to small\nbidirectional LSTMs, reaching BERT-base parity on multiple datasets using 30x\nfewer parameters. The primary contribution of our paper is improved baselines\nthat can provide the foundation for future work.", "journal": ""}
{"doi": "10.48550/arXiv.1911.03829", "date": "2019-11-10", "title": "Distilling Knowledge Learned in BERT for Text Generation", "authors": "Yen-Chun Chen, Zhe Gan, Yu Cheng, Jingzhou Liu, Jingjing Liu", "abstract": "Large-scale pre-trained language model such as BERT has achieved great\nsuccess in language understanding tasks. However, it remains an open question\nhow to utilize BERT for language generation. In this paper, we present a novel\napproach, Conditional Masked Language Modeling (C-MLM), to enable the\nfinetuning of BERT on target generation tasks. The finetuned BERT (teacher) is\nexploited as extra supervision to improve conventional Seq2Seq models (student)\nfor better text generation performance. By leveraging BERT's idiosyncratic\nbidirectional nature, distilling knowledge learned in BERT can encourage\nauto-regressive Seq2Seq models to plan ahead, imposing global sequence-level\nsupervision for coherent text generation. Experiments show that the proposed\napproach significantly outperforms strong Transformer baselines on multiple\nlanguage generation tasks such as machine translation and text summarization.\nOur proposed model also achieves new state of the art on IWSLT German-English\nand English-Vietnamese MT datasets. Code is available at\nhttps://github.com/ChenRocks/Distill-BERT-Textgen.", "journal": ""}
{"doi": "10.48550/arXiv.2002.12591", "date": "2020-02-28", "title": "DC-BERT: Decoupling Question and Document for Efficient Contextual Encoding", "authors": "Yuyu Zhang, Ping Nie, Xiubo Geng, Arun Ramamurthy, Le Song, Daxin Jiang", "abstract": "Recent studies on open-domain question answering have achieved prominent\nperformance improvement using pre-trained language models such as BERT.\nState-of-the-art approaches typically follow the \"retrieve and read\" pipeline\nand employ BERT-based reranker to filter retrieved documents before feeding\nthem into the reader module. The BERT retriever takes as input the\nconcatenation of question and each retrieved document. Despite the success of\nthese approaches in terms of QA accuracy, due to the concatenation, they can\nbarely handle high-throughput of incoming questions each with a large\ncollection of retrieved documents. To address the efficiency problem, we\npropose DC-BERT, a decoupled contextual encoding framework that has dual BERT\nmodels: an online BERT which encodes the question only once, and an offline\nBERT which pre-encodes all the documents and caches their encodings. On SQuAD\nOpen and Natural Questions Open datasets, DC-BERT achieves 10x speedup on\ndocument retrieval, while retaining most (about 98%) of the QA performance\ncompared to state-of-the-art approaches for open-domain question answering.", "journal": ""}
{"doi": "10.48550/arXiv.2004.03097", "date": "2020-04-07", "title": "Towards Non-task-specific Distillation of BERT via Sentence Representation Approximation", "authors": "Bowen Wu, Huan Zhang, Mengyuan Li, Zongsheng Wang, Qihang Feng, Junhong Huang, Baoxun Wang", "abstract": "Recently, BERT has become an essential ingredient of various NLP deep models\ndue to its effectiveness and universal-usability. However, the online\ndeployment of BERT is often blocked by its large-scale parameters and high\ncomputational cost. There are plenty of studies showing that the knowledge\ndistillation is efficient in transferring the knowledge from BERT into the\nmodel with a smaller size of parameters. Nevertheless, current BERT\ndistillation approaches mainly focus on task-specified distillation, such\nmethodologies lead to the loss of the general semantic knowledge of BERT for\nuniversal-usability. In this paper, we propose a sentence representation\napproximating oriented distillation framework that can distill the pre-trained\nBERT into a simple LSTM based model without specifying tasks. Consistent with\nBERT, our distilled model is able to perform transfer learning via fine-tuning\nto adapt to any sentence-level downstream task. Besides, our model can further\ncooperate with task-specific distillation procedures. The experimental results\non multiple NLP tasks from the GLUE benchmark show that our approach\noutperforms other task-specific distillation methods or even much larger\nmodels, i.e., ELMO, with efficiency well-improved.", "journal": ""}
{"doi": "10.48550/arXiv.2005.00766", "date": "2020-05-02", "title": "BERT-kNN: Adding a kNN Search Component to Pretrained Language Models for Better QA", "authors": "Nora Kassner, Hinrich Sch\u00fctze", "abstract": "Khandelwal et al. (2020) use a k-nearest-neighbor (kNN) component to improve\nlanguage model performance. We show that this idea is beneficial for\nopen-domain question answering (QA). To improve the recall of facts encountered\nduring training, we combine BERT (Devlin et al., 2019) with a traditional\ninformation retrieval step (IR) and a kNN search over a large datastore of an\nembedded text collection. Our contributions are as follows: i) BERT-kNN\noutperforms BERT on cloze-style QA by large margins without any further\ntraining. ii) We show that BERT often identifies the correct response category\n(e.g., US city), but only kNN recovers the factually correct answer (e.g.,\n\"Miami\"). iii) Compared to BERT, BERT-kNN excels for rare facts. iv) BERT-kNN\ncan easily handle facts not covered by BERT's training set, e.g., recent\nevents.", "journal": ""}
{"doi": "10.48550/arXiv.2010.00454", "date": "2020-10-01", "title": "Evaluating Multilingual BERT for Estonian", "authors": "Claudia Kittask, Kirill Milintsevich, Kairit Sirts", "abstract": "Recently, large pre-trained language models, such as BERT, have reached\nstate-of-the-art performance in many natural language processing tasks, but for\nmany languages, including Estonian, BERT models are not yet available. However,\nthere exist several multilingual BERT models that can handle multiple languages\nsimultaneously and that have been trained also on Estonian data. In this paper,\nwe evaluate four multilingual models -- multilingual BERT, multilingual\ndistilled BERT, XLM and XLM-RoBERTa -- on several NLP tasks including POS and\nmorphological tagging, NER and text classification. Our aim is to establish a\ncomparison between these multilingual BERT models and the existing baseline\nneural models for these tasks. Our results show that multilingual BERT models\ncan generalise well on different Estonian NLP tasks outperforming all baselines\nmodels for POS and morphological tagging and text classification, and reaching\nthe comparable level with the best baseline for NER, with XLM-RoBERTa achieving\nthe highest results compared with other multilingual models.", "journal": ""}
{"doi": "10.48550/arXiv.2010.02559", "date": "2020-10-06", "title": "LEGAL-BERT: The Muppets straight out of Law School", "authors": "Ilias Chalkidis, Manos Fergadiotis, Prodromos Malakasiotis, Nikolaos Aletras, Ion Androutsopoulos", "abstract": "BERT has achieved impressive performance in several NLP tasks. However, there\nhas been limited investigation on its adaptation guidelines in specialised\ndomains. Here we focus on the legal domain, where we explore several approaches\nfor applying BERT models to downstream legal tasks, evaluating on multiple\ndatasets. Our findings indicate that the previous guidelines for pre-training\nand fine-tuning, often blindly followed, do not always generalize well in the\nlegal domain. Thus we propose a systematic investigation of the available\nstrategies when applying BERT in specialised domains. These are: (a) use the\noriginal BERT out of the box, (b) adapt BERT by additional pre-training on\ndomain-specific corpora, and (c) pre-train BERT from scratch on domain-specific\ncorpora. We also propose a broader hyper-parameter search space when\nfine-tuning for downstream tasks and we release LEGAL-BERT, a family of BERT\nmodels intended to assist legal NLP research, computational law, and legal\ntechnology applications.", "journal": ""}
{"doi": "10.48550/arXiv.2010.08983", "date": "2020-10-18", "title": "Towards Interpreting BERT for Reading Comprehension Based QA", "authors": "Sahana Ramnath, Preksha Nema, Deep Sahni, Mitesh M. Khapra", "abstract": "BERT and its variants have achieved state-of-the-art performance in various\nNLP tasks. Since then, various works have been proposed to analyze the\nlinguistic information being captured in BERT. However, the current works do\nnot provide an insight into how BERT is able to achieve near human-level\nperformance on the task of Reading Comprehension based Question Answering. In\nthis work, we attempt to interpret BERT for RCQA. Since BERT layers do not have\npredefined roles, we define a layer's role or functionality using Integrated\nGradients. Based on the defined roles, we perform a preliminary analysis across\nall layers. We observed that the initial layers focus on query-passage\ninteraction, whereas later layers focus more on contextual understanding and\nenhancing the answer prediction. Specifically for quantifier questions (how\nmuch/how many), we notice that BERT focuses on confusing words (i.e., on other\nnumerical quantities in the passage) in the later layers, but still manages to\npredict the answer correctly. The fine-tuning and analysis scripts will be\npublicly available at https://github.com/iitmnlp/BERT-Analysis-RCQA .", "journal": ""}
{"doi": "10.48550/arXiv.2102.10934", "date": "2021-02-22", "title": "Using Prior Knowledge to Guide BERT's Attention in Semantic Textual Matching Tasks", "authors": "Tingyu Xia, Yue Wang, Yuan Tian, Yi Chang", "abstract": "We study the problem of incorporating prior knowledge into a deep\nTransformer-based model,i.e.,Bidirectional Encoder Representations from\nTransformers (BERT), to enhance its performance on semantic textual matching\ntasks. By probing and analyzing what BERT has already known when solving this\ntask, we obtain better understanding of what task-specific knowledge BERT needs\nthe most and where it is most needed. The analysis further motivates us to take\na different approach than most existing works. Instead of using prior knowledge\nto create a new training task for fine-tuning BERT, we directly inject\nknowledge into BERT's multi-head attention mechanism. This leads us to a simple\nyet effective approach that enjoys fast training stage as it saves the model\nfrom training on additional data or tasks other than the main task. Extensive\nexperiments demonstrate that the proposed knowledge-enhanced BERT is able to\nconsistently improve semantic textual matching performance over the original\nBERT model, and the performance benefit is most salient when training data is\nscarce.", "journal": ""}
{"doi": "10.48550/arXiv.2103.01126", "date": "2021-03-01", "title": "BERT based patent novelty search by training claims to their own description", "authors": "Michael Freunek, Andr\u00e9 Bodmer", "abstract": "In this paper we present a method to concatenate patent claims to their own\ndescription. By applying this method, BERT trains suitable descriptions for\nclaims. Such a trained BERT (claim-to-description- BERT) could be able to\nidentify novelty relevant descriptions for patents. In addition, we introduce a\nnew scoring scheme, relevance scoring or novelty scoring, to process the output\nof BERT in a meaningful way. We tested the method on patent applications by\ntraining BERT on the first claims of patents and corresponding descriptions.\nBERT's output has been processed according to the relevance score and the\nresults compared with the cited X documents in the search reports. The test\nshowed that BERT has scored some of the cited X documents as highly relevant.", "journal": ""}
{"doi": "10.48550/arXiv.2112.10925", "date": "2021-12-21", "title": "DB-BERT: a Database Tuning Tool that \"Reads the Manual\"", "authors": "Immanuel Trummer", "abstract": "DB-BERT is a database tuning tool that exploits information gained via\nnatural language analysis of manuals and other relevant text documents. It uses\ntext to identify database system parameters to tune as well as recommended\nparameter values. DB-BERT applies large, pre-trained language models\n(specifically, the BERT model) for text analysis. During an initial training\nphase, it fine-tunes model weights in order to translate natural language hints\ninto recommended settings. At run time, DB-BERT learns to aggregate, adapt, and\nprioritize hints to achieve optimal performance for a specific database system\nand benchmark. Both phases are iterative and use reinforcement learning to\nguide the selection of tuning settings to evaluate (penalizing settings that\nthe database system rejects while rewarding settings that improve performance).\nIn our experiments, we leverage hundreds of text documents about database\ntuning as input for DB-BERT. We compare DB-BERT against various baselines,\nconsidering different benchmarks (TPC-C and TPC-H), metrics (throughput and run\ntime), as well as database systems (Postgres and MySQL). In all cases, DB-BERT\nfinds the best parameter settings among all compared methods. The code of\nDB-BERT is available online at https://itrummer.github.io/dbbert/.", "journal": ""}
{"doi": "10.48550/arXiv.2203.13483", "date": "2022-03-25", "title": "MKQ-BERT: Quantized BERT with 4-bits Weights and Activations", "authors": "Hanlin Tang, Xipeng Zhang, Kai Liu, Jianchen Zhu, Zhanhui Kang", "abstract": "Recently, pre-trained Transformer based language models, such as BERT, have\nshown great superiority over the traditional methods in many Natural Language\nProcessing (NLP) tasks. However, the computational cost for deploying these\nmodels is prohibitive on resource-restricted devices. One method to alleviate\nthis computation overhead is to quantize the original model into fewer bits\nrepresentation, and previous work has proved that we can at most quantize both\nweights and activations of BERT into 8-bits, without degrading its performance.\nIn this work, we propose MKQ-BERT, which further improves the compression level\nand uses 4-bits for quantization. In MKQ-BERT, we propose a novel way for\ncomputing the gradient of the quantization scale, combined with an advanced\ndistillation strategy. On the one hand, we prove that MKQ-BERT outperforms the\nexisting BERT quantization methods for achieving a higher accuracy under the\nsame compression level. On the other hand, we are the first work that\nsuccessfully deploys the 4-bits BERT and achieves an end-to-end speedup for\ninference. Our results suggest that we could achieve 5.3x of bits reduction\nwithout degrading the model accuracy, and the inference speed of one int4 layer\nis 15x faster than a float32 layer in Transformer based model.", "journal": ""}
{"doi": "10.48550/arXiv.2205.06603", "date": "2022-05-13", "title": "Improving Contextual Representation with Gloss Regularized Pre-training", "authors": "Yu Lin, Zhecheng An, Peihao Wu, Zejun Ma", "abstract": "Though achieving impressive results on many NLP tasks, the BERT-like masked\nlanguage models (MLM) encounter the discrepancy between pre-training and\ninference. In light of this gap, we investigate the contextual representation\nof pre-training and inference from the perspective of word probability\ndistribution. We discover that BERT risks neglecting the contextual word\nsimilarity in pre-training. To tackle this issue, we propose an auxiliary gloss\nregularizer module to BERT pre-training (GR-BERT), to enhance word semantic\nsimilarity. By predicting masked words and aligning contextual embeddings to\ncorresponding glosses simultaneously, the word similarity can be explicitly\nmodeled. We design two architectures for GR-BERT and evaluate our model in\ndownstream tasks. Experimental results show that the gloss regularizer benefits\nBERT in word-level and sentence-level semantic representation. The GR-BERT\nachieves new state-of-the-art in lexical substitution task and greatly promotes\nBERT sentence representation in both unsupervised and supervised STS tasks.", "journal": ""}
{"doi": "10.48550/arXiv.2210.08284", "date": "2022-10-15", "title": "AraLegal-BERT: A pretrained language model for Arabic Legal text", "authors": "Muhammad AL-Qurishi, Sarah AlQaseemi, Riad Soussi", "abstract": "The effectiveness of the BERT model on multiple linguistic tasks has been\nwell documented. On the other hand, its potentials for narrow and specific\ndomains such as Legal, have not been fully explored. In this paper, we examine\nhow BERT can be used in the Arabic legal domain and try customizing this\nlanguage model for several downstream tasks using several different\ndomain-relevant training and testing datasets to train BERT from scratch. We\nintroduce the AraLegal-BERT, a bidirectional encoder Transformer-based model\nthat have been thoroughly tested and carefully optimized with the goal to\namplify the impact of NLP-driven solution concerning jurisprudence, legal\ndocuments, and legal practice. We fine-tuned AraLegal-BERT and evaluated it\nagainst three BERT variations for Arabic language in three natural languages\nunderstanding (NLU) tasks. The results show that the base version of\nAraLegal-BERT achieve better accuracy than the general and original BERT over\nthe Legal text.", "journal": ""}
{"doi": "10.48550/arXiv.2210.16663", "date": "2022-10-29", "title": "BERT Meets CTC: New Formulation of End-to-End Speech Recognition with Pre-trained Masked Language Model", "authors": "Yosuke Higuchi, Brian Yan, Siddhant Arora, Tetsuji Ogawa, Tetsunori Kobayashi, Shinji Watanabe", "abstract": "This paper presents BERT-CTC, a novel formulation of end-to-end speech\nrecognition that adapts BERT for connectionist temporal classification (CTC).\nOur formulation relaxes the conditional independence assumptions used in\nconventional CTC and incorporates linguistic knowledge through the explicit\noutput dependency obtained by BERT contextual embedding. BERT-CTC attends to\nthe full contexts of the input and hypothesized output sequences via the\nself-attention mechanism. This mechanism encourages a model to learn\ninner/inter-dependencies between the audio and token representations while\nmaintaining CTC's training efficiency. During inference, BERT-CTC combines a\nmask-predict algorithm with CTC decoding, which iteratively refines an output\nsequence. The experimental results reveal that BERT-CTC improves over\nconventional approaches across variations in speaking styles and languages.\nFinally, we show that the semantic representations in BERT-CTC are beneficial\ntowards downstream spoken language understanding tasks.", "journal": ""}
{"doi": "10.48550/arXiv.2310.17742", "date": "2023-10-26", "title": "BERT-PIN: A BERT-based Framework for Recovering Missing Data Segments in Time-series Load Profiles", "authors": "Yi Hu, Kai Ye, Hyeonjin Kim, Ning Lu", "abstract": "Inspired by the success of the Transformer model in natural language\nprocessing and computer vision, this paper introduces BERT-PIN, a Bidirectional\nEncoder Representations from Transformers (BERT) powered Profile Inpainting\nNetwork. BERT-PIN recovers multiple missing data segments (MDSs) using load and\ntemperature time-series profiles as inputs. To adopt a standard Transformer\nmodel structure for profile inpainting, we segment the load and temperature\nprofiles into line segments, treating each segment as a word and the entire\nprofile as a sentence. We incorporate a top candidates selection process in\nBERT-PIN, enabling it to produce a sequence of probability distributions, based\non which users can generate multiple plausible imputed data sets, each\nreflecting different confidence levels. We develop and evaluate BERT-PIN using\nreal-world dataset for two applications: multiple MDSs recovery and demand\nresponse baseline estimation. Simulation results show that BERT-PIN outperforms\nthe existing methods in accuracy while is capable of restoring multiple MDSs\nwithin a longer window. BERT-PIN, served as a pre-trained model, can be\nfine-tuned for conducting many downstream tasks, such as classification and\nsuper resolution.", "journal": ""}
{"doi": "10.48550/arXiv.2312.10702", "date": "2023-12-17", "title": "Can persistent homology whiten Transformer-based black-box models? A case study on BERT compression", "authors": "Luis Balderas, Miguel Lastra, Jos\u00e9 M. Ben\u00edtez", "abstract": "Large Language Models (LLMs) like BERT have gained significant prominence due\nto their remarkable performance in various natural language processing tasks.\nHowever, they come with substantial computational and memory costs.\nAdditionally, they are essentially black-box models, challenging to explain and\ninterpret. In this article, we propose Optimus BERT Compression and\nExplainability (OBCE), a methodology to bring explainability to BERT models\nusing persistent homology, aiming to measure the importance of each neuron by\nstudying the topological characteristics of their outputs. As a result, we can\ncompress BERT significantly by reducing the number of parameters (58.47% of the\noriginal parameters for BERT Base, 52.3% for BERT Large). We evaluated our\nmethodology on the standard GLUE Benchmark, comparing the results with\nstate-of-the-art techniques and achieving outstanding results. Consequently,\nour methodology can \"whiten\" BERT models by providing explainability to its\nneurons and reducing the model's size, making it more suitable for deployment\non resource-constrained devices.", "journal": "Applied Sciences. 2025, 15, 390"}
{"doi": "10.48550/arXiv.2403.12400", "date": "2024-03-19", "title": "Finding the Missing Data: A BERT-inspired Approach Against Package Loss in Wireless Sensing", "authors": "Zijian Zhao, Tingwei Chen, Fanyi Meng, Hang Li, Xiaoyang Li, Guangxu Zhu", "abstract": "Despite the development of various deep learning methods for Wi-Fi sensing,\npackage loss often results in noncontinuous estimation of the Channel State\nInformation (CSI), which negatively impacts the performance of the learning\nmodels. To overcome this challenge, we propose a deep learning model based on\nBidirectional Encoder Representations from Transformers (BERT) for CSI\nrecovery, named CSI-BERT. CSI-BERT can be trained in an self-supervised manner\non the target dataset without the need for additional data. Furthermore, unlike\ntraditional interpolation methods that focus on one subcarrier at a time,\nCSI-BERT captures the sequential relationships across different subcarriers.\nExperimental results demonstrate that CSI-BERT achieves lower error rates and\nfaster speed compared to traditional interpolation methods, even when facing\nwith high loss rates. Moreover, by harnessing the recovered CSI obtained from\nCSI-BERT, other deep learning models like Residual Network and Recurrent Neural\nNetwork can achieve an average increase in accuracy of approximately 15\\% in\nWi-Fi sensing tasks. The collected dataset WiGesture and code for our model are\npublicly available at https://github.com/RS2002/CSI-BERT.", "journal": ""}
{"doi": "10.48550/arXiv.2408.15265", "date": "2024-08-11", "title": "Multitask Fine-Tuning and Generative Adversarial Learning for Improved Auxiliary Classification", "authors": "Christopher Sun, Abishek Satish", "abstract": "In this study, we implement a novel BERT architecture for multitask\nfine-tuning on three downstream tasks: sentiment classification, paraphrase\ndetection, and semantic textual similarity prediction. Our model, Multitask\nBERT, incorporates layer sharing and a triplet architecture, custom sentence\npair tokenization, loss pairing, and gradient surgery. Such optimizations yield\na 0.516 sentiment classification accuracy, 0.886 paraphase detection accuracy,\nand 0.864 semantic textual similarity correlation on test data. We also apply\ngenerative adversarial learning to BERT, constructing a conditional generator\nmodel that maps from latent space to create fake embeddings in\n$\\mathbb{R}^{768}$. These fake embeddings are concatenated with real BERT\nembeddings and passed into a discriminator model for auxiliary classification.\nUsing this framework, which we refer to as AC-GAN-BERT, we conduct\nsemi-supervised sensitivity analyses to investigate the effect of increasing\namounts of unlabeled training data on AC-GAN-BERT's test accuracy. Overall,\naside from implementing a high-performing multitask classification system, our\nnovelty lies in the application of adversarial learning to construct a\ngenerator that mimics BERT. We find that the conditional generator successfully\nproduces rich embeddings with clear spatial correlation with class labels,\ndemonstrating avoidance of mode collapse. Our findings validate the GAN-BERT\napproach and point to future directions of generator-aided knowledge\ndistillation.", "journal": ""}
{"doi": "10.48550/arXiv.2410.00528", "date": "2024-10-01", "title": "End-to-End Speech Recognition with Pre-trained Masked Language Model", "authors": "Yosuke Higuchi, Tetsuji Ogawa, Tetsunori Kobayashi, Shinji Watanabe", "abstract": "We present a novel approach to end-to-end automatic speech recognition (ASR)\nthat utilizes pre-trained masked language models (LMs) to facilitate the\nextraction of linguistic information. The proposed models, BERT-CTC and BECTRA,\nare specifically designed to effectively integrate pre-trained LMs (e.g., BERT)\ninto end-to-end ASR models. BERT-CTC adapts BERT for connectionist temporal\nclassification (CTC) by addressing the constraint of the conditional\nindependence assumption between output tokens. This enables explicit\nconditioning of BERT's contextualized embeddings in the ASR process, seamlessly\nmerging audio and linguistic information through an iterative refinement\nalgorithm. BECTRA extends BERT-CTC to the transducer framework and trains the\ndecoder network using a vocabulary suitable for ASR training. This aims to\nbridge the gap between the text processed in end-to-end ASR and BERT, as these\nmodels have distinct vocabularies with varying text formats and styles, such as\nthe presence of punctuation. Experimental results on various ASR tasks\ndemonstrate that the proposed models improve over both the CTC and\ntransducer-based baselines, owing to the incorporation of BERT knowledge.\nMoreover, our in-depth analysis and investigation verify the effectiveness of\nthe proposed formulations and architectural designs.", "journal": ""}
{"doi": "10.48550/arXiv.1905.01758", "date": "2019-05-05", "title": "Investigating the Successes and Failures of BERT for Passage Re-Ranking", "authors": "Harshith Padigela, Hamed Zamani, W. Bruce Croft", "abstract": "The bidirectional encoder representations from transformers (BERT) model has\nrecently advanced the state-of-the-art in passage re-ranking. In this paper, we\nanalyze the results produced by a fine-tuned BERT model to better understand\nthe reasons behind such substantial improvements. To this aim, we focus on the\nMS MARCO passage re-ranking dataset and provide potential reasons for the\nsuccesses and failures of BERT for retrieval. In more detail, we empirically\nstudy a set of hypotheses and provide additional analysis to explain the\nsuccessful performance of BERT.", "journal": ""}
{"doi": "10.48550/arXiv.2105.05915", "date": "2021-05-12", "title": "Better than BERT but Worse than Baseline", "authors": "Boxiang Liu, Jiaji Huang, Xingyu Cai, Kenneth Church", "abstract": "This paper compares BERT-SQuAD and Ab3P on the Abbreviation Definition\nIdentification (ADI) task. ADI inputs a text and outputs short forms\n(abbreviations/acronyms) and long forms (expansions). BERT with reranking\nimproves over BERT without reranking but fails to reach the Ab3P rule-based\nbaseline. What is BERT missing? Reranking introduces two new features:\ncharmatch and freq. The first feature identifies opportunities to take\nadvantage of character constraints in acronyms and the second feature\nidentifies opportunities to take advantage of frequency constraints across\ndocuments.", "journal": ""}
{"doi": "10.48550/arXiv.2002.10345", "date": "2020-02-24", "title": "Improving BERT Fine-Tuning via Self-Ensemble and Self-Distillation", "authors": "Yige Xu, Xipeng Qiu, Ligao Zhou, Xuanjing Huang", "abstract": "Fine-tuning pre-trained language models like BERT has become an effective way\nin NLP and yields state-of-the-art results on many downstream tasks. Recent\nstudies on adapting BERT to new tasks mainly focus on modifying the model\nstructure, re-designing the pre-train tasks, and leveraging external data and\nknowledge. The fine-tuning strategy itself has yet to be fully explored. In\nthis paper, we improve the fine-tuning of BERT with two effective mechanisms:\nself-ensemble and self-distillation. The experiments on text classification and\nnatural language inference tasks show our proposed methods can significantly\nimprove the adaption of BERT without any external data or knowledge.", "journal": ""}
{"doi": "10.48550/arXiv.2009.08180", "date": "2020-09-17", "title": "DSC IIT-ISM at SemEval-2020 Task 6: Boosting BERT with Dependencies for Definition Extraction", "authors": "Aadarsh Singh, Priyanshu Kumar, Aman Sinha", "abstract": "We explore the performance of Bidirectional Encoder Representations from\nTransformers (BERT) at definition extraction. We further propose a joint model\nof BERT and Text Level Graph Convolutional Network so as to incorporate\ndependencies into the model. Our proposed model produces better results than\nBERT and achieves comparable results to BERT with fine tuned language model in\nDeftEval (Task 6 of SemEval 2020), a shared task of classifying whether a\nsentence contains a definition or not (Subtask 1).", "journal": ""}
{"doi": "10.48550/arXiv.2109.11888", "date": "2021-09-24", "title": "Robustness and Sensitivity of BERT Models Predicting Alzheimer's Disease from Text", "authors": "Jekaterina Novikova", "abstract": "Understanding robustness and sensitivity of BERT models predicting\nAlzheimer's disease from text is important for both developing better\nclassification models and for understanding their capabilities and limitations.\nIn this paper, we analyze how a controlled amount of desired and undesired text\nalterations impacts performance of BERT. We show that BERT is robust to natural\nlinguistic variations in text. On the other hand, we show that BERT is not\nsensitive to removing clinically important information from text.", "journal": ""}
{"doi": "10.48550/arXiv.2110.10027", "date": "2021-09-11", "title": "Clinical Trial Information Extraction with BERT", "authors": "Xiong Liu, Greg L. Hersch, Iya Khalil, Murthy Devarakonda", "abstract": "Natural language processing (NLP) of clinical trial documents can be useful\nin new trial design. Here we identify entity types relevant to clinical trial\ndesign and propose a framework called CT-BERT for information extraction from\nclinical trial text. We trained named entity recognition (NER) models to\nextract eligibility criteria entities by fine-tuning a set of pre-trained BERT\nmodels. We then compared the performance of CT-BERT with recent baseline\nmethods including attention-based BiLSTM and Criteria2Query. The results\ndemonstrate the superiority of CT-BERT in clinical trial NLP.", "journal": ""}
{"doi": "10.48550/arXiv.2402.14408", "date": "2024-02-22", "title": "Transferring BERT Capabilities from High-Resource to Low-Resource Languages Using Vocabulary Matching", "authors": "Piotr Rybak", "abstract": "Pre-trained language models have revolutionized the natural language\nunderstanding landscape, most notably BERT (Bidirectional Encoder\nRepresentations from Transformers). However, a significant challenge remains\nfor low-resource languages, where limited data hinders the effective training\nof such models. This work presents a novel approach to bridge this gap by\ntransferring BERT capabilities from high-resource to low-resource languages\nusing vocabulary matching. We conduct experiments on the Silesian and Kashubian\nlanguages and demonstrate the effectiveness of our approach to improve the\nperformance of BERT models even when the target language has minimal training\ndata. Our results highlight the potential of the proposed technique to\neffectively train BERT models for low-resource languages, thus democratizing\naccess to advanced language understanding models.", "journal": ""}
{"doi": "10.48550/arXiv.2404.03184", "date": "2024-04-04", "title": "The Death of Feature Engineering? BERT with Linguistic Features on SQuAD 2.0", "authors": "Jiawei Li, Yue Zhang", "abstract": "Machine reading comprehension is an essential natural language processing\ntask, which takes into a pair of context and query and predicts the\ncorresponding answer to query. In this project, we developed an end-to-end\nquestion answering model incorporating BERT and additional linguistic features.\nWe conclude that the BERT base model will be improved by incorporating the\nfeatures. The EM score and F1 score are improved 2.17 and 2.14 compared with\nBERT(base). Our best single model reaches EM score 76.55 and F1 score 79.97 in\nthe hidden test set. Our error analysis also shows that the linguistic\narchitecture can help model understand the context better in that it can locate\nanswers that BERT only model predicted \"No Answer\" wrongly.", "journal": ""}
{"doi": "10.48550/arXiv.2003.12932", "date": "2020-03-29", "title": "Noisy Text Data: Achilles' Heel of BERT", "authors": "Ankit Kumar, Piyush Makhija, Anuj Gupta", "abstract": "Owing to the phenomenal success of BERT on various NLP tasks and benchmark\ndatasets, industry practitioners are actively experimenting with fine-tuning\nBERT to build NLP applications for solving industry use cases. For most\ndatasets that are used by practitioners to build industrial NLP applications,\nit is hard to guarantee absence of any noise in the data. While BERT has\nperformed exceedingly well for transferring the learnings from one use case to\nanother, it remains unclear how BERT performs when fine-tuned on noisy text. In\nthis work, we explore the sensitivity of BERT to noise in the data. We work\nwith most commonly occurring noise (spelling mistakes, typos) and show that\nthis results in significant degradation in the performance of BERT. We present\nexperimental results to show that BERT's performance on fundamental NLP tasks\nlike sentiment analysis and textual similarity drops significantly in the\npresence of (simulated) noise on benchmark datasets viz. IMDB Movie Review,\nSTS-B, SST-2. Further, we identify shortcomings in the existing BERT pipeline\nthat are responsible for this drop in performance. Our findings suggest that\npractitioners need to be vary of presence of noise in their datasets while\nfine-tuning BERT to solve industry use cases.", "journal": ""}
{"doi": "10.48550/arXiv.2105.10909", "date": "2021-05-23", "title": "Killing One Bird with Two Stones: Model Extraction and Attribute Inference Attacks against BERT-based APIs", "authors": "Chen Chen, Xuanli He, Lingjuan Lyu, Fangzhao Wu", "abstract": "The collection and availability of big data, combined with advances in\npre-trained models (e.g., BERT, XLNET, etc), have revolutionized the predictive\nperformance of modern natural language processing tasks, ranging from text\nclassification to text generation. This allows corporations to provide machine\nlearning as a service (MLaaS) by encapsulating fine-tuned BERT-based models as\nAPIs. However, BERT-based APIs have exhibited a series of security and privacy\nvulnerabilities. For example, prior work has exploited the security issues of\nthe BERT-based APIs through the adversarial examples crafted by the extracted\nmodel. However, the privacy leakage problems of the BERT-based APIs through the\nextracted model have not been well studied. On the other hand, due to the high\ncapacity of BERT-based APIs, the fine-tuned model is easy to be overlearned,\nbut what kind of information can be leaked from the extracted model remains\nunknown. In this work, we bridge this gap by first presenting an effective\nmodel extraction attack, where the adversary can practically steal a BERT-based\nAPI (the target/victim model) by only querying a limited number of queries. We\nfurther develop an effective attribute inference attack which can infer the\nsensitive attribute of the training data used by the BERT-based APIs. Our\nextensive experiments on benchmark datasets under various realistic settings\nvalidate the potential vulnerabilities of BERT-based APIs. Moreover, we\ndemonstrate that two promising defense methods become ineffective against our\nattacks, which calls for more effective defense methods.", "journal": ""}
{"doi": "10.48550/arXiv.2001.05140", "date": "2020-01-15", "title": "Graph-Bert: Only Attention is Needed for Learning Graph Representations", "authors": "Jiawei Zhang, Haopeng Zhang, Congying Xia, Li Sun", "abstract": "The dominant graph neural networks (GNNs) over-rely on the graph links,\nseveral serious performance problems with which have been witnessed already,\ne.g., suspended animation problem and over-smoothing problem. What's more, the\ninherently inter-connected nature precludes parallelization within the graph,\nwhich becomes critical for large-sized graph, as memory constraints limit\nbatching across the nodes. In this paper, we will introduce a new graph neural\nnetwork, namely GRAPH-BERT (Graph based BERT), solely based on the attention\nmechanism without any graph convolution or aggregation operators. Instead of\nfeeding GRAPH-BERT with the complete large input graph, we propose to train\nGRAPH-BERT with sampled linkless subgraphs within their local contexts.\nGRAPH-BERT can be learned effectively in a standalone mode. Meanwhile, a\npre-trained GRAPH-BERT can also be transferred to other application tasks\ndirectly or with necessary fine-tuning if any supervised label information or\ncertain application oriented objective is available. We have tested the\neffectiveness of GRAPH-BERT on several graph benchmark datasets. Based the\npre-trained GRAPH-BERT with the node attribute reconstruction and structure\nrecovery tasks, we further fine-tune GRAPH-BERT on node classification and\ngraph clustering tasks specifically. The experimental results have demonstrated\nthat GRAPH-BERT can out-perform the existing GNNs in both the learning\neffectiveness and efficiency.", "journal": ""}
{"doi": "10.48550/arXiv.2001.08950", "date": "2020-01-24", "title": "PoWER-BERT: Accelerating BERT Inference via Progressive Word-vector Elimination", "authors": "Saurabh Goyal, Anamitra R. Choudhury, Saurabh M. Raje, Venkatesan T. Chakaravarthy, Yogish Sabharwal, Ashish Verma", "abstract": "We develop a novel method, called PoWER-BERT, for improving the inference\ntime of the popular BERT model, while maintaining the accuracy. It works by: a)\nexploiting redundancy pertaining to word-vectors (intermediate encoder outputs)\nand eliminating the redundant vectors. b) determining which word-vectors to\neliminate by developing a strategy for measuring their significance, based on\nthe self-attention mechanism. c) learning how many word-vectors to eliminate by\naugmenting the BERT model and the loss function. Experiments on the standard\nGLUE benchmark shows that PoWER-BERT achieves up to 4.5x reduction in inference\ntime over BERT with <1% loss in accuracy. We show that PoWER-BERT offers\nsignificantly better trade-off between accuracy and inference time compared to\nprior methods. We demonstrate that our method attains up to 6.8x reduction in\ninference time with <1% loss in accuracy when applied over ALBERT, a highly\ncompressed version of BERT. The code for PoWER-BERT is publicly available at\nhttps://github.com/IBM/PoWER-BERT.", "journal": ""}
{"doi": "10.48550/arXiv.2011.13633", "date": "2020-11-27", "title": "CoRe: An Efficient Coarse-refined Training Framework for BERT", "authors": "Cheng Yang, Shengnan Wang, Yuechuan Li, Chao Yang, Ming Yan, Jingqiao Zhang, Fangquan Lin", "abstract": "In recent years, BERT has made significant breakthroughs on many natural\nlanguage processing tasks and attracted great attentions. Despite its accuracy\ngains, the BERT model generally involves a huge number of parameters and needs\nto be trained on massive datasets, so training such a model is computationally\nvery challenging and time-consuming. Hence, training efficiency should be a\ncritical issue. In this paper, we propose a novel coarse-refined training\nframework named CoRe to speed up the training of BERT. Specifically, we\ndecompose the training process of BERT into two phases. In the first phase, by\nintroducing fast attention mechanism and decomposing the large parameters in\nthe feed-forward network sub-layer, we construct a relaxed BERT model which has\nmuch less parameters and much lower model complexity than the original BERT, so\nthe relaxed model can be quickly trained. In the second phase, we transform the\ntrained relaxed BERT model into the original BERT and further retrain the\nmodel. Thanks to the desired initialization provided by the relaxed model, the\nretraining phase requires much less training steps, compared with training an\noriginal BERT model from scratch with a random initialization. Experimental\nresults show that the proposed CoRe framework can greatly reduce the training\ntime without reducing the performance.", "journal": ""}
{"doi": "10.48550/arXiv.2205.00820", "date": "2022-05-02", "title": "Entity-aware Transformers for Entity Search", "authors": "Emma J. Gerritse, Faegheh Hasibi, Arjen P. de Vries", "abstract": "Pre-trained language models such as BERT have been a key ingredient to\nachieve state-of-the-art results on a variety of tasks in natural language\nprocessing and, more recently, also in information retrieval.Recent research\neven claims that BERT is able to capture factual knowledge about entity\nrelations and properties, the information that is commonly obtained from\nknowledge graphs. This paper investigates the following question: Do BERT-based\nentity retrieval models benefit from additional entity information stored in\nknowledge graphs? To address this research question, we map entity embeddings\ninto the same input space as a pre-trained BERT model and inject these entity\nembeddings into the BERT model. This entity-enriched language model is then\nemployed on the entity retrieval task. We show that the entity-enriched BERT\nmodel improves effectiveness on entity-oriented queries over a regular BERT\nmodel, establishing a new state-of-the-art result for the entity retrieval\ntask, with substantial improvements for complex natural language queries and\nqueries requesting a list of entities with a certain property. Additionally, we\nshow that the entity information provided by our entity-enriched model\nparticularly helps queries related to less popular entities. Last, we observe\nempirically that the entity-enriched BERT models enable fine-tuning on limited\ntraining data, which otherwise would not be feasible due to the known\ninstabilities of BERT in few-sample fine-tuning, thereby contributing to\ndata-efficient training of BERT for entity search.", "journal": ""}
{"doi": "10.48550/arXiv.2407.21073", "date": "2024-07-29", "title": "Enhancing Adversarial Text Attacks on BERT Models with Projected Gradient Descent", "authors": "Hetvi Waghela, Jaydip Sen, Sneha Rakshit", "abstract": "Adversarial attacks against deep learning models represent a major threat to\nthe security and reliability of natural language processing (NLP) systems. In\nthis paper, we propose a modification to the BERT-Attack framework, integrating\nProjected Gradient Descent (PGD) to enhance its effectiveness and robustness.\nThe original BERT-Attack, designed for generating adversarial examples against\nBERT-based models, suffers from limitations such as a fixed perturbation budget\nand a lack of consideration for semantic similarity. The proposed approach in\nthis work, PGD-BERT-Attack, addresses these limitations by leveraging PGD to\niteratively generate adversarial examples while ensuring both imperceptibility\nand semantic similarity to the original input. Extensive experiments are\nconducted to evaluate the performance of PGD-BERT-Attack compared to the\noriginal BERT-Attack and other baseline methods. The results demonstrate that\nPGD-BERT-Attack achieves higher success rates in causing misclassification\nwhile maintaining low perceptual changes. Furthermore, PGD-BERT-Attack produces\nadversarial instances that exhibit greater semantic resemblance to the initial\ninput, enhancing their applicability in real-world scenarios. Overall, the\nproposed modification offers a more effective and robust approach to\nadversarial attacks on BERT-based models, thus contributing to the advancement\nof defense against attacks on NLP systems.", "journal": ""}
{"doi": "10.48550/arXiv.1812.06705", "date": "2018-12-17", "title": "Conditional BERT Contextual Augmentation", "authors": "Xing Wu, Shangwen Lv, Liangjun Zang, Jizhong Han, Songlin Hu", "abstract": "We propose a novel data augmentation method for labeled sentences called\nconditional BERT contextual augmentation. Data augmentation methods are often\napplied to prevent overfitting and improve generalization of deep neural\nnetwork models. Recently proposed contextual augmentation augments labeled\nsentences by randomly replacing words with more varied substitutions predicted\nby language model. BERT demonstrates that a deep bidirectional language model\nis more powerful than either an unidirectional language model or the shallow\nconcatenation of a forward and backward model. We retrofit BERT to conditional\nBERT by introducing a new conditional masked language model\\footnote{The term\n\"conditional masked language model\" appeared once in original BERT paper, which\nindicates context-conditional, is equivalent to term \"masked language model\".\nIn our paper, \"conditional masked language model\" indicates we apply extra\nlabel-conditional constraint to the \"masked language model\".} task. The well\ntrained conditional BERT can be applied to enhance contextual augmentation.\nExperiments on six various different text classification tasks show that our\nmethod can be easily applied to both convolutional or recurrent neural networks\nclassifier to obtain obvious improvement.", "journal": ""}
{"doi": "10.48550/arXiv.1905.12848", "date": "2019-05-30", "title": "A Simple but Effective Method to Incorporate Multi-turn Context with BERT for Conversational Machine Comprehension", "authors": "Yasuhito Ohsugi, Itsumi Saito, Kyosuke Nishida, Hisako Asano, Junji Tomita", "abstract": "Conversational machine comprehension (CMC) requires understanding the context\nof multi-turn dialogue. Using BERT, a pre-training language model, has been\nsuccessful for single-turn machine comprehension, while modeling multiple turns\nof question answering with BERT has not been established because BERT has a\nlimit on the number and the length of input sequences. In this paper, we\npropose a simple but effective method with BERT for CMC. Our method uses BERT\nto encode a paragraph independently conditioned with each question and each\nanswer in a multi-turn context. Then, the method predicts an answer on the\nbasis of the paragraph representations encoded with BERT. The experiments with\nrepresentative CMC datasets, QuAC and CoQA, show that our method outperformed\nrecently published methods (+0.8 F1 on QuAC and +2.1 F1 on CoQA). In addition,\nwe conducted a detailed analysis of the effects of the number and types of\ndialogue history on the accuracy of CMC, and we found that the gold answer\nhistory, which may not be given in an actual conversation, contributed to the\nmodel performance most on both datasets.", "journal": ""}
{"doi": "10.48550/arXiv.1909.02209", "date": "2019-09-05", "title": "Semantics-aware BERT for Language Understanding", "authors": "Zhuosheng Zhang, Yuwei Wu, Hai Zhao, Zuchao Li, Shuailiang Zhang, Xi Zhou, Xiang Zhou", "abstract": "The latest work on language representations carefully integrates\ncontextualized features into language model training, which enables a series of\nsuccess especially in various machine reading comprehension and natural\nlanguage inference tasks. However, the existing language representation models\nincluding ELMo, GPT and BERT only exploit plain context-sensitive features such\nas character or word embeddings. They rarely consider incorporating structured\nsemantic information which can provide rich semantics for language\nrepresentation. To promote natural language understanding, we propose to\nincorporate explicit contextual semantics from pre-trained semantic role\nlabeling, and introduce an improved language representation model,\nSemantics-aware BERT (SemBERT), which is capable of explicitly absorbing\ncontextual semantics over a BERT backbone. SemBERT keeps the convenient\nusability of its BERT precursor in a light fine-tuning way without substantial\ntask-specific modifications. Compared with BERT, semantics-aware BERT is as\nsimple in concept but more powerful. It obtains new state-of-the-art or\nsubstantially improves results on ten reading comprehension and language\ninference tasks.", "journal": ""}
{"doi": "10.48550/arXiv.1904.07531", "date": "2019-04-16", "title": "Understanding the Behaviors of BERT in Ranking", "authors": "Yifan Qiao, Chenyan Xiong, Zhenghao Liu, Zhiyuan Liu", "abstract": "This paper studies the performances and behaviors of BERT in ranking tasks.\nWe explore several different ways to leverage the pre-trained BERT and\nfine-tune it on two ranking tasks: MS MARCO passage reranking and TREC Web\nTrack ad hoc document ranking. Experimental results on MS MARCO demonstrate the\nstrong effectiveness of BERT in question-answering focused passage ranking\ntasks, as well as the fact that BERT is a strong interaction-based seq2seq\nmatching model. Experimental results on TREC show the gaps between the BERT\npre-trained on surrounding contexts and the needs of ad hoc document ranking.\nAnalyses illustrate how BERT allocates its attentions between query-document\ntokens in its Transformer layers, how it prefers semantic matches between\nparaphrase tokens, and how that differs with the soft match patterns learned by\na click-trained neural ranker.", "journal": ""}
{"doi": "10.48550/arXiv.1910.07973", "date": "2019-10-17", "title": "Universal Text Representation from BERT: An Empirical Study", "authors": "Xiaofei Ma, Zhiguo Wang, Patrick Ng, Ramesh Nallapati, Bing Xiang", "abstract": "We present a systematic investigation of layer-wise BERT activations for\ngeneral-purpose text representations to understand what linguistic information\nthey capture and how transferable they are across different tasks.\nSentence-level embeddings are evaluated against two state-of-the-art models on\ndownstream and probing tasks from SentEval, while passage-level embeddings are\nevaluated on four question-answering (QA) datasets under a learning-to-rank\nproblem setting. Embeddings from the pre-trained BERT model perform poorly in\nsemantic similarity and sentence surface information probing tasks. Fine-tuning\nBERT on natural language inference data greatly improves the quality of the\nembeddings. Combining embeddings from different BERT layers can further boost\nperformance. BERT embeddings outperform BM25 baseline significantly on factoid\nQA datasets at the passage level, but fail to perform better than BM25 on\nnon-factoid datasets. For all QA datasets, there is a gap between\nembedding-based method and in-domain fine-tuned BERT (we report new\nstate-of-the-art results on two datasets), which suggests deep interactions\nbetween question and answer pairs are critical for those hard tasks.", "journal": ""}
{"doi": "10.48550/arXiv.1910.12391", "date": "2019-10-28", "title": "What does BERT Learn from Multiple-Choice Reading Comprehension Datasets?", "authors": "Chenglei Si, Shuohang Wang, Min-Yen Kan, Jing Jiang", "abstract": "Multiple-Choice Reading Comprehension (MCRC) requires the model to read the\npassage and question, and select the correct answer among the given options.\nRecent state-of-the-art models have achieved impressive performance on multiple\nMCRC datasets. However, such performance may not reflect the model's true\nability of language understanding and reasoning. In this work, we adopt two\napproaches to investigate what BERT learns from MCRC datasets: 1) an\nun-readable data attack, in which we add keywords to confuse BERT, leading to a\nsignificant performance drop; and 2) an un-answerable data training, in which\nwe train BERT on partial or shuffled input. Under un-answerable data training,\nBERT achieves unexpectedly high performance. Based on our experiments on the 5\nkey MCRC datasets - RACE, MCTest, MCScript, MCScript2.0, DREAM - we observe\nthat 1) fine-tuned BERT mainly learns how keywords lead to correct prediction,\ninstead of learning semantic understanding and reasoning; and 2) BERT does not\nneed correct syntactic information to solve the task; 3) there exists artifacts\nin these datasets such that they can be solved even without the full context.", "journal": ""}
{"doi": "10.48550/arXiv.2105.11618", "date": "2021-05-25", "title": "TR-BERT: Dynamic Token Reduction for Accelerating BERT Inference", "authors": "Deming Ye, Yankai Lin, Yufei Huang, Maosong Sun", "abstract": "Existing pre-trained language models (PLMs) are often computationally\nexpensive in inference, making them impractical in various resource-limited\nreal-world applications. To address this issue, we propose a dynamic token\nreduction approach to accelerate PLMs' inference, named TR-BERT, which could\nflexibly adapt the layer number of each token in inference to avoid redundant\ncalculation. Specially, TR-BERT formulates the token reduction process as a\nmulti-step token selection problem and automatically learns the selection\nstrategy via reinforcement learning. The experimental results on several\ndownstream NLP tasks show that TR-BERT is able to speed up BERT by 2-5 times to\nsatisfy various performance demands. Moreover, TR-BERT can also achieve better\nperformance with less computation in a suite of long-text tasks since its\ntoken-level layer number adaption greatly accelerates the self-attention\noperation in PLMs. The source code and experiment details of this paper can be\nobtained from https://github.com/thunlp/TR-BERT.", "journal": ""}
{"doi": "10.48550/arXiv.2201.02010", "date": "2022-01-06", "title": "Self-Training Vision Language BERTs with a Unified Conditional Model", "authors": "Xiaofeng Yang, Fengmao Lv, Fayao Liu, Guosheng Lin", "abstract": "Natural language BERTs are trained with language corpus in a self-supervised\nmanner. Unlike natural language BERTs, vision language BERTs need paired data\nto train, which restricts the scale of VL-BERT pretraining. We propose a\nself-training approach that allows training VL-BERTs from unlabeled image data.\nThe proposed method starts with our unified conditional model -- a vision\nlanguage BERT model that can perform zero-shot conditional generation. Given\ndifferent conditions, the unified conditional model can generate captions,\ndense captions, and even questions. We use the labeled image data to train a\nteacher model and use the trained model to generate pseudo captions on\nunlabeled image data. We then combine the labeled data and pseudo labeled data\nto train a student model. The process is iterated by putting the student model\nas a new teacher. By using the proposed self-training approach and only 300k\nunlabeled extra data, we are able to get competitive or even better\nperformances compared to the models of similar model size trained with 3\nmillion extra image data.", "journal": ""}
{"doi": "10.48550/arXiv.2201.03382", "date": "2022-01-10", "title": "BERT for Sentiment Analysis: Pre-trained and Fine-Tuned Alternatives", "authors": "Frederico Souza, Jo\u00e3o Filho", "abstract": "BERT has revolutionized the NLP field by enabling transfer learning with\nlarge language models that can capture complex textual patterns, reaching the\nstate-of-the-art for an expressive number of NLP applications. For text\nclassification tasks, BERT has already been extensively explored. However,\naspects like how to better cope with the different embeddings provided by the\nBERT output layer and the usage of language-specific instead of multilingual\nmodels are not well studied in the literature, especially for the Brazilian\nPortuguese language. The purpose of this article is to conduct an extensive\nexperimental study regarding different strategies for aggregating the features\nproduced in the BERT output layer, with a focus on the sentiment analysis task.\nThe experiments include BERT models trained with Brazilian Portuguese corpora\nand the multilingual version, contemplating multiple aggregation strategies and\nopen-source datasets with predefined training, validation, and test partitions\nto facilitate the reproducibility of the results. BERT achieved the highest\nROC-AUC values for the majority of cases as compared to TF-IDF. Nonetheless,\nTF-IDF represents a good trade-off between the predictive performance and\ncomputational cost.", "journal": ""}
{"doi": "10.48550/arXiv.1906.01698", "date": "2019-06-04", "title": "Open Sesame: Getting Inside BERT's Linguistic Knowledge", "authors": "Yongjie Lin, Yi Chern Tan, Robert Frank", "abstract": "How and to what extent does BERT encode syntactically-sensitive hierarchical\ninformation or positionally-sensitive linear information? Recent work has shown\nthat contextual representations like BERT perform well on tasks that require\nsensitivity to linguistic structure. We present here two studies which aim to\nprovide a better understanding of the nature of BERT's representations. The\nfirst of these focuses on the identification of structurally-defined elements\nusing diagnostic classifiers, while the second explores BERT's representation\nof subject-verb agreement and anaphor-antecedent dependencies through a\nquantitative assessment of self-attention vectors. In both cases, we find that\nBERT encodes positional information about word tokens well on its lower layers,\nbut switches to a hierarchically-oriented encoding on higher layers. We\nconclude then that BERT's representations do indeed model linguistically\nrelevant aspects of hierarchical structure, though they do not appear to show\nthe sharp sensitivity to hierarchical structure that is found in human\nprocessing of reflexive anaphora.", "journal": ""}
{"doi": "10.48550/arXiv.1911.03681", "date": "2019-11-09", "title": "E-BERT: Efficient-Yet-Effective Entity Embeddings for BERT", "authors": "Nina Poerner, Ulli Waltinger, Hinrich Sch\u00fctze", "abstract": "We present a novel way of injecting factual knowledge about entities into the\npretrained BERT model (Devlin et al., 2019): We align Wikipedia2Vec entity\nvectors (Yamada et al., 2016) with BERT's native wordpiece vector space and use\nthe aligned entity vectors as if they were wordpiece vectors. The resulting\nentity-enhanced version of BERT (called E-BERT) is similar in spirit to ERNIE\n(Zhang et al., 2019) and KnowBert (Peters et al., 2019), but it requires no\nexpensive further pretraining of the BERT encoder. We evaluate E-BERT on\nunsupervised question answering (QA), supervised relation classification (RC)\nand entity linking (EL). On all three tasks, E-BERT outperforms BERT and other\nbaselines. We also show quantitatively that the original BERT model is overly\nreliant on the surface form of entity names (e.g., guessing that someone with\nan Italian-sounding name speaks Italian), and that E-BERT mitigates this\nproblem.", "journal": ""}
{"doi": "10.48550/arXiv.2005.13012", "date": "2020-05-26", "title": "Comparing BERT against traditional machine learning text classification", "authors": "Santiago Gonz\u00e1lez-Carvajal, Eduardo C. Garrido-Merch\u00e1n", "abstract": "The BERT model has arisen as a popular state-of-the-art machine learning\nmodel in the recent years that is able to cope with multiple NLP tasks such as\nsupervised text classification without human supervision. Its flexibility to\ncope with any type of corpus delivering great results has make this approach\nvery popular not only in academia but also in the industry. Although, there are\nlots of different approaches that have been used throughout the years with\nsuccess. In this work, we first present BERT and include a little review on\nclassical NLP approaches. Then, we empirically test with a suite of experiments\ndealing different scenarios the behaviour of BERT against the traditional\nTF-IDF vocabulary fed to machine learning algorithms. Our purpose of this work\nis to add empirical evidence to support or refuse the use of BERT as a default\non NLP tasks. Experiments show the superiority of BERT and its independence of\nfeatures of the NLP problem such as the language of the text adding empirical\nevidence to use BERT as a default technique to be used in NLP problems.", "journal": "Journal of Computational and Cognitive Engineering (2023)"}
{"doi": "10.48550/arXiv.2007.01658", "date": "2020-07-03", "title": "Playing with Words at the National Library of Sweden -- Making a Swedish BERT", "authors": "Martin Malmsten, Love B\u00f6rjeson, Chris Haffenden", "abstract": "This paper introduces the Swedish BERT (\"KB-BERT\") developed by the KBLab for\ndata-driven research at the National Library of Sweden (KB). Building on recent\nefforts to create transformer-based BERT models for languages other than\nEnglish, we explain how we used KB's collections to create and train a new\nlanguage-specific BERT model for Swedish. We also present the results of our\nmodel in comparison with existing models - chiefly that produced by the Swedish\nPublic Employment Service, Arbetsf\\\"ormedlingen, and Google's multilingual\nM-BERT - where we demonstrate that KB-BERT outperforms these in a range of NLP\ntasks from named entity recognition (NER) to part-of-speech tagging (POS). Our\ndiscussion highlights the difficulties that continue to exist given the lack of\ntraining data and testbeds for smaller languages like Swedish. We release our\nmodel for further exploration and research here:\nhttps://github.com/Kungbib/swedish-bert-models .", "journal": ""}
{"doi": "10.48550/arXiv.2010.07523", "date": "2020-10-15", "title": "Context-Guided BERT for Targeted Aspect-Based Sentiment Analysis", "authors": "Zhengxuan Wu, Desmond C. Ong", "abstract": "Aspect-based sentiment analysis (ABSA) and Targeted ASBA (TABSA) allow\nfiner-grained inferences about sentiment to be drawn from the same text,\ndepending on context. For example, a given text can have different targets\n(e.g., neighborhoods) and different aspects (e.g., price or safety), with\ndifferent sentiment associated with each target-aspect pair. In this paper, we\ninvestigate whether adding context to self-attention models improves\nperformance on (T)ABSA. We propose two variants of Context-Guided BERT\n(CG-BERT) that learn to distribute attention under different contexts. We first\nadapt a context-aware Transformer to produce a CG-BERT that uses context-guided\nsoftmax-attention. Next, we propose an improved Quasi-Attention CG-BERT model\nthat learns a compositional attention that supports subtractive attention. We\ntrain both models with pretrained BERT on two (T)ABSA datasets: SentiHood and\nSemEval-2014 (Task 4). Both models achieve new state-of-the-art results with\nour QACG-BERT model having the best performance. Furthermore, we provide\nanalyses of the impact of context in the our proposed models. Our work provides\nmore evidence for the utility of adding context-dependencies to pretrained\nself-attention-based language models for context-based natural language tasks.", "journal": "AAAI 2021"}
{"doi": "10.48550/arXiv.2011.06153", "date": "2020-11-12", "title": "Augmenting BERT Carefully with Underrepresented Linguistic Features", "authors": "Aparna Balagopalan, Jekaterina Novikova", "abstract": "Fine-tuned Bidirectional Encoder Representations from Transformers\n(BERT)-based sequence classification models have proven to be effective for\ndetecting Alzheimer's Disease (AD) from transcripts of human speech. However,\nprevious research shows it is possible to improve BERT's performance on various\ntasks by augmenting the model with additional information. In this work, we use\nprobing tasks as introspection techniques to identify linguistic information\nnot well-represented in various layers of BERT, but important for the AD\ndetection task. We supplement these linguistic features in which\nrepresentations from BERT are found to be insufficient with hand-crafted\nfeatures externally, and show that jointly fine-tuning BERT in combination with\nthese features improves the performance of AD classification by upto 5\\% over\nfine-tuned BERT alone.", "journal": ""}
{"doi": "10.48550/arXiv.2011.10426", "date": "2020-11-20", "title": "Fine-Tuning BERT for Sentiment Analysis of Vietnamese Reviews", "authors": "Quoc Thai Nguyen, Thoai Linh Nguyen, Ngoc Hoang Luong, Quoc Hung Ngo", "abstract": "Sentiment analysis is an important task in the field ofNature Language\nProcessing (NLP), in which users' feedbackdata on a specific issue are\nevaluated and analyzed. Manydeep learning models have been proposed to tackle\nthis task, including the recently-introduced Bidirectional Encoder\nRep-resentations from Transformers (BERT) model. In this paper,we experiment\nwith two BERT fine-tuning methods for thesentiment analysis task on datasets of\nVietnamese reviews: 1) a method that uses only the [CLS] token as the input for\nanattached feed-forward neural network, and 2) another methodin which all BERT\noutput vectors are used as the input forclassification. Experimental results on\ntwo datasets show thatmodels using BERT slightly outperform other models\nusingGloVe and FastText. Also, regarding the datasets employed inthis study,\nour proposed BERT fine-tuning method produces amodel with better performance\nthan the original BERT fine-tuning method.", "journal": ""}
{"doi": "10.48550/arXiv.2101.09755", "date": "2021-01-24", "title": "RomeBERT: Robust Training of Multi-Exit BERT", "authors": "Shijie Geng, Peng Gao, Zuohui Fu, Yongfeng Zhang", "abstract": "BERT has achieved superior performances on Natural Language Understanding\n(NLU) tasks. However, BERT possesses a large number of parameters and demands\ncertain resources to deploy. For acceleration, Dynamic Early Exiting for BERT\n(DeeBERT) has been proposed recently, which incorporates multiple exits and\nadopts a dynamic early-exit mechanism to ensure efficient inference. While\nobtaining an efficiency-performance tradeoff, the performances of early exits\nin multi-exit BERT are significantly worse than late exits. In this paper, we\nleverage gradient regularized self-distillation for RObust training of\nMulti-Exit BERT (RomeBERT), which can effectively solve the performance\nimbalance problem between early and late exits. Moreover, the proposed RomeBERT\nadopts a one-stage joint training strategy for multi-exits and the BERT\nbackbone while DeeBERT needs two stages that require more training time.\nExtensive experiments on GLUE datasets are performed to demonstrate the\nsuperiority of our approach. Our code is available at\nhttps://github.com/romebert/RomeBERT.", "journal": ""}
{"doi": "10.48550/arXiv.2103.02800", "date": "2021-03-04", "title": "Hardware Acceleration of Fully Quantized BERT for Efficient Natural Language Processing", "authors": "Zejian Liu, Gang Li, Jian Cheng", "abstract": "BERT is the most recent Transformer-based model that achieves\nstate-of-the-art performance in various NLP tasks. In this paper, we\ninvestigate the hardware acceleration of BERT on FPGA for edge computing. To\ntackle the issue of huge computational complexity and memory footprint, we\npropose to fully quantize the BERT (FQ-BERT), including weights, activations,\nsoftmax, layer normalization, and all the intermediate results. Experiments\ndemonstrate that the FQ-BERT can achieve 7.94x compression for weights with\nnegligible performance loss. We then propose an accelerator tailored for the\nFQ-BERT and evaluate on Xilinx ZCU102 and ZCU111 FPGA. It can achieve a\nperformance-per-watt of 3.18 fps/W, which is 28.91x and 12.72x over Intel(R)\nCore(TM) i7-8700 CPU and NVIDIA K80 GPU, respectively.", "journal": "Design, Automation & Test in Europe (DATE) 2021"}
{"doi": "10.48550/arXiv.2104.08145", "date": "2021-04-09", "title": "KI-BERT: Infusing Knowledge Context for Better Language and Domain Understanding", "authors": "Keyur Faldu, Amit Sheth, Prashant Kikani, Hemang Akbari", "abstract": "Contextualized entity representations learned by state-of-the-art\ntransformer-based language models (TLMs) like BERT, GPT, T5, etc., leverage the\nattention mechanism to learn the data context from training data corpus.\nHowever, these models do not use the knowledge context. Knowledge context can\nbe understood as semantics about entities and their relationship with\nneighboring entities in knowledge graphs. We propose a novel and effective\ntechnique to infuse knowledge context from multiple knowledge graphs for\nconceptual and ambiguous entities into TLMs during fine-tuning. It projects\nknowledge graph embeddings in the homogeneous vector-space, introduces new\ntoken-types for entities, aligns entity position ids, and a selective attention\nmechanism. We take BERT as a baseline model and implement the\n\"Knowledge-Infused BERT\" by infusing knowledge context from ConceptNet and\nWordNet, which significantly outperforms BERT and other recent knowledge-aware\nBERT variants like ERNIE, SenseBERT, and BERT_CS over eight different subtasks\nof GLUE benchmark. The KI-BERT-base model even significantly outperforms\nBERT-large for domain-specific tasks like SciTail and academic subsets of QQP,\nQNLI, and MNLI.", "journal": ""}
{"doi": "10.48550/arXiv.2104.08335", "date": "2021-04-14", "title": "Demystifying BERT: Implications for Accelerator Design", "authors": "Suchita Pati, Shaizeen Aga, Nuwan Jayasena, Matthew D. Sinclair", "abstract": "Transfer learning in natural language processing (NLP), as realized using\nmodels like BERT (Bi-directional Encoder Representation from Transformer), has\nsignificantly improved language representation with models that can tackle\nchallenging language problems. Consequently, these applications are driving the\nrequirements of future systems. Thus, we focus on BERT, one of the most popular\nNLP transfer learning algorithms, to identify how its algorithmic behavior can\nguide future accelerator design. To this end, we carefully profile BERT\ntraining and identify key algorithmic behaviors which are worthy of attention\nin accelerator design.\n  We observe that while computations which manifest as matrix multiplication\ndominate BERT's overall runtime, as in many convolutional neural networks,\nmemory-intensive computations also feature prominently. We characterize these\ncomputations, which have received little attention so far. Further, we also\nidentify heterogeneity in compute-intensive BERT computations and discuss\nsoftware and possible hardware mechanisms to further optimize these\ncomputations. Finally, we discuss implications of these behaviors as networks\nget larger and use distributed training environments, and how techniques such\nas micro-batching and mixed-precision training scale. Overall, our analysis\nidentifies holistic solutions to optimize systems for BERT-like models.", "journal": ""}
{"doi": "10.48550/arXiv.2109.06304", "date": "2021-09-13", "title": "Phrase-BERT: Improved Phrase Embeddings from BERT with an Application to Corpus Exploration", "authors": "Shufan Wang, Laure Thompson, Mohit Iyyer", "abstract": "Phrase representations derived from BERT often do not exhibit complex phrasal\ncompositionality, as the model relies instead on lexical similarity to\ndetermine semantic relatedness. In this paper, we propose a contrastive\nfine-tuning objective that enables BERT to produce more powerful phrase\nembeddings. Our approach (Phrase-BERT) relies on a dataset of diverse phrasal\nparaphrases, which is automatically generated using a paraphrase generation\nmodel, as well as a large-scale dataset of phrases in context mined from the\nBooks3 corpus. Phrase-BERT outperforms baselines across a variety of\nphrase-level similarity tasks, while also demonstrating increased lexical\ndiversity between nearest neighbors in the vector space. Finally, as a case\nstudy, we show that Phrase-BERT embeddings can be easily integrated with a\nsimple autoencoder to build a phrase-based neural topic model that interprets\ntopics as mixtures of words and phrases by performing a nearest neighbor search\nin the embedding space. Crowdsourced evaluations demonstrate that this\nphrase-based topic model produces more coherent and meaningful topics than\nbaseline word and phrase-level topic models, further validating the utility of\nPhrase-BERT.", "journal": ""}
{"doi": "10.48550/arXiv.2206.14861", "date": "2022-06-29", "title": "Two-Stage COVID19 Classification Using BERT Features", "authors": "Weijun Tan, Qi Yao, Jingfeng Liu", "abstract": "We propose an automatic COVID1-19 diagnosis framework from lung CT-scan slice\nimages using double BERT feature extraction. In the first BERT feature\nextraction, A 3D-CNN is first used to extract CNN internal feature maps.\nInstead of using the global average pooling, a late BERT temporal pooing is\nused to aggregate the temporal information in these feature maps, followed by a\nclassification layer. This 3D-CNN-BERT classification network is first trained\non sampled fixed number of slice images from every original CT scan volume. In\nthe second stage, the 3D-CNN-BERT embedding features are extracted on all slice\nimages of every CT scan volume, and these features are averaged into a fixed\nnumber of segments. Then another BERT network is used to aggregate these\nmultiple features into a single feature followed by another classification\nlayer. The classification results of both stages are combined to generate final\noutputs. On the validation dataset, we achieve macro F1 score of 0.9164.", "journal": ""}
{"doi": "10.48550/arXiv.2207.12089", "date": "2022-07-01", "title": "A Polyphone BERT for Polyphone Disambiguation in Mandarin Chinese", "authors": "Song Zhang, Ken Zheng, Xiaoxu Zhu, Baoxiang Li", "abstract": "Grapheme-to-phoneme (G2P) conversion is an indispensable part of the Chinese\nMandarin text-to-speech (TTS) system, and the core of G2P conversion is to\nsolve the problem of polyphone disambiguation, which is to pick up the correct\npronunciation for several candidates for a Chinese polyphonic character. In\nthis paper, we propose a Chinese polyphone BERT model to predict the\npronunciations of Chinese polyphonic characters. Firstly, we create 741 new\nChinese monophonic characters from 354 source Chinese polyphonic characters by\npronunciation. Then we get a Chinese polyphone BERT by extending a pre-trained\nChinese BERT with 741 new Chinese monophonic characters and adding a\ncorresponding embedding layer for new tokens, which is initialized by the\nembeddings of source Chinese polyphonic characters. In this way, we can turn\nthe polyphone disambiguation task into a pre-training task of the Chinese\npolyphone BERT. Experimental results demonstrate the effectiveness of the\nproposed model, and the polyphone BERT model obtain 2% (from 92.1% to 94.1%)\nimprovement of average accuracy compared with the BERT-based classifier model,\nwhich is the prior state-of-the-art in polyphone disambiguation.", "journal": ""}
{"doi": "10.48550/arXiv.2209.05741", "date": "2022-09-13", "title": "SkIn: Skimming-Intensive Long-Text Classification Using BERT for Medical Corpus", "authors": "Yufeng Zhao, Haiying Che", "abstract": "BERT is a widely used pre-trained model in natural language processing.\nHowever, since BERT is quadratic to the text length, the BERT model is\ndifficult to be used directly on the long-text corpus. In some fields, the\ncollected text data may be quite long, such as in the health care field.\nTherefore, to apply the pre-trained language knowledge of BERT to long text, in\nthis paper, imitating the skimming-intensive reading method used by humans when\nreading a long paragraph, the Skimming-Intensive Model (SkIn) is proposed. It\ncan dynamically select the critical information in the text so that the\nsentence input into the BERT-Base model is significantly shortened, which can\neffectively save the cost of the classification algorithm. Experiments show\nthat the SkIn method has achieved superior accuracy than the baselines on\nlong-text classification datasets in the medical field, while its time and\nspace requirements increase linearly with the text length, alleviating the time\nand space overflow problem of basic BERT on long-text data.", "journal": ""}
{"doi": "10.48550/arXiv.2209.06178", "date": "2022-09-13", "title": "Automated classification for open-ended questions with BERT", "authors": "Hyukjun Gweon, Matthias Schonlau", "abstract": "Manual coding of text data from open-ended questions into different\ncategories is time consuming and expensive. Automated coding uses\nstatistical/machine learning to train on a small subset of manually coded text\nanswers. Recently, pre-training a general language model on vast amounts of\nunrelated data and then adapting the model to the specific application has\nproven effective in natural language processing. Using two data sets, we\nempirically investigate whether BERT, the currently dominant pre-trained\nlanguage model, is more effective at automated coding of answers to open-ended\nquestions than other non-pre-trained statistical learning approaches. We found\nfine-tuning the pre-trained BERT parameters is essential as otherwise BERT's is\nnot competitive. Second, we found fine-tuned BERT barely beats the\nnon-pre-trained statistical learning approaches in terms of classification\naccuracy when trained on 100 manually coded observations. However, BERT's\nrelative advantage increases rapidly when more manually coded observations\n(e.g. 200-400) are available for training. We conclude that for automatically\ncoding answers to open-ended questions BERT is preferable to non-pretrained\nmodels such as support vector machines and boosting.", "journal": ""}
{"doi": "10.48550/arXiv.2210.13391", "date": "2022-10-24", "title": "Explaining Translationese: why are Neural Classifiers Better and what do they Learn?", "authors": "Kwabena Amponsah-Kaakyire, Daria Pylypenko, Josef van Genabith, Cristina Espa\u00f1a-Bonet", "abstract": "Recent work has shown that neural feature- and representation-learning, e.g.\nBERT, achieves superior performance over traditional manual feature engineering\nbased approaches, with e.g. SVMs, in translationese classification tasks.\nPrevious research did not show $(i)$ whether the difference is because of the\nfeatures, the classifiers or both, and $(ii)$ what the neural classifiers\nactually learn. To address $(i)$, we carefully design experiments that swap\nfeatures between BERT- and SVM-based classifiers. We show that an SVM fed with\nBERT representations performs at the level of the best BERT classifiers, while\nBERT learning and using handcrafted features performs at the level of an SVM\nusing handcrafted features. This shows that the performance differences are due\nto the features. To address $(ii)$ we use integrated gradients and find that\n$(a)$ there is indication that information captured by hand-crafted features is\nonly a subset of what BERT learns, and $(b)$ part of BERT's top performance\nresults are due to BERT learning topic differences and spurious correlations\nwith translationese.", "journal": ""}
{"doi": "10.48550/arXiv.2210.15976", "date": "2022-10-28", "title": "BEBERT: Efficient and Robust Binary Ensemble BERT", "authors": "Jiayi Tian, Chao Fang, Haonan Wang, Zhongfeng Wang", "abstract": "Pre-trained BERT models have achieved impressive accuracy on natural language\nprocessing (NLP) tasks. However, their excessive amount of parameters hinders\nthem from efficient deployment on edge devices. Binarization of the BERT models\ncan significantly alleviate this issue but comes with a severe accuracy drop\ncompared with their full-precision counterparts. In this paper, we propose an\nefficient and robust binary ensemble BERT (BEBERT) to bridge the accuracy gap.\nTo the best of our knowledge, this is the first work employing ensemble\ntechniques on binary BERTs, yielding BEBERT, which achieves superior accuracy\nwhile retaining computational efficiency. Furthermore, we remove the knowledge\ndistillation procedures during ensemble to speed up the training process\nwithout compromising accuracy. Experimental results on the GLUE benchmark show\nthat the proposed BEBERT significantly outperforms the existing binary BERT\nmodels in accuracy and robustness with a 2x speedup on training time. Moreover,\nour BEBERT has only a negligible accuracy loss of 0.3% compared to the\nfull-precision baseline while saving 15x and 13x in FLOPs and model size,\nrespectively. In addition, BEBERT also outperforms other compressed BERTs in\naccuracy by up to 6.7%.", "journal": ""}
{"doi": "10.48550/arXiv.2212.08321", "date": "2022-12-16", "title": "Investigation of Japanese PnG BERT language model in text-to-speech synthesis for pitch accent language", "authors": "Yusuke Yasuda, Tomoki Toda", "abstract": "End-to-end text-to-speech synthesis (TTS) can generate highly natural\nsynthetic speech from raw text. However, rendering the correct pitch accents is\nstill a challenging problem for end-to-end TTS. To tackle the challenge of\nrendering correct pitch accent in Japanese end-to-end TTS, we adopt PnG~BERT, a\nself-supervised pretrained model in the character and phoneme domain for TTS.\nWe investigate the effects of features captured by PnG~BERT on Japanese TTS by\nmodifying the fine-tuning condition to determine the conditions helpful\ninferring pitch accents. We manipulate content of PnG~BERT features from being\ntext-oriented to speech-oriented by changing the number of fine-tuned layers\nduring TTS. In addition, we teach PnG~BERT pitch accent information by\nfine-tuning with tone prediction as an additional downstream task. Our\nexperimental results show that the features of PnG~BERT captured by pretraining\ncontain information helpful inferring pitch accent, and PnG~BERT outperforms\nbaseline Tacotron on accent correctness in a listening test.", "journal": "IEEE Journal of Selected Topics in Signal Processing (Volume: 16,\n  Issue: 6, October 2022)"}
{"doi": "10.48550/arXiv.2308.03782", "date": "2023-08-02", "title": "Bio+Clinical BERT, BERT Base, and CNN Performance Comparison for Predicting Drug-Review Satisfaction", "authors": "Yue Ling", "abstract": "The objective of this study is to develop natural language processing (NLP)\nmodels that can analyze patients' drug reviews and accurately classify their\nsatisfaction levels as positive, neutral, or negative. Such models would reduce\nthe workload of healthcare professionals and provide greater insight into\npatients' quality of life, which is a critical indicator of treatment\neffectiveness. To achieve this, we implemented and evaluated several\nclassification models, including a BERT base model, Bio+Clinical BERT, and a\nsimpler CNN. Results indicate that the medical domain-specific Bio+Clinical\nBERT model significantly outperformed the general domain base BERT model,\nachieving macro f1 and recall score improvement of 11%, as shown in Table 2.\nFuture research could explore how to capitalize on the specific strengths of\neach model. Bio+Clinical BERT excels in overall performance, particularly with\nmedical jargon, while the simpler CNN demonstrates the ability to identify\ncrucial words and accurately classify sentiment in texts with conflicting\nsentiments.", "journal": ""}
{"doi": "10.48550/arXiv.2408.04849", "date": "2024-08-09", "title": "Ensemble BERT: A student social network text sentiment classification model based on ensemble learning and BERT architecture", "authors": "Kai Jiang, Honghao Yang, Yuexian Wang, Qianru Chen, Yiming Luo", "abstract": "The mental health assessment of middle school students has always been one of\nthe focuses in the field of education. This paper introduces a new ensemble\nlearning network based on BERT, employing the concept of enhancing model\nperformance by integrating multiple classifiers. We trained a range of\nBERT-based learners, which combined using the majority voting method. We\ncollect social network text data of middle school students through China's\nWeibo and apply the method to the task of classifying emotional tendencies in\nmiddle school students' social network texts. Experimental results suggest that\nthe ensemble learning network has a better performance than the base model and\nthe performance of the ensemble learning model, consisting of three\nsingle-layer BERT models, is barely the same as a three-layer BERT model but\nrequires 11.58% more training time. Therefore, in terms of balancing prediction\neffect and efficiency, the deeper BERT network should be preferred for\ntraining. However, for interpretability, network ensembles can provide\nacceptable solutions.", "journal": ""}
{"doi": "10.48550/arXiv.1909.05840", "date": "2019-09-12", "title": "Q-BERT: Hessian Based Ultra Low Precision Quantization of BERT", "authors": "Sheng Shen, Zhen Dong, Jiayu Ye, Linjian Ma, Zhewei Yao, Amir Gholami, Michael W. Mahoney, Kurt Keutzer", "abstract": "Transformer based architectures have become de-facto models used for a range\nof Natural Language Processing tasks. In particular, the BERT based models\nachieved significant accuracy gain for GLUE tasks, CoNLL-03 and SQuAD. However,\nBERT based models have a prohibitive memory footprint and latency. As a result,\ndeploying BERT based models in resource constrained environments has become a\nchallenging task. In this work, we perform an extensive analysis of fine-tuned\nBERT models using second order Hessian information, and we use our results to\npropose a novel method for quantizing BERT models to ultra low precision. In\nparticular, we propose a new group-wise quantization scheme, and we use a\nHessian based mix-precision method to compress the model further. We\nextensively test our proposed method on BERT downstream tasks of SST-2, MNLI,\nCoNLL-03, and SQuAD. We can achieve comparable performance to baseline with at\nmost $2.3\\%$ performance degradation, even with ultra-low precision\nquantization down to 2 bits, corresponding up to $13\\times$ compression of the\nmodel parameters, and up to $4\\times$ compression of the embedding table as\nwell as activations. Among all tasks, we observed the highest performance loss\nfor BERT fine-tuned on SQuAD. By probing into the Hessian based analysis as\nwell as visualization, we show that this is related to the fact that current\ntraining/fine-tuning strategy of BERT does not converge for SQuAD.", "journal": "AAAI 2020"}
{"doi": "10.48550/arXiv.2004.04037", "date": "2020-04-08", "title": "DynaBERT: Dynamic BERT with Adaptive Width and Depth", "authors": "Lu Hou, Zhiqi Huang, Lifeng Shang, Xin Jiang, Xiao Chen, Qun Liu", "abstract": "The pre-trained language models like BERT, though powerful in many natural\nlanguage processing tasks, are both computation and memory expensive. To\nalleviate this problem, one approach is to compress them for specific tasks\nbefore deployment. However, recent works on BERT compression usually compress\nthe large BERT model to a fixed smaller size. They can not fully satisfy the\nrequirements of different edge devices with various hardware performances. In\nthis paper, we propose a novel dynamic BERT model (abbreviated as DynaBERT),\nwhich can flexibly adjust the size and latency by selecting adaptive width and\ndepth. The training process of DynaBERT includes first training a\nwidth-adaptive BERT and then allowing both adaptive width and depth, by\ndistilling knowledge from the full-sized model to small sub-networks. Network\nrewiring is also used to keep the more important attention heads and neurons\nshared by more sub-networks. Comprehensive experiments under various efficiency\nconstraints demonstrate that our proposed dynamic BERT (or RoBERTa) at its\nlargest size has comparable performance as BERT-base (or RoBERTa-base), while\nat smaller widths and depths consistently outperforms existing BERT compression\nmethods. Code is available at\nhttps://github.com/huawei-noah/Pretrained-Language-Model/tree/master/DynaBERT.", "journal": ""}
{"doi": "10.48550/arXiv.2101.10196", "date": "2021-01-25", "title": "A Hybrid Approach to Measure Semantic Relatedness in Biomedical Concepts", "authors": "Katikapalli Subramanyam Kalyan, Sivanesan Sangeetha", "abstract": "Objective: This work aimed to demonstrate the effectiveness of a hybrid\napproach based on Sentence BERT model and retrofitting algorithm to compute\nrelatedness between any two biomedical concepts. Materials and Methods: We\ngenerated concept vectors by encoding concept preferred terms using ELMo, BERT,\nand Sentence BERT models. We used BioELMo and Clinical ELMo. We used Ontology\nKnowledge Free (OKF) models like PubMedBERT, BioBERT, BioClinicalBERT, and\nOntology Knowledge Injected (OKI) models like SapBERT, CoderBERT, KbBERT, and\nUmlsBERT. We trained all the BERT models using Siamese network on SNLI and STSb\ndatasets to allow the models to learn more semantic information at the phrase\nor sentence level so that they can represent multi-word concepts better.\nFinally, to inject ontology relationship knowledge into concept vectors, we\nused retrofitting algorithm and concepts from various UMLS relationships. We\nevaluated our hybrid approach on four publicly available datasets which also\nincludes the recently released EHR-RelB dataset. EHR-RelB is the largest\npublicly available relatedness dataset in which 89% of terms are multi-word\nwhich makes it more challenging. Results: Sentence BERT models mostly\noutperformed corresponding BERT models. The concept vectors generated using the\nSentence BERT model based on SapBERT and retrofitted using UMLS-related\nconcepts achieved the best results on all four datasets. Conclusions: Sentence\nBERT models are more effective compared to BERT models in computing relatedness\nscores in most of the cases. Injecting ontology knowledge into concept vectors\nfurther enhances their quality and contributes to better relatedness scores.", "journal": ""}
{"doi": "10.48550/arXiv.2106.02435", "date": "2021-06-04", "title": "You Only Compress Once: Towards Effective and Elastic BERT Compression via Exploit-Explore Stochastic Nature Gradient", "authors": "Shaokun Zhang, Xiawu Zheng, Chenyi Yang, Yuchao Li, Yan Wang, Fei Chao, Mengdi Wang, Shen Li, Jun Yang, Rongrong Ji", "abstract": "Despite superior performance on various natural language processing tasks,\npre-trained models such as BERT are challenged by deploying on\nresource-constraint devices. Most existing model compression approaches require\nre-compression or fine-tuning across diverse constraints to accommodate various\nhardware deployments. This practically limits the further application of model\ncompression. Moreover, the ineffective training and searching process of\nexisting elastic compression paradigms[4,27] prevents the direct migration to\nBERT compression. Motivated by the necessity of efficient inference across\nvarious constraints on BERT, we propose a novel approach, YOCO-BERT, to achieve\ncompress once and deploy everywhere. Specifically, we first construct a huge\nsearch space with 10^13 architectures, which covers nearly all configurations\nin BERT model. Then, we propose a novel stochastic nature gradient optimization\nmethod to guide the generation of optimal candidate architecture which could\nkeep a balanced trade-off between explorations and exploitation. When a certain\nresource constraint is given, a lightweight distribution optimization approach\nis utilized to obtain the optimal network for target deployment without\nfine-tuning. Compared with state-of-the-art algorithms, YOCO-BERT provides more\ncompact models, yet achieving 2.1%-4.5% average accuracy improvement on the\nGLUE benchmark. Besides, YOCO-BERT is also more effective, e.g.,the training\ncomplexity is O(1)for N different devices. Code is\navailablehttps://github.com/MAC-AutoML/YOCO-BERT.", "journal": ""}
{"doi": "10.48550/arXiv.2106.02581", "date": "2021-06-04", "title": "BERT-Based Sentiment Analysis: A Software Engineering Perspective", "authors": "Himanshu Batra, Narinder Singh Punn, Sanjay Kumar Sonbhadra, Sonali Agarwal", "abstract": "Sentiment analysis can provide a suitable lead for the tools used in software\nengineering along with the API recommendation systems and relevant libraries to\nbe used. In this context, the existing tools like SentiCR, SentiStrength-SE,\netc. exhibited low f1-scores that completely defeats the purpose of deployment\nof such strategies, thereby there is enough scope for performance improvement.\nRecent advancements show that transformer based pre-trained models (e.g., BERT,\nRoBERTa, ALBERT, etc.) have displayed better results in the text classification\ntask. Following this context, the present research explores different\nBERT-based models to analyze the sentences in GitHub comments, Jira comments,\nand Stack Overflow posts. The paper presents three different strategies to\nanalyse BERT based model for sentiment analysis, where in the first strategy\nthe BERT based pre-trained models are fine-tuned; in the second strategy an\nensemble model is developed from BERT variants, and in the third strategy a\ncompressed model (Distil BERT) is used. The experimental results show that the\nBERT based ensemble approach and the compressed BERT model attain improvements\nby 6-12% over prevailing tools for the F1 measure on all three datasets.", "journal": ""}
{"doi": "10.48550/arXiv.2107.06785", "date": "2021-07-14", "title": "Large-Scale News Classification using BERT Language Model: Spark NLP Approach", "authors": "Kuncahyo Setyo Nugroho, Anantha Yullian Sukmadewa, Novanto Yudistira", "abstract": "The rise of big data analytics on top of NLP increases the computational\nburden for text processing at scale. The problems faced in NLP are very high\ndimensional text, so it takes a high computation resource. The MapReduce allows\nparallelization of large computations and can improve the efficiency of text\nprocessing. This research aims to study the effect of big data processing on\nNLP tasks based on a deep learning approach. We classify a big text of news\ntopics with fine-tuning BERT used pre-trained models. Five pre-trained models\nwith a different number of parameters were used in this study. To measure the\nefficiency of this method, we compared the performance of the BERT with the\npipelines from Spark NLP. The result shows that BERT without Spark NLP gives\nhigher accuracy compared to BERT with Spark NLP. The accuracy average and\ntraining time of all models using BERT is 0.9187 and 35 minutes while using\nBERT with Spark NLP pipeline is 0.8444 and 9 minutes. The bigger model will\ntake more computation resources and need a longer time to complete the tasks.\nHowever, the accuracy of BERT with Spark NLP only decreased by an average of\n5.7%, while the training time was reduced significantly by 62.9% compared to\nBERT without Spark NLP.", "journal": "SIET '21: 6th International Conference on Sustainable Information\n  Engineering and Technology 2021"}
{"doi": "10.48550/arXiv.2109.13348", "date": "2021-09-14", "title": "Evaluating Biomedical BERT Models for Vocabulary Alignment at Scale in the UMLS Metathesaurus", "authors": "Goonmeet Bajaj, Vinh Nguyen, Thilini Wijesiriwardene, Hong Yung Yip, Vishesh Javangula, Srinivasan Parthasarathy, Amit Sheth, Olivier Bodenreider", "abstract": "The current UMLS (Unified Medical Language System) Metathesaurus construction\nprocess for integrating over 200 biomedical source vocabularies is expensive\nand error-prone as it relies on the lexical algorithms and human editors for\ndeciding if the two biomedical terms are synonymous. Recent advances in Natural\nLanguage Processing such as Transformer models like BERT and its biomedical\nvariants with contextualized word embeddings have achieved state-of-the-art\n(SOTA) performance on downstream tasks. We aim to validate if these approaches\nusing the BERT models can actually outperform the existing approaches for\npredicting synonymy in the UMLS Metathesaurus. In the existing Siamese Networks\nwith LSTM and BioWordVec embeddings, we replace the BioWordVec embeddings with\nthe biomedical BERT embeddings extracted from each BERT model using different\nways of extraction. In the Transformer architecture, we evaluate the use of the\ndifferent biomedical BERT models that have been pre-trained using different\ndatasets and tasks. Given the SOTA performance of these BERT models for other\ndownstream tasks, our experiments yield surprisingly interesting results: (1)\nin both model architectures, the approaches employing these biomedical\nBERT-based models do not outperform the existing approaches using Siamese\nNetwork with BioWordVec embeddings for the UMLS synonymy prediction task, (2)\nthe original BioBERT large model that has not been pre-trained with the UMLS\noutperforms the SapBERT models that have been pre-trained with the UMLS, and\n(3) using the Siamese Networks yields better performance for synonymy\nprediction when compared to using the biomedical BERT models.", "journal": ""}
{"doi": "10.48550/arXiv.2205.03695", "date": "2022-05-07", "title": "AKI-BERT: a Pre-trained Clinical Language Model for Early Prediction of Acute Kidney Injury", "authors": "Chengsheng Mao, Liang Yao, Yuan Luo", "abstract": "Acute kidney injury (AKI) is a common clinical syndrome characterized by a\nsudden episode of kidney failure or kidney damage within a few hours or a few\ndays. Accurate early prediction of AKI for patients in ICU who are more likely\nthan others to have AKI can enable timely interventions, and reduce the\ncomplications of AKI. Much of the clinical information relevant to AKI is\ncaptured in clinical notes that are largely unstructured text and requires\nadvanced natural language processing (NLP) for useful information extraction.\nOn the other hand, pre-trained contextual language models such as Bidirectional\nEncoder Representations from Transformers (BERT) have improved performances for\nmany NLP tasks in general domain recently. However, few have explored BERT on\ndisease-specific medical domain tasks such as AKI early prediction. In this\npaper, we try to apply BERT to specific diseases and present an AKI\ndomain-specific pre-trained language model based on BERT (AKI-BERT) that could\nbe used to mine the clinical notes for early prediction of AKI. AKI-BERT is a\nBERT model pre-trained on the clinical notes of patients having risks for AKI.\nOur experiments on Medical Information Mart for Intensive Care III (MIMIC-III)\ndataset demonstrate that AKI-BERT can yield performance improvements for early\nAKI prediction, thus expanding the utility of the BERT model from general\nclinical domain to disease-specific domain.", "journal": ""}
{"doi": "10.48550/arXiv.2208.08124", "date": "2022-08-17", "title": "Boosting Distributed Training Performance of the Unpadded BERT Model", "authors": "Jinle Zeng, Min Li, Zhihua Wu, Jiaqi Liu, Yuang Liu, Dianhai Yu, Yanjun Ma", "abstract": "Pre-training models are an important tool in Natural Language Processing\n(NLP), while the BERT model is a classic pre-training model whose structure has\nbeen widely adopted by followers. It was even chosen as the reference model for\nthe MLPerf training benchmark. The distributed training performance\noptimization of BERT models plays an important role in accelerating the\nsolutions of most NLP tasks. BERT model often uses padding tensors as its\ninputs, leading to excessive redundant computations. Thus, removing these\nredundant computations is essential to improve the distributed training\nperformance.\n  This paper designs a new approach to train BERT models with variable-length\ninputs efficiently. Firstly, we propose a general structure for the\nvariable-length BERT models, and accelerate the encoder layer via our grouped\nmulti-stream FMHA (Fused Multi-Head Attention) method. Secondly, through data\nexchange, we address the unbalanced workload problem caused by the\nvariable-length inputs, which overlaps highly with the training process.\nFinally, we optimize the overall performance of the BERT model, such as kernel\nfusion, and operator optimization. Our experimental results show that our\nhighly optimized BERT model achieves state-of-the-art throughput and ranks\nfirst in MLPerf Training v2.0 within the same GPU configuration. The\noptimizations in this paper can be applied to more BERT-like models in our\nfuture works.", "journal": ""}
{"doi": "10.48550/arXiv.2211.00792", "date": "2022-11-02", "title": "BECTRA: Transducer-based End-to-End ASR with BERT-Enhanced Encoder", "authors": "Yosuke Higuchi, Tetsuji Ogawa, Tetsunori Kobayashi, Shinji Watanabe", "abstract": "We present BERT-CTC-Transducer (BECTRA), a novel end-to-end automatic speech\nrecognition (E2E-ASR) model formulated by the transducer with a BERT-enhanced\nencoder. Integrating a large-scale pre-trained language model (LM) into E2E-ASR\nhas been actively studied, aiming to utilize versatile linguistic knowledge for\ngenerating accurate text. One crucial factor that makes this integration\nchallenging lies in the vocabulary mismatch; the vocabulary constructed for a\npre-trained LM is generally too large for E2E-ASR training and is likely to\nhave a mismatch against a target ASR domain. To overcome such an issue, we\npropose BECTRA, an extended version of our previous BERT-CTC, that realizes\nBERT-based E2E-ASR using a vocabulary of interest. BECTRA is a transducer-based\nmodel, which adopts BERT-CTC for its encoder and trains an ASR-specific decoder\nusing a vocabulary suitable for a target task. With the combination of the\ntransducer and BERT-CTC, we also propose a novel inference algorithm for taking\nadvantage of both autoregressive and non-autoregressive decoding. Experimental\nresults on several ASR tasks, varying in amounts of data, speaking styles, and\nlanguages, demonstrate that BECTRA outperforms BERT-CTC by effectively dealing\nwith the vocabulary mismatch while exploiting BERT knowledge.", "journal": ""}
{"doi": "10.48550/arXiv.2211.11187", "date": "2022-11-21", "title": "L3Cube-MahaSBERT and HindSBERT: Sentence BERT Models and Benchmarking BERT Sentence Representations for Hindi and Marathi", "authors": "Ananya Joshi, Aditi Kajale, Janhavi Gadre, Samruddhi Deode, Raviraj Joshi", "abstract": "Sentence representation from vanilla BERT models does not work well on\nsentence similarity tasks. Sentence-BERT models specifically trained on STS or\nNLI datasets are shown to provide state-of-the-art performance. However,\nbuilding these models for low-resource languages is not straightforward due to\nthe lack of these specialized datasets. This work focuses on two low-resource\nIndian languages, Hindi and Marathi. We train sentence-BERT models for these\nlanguages using synthetic NLI and STS datasets prepared using machine\ntranslation. We show that the strategy of NLI pre-training followed by STSb\nfine-tuning is effective in generating high-performance sentence-similarity\nmodels for Hindi and Marathi. The vanilla BERT models trained using this simple\nstrategy outperform the multilingual LaBSE trained using a complex training\nstrategy. These models are evaluated on downstream text classification and\nsimilarity tasks. We evaluate these models on real text classification datasets\nto show embeddings obtained from synthetic data training are generalizable to\nreal datasets as well and thus represent an effective training strategy for\nlow-resource languages. We also provide a comparative analysis of sentence\nembeddings from fast text models, multilingual BERT models (mBERT, IndicBERT,\nxlm-RoBERTa, MuRIL), multilingual sentence embedding models (LASER, LaBSE), and\nmonolingual BERT models based on L3Cube-MahaBERT and HindBERT. We release\nL3Cube-MahaSBERT and HindSBERT, the state-of-the-art sentence-BERT models for\nMarathi and Hindi respectively. Our work also serves as a guide to building\nlow-resource sentence embedding models.", "journal": ""}
{"doi": "10.48550/arXiv.2312.04891", "date": "2023-12-08", "title": "Cross-BERT for Point Cloud Pretraining", "authors": "Xin Li, Peng Li, Zeyong Wei, Zhe Zhu, Mingqiang Wei, Junhui Hou, Liangliang Nan, Jing Qin, Haoran Xie, Fu Lee Wang", "abstract": "Introducing BERT into cross-modal settings raises difficulties in its\noptimization for handling multiple modalities. Both the BERT architecture and\ntraining objective need to be adapted to incorporate and model information from\ndifferent modalities. In this paper, we address these challenges by exploring\nthe implicit semantic and geometric correlations between 2D and 3D data of the\nsame objects/scenes. We propose a new cross-modal BERT-style self-supervised\nlearning paradigm, called Cross-BERT. To facilitate pretraining for irregular\nand sparse point clouds, we design two self-supervised tasks to boost\ncross-modal interaction. The first task, referred to as Point-Image Alignment,\naims to align features between unimodal and cross-modal representations to\ncapture the correspondences between the 2D and 3D modalities. The second task,\ntermed Masked Cross-modal Modeling, further improves mask modeling of BERT by\nincorporating high-dimensional semantic information obtained by cross-modal\ninteraction. By performing cross-modal interaction, Cross-BERT can smoothly\nreconstruct the masked tokens during pretraining, leading to notable\nperformance enhancements for downstream tasks. Through empirical evaluation, we\ndemonstrate that Cross-BERT outperforms existing state-of-the-art methods in 3D\ndownstream applications. Our work highlights the effectiveness of leveraging\ncross-modal 2D knowledge to strengthen 3D point cloud representation and the\ntransferable capability of BERT across modalities.", "journal": ""}
{"doi": "10.48550/arXiv.1903.10318", "date": "2019-03-25", "title": "Fine-tune BERT for Extractive Summarization", "authors": "Yang Liu", "abstract": "BERT, a pre-trained Transformer model, has achieved ground-breaking\nperformance on multiple NLP tasks. In this paper, we describe BERTSUM, a simple\nvariant of BERT, for extractive summarization. Our system is the state of the\nart on the CNN/Dailymail dataset, outperforming the previous best-performed\nsystem by 1.65 on ROUGE-L. The codes to reproduce our results are available at\nhttps://github.com/nlpyang/BertSum", "journal": ""}
{"doi": "10.48550/arXiv.1901.05287", "date": "2019-01-16", "title": "Assessing BERT's Syntactic Abilities", "authors": "Yoav Goldberg", "abstract": "I assess the extent to which the recently introduced BERT model captures\nEnglish syntactic phenomena, using (1) naturally-occurring subject-verb\nagreement stimuli; (2) \"coloreless green ideas\" subject-verb agreement stimuli,\nin which content words in natural sentences are randomly replaced with words\nsharing the same part-of-speech and inflection; and (3) manually crafted\nstimuli for subject-verb agreement and reflexive anaphora phenomena. The BERT\nmodel performs remarkably well on all cases.", "journal": ""}
{"doi": "10.48550/arXiv.2105.06514", "date": "2021-05-13", "title": "Distilling BERT for low complexity network training", "authors": "Bansidhar Mangalwedhekar", "abstract": "This paper studies the efficiency of transferring BERT learnings to low\ncomplexity models like BiLSTM, BiLSTM with attention and shallow CNNs using\nsentiment analysis on SST-2 dataset. It also compares the complexity of\ninference of the BERT model with these lower complexity models and underlines\nthe importance of these techniques in enabling high performance NLP models on\nedge devices like mobiles, tablets and MCU development boards like Raspberry Pi\netc. and enabling exciting new applications.", "journal": ""}
{"doi": "10.48550/arXiv.1905.05583", "date": "2019-05-14", "title": "How to Fine-Tune BERT for Text Classification?", "authors": "Chi Sun, Xipeng Qiu, Yige Xu, Xuanjing Huang", "abstract": "Language model pre-training has proven to be useful in learning universal\nlanguage representations. As a state-of-the-art language model pre-training\nmodel, BERT (Bidirectional Encoder Representations from Transformers) has\nachieved amazing results in many language understanding tasks. In this paper,\nwe conduct exhaustive experiments to investigate different fine-tuning methods\nof BERT on text classification task and provide a general solution for BERT\nfine-tuning. Finally, the proposed solution obtains new state-of-the-art\nresults on eight widely-studied text classification datasets.", "journal": ""}
{"doi": "10.48550/arXiv.1908.06780", "date": "2019-08-19", "title": "A Study of BERT for Non-Factoid Question-Answering under Passage Length Constraints", "authors": "Yosi Mass, Haggai Roitman, Shai Erera, Or Rivlin, Bar Weiner, David Konopnicki", "abstract": "We study the use of BERT for non-factoid question-answering, focusing on the\npassage re-ranking task under varying passage lengths. To this end, we explore\nthe fine-tuning of BERT in different learning-to-rank setups, comprising both\npoint-wise and pair-wise methods, resulting in substantial improvements over\nthe state-of-the-art. We then analyze the effectiveness of BERT for different\npassage lengths and suggest how to cope with large passages.", "journal": ""}
{"doi": "10.48550/arXiv.1908.09091", "date": "2019-08-24", "title": "BERT for Coreference Resolution: Baselines and Analysis", "authors": "Mandar Joshi, Omer Levy, Daniel S. Weld, Luke Zettlemoyer", "abstract": "We apply BERT to coreference resolution, achieving strong improvements on the\nOntoNotes (+3.9 F1) and GAP (+11.5 F1) benchmarks. A qualitative analysis of\nmodel predictions indicates that, compared to ELMo and BERT-base, BERT-large is\nparticularly better at distinguishing between related but distinct entities\n(e.g., President and CEO). However, there is still room for improvement in\nmodeling document-level context, conversations, and mention paraphrasing. Our\ncode and models are publicly available.", "journal": ""}
{"doi": "10.48550/arXiv.1908.09892", "date": "2019-08-26", "title": "Does BERT agree? Evaluating knowledge of structure dependence through agreement relations", "authors": "Geoff Bacon, Terry Regier", "abstract": "Learning representations that accurately model semantics is an important goal\nof natural language processing research. Many semantic phenomena depend on\nsyntactic structure. Recent work examines the extent to which state-of-the-art\nmodels for pre-training representations, such as BERT, capture such\nstructure-dependent phenomena, but is largely restricted to one phenomenon in\nEnglish: number agreement between subjects and verbs. We evaluate BERT's\nsensitivity to four types of structure-dependent agreement relations in a new\nsemi-automatically curated dataset across 26 languages. We show that both the\nsingle-language and multilingual BERT models capture syntax-sensitive agreement\npatterns well in general, but we also highlight the specific linguistic\ncontexts in which their performance degrades.", "journal": ""}
{"doi": "10.48550/arXiv.1904.05255", "date": "2019-04-10", "title": "Simple BERT Models for Relation Extraction and Semantic Role Labeling", "authors": "Peng Shi, Jimmy Lin", "abstract": "We present simple BERT-based models for relation extraction and semantic role\nlabeling. In recent years, state-of-the-art performance has been achieved using\nneural models by incorporating lexical and syntactic features such as\npart-of-speech tags and dependency trees. In this paper, extensive experiments\non datasets for these two tasks show that without using any external features,\na simple BERT-based model can achieve state-of-the-art performance. To our\nknowledge, we are the first to successfully apply BERT in this manner. Our\nmodels provide strong baselines for future research.", "journal": ""}
{"doi": "10.48550/arXiv.1910.00883", "date": "2019-10-02", "title": "Exploiting BERT for End-to-End Aspect-based Sentiment Analysis", "authors": "Xin Li, Lidong Bing, Wenxuan Zhang, Wai Lam", "abstract": "In this paper, we investigate the modeling power of contextualized embeddings\nfrom pre-trained language models, e.g. BERT, on the E2E-ABSA task.\nSpecifically, we build a series of simple yet insightful neural baselines to\ndeal with E2E-ABSA. The experimental results show that even with a simple\nlinear classification layer, our BERT-based architecture can outperform\nstate-of-the-art works. Besides, we also standardize the comparative study by\nconsistently utilizing a hold-out validation dataset for model selection, which\nis largely ignored by previous works. Therefore, our work can serve as a\nBERT-based benchmark for E2E-ABSA.", "journal": ""}
{"doi": "10.48550/arXiv.1910.07713", "date": "2019-10-17", "title": "BIG MOOD: Relating Transformers to Explicit Commonsense Knowledge", "authors": "Jeff Da", "abstract": "We introduce a simple yet effective method of integrating contextual\nembeddings with commonsense graph embeddings, dubbed BERT Infused Graphs:\nMatching Over Other embeDdings. First, we introduce a preprocessing method to\nimprove the speed of querying knowledge bases. Then, we develop a method of\ncreating knowledge embeddings from each knowledge base. We introduce a method\nof aligning tokens between two misaligned tokenization methods. Finally, we\ncontribute a method of contextualizing BERT after combining with knowledge base\nembeddings. We also show BERTs tendency to correct lower accuracy question\ntypes. Our model achieves a higher accuracy than BERT, and we score fifth on\nthe official leaderboard of the shared task and score the highest without any\nadditional language model pretraining.", "journal": ""}
{"doi": "10.48550/arXiv.2002.12327", "date": "2020-02-27", "title": "A Primer in BERTology: What we know about how BERT works", "authors": "Anna Rogers, Olga Kovaleva, Anna Rumshisky", "abstract": "Transformer-based models have pushed state of the art in many areas of NLP,\nbut our understanding of what is behind their success is still limited. This\npaper is the first survey of over 150 studies of the popular BERT model. We\nreview the current state of knowledge about how BERT works, what kind of\ninformation it learns and how it is represented, common modifications to its\ntraining objectives and architecture, the overparameterization issue and\napproaches to compression. We then outline directions for future research.", "journal": ""}
{"doi": "10.48550/arXiv.2004.14135", "date": "2020-03-29", "title": "BERT Fine-tuning For Arabic Text Summarization", "authors": "Khalid N. Elmadani, Mukhtar Elgezouli, Anas Showk", "abstract": "Fine-tuning a pretrained BERT model is the state of the art method for\nextractive/abstractive text summarization, in this paper we showcase how this\nfine-tuning method can be applied to the Arabic language to both construct the\nfirst documented model for abstractive Arabic text summarization and show its\nperformance in Arabic extractive summarization. Our model works with\nmultilingual BERT (as Arabic language does not have a pretrained BERT of its\nown). We show its performance in English corpus first before applying it to\nArabic corpora in both extractive and abstractive tasks.", "journal": ""}
{"doi": "10.48550/arXiv.2005.00672", "date": "2020-05-02", "title": "DagoBERT: Generating Derivational Morphology with a Pretrained Language Model", "authors": "Valentin Hofmann, Janet B. Pierrehumbert, Hinrich Sch\u00fctze", "abstract": "Can pretrained language models (PLMs) generate derivationally complex words?\nWe present the first study investigating this question, taking BERT as the\nexample PLM. We examine BERT's derivational capabilities in different settings,\nranging from using the unmodified pretrained model to full finetuning. Our best\nmodel, DagoBERT (Derivationally and generatively optimized BERT), clearly\noutperforms the previous state of the art in derivation generation (DG).\nFurthermore, our experiments show that the input segmentation crucially impacts\nBERT's derivational knowledge, suggesting that the performance of PLMs could be\nfurther improved if a morphologically informed vocabulary of units were used.", "journal": ""}
{"doi": "10.48550/arXiv.2005.11313", "date": "2020-05-22", "title": "Comparative Study of Machine Learning Models and BERT on SQuAD", "authors": "Devshree Patel, Param Raval, Ratnam Parikh, Yesha Shastri", "abstract": "This study aims to provide a comparative analysis of performance of certain\nmodels popular in machine learning and the BERT model on the Stanford Question\nAnswering Dataset (SQuAD). The analysis shows that the BERT model, which was\nonce state-of-the-art on SQuAD, gives higher accuracy in comparison to other\nmodels. However, BERT requires a greater execution time even when only 100\nsamples are used. This shows that with increasing accuracy more amount of time\nis invested in training the data. Whereas in case of preliminary machine\nlearning models, execution time for full data is lower but accuracy is\ncompromised.", "journal": ""}
{"doi": "10.48550/arXiv.2007.14310", "date": "2020-07-28", "title": "Improving Results on Russian Sentiment Datasets", "authors": "Anton Golubev, Natalia Loukachevitch", "abstract": "In this study, we test standard neural network architectures (CNN, LSTM,\nBiLSTM) and recently appeared BERT architectures on previous Russian sentiment\nevaluation datasets. We compare two variants of Russian BERT and show that for\nall sentiment tasks in this study the conversational variant of Russian BERT\nperforms better. The best results were achieved by BERT-NLI model, which treats\nsentiment classification tasks as a natural language inference task. On one of\nthe datasets, this model practically achieves the human level.", "journal": ""}
{"doi": "10.48550/arXiv.2011.02788", "date": "2020-11-05", "title": "NUAA-QMUL at SemEval-2020 Task 8: Utilizing BERT and DenseNet for Internet Meme Emotion Analysis", "authors": "Xiaoyu Guo, Jing Ma, Arkaitz Zubiaga", "abstract": "This paper describes our contribution to SemEval 2020 Task 8: Memotion\nAnalysis. Our system learns multi-modal embeddings from text and images in\norder to classify Internet memes by sentiment. Our model learns text embeddings\nusing BERT and extracts features from images with DenseNet, subsequently\ncombining both features through concatenation. We also compare our results with\nthose produced by DenseNet, ResNet, BERT, and BERT-ResNet. Our results show\nthat image classification models have the potential to help classifying memes,\nwith DenseNet outperforming ResNet. Adding text features is however not always\nhelpful for Memotion Analysis.", "journal": ""}
{"doi": "10.48550/arXiv.2101.00396", "date": "2021-01-02", "title": "Lex-BERT: Enhancing BERT based NER with lexicons", "authors": "Wei Zhu, Daniel Cheung", "abstract": "In this work, we represent Lex-BERT, which incorporates the lexicon\ninformation into Chinese BERT for named entity recognition (NER) tasks in a\nnatural manner. Instead of using word embeddings and a newly designed\ntransformer layer as in FLAT, we identify the boundary of words in the\nsentences using special tokens, and the modified sentence will be encoded\ndirectly by BERT. Our model does not introduce any new parameters and are more\nefficient than FLAT. In addition, we do not require any word embeddings\naccompanying the lexicon collection. Experiments on Ontonotes and ZhCrossNER\nshow that our model outperforms FLAT and other baselines.", "journal": ""}
{"doi": "10.48550/arXiv.2101.09007", "date": "2021-01-22", "title": "HASOCOne@FIRE-HASOC2020: Using BERT and Multilingual BERT models for Hate Speech Detection", "authors": "Suman Dowlagar, Radhika Mamidi", "abstract": "Hateful and Toxic content has become a significant concern in today's world\ndue to an exponential rise in social media. The increase in hate speech and\nharmful content motivated researchers to dedicate substantial efforts to the\nchallenging direction of hateful content identification. In this task, we\npropose an approach to automatically classify hate speech and offensive\ncontent. We have used the datasets obtained from FIRE 2019 and 2020 shared\ntasks. We perform experiments by taking advantage of transfer learning models.\nWe observed that the pre-trained BERT model and the multilingual-BERT model\ngave the best results. The code is made publically available at\nhttps://github.com/suman101112/hasoc-fire-2020.", "journal": ""}
{"doi": "10.48550/arXiv.2103.07259", "date": "2021-03-12", "title": "Explaining and Improving BERT Performance on Lexical Semantic Change Detection", "authors": "Severin Laicher, Sinan Kurtyigit, Dominik Schlechtweg, Jonas Kuhn, Sabine Schulte im Walde", "abstract": "Type- and token-based embedding architectures are still competing in lexical\nsemantic change detection. The recent success of type-based models in\nSemEval-2020 Task 1 has raised the question why the success of token-based\nmodels on a variety of other NLP tasks does not translate to our field. We\ninvestigate the influence of a range of variables on clusterings of BERT\nvectors and show that its low performance is largely due to orthographic\ninformation on the target word, which is encoded even in the higher layers of\nBERT representations. By reducing the influence of orthography we considerably\nimprove BERT's performance.", "journal": ""}
{"doi": "10.48550/arXiv.2107.02893", "date": "2021-06-26", "title": "Answering Chinese Elementary School Social Study Multiple Choice Questions", "authors": "Daniel Lee, Chao-Chun Liang, Keh-Yih Su", "abstract": "We present a novel approach to answer the Chinese elementary school Social\nStudy Multiple Choice questions. Although BERT has demonstrated excellent\nperformance on Reading Comprehension tasks, it is found not good at handling\nsome specific types of questions, such as Negation, All-of-the-above, and\nNone-of-the-above. We thus propose a novel framework to cascade BERT with a\nPre-Processor and an Answer-Selector modules to tackle the above challenges.\nExperimental results show the proposed approach effectively improves the\nperformance of BERT, and thus demonstrate the feasibility of supplementing BERT\nwith additional modules.", "journal": ""}
{"doi": "10.48550/arXiv.2205.12335", "date": "2022-05-24", "title": "K-12BERT: BERT for K-12 education", "authors": "Vasu Goel, Dhruv Sahnan, Venktesh V, Gaurav Sharma, Deep Dwivedi, Mukesh Mohania", "abstract": "Online education platforms are powered by various NLP pipelines, which\nutilize models like BERT to aid in content curation. Since the inception of the\npre-trained language models like BERT, there have also been many efforts toward\nadapting these pre-trained models to specific domains. However, there has not\nbeen a model specifically adapted for the education domain (particularly K-12)\nacross subjects to the best of our knowledge. In this work, we propose to train\na language model on a corpus of data curated by us across multiple subjects\nfrom various sources for K-12 education. We also evaluate our model, K12-BERT,\non downstream tasks like hierarchical taxonomy tagging.", "journal": ""}
{"doi": "10.48550/arXiv.2304.01894", "date": "2023-04-04", "title": "San-BERT: Extractive Summarization for Sanskrit Documents using BERT and it's variants", "authors": "Kartik Bhatnagar, Sampath Lonka, Jammi Kunal, Mahabala Rao M G", "abstract": "In this work, we develop language models for the Sanskrit language, namely\nBidirectional Encoder Representations from Transformers (BERT) and its\nvariants: A Lite BERT (ALBERT), and Robustly Optimized BERT (RoBERTa) using\nDevanagari Sanskrit text corpus. Then we extracted the features for the given\ntext from these models. We applied the dimensional reduction and clustering\ntechniques on the features to generate an extractive summary for a given\nSanskrit document. Along with the extractive text summarization techniques, we\nhave also created and released a Sanskrit Devanagari text corpus publicly.", "journal": ""}
{"doi": "10.48550/arXiv.2305.04673", "date": "2023-05-08", "title": "PreCog: Exploring the Relation between Memorization and Performance in Pre-trained Language Models", "authors": "Leonardo Ranaldi, Elena Sofia Ruzzetti, Fabio Massimo Zanzotto", "abstract": "Pre-trained Language Models such as BERT are impressive machines with the\nability to memorize, possibly generalized learning examples. We present here a\nsmall, focused contribution to the analysis of the interplay between\nmemorization and performance of BERT in downstream tasks. We propose PreCog, a\nmeasure for evaluating memorization from pre-training, and we analyze its\ncorrelation with the BERT's performance. Our experiments show that highly\nmemorized examples are better classified, suggesting memorization is an\nessential key to success for BERT.", "journal": "2023.ranlp-1.103"}
{"doi": "10.48550/arXiv.2404.08836", "date": "2024-04-12", "title": "BERT-LSH: Reducing Absolute Compute For Attention", "authors": "Zezheng Li, Kingston Yip", "abstract": "This study introduces a novel BERT-LSH model that incorporates Locality\nSensitive Hashing (LSH) to approximate the attention mechanism in the BERT\narchitecture. We examine the computational efficiency and performance of this\nmodel compared to a standard baseline BERT model. Our findings reveal that\nBERT-LSH significantly reduces computational demand for the self-attention\nlayer while unexpectedly outperforming the baseline model in pretraining and\nfine-tuning tasks. These results suggest that the LSH-based attention mechanism\nnot only offers computational advantages but also may enhance the model's\nability to generalize from its training data. For more information, visit our\nGitHub repository: https://github.com/leo4life2/algoml-final", "journal": ""}
{"doi": "10.48550/arXiv.0011242", "date": "2000-11-27", "title": "Dielectric branes and spontaneous symmetry breaking", "authors": "Cesar Gomez, Bert Janssen, Pedro J. Silva", "abstract": "A stable non-commutative solution with symmetry breaking is presented for a\nsystem of Dp-branes in the presence of a RR (p+5)-form.", "journal": ""}
{"doi": "10.48550/arXiv.0603118", "date": "2006-03-15", "title": "Title Positivity and Integrability (Mathematical Physics at the FU-Berlin)", "authors": "Bert Schroer", "abstract": "Based on past contributions by Robert Schrader and Michael Karowski I review\nthe problem of existence of interacting quantum field theory and present recent\nideas and results on rigorous constructions.", "journal": ""}
{"doi": "10.48550/arXiv.1302.3863", "date": "2013-02-15", "title": "Upper bounds for the complexity of torus knot complements", "authors": "Evgeny Fominykh, Bert Wiest", "abstract": "We establish upper bounds for the complexity of Seifert fibered manifolds\nwith nonempty boundary. In particular, we obtain potentially sharp bounds on\nthe complexity of torus knot complements.", "journal": ""}
{"doi": "10.48550/arXiv.2202.02366", "date": "2022-02-04", "title": "Conjectures on Symmetric Queues in Heavy Traffic", "authors": "Bert Zwart", "abstract": "This note describes several open questions concerning scaling limits of\nqueue-length processes of symmetric queues in heavy traffic, distinguishing\nbetween service-time distributions with finite and infinite variance.", "journal": ""}
{"doi": "10.48550/arXiv.2001.04246", "date": "2020-01-13", "title": "AdaBERT: Task-Adaptive BERT Compression with Differentiable Neural Architecture Search", "authors": "Daoyuan Chen, Yaliang Li, Minghui Qiu, Zhen Wang, Bofang Li, Bolin Ding, Hongbo Deng, Jun Huang, Wei Lin, Jingren Zhou", "abstract": "Large pre-trained language models such as BERT have shown their effectiveness\nin various natural language processing tasks. However, the huge parameter size\nmakes them difficult to be deployed in real-time applications that require\nquick inference with limited resources. Existing methods compress BERT into\nsmall models while such compression is task-independent, i.e., the same\ncompressed BERT for all different downstream tasks. Motivated by the necessity\nand benefits of task-oriented BERT compression, we propose a novel compression\nmethod, AdaBERT, that leverages differentiable Neural Architecture Search to\nautomatically compress BERT into task-adaptive small models for specific tasks.\nWe incorporate a task-oriented knowledge distillation loss to provide search\nhints and an efficiency-aware loss as search constraints, which enables a good\ntrade-off between efficiency and effectiveness for task-adaptive BERT\ncompression. We evaluate AdaBERT on several NLP tasks, and the results\ndemonstrate that those task-adaptive compressed models are 12.7x to 29.3x\nfaster than BERT in inference time and 11.5x to 17.0x smaller in terms of\nparameter size, while comparable performance is maintained.", "journal": ""}
{"doi": "10.48550/arXiv.2008.08802", "date": "2020-08-20", "title": "Garside groups and geometry", "authors": "Bert Wiest", "abstract": "This article in memory of Patrick Dehornoy (1952-2019) is an invitation to\nGarside theory for mainstream geometric group theorists interested in mapping\nclass groups, curve complexes, and the geometry of Artin-Tits groups.", "journal": ""}
{"doi": "10.48550/arXiv.2101.09244", "date": "2021-01-22", "title": "Extracting Lifestyle Factors for Alzheimer's Disease from Clinical Notes Using Deep Learning with Weak Supervision", "authors": "Zitao Shen, Yoonkwon Yi, Anusha Bompelli, Fang Yu, Yanshan Wang, Rui Zhang", "abstract": "Since no effective therapies exist for Alzheimer's disease (AD), prevention\nhas become more critical through lifestyle factor changes and interventions.\nAnalyzing electronic health records (EHR) of patients with AD can help us\nbetter understand lifestyle's effect on AD. However, lifestyle information is\ntypically stored in clinical narratives. Thus, the objective of the study was\nto demonstrate the feasibility of natural language processing (NLP) models to\nclassify lifestyle factors (e.g., physical activity and excessive diet) from\nclinical texts. We automatically generated labels for the training data by\nusing a rule-based NLP algorithm. We conducted weak supervision for pre-trained\nBidirectional Encoder Representations from Transformers (BERT) models on the\nweakly labeled training corpus. These models include the BERT base model,\nPubMedBERT(abstracts + full text), PubMedBERT(only abstracts), Unified Medical\nLanguage System (UMLS) BERT, Bio BERT, and Bio-clinical BERT. We performed two\ncase studies: physical activity and excessive diet, in order to validate the\neffectiveness of BERT models in classifying lifestyle factors for AD. These\nmodels were compared on the developed Gold Standard Corpus (GSC) on the two\ncase studies. The PubmedBERT(Abs) model achieved the best performance for\nphysical activity, with its precision, recall, and F-1 scores of 0.96, 0.96,\nand 0.96, respectively. Regarding classifying excessive diet, the Bio BERT\nmodel showed the highest performance with perfect precision, recall, and F-1\nscores. The proposed approach leveraging weak supervision could significantly\nincrease the sample size, which is required for training the deep learning\nmodels. The study also demonstrates the effectiveness of BERT models for\nextracting lifestyle factors for Alzheimer's disease from clinical notes.", "journal": ""}
{"doi": "10.48550/arXiv.2103.11367", "date": "2021-03-21", "title": "ROSITA: Refined BERT cOmpreSsion with InTegrAted techniques", "authors": "Yuanxin Liu, Zheng Lin, Fengcheng Yuan", "abstract": "Pre-trained language models of the BERT family have defined the\nstate-of-the-arts in a wide range of NLP tasks. However, the performance of\nBERT-based models is mainly driven by the enormous amount of parameters, which\nhinders their application to resource-limited scenarios. Faced with this\nproblem, recent studies have been attempting to compress BERT into a\nsmall-scale model. However, most previous work primarily focuses on a single\nkind of compression technique, and few attention has been paid to the\ncombination of different methods. When BERT is compressed with integrated\ntechniques, a critical question is how to design the entire compression\nframework to obtain the optimal performance. In response to this question, we\nintegrate three kinds of compression methods (weight pruning, low-rank\nfactorization and knowledge distillation (KD)) and explore a range of designs\nconcerning model architecture, KD strategy, pruning frequency and learning rate\nschedule. We find that a careful choice of the designs is crucial to the\nperformance of the compressed model. Based on the empirical findings, our best\ncompressed model, dubbed Refined BERT cOmpreSsion with InTegrAted techniques\n(ROSITA), is $7.5 \\times$ smaller than BERT while maintains $98.5\\%$ of the\nperformance on five tasks of the GLUE benchmark, outperforming the previous\nBERT compression methods with similar parameter budget. The code is available\nat https://github.com/llyx97/Rosita.", "journal": ""}
{"doi": "10.48550/arXiv.2106.12280", "date": "2021-06-23", "title": "Ergodic Density Estimates for some diffusion processes", "authors": "Bert Koehler, Volker Krafft", "abstract": "For n-dimensional ergodic diffusion processes with values in\n$G=\\mathbb{R}_{+}^n$ we prove time-independent upper bounds for the\ntransitional density and so also for the unique ergodic density. We do not\nrequire geodesic completeness of the elliptic symbol towards the boundary of\n$G$.", "journal": ""}
{"doi": "10.48550/arXiv.2112.07209", "date": "2021-12-14", "title": "ACE-BERT: Adversarial Cross-modal Enhanced BERT for E-commerce Retrieval", "authors": "Boxuan Zhang, Chao Wei, Yan Jin, Weiru Zhang", "abstract": "Nowadays on E-commerce platforms, products are presented to the customers\nwith multiple modalities. These multiple modalities are significant for a\nretrieval system while providing attracted products for customers. Therefore,\nhow to take into account those multiple modalities simultaneously to boost the\nretrieval performance is crucial. This problem is a huge challenge to us due to\nthe following reasons: (1) the way of extracting patch features with the\npre-trained image model (e.g., CNN-based model) has much inductive bias. It is\ndifficult to capture the efficient information from the product image in\nE-commerce. (2) The heterogeneity of multimodal data makes it challenging to\nconstruct the representations of query text and product including title and\nimage in a common subspace. We propose a novel Adversarial Cross-modal Enhanced\nBERT (ACE-BERT) for efficient E-commerce retrieval. In detail, ACE-BERT\nleverages the patch features and pixel features as image representation. Thus\nthe Transformer architecture can be applied directly to the raw image\nsequences. With the pre-trained enhanced BERT as the backbone network, ACE-BERT\nfurther adopts adversarial learning by adding a domain classifier to ensure the\ndistribution consistency of different modality representations for the purpose\nof narrowing down the representation gap between query and product.\nExperimental results demonstrate that ACE-BERT outperforms the state-of-the-art\napproaches on the retrieval task. It is remarkable that ACE-BERT has already\nbeen deployed in our E-commerce's search engine, leading to 1.46% increase in\nrevenue.", "journal": ""}
{"doi": "10.48550/arXiv.2203.06390", "date": "2022-03-12", "title": "BiBERT: Accurate Fully Binarized BERT", "authors": "Haotong Qin, Yifu Ding, Mingyuan Zhang, Qinghua Yan, Aishan Liu, Qingqing Dang, Ziwei Liu, Xianglong Liu", "abstract": "The large pre-trained BERT has achieved remarkable performance on Natural\nLanguage Processing (NLP) tasks but is also computation and memory expensive.\nAs one of the powerful compression approaches, binarization extremely reduces\nthe computation and memory consumption by utilizing 1-bit parameters and\nbitwise operations. Unfortunately, the full binarization of BERT (i.e., 1-bit\nweight, embedding, and activation) usually suffer a significant performance\ndrop, and there is rare study addressing this problem. In this paper, with the\ntheoretical justification and empirical analysis, we identify that the severe\nperformance drop can be mainly attributed to the information degradation and\noptimization direction mismatch respectively in the forward and backward\npropagation, and propose BiBERT, an accurate fully binarized BERT, to eliminate\nthe performance bottlenecks. Specifically, BiBERT introduces an efficient\nBi-Attention structure for maximizing representation information statistically\nand a Direction-Matching Distillation (DMD) scheme to optimize the full\nbinarized BERT accurately. Extensive experiments show that BiBERT outperforms\nboth the straightforward baseline and existing state-of-the-art quantized BERTs\nwith ultra-low bit activations by convincing margins on the NLP benchmark. As\nthe first fully binarized BERT, our method yields impressive 56.3 times and\n31.2 times saving on FLOPs and model size, demonstrating the vast advantages\nand potential of the fully binarized BERT model in real-world\nresource-constrained scenarios.", "journal": "International Conference on Learning Representations (ICLR) 2022"}
{"doi": "10.48550/arXiv.2203.17190", "date": "2022-03-31", "title": "Mixed-Phoneme BERT: Improving BERT with Mixed Phoneme and Sup-Phoneme Representations for Text to Speech", "authors": "Guangyan Zhang, Kaitao Song, Xu Tan, Daxin Tan, Yuzi Yan, Yanqing Liu, Gang Wang, Wei Zhou, Tao Qin, Tan Lee, Sheng Zhao", "abstract": "Recently, leveraging BERT pre-training to improve the phoneme encoder in text\nto speech (TTS) has drawn increasing attention. However, the works apply\npre-training with character-based units to enhance the TTS phoneme encoder,\nwhich is inconsistent with the TTS fine-tuning that takes phonemes as input.\nPre-training only with phonemes as input can alleviate the input mismatch but\nlack the ability to model rich representations and semantic information due to\nlimited phoneme vocabulary. In this paper, we propose MixedPhoneme BERT, a\nnovel variant of the BERT model that uses mixed phoneme and sup-phoneme\nrepresentations to enhance the learning capability. Specifically, we merge the\nadjacent phonemes into sup-phonemes and combine the phoneme sequence and the\nmerged sup-phoneme sequence as the model input, which can enhance the model\ncapacity to learn rich contextual representations. Experiment results\ndemonstrate that our proposed Mixed-Phoneme BERT significantly improves the TTS\nperformance with 0.30 CMOS gain compared with the FastSpeech 2 baseline. The\nMixed-Phoneme BERT achieves 3x inference speedup and similar voice quality to\nthe previous TTS pre-trained model PnG BERT", "journal": ""}
{"doi": "10.48550/arXiv.2204.04793", "date": "2022-04-10", "title": "Fake news detection using parallel BERT deep neural networks", "authors": "Mahmood Farokhian, Vahid Rafe, Hadi Veisi", "abstract": "Fake news is a growing challenge for social networks and media. Detection of\nfake news always has been a problem for many years, but after the evolution of\nsocial networks and increasing speed of news dissemination in recent years has\nbeen considered again. There are several approaches to solving this problem,\none of which is to detect fake news based on its text style using deep neural\nnetworks. In recent years, one of the most used forms of deep neural networks\nfor natural language processing is transfer learning with transformers. BERT is\none of the most promising transformers who outperforms other models in many NLP\nbenchmarks. This article, we introduce MWPBert, which uses two parallel BERT\nnetworks to perform veracity detection on full-text news articles. One of the\nBERT networks encodes news headline, and another encodes news body. Since the\ninput length of the BERT network is limited and constant and the news body is\nusually a long text, we cannot fed the whole news text into the BERT.\nTherefore, using the MaxWorth algorithm, we selected the part of the news text\nthat is more valuable for fact-checking, and fed it into the BERT network.\nFinally, we encode the output of the two BERT networks to an output network to\nclassify the news. The experiment results showed that the proposed model\noutperformed previous models in terms of accuracy and other performance\nmeasures.", "journal": ""}
{"doi": "10.48550/arXiv.2206.10461", "date": "2022-06-21", "title": "An Automatic and Efficient BERT Pruning for Edge AI Systems", "authors": "Shaoyi Huang, Ning Liu, Yueying Liang, Hongwu Peng, Hongjia Li, Dongkuan Xu, Mimi Xie, Caiwen Ding", "abstract": "With the yearning for deep learning democratization, there are increasing\ndemands to implement Transformer-based natural language processing (NLP) models\non resource-constrained devices for low-latency and high accuracy. Existing\nBERT pruning methods require domain experts to heuristically handcraft\nhyperparameters to strike a balance among model size, latency, and accuracy. In\nthis work, we propose AE-BERT, an automatic and efficient BERT pruning\nframework with efficient evaluation to select a \"good\" sub-network candidate\n(with high accuracy) given the overall pruning ratio constraints. Our proposed\nmethod requires no human experts experience and achieves a better accuracy\nperformance on many NLP tasks. Our experimental results on General Language\nUnderstanding Evaluation (GLUE) benchmark show that AE-BERT outperforms the\nstate-of-the-art (SOTA) hand-crafted pruning methods on BERT$_{\\mathrm{BASE}}$.\nOn QNLI and RTE, we obtain 75\\% and 42.8\\% more overall pruning ratio while\nachieving higher accuracy. On MRPC, we obtain a 4.6 higher score than the SOTA\nat the same overall pruning ratio of 0.5. On STS-B, we can achieve a 40\\%\nhigher pruning ratio with a very small loss in Spearman correlation compared to\nSOTA hand-crafted pruning methods. Experimental results also show that after\nmodel compression, the inference time of a single BERT$_{\\mathrm{BASE}}$\nencoder on Xilinx Alveo U200 FPGA board has a 1.83$\\times$ speedup compared to\nIntel(R) Xeon(R) Gold 5218 (2.30GHz) CPU, which shows the reasonableness of\ndeploying the proposed method generated subnets of BERT$_{\\mathrm{BASE}}$ model\non computation restricted devices.", "journal": ""}
{"doi": "10.48550/arXiv.2212.03749", "date": "2022-12-07", "title": "Memorization of Named Entities in Fine-tuned BERT Models", "authors": "Andor Diera, Nicolas Lell, Aygul Garifullina, Ansgar Scherp", "abstract": "Privacy preserving deep learning is an emerging field in machine learning\nthat aims to mitigate the privacy risks in the use of deep neural networks. One\nsuch risk is training data extraction from language models that have been\ntrained on datasets, which contain personal and privacy sensitive information.\nIn our study, we investigate the extent of named entity memorization in\nfine-tuned BERT models. We use single-label text classification as\nrepresentative downstream task and employ three different fine-tuning setups in\nour experiments, including one with Differential Privacy (DP). We create a\nlarge number of text samples from the fine-tuned BERT models utilizing a custom\nsequential sampling strategy with two prompting strategies. We search in these\nsamples for named entities and check if they are also present in the\nfine-tuning datasets. We experiment with two benchmark datasets in the domains\nof emails and blogs. We show that the application of DP has a detrimental\neffect on the text generation capabilities of BERT. Furthermore, we show that a\nfine-tuned BERT does not generate more named entities specific to the\nfine-tuning dataset than a BERT model that is pre-trained only. This suggests\nthat BERT is unlikely to emit personal or privacy sensitive named entities.\nOverall, our results are important to understand to what extent BERT-based\nservices are prone to training data extraction attacks.", "journal": ""}
{"doi": "10.48550/arXiv.2301.01953", "date": "2023-01-05", "title": "Learning Trajectory-Word Alignments for Video-Language Tasks", "authors": "Xu Yang, Zhangzikang Li, Haiyang Xu, Hanwang Zhang, Qinghao Ye, Chenliang Li, Ming Yan, Yu Zhang, Fei Huang, Songfang Huang", "abstract": "In a video, an object usually appears as the trajectory, i.e., it spans over\na few spatial but longer temporal patches, that contains abundant\nspatiotemporal contexts. However, modern Video-Language BERTs (VDL-BERTs)\nneglect this trajectory characteristic that they usually follow image-language\nBERTs (IL-BERTs) to deploy the patch-to-word (P2W) attention that may\nover-exploit trivial spatial contexts and neglect significant temporal\ncontexts. To amend this, we propose a novel TW-BERT to learn Trajectory-Word\nalignment by a newly designed trajectory-to-word (T2W) attention for solving\nvideo-language tasks. Moreover, previous VDL-BERTs usually uniformly sample a\nfew frames into the model while different trajectories have diverse graininess,\ni.e., some trajectories span longer frames and some span shorter, and using a\nfew frames will lose certain useful temporal contexts. However, simply sampling\nmore frames will also make pre-training infeasible due to the largely increased\ntraining burdens. To alleviate the problem, during the fine-tuning stage, we\ninsert a novel Hierarchical Frame-Selector (HFS) module into the video encoder.\nHFS gradually selects the suitable frames conditioned on the text context for\nthe later cross-modal encoder to learn better trajectory-word alignments. By\nthe proposed T2W attention and HFS, our TW-BERT achieves SOTA performances on\ntext-to-video retrieval tasks, and comparable performances on video\nquestion-answering tasks with some VDL-BERTs trained on much more data. The\ncode will be available in the supplementary material.", "journal": ""}
{"doi": "10.48550/arXiv.2302.04792", "date": "2023-02-09", "title": "Using Language Models for Enhancing the Completeness of Natural-language Requirements", "authors": "Dipeeka Luitel, Shabnam Hassani, Mehrdad Sabetzadeh", "abstract": "[Context and motivation] Incompleteness in natural-language requirements is a\nchallenging problem. [Question/problem] A common technique for detecting\nincompleteness in requirements is checking the requirements against external\nsources. With the emergence of language models such as BERT, an interesting\nquestion is whether language models are useful external sources for finding\npotential incompleteness in requirements. [Principal ideas/results] We mask\nwords in requirements and have BERT's masked language model (MLM) generate\ncontextualized predictions for filling the masked slots. We simulate\nincompleteness by withholding content from requirements and measure BERT's\nability to predict terminology that is present in the withheld content but\nabsent in the content disclosed to BERT. [Contribution] BERT can be configured\nto generate multiple predictions per mask. Our first contribution is to\ndetermine how many predictions per mask is an optimal trade-off between\neffectively discovering omissions in requirements and the level of noise in the\npredictions. Our second contribution is devising a machine learning-based\nfilter that post-processes predictions made by BERT to further reduce noise. We\nempirically evaluate our solution over 40 requirements specifications drawn\nfrom the PURE dataset [1]. Our results indicate that: (1) predictions made by\nBERT are highly effective at pinpointing terminology that is missing from\nrequirements, and (2) our filter can substantially reduce noise from the\npredictions, thus making BERT a more compelling aid for improving completeness\nin requirements.", "journal": ""}
{"doi": "10.48550/arXiv.2308.03784", "date": "2023-08-03", "title": "Improving Requirements Completeness: Automated Assistance through Large Language Models", "authors": "Dipeeka Luitel, Shabnam Hassani, Mehrdad Sabetzadeh", "abstract": "Natural language (NL) is arguably the most prevalent medium for expressing\nsystems and software requirements. Detecting incompleteness in NL requirements\nis a major challenge. One approach to identify incompleteness is to compare\nrequirements with external sources. Given the rise of large language models\n(LLMs), an interesting question arises: Are LLMs useful external sources of\nknowledge for detecting potential incompleteness in NL requirements? This\narticle explores this question by utilizing BERT. Specifically, we employ\nBERT's masked language model (MLM) to generate contextualized predictions for\nfilling masked slots in requirements. To simulate incompleteness, we withhold\ncontent from the requirements and assess BERT's ability to predict terminology\nthat is present in the withheld content but absent in the disclosed content.\nBERT can produce multiple predictions per mask. Our first contribution is\ndetermining the optimal number of predictions per mask, striking a balance\nbetween effectively identifying omissions in requirements and mitigating noise\npresent in the predictions. Our second contribution involves designing a\nmachine learning-based filter to post-process BERT's predictions and further\nreduce noise. We conduct an empirical evaluation using 40 requirements\nspecifications from the PURE dataset. Our findings indicate that: (1) BERT's\npredictions effectively highlight terminology that is missing from\nrequirements, (2) BERT outperforms simpler baselines in identifying relevant\nyet missing terminology, and (3) our filter significantly reduces noise in the\npredictions, enhancing BERT's effectiveness as a tool for completeness checking\nof requirements.", "journal": ""}
{"doi": "10.48550/arXiv.2409.13701", "date": "2024-09-05", "title": "CA-BERT: Leveraging Context Awareness for Enhanced Multi-Turn Chat Interaction", "authors": "Minghao Liu, Mingxiu Sui, Yi Nan, Cangqing Wang, Zhijie Zhou", "abstract": "Effective communication in automated chat systems hinges on the ability to\nunderstand and respond to context. Traditional models often struggle with\ndetermining when additional context is necessary for generating appropriate\nresponses. This paper introduces Context-Aware BERT (CA-BERT), a\ntransformer-based model specifically fine-tuned to address this challenge.\nCA-BERT innovatively applies deep learning techniques to discern context\nnecessity in multi-turn chat interactions, enhancing both the relevance and\naccuracy of responses.\n  We describe the development of CA-BERT, which adapts the robust architecture\nof BERT with a novel training regimen focused on a specialized dataset of chat\ndialogues. The model is evaluated on its ability to classify context necessity,\ndemonstrating superior performance over baseline BERT models in terms of\naccuracy and efficiency. Furthermore, CA-BERT's implementation showcases\nsignificant reductions in training time and resource usage, making it feasible\nfor real-time applications.\n  The results indicate that CA-BERT can effectively enhance the functionality\nof chatbots by providing a nuanced understanding of context, thereby improving\nuser experience and interaction quality in automated systems. This study not\nonly advances the field of NLP in chat applications but also provides a\nframework for future research into context-sensitive AI developments.", "journal": ""}
{"doi": "10.48550/arXiv.1908.05620", "date": "2019-08-15", "title": "Visualizing and Understanding the Effectiveness of BERT", "authors": "Yaru Hao, Li Dong, Furu Wei, Ke Xu", "abstract": "Language model pre-training, such as BERT, has achieved remarkable results in\nmany NLP tasks. However, it is unclear why the pre-training-then-fine-tuning\nparadigm can improve performance and generalization capability across different\ntasks. In this paper, we propose to visualize loss landscapes and optimization\ntrajectories of fine-tuning BERT on specific datasets. First, we find that\npre-training reaches a good initial point across downstream tasks, which leads\nto wider optima and easier optimization compared with training from scratch. We\nalso demonstrate that the fine-tuning procedure is robust to overfitting, even\nthough BERT is highly over-parameterized for downstream tasks. Second, the\nvisualization results indicate that fine-tuning BERT tends to generalize better\nbecause of the flat and wide optima, and the consistency between the training\nloss surface and the generalization error surface. Third, the lower layers of\nBERT are more invariant during fine-tuning, which suggests that the layers that\nare close to input learn more transferable representations of language.", "journal": ""}
{"doi": "10.48550/arXiv.1909.00931", "date": "2019-09-03", "title": "Transfer Fine-Tuning: A BERT Case Study", "authors": "Yuki Arase, Junichi Tsujii", "abstract": "A semantic equivalence assessment is defined as a task that assesses semantic\nequivalence in a sentence pair by binary judgment (i.e., paraphrase\nidentification) or grading (i.e., semantic textual similarity measurement). It\nconstitutes a set of tasks crucial for research on natural language\nunderstanding. Recently, BERT realized a breakthrough in sentence\nrepresentation learning (Devlin et al., 2019), which is broadly transferable to\nvarious NLP tasks. While BERT's performance improves by increasing its model\nsize, the required computational power is an obstacle preventing practical\napplications from adopting the technology. Herein, we propose to inject phrasal\nparaphrase relations into BERT in order to generate suitable representations\nfor semantic equivalence assessment instead of increasing the model size.\nExperiments on standard natural language understanding tasks confirm that our\nmethod effectively improves a smaller BERT model while maintaining the model\nsize. The generated model exhibits superior performance compared to a larger\nBERT model on semantic equivalence assessment tasks. Furthermore, it achieves\nlarger performance gains on tasks with limited training datasets for\nfine-tuning, which is a property desirable for transfer learning.", "journal": ""}
{"doi": "10.48550/arXiv.1909.02339", "date": "2019-09-05", "title": "Specializing Unsupervised Pretraining Models for Word-Level Semantic Similarity", "authors": "Anne Lauscher, Ivan Vuli\u0107, Edoardo Maria Ponti, Anna Korhonen, Goran Glava\u0161", "abstract": "Unsupervised pretraining models have been shown to facilitate a wide range of\ndownstream NLP applications. These models, however, retain some of the\nlimitations of traditional static word embeddings. In particular, they encode\nonly the distributional knowledge available in raw text corpora, incorporated\nthrough language modeling objectives. In this work, we complement such\ndistributional knowledge with external lexical knowledge, that is, we integrate\nthe discrete knowledge on word-level semantic similarity into pretraining. To\nthis end, we generalize the standard BERT model to a multi-task learning\nsetting where we couple BERT's masked language modeling and next sentence\nprediction objectives with an auxiliary task of binary word relation\nclassification. Our experiments suggest that our \"Lexically Informed\" BERT\n(LIBERT), specialized for the word-level semantic similarity, yields better\nperformance than the lexically blind \"vanilla\" BERT on several language\nunderstanding tasks. Concretely, LIBERT outperforms BERT in 9 out of 10 tasks\nof the GLUE benchmark and is on a par with BERT in the remaining one. Moreover,\nwe show consistent gains on 3 benchmarks for lexical simplification, a task\nwhere knowledge about word-level semantic similarity is paramount.", "journal": ""}
{"doi": "10.48550/arXiv.1909.03415", "date": "2019-09-08", "title": "Commonsense Knowledge + BERT for Level 2 Reading Comprehension Ability Test", "authors": "Yidan Hu, Gongqi Lin, Yuan Miao, Chunyan Miao", "abstract": "Commonsense knowledge plays an important role when we read. The performance\nof BERT on SQuAD dataset shows that the accuracy of BERT can be better than\nhuman users. However, it does not mean that computers can surpass the human\nbeing in reading comprehension. CommonsenseQA is a large-scale dataset which is\ndesigned based on commonsense knowledge. BERT only achieved an accuracy of\n55.9% on it. The result shows that computers cannot apply commonsense knowledge\nlike human beings to answer questions. Comprehension Ability Test (CAT) divided\nthe reading comprehension ability at four levels. We can achieve human like\ncomprehension ability level by level. BERT has performed well at level 1 which\ndoes not require common knowledge. In this research, we propose a system which\naims to allow computers to read articles and answer related questions with\ncommonsense knowledge like a human being for CAT level 2. This system consists\nof three parts. Firstly, we built a commonsense knowledge graph; and then\nautomatically constructed the commonsense knowledge question dataset according\nto it. Finally, BERT is combined with the commonsense knowledge to achieve the\nreading comprehension ability at CAT level 2. Experiments show that it can pass\nthe CAT as long as the required common knowledge is included in the knowledge\nbase.", "journal": ""}
{"doi": "10.48550/arXiv.1909.09292", "date": "2019-09-20", "title": "BERT Meets Chinese Word Segmentation", "authors": "Haiqin Yang", "abstract": "Chinese word segmentation (CWS) is a fundamental task for Chinese language\nunderstanding. Recently, neural network-based models have attained superior\nperformance in solving the in-domain CWS task. Last year, Bidirectional Encoder\nRepresentation from Transformers (BERT), a new language representation model,\nhas been proposed as a backbone model for many natural language tasks and\nredefined the corresponding performance. The excellent performance of BERT\nmotivates us to apply it to solve the CWS task. By conducting intensive\nexperiments in the benchmark datasets from the second International Chinese\nWord Segmentation Bake-off, we obtain several keen observations. BERT can\nslightly improve the performance even when the datasets contain the issue of\nlabeling inconsistency. When applying sufficiently learned features, Softmax, a\nsimpler classifier, can attain the same performance as that of a more\ncomplicated classifier, e.g., Conditional Random Field (CRF). The performance\nof BERT usually increases as the model size increases. The features extracted\nby BERT can be also applied as good candidates for other neural network models.", "journal": ""}
{"doi": "10.48550/arXiv.2006.07890", "date": "2020-06-14", "title": "FinEst BERT and CroSloEngual BERT: less is more in multilingual models", "authors": "Matej Ul\u010dar, Marko Robnik-\u0160ikonja", "abstract": "Large pretrained masked language models have become state-of-the-art\nsolutions for many NLP problems. The research has been mostly focused on\nEnglish language, though. While massively multilingual models exist, studies\nhave shown that monolingual models produce much better results. We train two\ntrilingual BERT-like models, one for Finnish, Estonian, and English, the other\nfor Croatian, Slovenian, and English. We evaluate their performance on several\ndownstream tasks, NER, POS-tagging, and dependency parsing, using the\nmultilingual BERT and XLM-R as baselines. The newly created FinEst BERT and\nCroSloEngual BERT improve the results on all tasks in most monolingual and\ncross-lingual situations", "journal": "Proceedings of the 23rd Internetional Conference on Text, Speech,\n  and Dialogue (TSD 2020), pages 104-111"}
{"doi": "10.48550/arXiv.1904.03339", "date": "2019-04-06", "title": "ThisIsCompetition at SemEval-2019 Task 9: BERT is unstable for out-of-domain samples", "authors": "Cheoneum Park, Juae Kim, Hyeon-gu Lee, Reinald Kim Amplayo, Harksoo Kim, Jungyun Seo, Changki Lee", "abstract": "This paper describes our system, Joint Encoders for Stable Suggestion\nInference (JESSI), for the SemEval 2019 Task 9: Suggestion Mining from Online\nReviews and Forums. JESSI is a combination of two sentence encoders: (a) one\nusing multiple pre-trained word embeddings learned from log-bilinear regression\n(GloVe) and translation (CoVe) models, and (b) one on top of word encodings\nfrom a pre-trained deep bidirectional transformer (BERT). We include a domain\nadversarial training module when training for out-of-domain samples. Our\nexperiments show that while BERT performs exceptionally well for in-domain\nsamples, several runs of the model show that it is unstable for out-of-domain\nsamples. The problem is mitigated tremendously by (1) combining BERT with a\nnon-BERT encoder, and (2) using an RNN-based classifier on top of BERT. Our\nfinal models obtained second place with 77.78\\% F-Score on Subtask A (i.e.\nin-domain) and achieved an F-Score of 79.59\\% on Subtask B (i.e.\nout-of-domain), even without using any additional external data.", "journal": ""}
{"doi": "10.48550/arXiv.2105.00817", "date": "2021-04-12", "title": "BERT based freedom to operate patent analysis", "authors": "Michael Freunek, Andr\u00e9 Bodmer", "abstract": "In this paper we present a method to apply BERT to freedom to operate patent\nanalysis and patent searches. According to the method, BERT is fine-tuned by\ntraining patent descriptions to the independent claims. Each description\nrepresents an invention which is protected by the corresponding claims. Such a\ntrained BERT could be able to identify or order freedom to operate relevant\npatents based on a short description of an invention or product. We tested the\nmethod by training BERT on the patent class G06T1/00 and applied the trained\nBERT on five inventions classified in G06T1/60, described via DOCDB abstracts.\nThe DOCDB abstract are available on ESPACENET of the European Patent Office.", "journal": ""}
{"doi": "10.48550/arXiv.2105.06020", "date": "2021-05-13", "title": "Are Larger Pretrained Language Models Uniformly Better? Comparing Performance at the Instance Level", "authors": "Ruiqi Zhong, Dhruba Ghosh, Dan Klein, Jacob Steinhardt", "abstract": "Larger language models have higher accuracy on average, but are they better\non every single instance (datapoint)? Some work suggests larger models have\nhigher out-of-distribution robustness, while other work suggests they have\nlower accuracy on rare subgroups. To understand these differences, we\ninvestigate these models at the level of individual instances. However, one\nmajor challenge is that individual predictions are highly sensitive to noise in\nthe randomness in training. We develop statistically rigorous methods to\naddress this, and after accounting for pretraining and finetuning noise, we\nfind that our BERT-Large is worse than BERT-Mini on at least 1-4% of instances\nacross MNLI, SST-2, and QQP, compared to the overall accuracy improvement of\n2-10%. We also find that finetuning noise increases with model size and that\ninstance-level accuracy has momentum: improvement from BERT-Mini to BERT-Medium\ncorrelates with improvement from BERT-Medium to BERT-Large. Our findings\nsuggest that instance-level predictions provide a rich source of information;\nwe therefore, recommend that researchers supplement model weights with model\npredictions.", "journal": ""}
{"doi": "10.48550/arXiv.2105.07148", "date": "2021-05-15", "title": "Lexicon Enhanced Chinese Sequence Labeling Using BERT Adapter", "authors": "Wei Liu, Xiyan Fu, Yue Zhang, Wenming Xiao", "abstract": "Lexicon information and pre-trained models, such as BERT, have been combined\nto explore Chinese sequence labelling tasks due to their respective strengths.\nHowever, existing methods solely fuse lexicon features via a shallow and random\ninitialized sequence layer and do not integrate them into the bottom layers of\nBERT. In this paper, we propose Lexicon Enhanced BERT (LEBERT) for Chinese\nsequence labelling, which integrates external lexicon knowledge into BERT\nlayers directly by a Lexicon Adapter layer. Compared with the existing methods,\nour model facilitates deep lexicon knowledge fusion at the lower layers of\nBERT. Experiments on ten Chinese datasets of three tasks including Named Entity\nRecognition, Word Segmentation, and Part-of-Speech tagging, show that LEBERT\nachieves the state-of-the-art results.", "journal": ""}
{"doi": "10.48550/arXiv.2201.00558", "date": "2022-01-03", "title": "Which Student is Best? A Comprehensive Knowledge Distillation Exam for Task-Specific BERT Models", "authors": "Made Nindyatama Nityasya, Haryo Akbarianto Wibowo, Rendi Chevi, Radityo Eko Prasojo, Alham Fikri Aji", "abstract": "We perform knowledge distillation (KD) benchmark from task-specific BERT-base\nteacher models to various student models: BiLSTM, CNN, BERT-Tiny, BERT-Mini,\nand BERT-Small. Our experiment involves 12 datasets grouped in two tasks: text\nclassification and sequence labeling in the Indonesian language. We also\ncompare various aspects of distillations including the usage of word embeddings\nand unlabeled data augmentation. Our experiments show that, despite the rising\npopularity of Transformer-based models, using BiLSTM and CNN student models\nprovide the best trade-off between performance and computational resource (CPU,\nRAM, and storage) compared to pruned BERT models. We further propose some quick\nwins on performing KD to produce small NLP models via efficient KD training\nmechanisms involving simple choices of loss functions, word embeddings, and\nunlabeled data preparation.", "journal": ""}
{"doi": "10.48550/arXiv.2202.12191", "date": "2022-02-24", "title": "Finding Inverse Document Frequency Information in BERT", "authors": "Jaekeol Choi, Euna Jung, Sungjun Lim, Wonjong Rhee", "abstract": "For many decades, BM25 and its variants have been the dominant document\nretrieval approach, where their two underlying features are Term Frequency (TF)\nand Inverse Document Frequency (IDF). The traditional approach, however, is\nbeing rapidly replaced by Neural Ranking Models (NRMs) that can exploit\nsemantic features. In this work, we consider BERT-based NRMs and study if IDF\ninformation is present in the NRMs. This simple question is interesting because\nIDF has been indispensable for the traditional lexical matching, but global\nfeatures like IDF are not explicitly learned by neural language models\nincluding BERT. We adopt linear probing as the main analysis tool because\ntypical BERT based NRMs utilize linear or inner-product based score\naggregators. We analyze input embeddings, representations of all BERT layers,\nand the self-attention weights of CLS. By studying MS-MARCO dataset with three\nBERT-based models, we show that all of them contain information that is\nstrongly dependent on IDF.", "journal": ""}
{"doi": "10.48550/arXiv.1906.04341", "date": "2019-06-11", "title": "What Does BERT Look At? An Analysis of BERT's Attention", "authors": "Kevin Clark, Urvashi Khandelwal, Omer Levy, Christopher D. Manning", "abstract": "Large pre-trained neural networks such as BERT have had great recent success\nin NLP, motivating a growing body of research investigating what aspects of\nlanguage they are able to learn from unlabeled data. Most recent analysis has\nfocused on model outputs (e.g., language model surprisal) or internal vector\nrepresentations (e.g., probing classifiers). Complementary to these works, we\npropose methods for analyzing the attention mechanisms of pre-trained models\nand apply them to BERT. BERT's attention heads exhibit patterns such as\nattending to delimiter tokens, specific positional offsets, or broadly\nattending over the whole sentence, with heads in the same layer often\nexhibiting similar behaviors. We further show that certain attention heads\ncorrespond well to linguistic notions of syntax and coreference. For example,\nwe find heads that attend to the direct objects of verbs, determiners of nouns,\nobjects of prepositions, and coreferent mentions with remarkably high accuracy.\nLastly, we propose an attention-based probing classifier and use it to further\ndemonstrate that substantial syntactic information is captured in BERT's\nattention.", "journal": ""}
{"doi": "10.48550/arXiv.1911.00225", "date": "2019-11-01", "title": "When Choosing Plausible Alternatives, Clever Hans can be Clever", "authors": "Pride Kavumba, Naoya Inoue, Benjamin Heinzerling, Keshav Singh, Paul Reisert, Kentaro Inui", "abstract": "Pretrained language models, such as BERT and RoBERTa, have shown large\nimprovements in the commonsense reasoning benchmark COPA. However, recent work\nfound that many improvements in benchmarks of natural language understanding\nare not due to models learning the task, but due to their increasing ability to\nexploit superficial cues, such as tokens that occur more often in the correct\nanswer than the wrong one. Are BERT's and RoBERTa's good performance on COPA\nalso caused by this? We find superficial cues in COPA, as well as evidence that\nBERT exploits these cues. To remedy this problem, we introduce Balanced COPA,\nan extension of COPA that does not suffer from easy-to-exploit single token\ncues. We analyze BERT's and RoBERTa's performance on original and Balanced\nCOPA, finding that BERT relies on superficial cues when they are present, but\nstill achieves comparable performance once they are made ineffective,\nsuggesting that BERT learns the task to a certain degree when forced to. In\ncontrast, RoBERTa does not appear to rely on superficial cues.", "journal": ""}
{"doi": "10.48550/arXiv.1912.09582", "date": "2019-12-19", "title": "BERTje: A Dutch BERT Model", "authors": "Wietse de Vries, Andreas van Cranenburgh, Arianna Bisazza, Tommaso Caselli, Gertjan van Noord, Malvina Nissim", "abstract": "The transformer-based pre-trained language model BERT has helped to improve\nstate-of-the-art performance on many natural language processing (NLP) tasks.\nUsing the same architecture and parameters, we developed and evaluated a\nmonolingual Dutch BERT model called BERTje. Compared to the multilingual BERT\nmodel, which includes Dutch but is only based on Wikipedia text, BERTje is\nbased on a large and diverse dataset of 2.4 billion tokens. BERTje consistently\noutperforms the equally-sized multilingual BERT model on downstream NLP tasks\n(part-of-speech tagging, named-entity recognition, semantic role labeling, and\nsentiment analysis). Our pre-trained Dutch BERT model is made available at\nhttps://github.com/wietsedv/bertje.", "journal": ""}
{"doi": "10.48550/arXiv.2002.04815", "date": "2020-02-12", "title": "Utilizing BERT Intermediate Layers for Aspect Based Sentiment Analysis and Natural Language Inference", "authors": "Youwei Song, Jiahai Wang, Zhiwei Liang, Zhiyue Liu, Tao Jiang", "abstract": "Aspect based sentiment analysis aims to identify the sentimental tendency\ntowards a given aspect in text. Fine-tuning of pretrained BERT performs\nexcellent on this task and achieves state-of-the-art performances. Existing\nBERT-based works only utilize the last output layer of BERT and ignore the\nsemantic knowledge in the intermediate layers. This paper explores the\npotential of utilizing BERT intermediate layers to enhance the performance of\nfine-tuning of BERT. To the best of our knowledge, no existing work has been\ndone on this research. To show the generality, we also apply this approach to a\nnatural language inference task. Experimental results demonstrate the\neffectiveness and generality of the proposed approach.", "journal": ""}
{"doi": "10.48550/arXiv.2002.08307", "date": "2020-02-19", "title": "Compressing BERT: Studying the Effects of Weight Pruning on Transfer Learning", "authors": "Mitchell A. Gordon, Kevin Duh, Nicholas Andrews", "abstract": "Pre-trained universal feature extractors, such as BERT for natural language\nprocessing and VGG for computer vision, have become effective methods for\nimproving deep learning models without requiring more labeled data. While\neffective, feature extractors like BERT may be prohibitively large for some\ndeployment scenarios. We explore weight pruning for BERT and ask: how does\ncompression during pre-training affect transfer learning? We find that pruning\naffects transfer learning in three broad regimes. Low levels of pruning\n(30-40%) do not affect pre-training loss or transfer to downstream tasks at\nall. Medium levels of pruning increase the pre-training loss and prevent useful\npre-training information from being transferred to downstream tasks. High\nlevels of pruning additionally prevent models from fitting downstream datasets,\nleading to further degradation. Finally, we observe that fine-tuning BERT on a\nspecific task does not improve its prunability. We conclude that BERT can be\npruned once during pre-training rather than separately for each task without\naffecting performance.", "journal": ""}
{"doi": "10.48550/arXiv.2004.01881", "date": "2020-04-04", "title": "CG-BERT: Conditional Text Generation with BERT for Generalized Few-shot Intent Detection", "authors": "Congying Xia, Chenwei Zhang, Hoang Nguyen, Jiawei Zhang, Philip Yu", "abstract": "In this paper, we formulate a more realistic and difficult problem setup for\nthe intent detection task in natural language understanding, namely Generalized\nFew-Shot Intent Detection (GFSID). GFSID aims to discriminate a joint label\nspace consisting of both existing intents which have enough labeled data and\nnovel intents which only have a few examples for each class. To approach this\nproblem, we propose a novel model, Conditional Text Generation with BERT\n(CG-BERT). CG-BERT effectively leverages a large pre-trained language model to\ngenerate text conditioned on the intent label. By modeling the utterance\ndistribution with variational inference, CG-BERT can generate diverse\nutterances for the novel intents even with only a few utterances available.\nExperimental results show that CG-BERT achieves state-of-the-art performance on\nthe GFSID task with 1-shot and 5-shot settings on two real-world datasets.", "journal": ""}
{"doi": "10.48550/arXiv.2004.05707", "date": "2020-04-12", "title": "VGCN-BERT: Augmenting BERT with Graph Embedding for Text Classification", "authors": "Zhibin Lu, Pan Du, Jian-Yun Nie", "abstract": "Much progress has been made recently on text classification with methods\nbased on neural networks. In particular, models using attention mechanism such\nas BERT have shown to have the capability of capturing the contextual\ninformation within a sentence or document. However, their ability of capturing\nthe global information about the vocabulary of a language is more limited. This\nlatter is the strength of Graph Convolutional Networks (GCN). In this paper, we\npropose VGCN-BERT model which combines the capability of BERT with a Vocabulary\nGraph Convolutional Network (VGCN). Local information and global information\ninteract through different layers of BERT, allowing them to influence mutually\nand to build together a final representation for classification. In our\nexperiments on several text classification datasets, our approach outperforms\nBERT and GCN alone, and achieve higher effectiveness than that reported in\nprevious studies.", "journal": "in J. M. Jose et al. (Eds.): ECIR 2020, LNCS 12035, pp.369-382,\n  2020"}
{"doi": "10.48550/arXiv.2005.12766", "date": "2020-05-16", "title": "CERT: Contrastive Self-supervised Learning for Language Understanding", "authors": "Hongchao Fang, Sicheng Wang, Meng Zhou, Jiayuan Ding, Pengtao Xie", "abstract": "Pretrained language models such as BERT, GPT have shown great effectiveness\nin language understanding. The auxiliary predictive tasks in existing\npretraining approaches are mostly defined on tokens, thus may not be able to\ncapture sentence-level semantics very well. To address this issue, we propose\nCERT: Contrastive self-supervised Encoder Representations from Transformers,\nwhich pretrains language representation models using contrastive\nself-supervised learning at the sentence level. CERT creates augmentations of\noriginal sentences using back-translation. Then it finetunes a pretrained\nlanguage encoder (e.g., BERT) by predicting whether two augmented sentences\noriginate from the same sentence. CERT is simple to use and can be flexibly\nplugged into any pretraining-finetuning NLP pipeline. We evaluate CERT on 11\nnatural language understanding tasks in the GLUE benchmark where CERT\noutperforms BERT on 7 tasks, achieves the same performance as BERT on 2 tasks,\nand performs worse than BERT on 2 tasks. On the averaged score of the 11 tasks,\nCERT outperforms BERT. The data and code are available at\nhttps://github.com/UCSD-AI4H/CERT", "journal": ""}
{"doi": "10.48550/arXiv.2008.12014", "date": "2020-08-27", "title": "GREEK-BERT: The Greeks visiting Sesame Street", "authors": "John Koutsikakis, Ilias Chalkidis, Prodromos Malakasiotis, Ion Androutsopoulos", "abstract": "Transformer-based language models, such as BERT and its variants, have\nachieved state-of-the-art performance in several downstream natural language\nprocessing (NLP) tasks on generic benchmark datasets (e.g., GLUE, SQUAD, RACE).\nHowever, these models have mostly been applied to the resource-rich English\nlanguage. In this paper, we present GREEK-BERT, a monolingual BERT-based\nlanguage model for modern Greek. We evaluate its performance in three NLP\ntasks, i.e., part-of-speech tagging, named entity recognition, and natural\nlanguage inference, obtaining state-of-the-art performance. Interestingly, in\ntwo of the benchmarks GREEK-BERT outperforms two multilingual Transformer-based\nmodels (M-BERT, XLM-R), as well as shallower neural baselines operating on\npre-trained word embeddings, by a large margin (5%-10%). Most importantly, we\nmake both GREEK-BERT and our training code publicly available, along with code\nillustrating how GREEK-BERT can be fine-tuned for downstream NLP tasks. We\nexpect these resources to boost NLP research and applications for modern Greek.", "journal": ""}
{"doi": "10.48550/arXiv.2009.08590", "date": "2020-09-18", "title": "NEU at WNUT-2020 Task 2: Data Augmentation To Tell BERT That Death Is Not Necessarily Informative", "authors": "Kumud Chauhan", "abstract": "Millions of people around the world are sharing COVID-19 related information\non social media platforms. Since not all the information shared on the social\nmedia is useful, a machine learning system to identify informative posts can\nhelp users in finding relevant information. In this paper, we present a BERT\nclassifier system for W-NUT2020 Shared Task 2: Identification of Informative\nCOVID-19 English Tweets. Further, we show that BERT exploits some easy signals\nto identify informative tweets, and adding simple patterns to uninformative\ntweets drastically degrades BERT performance. In particular, simply adding 10\ndeaths to tweets in dev set, reduces BERT F1- score from 92.63 to 7.28. We also\npropose a simple data augmentation technique that helps in improving the\nrobustness and generalization ability of the BERT classifier.", "journal": ""}
{"doi": "10.48550/arXiv.2009.14790", "date": "2020-09-30", "title": "BERT for Monolingual and Cross-Lingual Reverse Dictionary", "authors": "Hang Yan, Xiaonan Li, Xipeng Qiu", "abstract": "Reverse dictionary is the task to find the proper target word given the word\ndescription. In this paper, we tried to incorporate BERT into this task.\nHowever, since BERT is based on the byte-pair-encoding (BPE) subword encoding,\nit is nontrivial to make BERT generate a word given the description. We propose\na simple but effective method to make BERT generate the target word for this\nspecific task. Besides, the cross-lingual reverse dictionary is the task to\nfind the proper target word described in another language. Previous models have\nto keep two different word embeddings and learn to align these embeddings.\nNevertheless, by using the Multilingual BERT (mBERT), we can efficiently\nconduct the cross-lingual reverse dictionary with one subword embedding, and\nthe alignment between languages is not necessary. More importantly, mBERT can\nachieve remarkable cross-lingual reverse dictionary performance even without\nthe parallel corpus, which means it can conduct the cross-lingual reverse\ndictionary with only corresponding monolingual data. Code is publicly available\nat https://github.com/yhcc/BertForRD.git.", "journal": ""}
{"doi": "10.48550/arXiv.2010.05763", "date": "2020-10-12", "title": "Layer-wise Guided Training for BERT: Learning Incrementally Refined Document Representations", "authors": "Nikolaos Manginas, Ilias Chalkidis, Prodromos Malakasiotis", "abstract": "Although BERT is widely used by the NLP community, little is known about its\ninner workings. Several attempts have been made to shed light on certain\naspects of BERT, often with contradicting conclusions. A much raised concern\nfocuses on BERT's over-parameterization and under-utilization issues. To this\nend, we propose o novel approach to fine-tune BERT in a structured manner.\nSpecifically, we focus on Large Scale Multilabel Text Classification (LMTC)\nwhere documents are assigned with one or more labels from a large predefined\nset of hierarchically organized labels. Our approach guides specific BERT\nlayers to predict labels from specific hierarchy levels. Experimenting with two\nLMTC datasets we show that this structured fine-tuning approach not only yields\nbetter classification results but also leads to better parameter utilization.", "journal": ""}
{"doi": "10.48550/arXiv.2010.07711", "date": "2020-10-15", "title": "Does Chinese BERT Encode Word Structure?", "authors": "Yile Wang, Leyang Cui, Yue Zhang", "abstract": "Contextualized representations give significantly improved results for a wide\nrange of NLP tasks. Much work has been dedicated to analyzing the features\ncaptured by representative models such as BERT. Existing work finds that\nsyntactic, semantic and word sense knowledge are encoded in BERT. However,\nlittle work has investigated word features for character-based languages such\nas Chinese. We investigate Chinese BERT using both attention weight\ndistribution statistics and probing tasks, finding that (1) word information is\ncaptured by BERT; (2) word-level features are mostly in the middle\nrepresentation layers; (3) downstream tasks make different use of word features\nin BERT, with POS tagging and chunking relying the most on word features, and\nnatural language inference relying the least on such features.", "journal": ""}
{"doi": "10.48550/arXiv.2010.10499", "date": "2020-10-20", "title": "Optimal Subarchitecture Extraction For BERT", "authors": "Adrian de Wynter, Daniel J. Perry", "abstract": "We extract an optimal subset of architectural parameters for the BERT\narchitecture from Devlin et al. (2018) by applying recent breakthroughs in\nalgorithms for neural architecture search. This optimal subset, which we refer\nto as \"Bort\", is demonstrably smaller, having an effective (that is, not\ncounting the embedding layer) size of $5.5\\%$ the original BERT-large\narchitecture, and $16\\%$ of the net size. Bort is also able to be pretrained in\n$288$ GPU hours, which is $1.2\\%$ of the time required to pretrain the\nhighest-performing BERT parametric architectural variant, RoBERTa-large (Liu et\nal., 2019), and about $33\\%$ of that of the world-record, in GPU hours,\nrequired to train BERT-large on the same hardware. It is also $7.9$x faster on\na CPU, as well as being better performing than other compressed variants of the\narchitecture, and some of the non-compressed variants: it obtains performance\nimprovements of between $0.3\\%$ and $31\\%$, absolute, with respect to\nBERT-large, on multiple public natural language understanding (NLU) benchmarks.", "journal": ""}
{"doi": "10.48550/arXiv.2011.00169", "date": "2020-10-31", "title": "Understanding Pre-trained BERT for Aspect-based Sentiment Analysis", "authors": "Hu Xu, Lei Shu, Philip S. Yu, Bing Liu", "abstract": "This paper analyzes the pre-trained hidden representations learned from\nreviews on BERT for tasks in aspect-based sentiment analysis (ABSA). Our work\nis motivated by the recent progress in BERT-based language models for ABSA.\nHowever, it is not clear how the general proxy task of (masked) language model\ntrained on unlabeled corpus without annotations of aspects or opinions can\nprovide important features for downstream tasks in ABSA. By leveraging the\nannotated datasets in ABSA, we investigate both the attentions and the learned\nrepresentations of BERT pre-trained on reviews. We found that BERT uses very\nfew self-attention heads to encode context words (such as prepositions or\npronouns that indicating an aspect) and opinion words for an aspect. Most\nfeatures in the representation of an aspect are dedicated to the fine-grained\nsemantics of the domain (or product category) and the aspect itself, instead of\ncarrying summarized opinions from its context. We hope this investigation can\nhelp future research in improving self-supervised learning, unsupervised\nlearning and fine-tuning for ABSA. The pre-trained model and code can be found\nat https://github.com/howardhsu/BERT-for-RRC-ABSA.", "journal": ""}
{"doi": "10.48550/arXiv.2011.00398", "date": "2020-11-01", "title": "Investigation of BERT Model on Biomedical Relation Extraction Based on Revised Fine-tuning Mechanism", "authors": "Peng Su, K. Vijay-Shanker", "abstract": "With the explosive growth of biomedical literature, designing automatic tools\nto extract information from the literature has great significance in biomedical\nresearch. Recently, transformer-based BERT models adapted to the biomedical\ndomain have produced leading results. However, all the existing BERT models for\nrelation classification only utilize partial knowledge from the last layer. In\nthis paper, we will investigate the method of utilizing the entire layer in the\nfine-tuning process of BERT model. To the best of our knowledge, we are the\nfirst to explore this method. The experimental results illustrate that our\nmethod improves the BERT model performance and outperforms the state-of-the-art\nmethods on three benchmark datasets for different relation extraction tasks. In\naddition, further analysis indicates that the key knowledge about the relations\ncan be learned from the last layer of BERT model.", "journal": ""}
{"doi": "10.48550/arXiv.2011.04784", "date": "2020-11-09", "title": "EstBERT: A Pretrained Language-Specific BERT for Estonian", "authors": "Hasan Tanvir, Claudia Kittask, Sandra Eiche, Kairit Sirts", "abstract": "This paper presents EstBERT, a large pretrained transformer-based\nlanguage-specific BERT model for Estonian. Recent work has evaluated\nmultilingual BERT models on Estonian tasks and found them to outperform the\nbaselines. Still, based on existing studies on other languages, a\nlanguage-specific BERT model is expected to improve over the multilingual ones.\nWe first describe the EstBERT pretraining process and then present the results\nof the models based on finetuned EstBERT for multiple NLP tasks, including POS\nand morphological tagging, named entity recognition and text classification.\nThe evaluation results show that the models based on EstBERT outperform\nmultilingual BERT models on five tasks out of six, providing further evidence\ntowards a view that training language-specific BERT models are still useful,\neven when multilingual models are available.", "journal": ""}
{"doi": "10.48550/arXiv.2011.05864", "date": "2020-11-02", "title": "On the Sentence Embeddings from Pre-trained Language Models", "authors": "Bohan Li, Hao Zhou, Junxian He, Mingxuan Wang, Yiming Yang, Lei Li", "abstract": "Pre-trained contextual representations like BERT have achieved great success\nin natural language processing. However, the sentence embeddings from the\npre-trained language models without fine-tuning have been found to poorly\ncapture semantic meaning of sentences. In this paper, we argue that the\nsemantic information in the BERT embeddings is not fully exploited. We first\nreveal the theoretical connection between the masked language model\npre-training objective and the semantic similarity task theoretically, and then\nanalyze the BERT sentence embeddings empirically. We find that BERT always\ninduces a non-smooth anisotropic semantic space of sentences, which harms its\nperformance of semantic similarity. To address this issue, we propose to\ntransform the anisotropic sentence embedding distribution to a smooth and\nisotropic Gaussian distribution through normalizing flows that are learned with\nan unsupervised objective. Experimental results show that our proposed\nBERT-flow method obtains significant performance gains over the\nstate-of-the-art sentence embeddings on a variety of semantic textual\nsimilarity tasks. The code is available at\nhttps://github.com/bohanli/BERT-flow.", "journal": ""}
{"doi": "10.48550/arXiv.2011.12380", "date": "2020-11-24", "title": "Experiments on transfer learning architectures for biomedical relation extraction", "authors": "Walid Hafiane, Joel Legrand, Yannick Toussaint, Adrien Coulet", "abstract": "Relation extraction (RE) consists in identifying and structuring\nautomatically relations of interest from texts. Recently, BERT improved the top\nperformances for several NLP tasks, including RE. However, the best way to use\nBERT, within a machine learning architecture, and within a transfer learning\nstrategy is still an open question since it is highly dependent on each\nspecific task and domain. Here, we explore various BERT-based architectures and\ntransfer learning strategies (i.e., frozen or fine-tuned) for the task of\nbiomedical RE on two corpora. Among tested architectures and strategies, our\n*BERT-segMCNN with finetuning reaches performances higher than the\nstate-of-the-art on the two corpora (1.73 % and 32.77 % absolute improvement on\nChemProt and PGxCorpus corpora respectively). More generally, our experiments\nillustrate the expected interest of fine-tuning with BERT, but also the\nunexplored advantage of using structural information (with sentence\nsegmentation), in addition to the context classically leveraged by BERT.", "journal": ""}
{"doi": "10.48550/arXiv.2012.10275", "date": "2020-12-18", "title": "An Empirical Study of Using Pre-trained BERT Models for Vietnamese Relation Extraction Task at VLSP 2020", "authors": "Pham Quang Nhat Minh", "abstract": "In this paper, we present an empirical study of using pre-trained BERT models\nfor the relation extraction task at the VLSP 2020 Evaluation Campaign. We\napplied two state-of-the-art BERT-based models: R-BERT and BERT model with\nentity starts. For each model, we compared two pre-trained BERT models:\nFPTAI/vibert and NlpHUST/vibert4news. We found that NlpHUST/vibert4news model\nsignificantly outperforms FPTAI/vibert for the Vietnamese relation extraction\ntask. Finally, we proposed an ensemble model that combines R-BERT and BERT with\nentity starts. Our proposed ensemble model slightly improved against two single\nmodels on the development data and the test data provided by the task\norganizers.", "journal": ""}
{"doi": "10.48550/arXiv.2101.00196", "date": "2021-01-01", "title": "On Explaining Your Explanations of BERT: An Empirical Study with Sequence Classification", "authors": "Zhengxuan Wu, Desmond C. Ong", "abstract": "BERT, as one of the pretrianed language models, attracts the most attention\nin recent years for creating new benchmarks across GLUE tasks via fine-tuning.\nOne pressing issue is to open up the blackbox and explain the decision makings\nof BERT. A number of attribution techniques have been proposed to explain BERT\nmodels, but are often limited to sequence to sequence tasks. In this paper, we\nadapt existing attribution methods on explaining decision makings of BERT in\nsequence classification tasks. We conduct extensive analyses of four existing\nattribution methods by applying them to four different datasets in sentiment\nanalysis. We compare the reliability and robustness of each method via various\nablation studies. Furthermore, we test whether attribution methods explain\ngeneralized semantics across semantically similar tasks. Our work provides\nsolid guidance for using attribution methods to explain decision makings of\nBERT for downstream classification tasks.", "journal": ""}
{"doi": "10.48550/arXiv.2101.03700", "date": "2021-01-11", "title": "AT-BERT: Adversarial Training BERT for Acronym Identification Winning Solution for SDU@AAAI-21", "authors": "Danqing Zhu, Wangli Lin, Yang Zhang, Qiwei Zhong, Guanxiong Zeng, Weilin Wu, Jiayu Tang", "abstract": "Acronym identification focuses on finding the acronyms and the phrases that\nhave been abbreviated, which is crucial for scientific document understanding\ntasks. However, the limited size of manually annotated datasets hinders further\nimprovement for the problem. Recent breakthroughs of language models\npre-trained on large corpora clearly show that unsupervised pre-training can\nvastly improve the performance of downstream tasks. In this paper, we present\nan Adversarial Training BERT method named AT-BERT, our winning solution to\nacronym identification task for Scientific Document Understanding (SDU)\nChallenge of AAAI 2021. Specifically, the pre-trained BERT is adopted to\ncapture better semantic representation. Then we incorporate the FGM adversarial\ntraining strategy into the fine-tuning of BERT, which makes the model more\nrobust and generalized. Furthermore, an ensemble mechanism is devised to\ninvolve the representations learned from multiple BERT variants. Assembling all\nthese components together, the experimental results on the SciAI dataset show\nthat our proposed approach outperforms all other competitive state-of-the-art\nmethods.", "journal": ""}
{"doi": "10.48550/arXiv.2101.09647", "date": "2021-01-24", "title": "Does Dialog Length matter for Next Response Selection task? An Empirical Study", "authors": "Jatin Ganhotra, Sachindra Joshi", "abstract": "In the last few years, the release of BERT, a multilingual transformer based\nmodel, has taken the NLP community by storm. BERT-based models have achieved\nstate-of-the-art results on various NLP tasks, including dialog tasks. One of\nthe limitation of BERT is the lack of ability to handle long text sequence. By\ndefault, BERT has a maximum wordpiece token sequence length of 512. Recently,\nthere has been renewed interest to tackle the BERT limitation to handle long\ntext sequences with the addition of new self-attention based architectures.\nHowever, there has been little to no research on the impact of this limitation\nwith respect to dialog tasks. Dialog tasks are inherently different from other\nNLP tasks due to: a) the presence of multiple utterances from multiple\nspeakers, which may be interlinked to each other across different turns and b)\nlonger length of dialogs. In this work, we empirically evaluate the impact of\ndialog length on the performance of BERT model for the Next Response Selection\ndialog task on four publicly available and one internal multi-turn dialog\ndatasets. We observe that there is little impact on performance with long\ndialogs and even the simplest approach of truncating input works really well.", "journal": ""}
{"doi": "10.48550/arXiv.2102.11278", "date": "2021-02-22", "title": "RUBERT: A Bilingual Roman Urdu BERT Using Cross Lingual Transfer Learning", "authors": "Usama Khalid, Mirza Omer Beg, Muhammad Umair Arshad", "abstract": "In recent studies, it has been shown that Multilingual language models\nunderperform their monolingual counterparts. It is also a well-known fact that\ntraining and maintaining monolingual models for each language is a costly and\ntime-consuming process. Roman Urdu is a resource-starved language used\npopularly on social media platforms and chat apps. In this research, we propose\na novel dataset of scraped tweets containing 54M tokens and 3M sentences.\nAdditionally, we also propose RUBERT a bilingual Roman Urdu model created by\nadditional pretraining of English BERT. We compare its performance with a\nmonolingual Roman Urdu BERT trained from scratch and a multilingual Roman Urdu\nBERT created by additional pretraining of Multilingual BERT. We show through\nour experiments that additional pretraining of the English BERT produces the\nmost notable performance improvement.", "journal": ""}
{"doi": "10.48550/arXiv.2103.10013", "date": "2021-03-18", "title": "Model Extraction and Adversarial Transferability, Your BERT is Vulnerable!", "authors": "Xuanli He, Lingjuan Lyu, Qiongkai Xu, Lichao Sun", "abstract": "Natural language processing (NLP) tasks, ranging from text classification to\ntext generation, have been revolutionised by the pre-trained language models,\nsuch as BERT. This allows corporations to easily build powerful APIs by\nencapsulating fine-tuned BERT models for downstream tasks. However, when a\nfine-tuned BERT model is deployed as a service, it may suffer from different\nattacks launched by malicious users. In this work, we first present how an\nadversary can steal a BERT-based API service (the victim/target model) on\nmultiple benchmark datasets with limited prior knowledge and queries. We\nfurther show that the extracted model can lead to highly transferable\nadversarial attacks against the victim model. Our studies indicate that the\npotential vulnerabilities of BERT-based API services still hold, even when\nthere is an architectural mismatch between the victim model and the attack\nmodel. Finally, we investigate two defence strategies to protect the victim\nmodel and find that unless the performance of the victim model is sacrificed,\nboth model ex-traction and adversarial transferability can effectively\ncompromise the target models", "journal": ""}
{"doi": "10.48550/arXiv.2103.13799", "date": "2021-03-25", "title": "Bertinho: Galician BERT Representations", "authors": "David Vilares, Marcos Garcia, Carlos G\u00f3mez-Rodr\u00edguez", "abstract": "This paper presents a monolingual BERT model for Galician. We follow the\nrecent trend that shows that it is feasible to build robust monolingual BERT\nmodels even for relatively low-resource languages, while performing better than\nthe well-known official multilingual BERT (mBERT). More particularly, we\nrelease two monolingual Galician BERT models, built using 6 and 12 transformer\nlayers, respectively; trained with limited resources (~45 million tokens on a\nsingle GPU of 24GB). We then provide an exhaustive evaluation on a number of\ntasks such as POS-tagging, dependency parsing and named entity recognition. For\nthis purpose, all these tasks are cast in a pure sequence labeling setup in\norder to run BERT without the need to include any additional layers on top of\nit (we only use an output classification layer to map the contextualized\nrepresentations into the predicted label). The experiments show that our\nmodels, especially the 12-layer one, outperform the results of mBERT in most\ntasks.", "journal": "Procesamiento del Lenguaje Natural. 66 (2021) 13-26"}
{"doi": "10.48550/arXiv.2103.15060", "date": "2021-03-28", "title": "PnG BERT: Augmented BERT on Phonemes and Graphemes for Neural TTS", "authors": "Ye Jia, Heiga Zen, Jonathan Shen, Yu Zhang, Yonghui Wu", "abstract": "This paper introduces PnG BERT, a new encoder model for neural TTS. This\nmodel is augmented from the original BERT model, by taking both phoneme and\ngrapheme representations of text as input, as well as the word-level alignment\nbetween them. It can be pre-trained on a large text corpus in a self-supervised\nmanner, and fine-tuned in a TTS task. Experimental results show that a neural\nTTS model using a pre-trained PnG BERT as its encoder yields more natural\nprosody and more accurate pronunciation than a baseline model using only\nphoneme input with no pre-training. Subjective side-by-side preference\nevaluations show that raters have no statistically significant preference\nbetween the speech synthesized using a PnG BERT and ground truth recordings\nfrom professional speakers.", "journal": ""}
{"doi": "10.48550/arXiv.2104.07613", "date": "2021-04-15", "title": "SINA-BERT: A pre-trained Language Model for Analysis of Medical Texts in Persian", "authors": "Nasrin Taghizadeh, Ehsan Doostmohammadi, Elham Seifossadat, Hamid R. Rabiee, Maedeh S. Tahaei", "abstract": "We have released Sina-BERT, a language model pre-trained on BERT (Devlin et\nal., 2018) to address the lack of a high-quality Persian language model in the\nmedical domain. SINA-BERT utilizes pre-training on a large-scale corpus of\nmedical contents including formal and informal texts collected from a variety\nof online resources in order to improve the performance on health-care related\ntasks. We employ SINA-BERT to complete following representative tasks:\ncategorization of medical questions, medical sentiment analysis, and medical\nquestion retrieval. For each task, we have developed Persian annotated data\nsets for training and evaluation and learnt a representation for the data of\neach task especially complex and long medical questions. With the same\narchitecture being used across tasks, SINA-BERT outperforms BERT-based models\nthat were previously made available in the Persian language.", "journal": ""}
{"doi": "10.48550/arXiv.2104.08523", "date": "2021-04-17", "title": "Co-BERT: A Context-Aware BERT Retrieval Model Incorporating Local and Query-specific Context", "authors": "Xiaoyang Chen, Kai Hui, Ben He, Xianpei Han, Le Sun, Zheng Ye", "abstract": "BERT-based text ranking models have dramatically advanced the\nstate-of-the-art in ad-hoc retrieval, wherein most models tend to consider\nindividual query-document pairs independently. In the mean time, the importance\nand usefulness to consider the cross-documents interactions and the\nquery-specific characteristics in a ranking model have been repeatedly\nconfirmed, mostly in the context of learning to rank. The BERT-based ranking\nmodel, however, has not been able to fully incorporate these two types of\nranking context, thereby ignoring the inter-document relationships from the\nranking and the differences among queries. To mitigate this gap, in this work,\nan end-to-end transformer-based ranking model, named Co-BERT, has been proposed\nto exploit several BERT architectures to calibrate the query-document\nrepresentations using pseudo relevance feedback before modeling the relevance\nof a group of documents jointly. Extensive experiments on two standard test\ncollections confirm the effectiveness of the proposed model in improving the\nperformance of text re-ranking over strong fine-tuned BERT-Base baselines. We\nplan to make our implementation open source to enable further comparisons.", "journal": ""}
{"doi": "10.48550/arXiv.2104.11559", "date": "2021-04-23", "title": "Optimizing small BERTs trained for German NER", "authors": "Jochen Z\u00f6llner, Konrad Sperfeld, Christoph Wick, Roger Labahn", "abstract": "Currently, the most widespread neural network architecture for training\nlanguage models is the so called BERT which led to improvements in various\nNatural Language Processing (NLP) tasks. In general, the larger the number of\nparameters in a BERT model, the better the results obtained in these NLP tasks.\nUnfortunately, the memory consumption and the training duration drastically\nincreases with the size of these models. In this article, we investigate\nvarious training techniques of smaller BERT models: We combine different\nmethods from other BERT variants like ALBERT, RoBERTa, and relative positional\nencoding. In addition, we propose two new fine-tuning modifications leading to\nbetter performance: Class-Start-End tagging and a modified form of Linear Chain\nConditional Random Fields. Furthermore, we introduce Whole-Word Attention which\nreduces BERTs memory usage and leads to a small increase in performance\ncompared to classical Multi-Head-Attention. We evaluate these techniques on\nfive public German Named Entity Recognition (NER) tasks of which two are\nintroduced by this article.", "journal": "MDPI Information 2021, vol. 12 nr. 11, article-nr. 443"}
{"doi": "10.48550/arXiv.2104.13913", "date": "2021-04-28", "title": "Improving BERT Model Using Contrastive Learning for Biomedical Relation Extraction", "authors": "Peng Su, Yifan Peng, K. Vijay-Shanker", "abstract": "Contrastive learning has been used to learn a high-quality representation of\nthe image in computer vision. However, contrastive learning is not widely\nutilized in natural language processing due to the lack of a general method of\ndata augmentation for text data. In this work, we explore the method of\nemploying contrastive learning to improve the text representation from the BERT\nmodel for relation extraction. The key knob of our framework is a unique\ncontrastive pre-training step tailored for the relation extraction tasks by\nseamlessly integrating linguistic knowledge into the data augmentation.\nFurthermore, we investigate how large-scale data constructed from the external\nknowledge bases can enhance the generality of contrastive pre-training of BERT.\nThe experimental results on three relation extraction benchmark datasets\ndemonstrate that our method can improve the BERT model representation and\nachieve state-of-the-art performance. In addition, we explore the\ninterpretability of models by showing that BERT with contrastive pre-training\nrelies more on rationales for prediction. Our code and data are publicly\navailable at: https://github.com/udel-biotm-lab/BERT-CLRE.", "journal": ""}
{"doi": "10.48550/arXiv.2106.06169", "date": "2021-06-11", "title": "BoB: BERT Over BERT for Training Persona-based Dialogue Models from Limited Personalized Data", "authors": "Haoyu Song, Yan Wang, Kaiyan Zhang, Wei-Nan Zhang, Ting Liu", "abstract": "Maintaining consistent personas is essential for dialogue agents. Although\ntremendous advancements have been brought, the limited-scale of annotated\npersona-dense data are still barriers towards training robust and consistent\npersona-based dialogue models. In this work, we show how the challenges can be\naddressed by disentangling persona-based dialogue generation into two sub-tasks\nwith a novel BERT-over-BERT (BoB) model. Specifically, the model consists of a\nBERT-based encoder and two BERT-based decoders, where one decoder is for\nresponse generation, and another is for consistency understanding. In\nparticular, to learn the ability of consistency understanding from large-scale\nnon-dialogue inference data, we train the second decoder in an unlikelihood\nmanner. Under different limited data settings, both automatic and human\nevaluations demonstrate that the proposed model outperforms strong baselines in\nresponse quality and persona consistency.", "journal": ""}
{"doi": "10.48550/arXiv.2107.10614", "date": "2021-07-22", "title": "Evaluation of contextual embeddings on less-resourced languages", "authors": "Matej Ul\u010dar, Ale\u0161 \u017dagar, Carlos S. Armendariz, Andra\u017e Repar, Senja Pollak, Matthew Purver, Marko Robnik-\u0160ikonja", "abstract": "The current dominance of deep neural networks in natural language processing\nis based on contextual embeddings such as ELMo, BERT, and BERT derivatives.\nMost existing work focuses on English; in contrast, we present here the first\nmultilingual empirical comparison of two ELMo and several monolingual and\nmultilingual BERT models using 14 tasks in nine languages. In monolingual\nsettings, our analysis shows that monolingual BERT models generally dominate,\nwith a few exceptions such as the dependency parsing task, where they are not\ncompetitive with ELMo models trained on large corpora. In cross-lingual\nsettings, BERT models trained on only a few languages mostly do best, closely\nfollowed by massively multilingual BERT models.", "journal": ""}
{"doi": "10.48550/arXiv.2108.10197", "date": "2021-08-23", "title": "Deploying a BERT-based Query-Title Relevance Classifier in a Production System: a View from the Trenches", "authors": "Leonard Dahlmann, Tomer Lancewicki", "abstract": "The Bidirectional Encoder Representations from Transformers (BERT) model has\nbeen radically improving the performance of many Natural Language Processing\n(NLP) tasks such as Text Classification and Named Entity Recognition (NER)\napplications. However, it is challenging to scale BERT for low-latency and\nhigh-throughput industrial use cases due to its enormous size. We successfully\noptimize a Query-Title Relevance (QTR) classifier for deployment via a compact\nmodel, which we name BERT Bidirectional Long Short-Term Memory (BertBiLSTM).\nThe model is capable of inferring an input in at most 0.2ms on CPU. BertBiLSTM\nexceeds the off-the-shelf BERT model's performance in terms of accuracy and\nefficiency for the aforementioned real-world production task. We achieve this\nresult in two phases. First, we create a pre-trained model, called eBERT, which\nis the original BERT architecture trained with our unique item title corpus. We\nthen fine-tune eBERT for the QTR task. Second, we train the BertBiLSTM model to\nmimic the eBERT model's performance through a process called Knowledge\nDistillation (KD) and show the effect of data augmentation to achieve the\nresembling goal. Experimental results show that the proposed model outperforms\nother compact and production-ready models.", "journal": ""}
{"doi": "10.48550/arXiv.2109.09700", "date": "2021-09-20", "title": "BERT Cannot Align Characters", "authors": "Antonis Maronikolakis, Philipp Dufter, Hinrich Sch\u00fctze", "abstract": "In previous work, it has been shown that BERT can adequately align\ncross-lingual sentences on the word level. Here we investigate whether BERT can\nalso operate as a char-level aligner. The languages examined are English,\nFake-English, German and Greek. We show that the closer two languages are, the\nbetter BERT can align them on the character level. BERT indeed works well in\nEnglish to Fake-English alignment, but this does not generalize to natural\nlanguages to the same extent. Nevertheless, the proximity of two languages does\nseem to be a factor. English is more related to German than to Greek and this\nis reflected in how well BERT aligns them; English to German is better than\nEnglish to Greek. We examine multiple setups and show that the similarity\nmatrices for natural languages show weaker relations the further apart two\nlanguages are.", "journal": ""}
{"doi": "10.48550/arXiv.2110.04504", "date": "2021-10-09", "title": "An Isotropy Analysis in the Multilingual BERT Embedding Space", "authors": "Sara Rajaee, Mohammad Taher Pilehvar", "abstract": "Several studies have explored various advantages of multilingual pre-trained\nmodels (such as multilingual BERT) in capturing shared linguistic knowledge.\nHowever, less attention has been paid to their limitations. In this paper, we\ninvestigate the multilingual BERT for two known issues of the monolingual\nmodels: anisotropic embedding space and outlier dimensions. We show that,\nunlike its monolingual counterpart, the multilingual BERT model exhibits no\noutlier dimension in its representations while it has a highly anisotropic\nspace. There are a few dimensions in the monolingual BERT with high\ncontributions to the anisotropic distribution. However, we observe no such\ndimensions in the multilingual BERT. Furthermore, our experimental results\ndemonstrate that increasing the isotropy of multilingual space can\nsignificantly improve its representation power and performance, similarly to\nwhat had been observed for monolingual CWRs on semantic similarity tasks. Our\nanalysis indicates that, despite having different degenerated directions, the\nembedding spaces in various languages tend to be partially similar with respect\nto their structures.", "journal": ""}
{"doi": "10.48550/arXiv.2203.00328", "date": "2022-03-01", "title": "BERT-LID: Leveraging BERT to Improve Spoken Language Identification", "authors": "Yuting Nie, Junhong Zhao, Wei-Qiang Zhang, Jinfeng Bai", "abstract": "Language identification is the task of automatically determining the identity\nof a language conveyed by a spoken segment. It has a profound impact on the\nmultilingual interoperability of an intelligent speech system. Despite language\nidentification attaining high accuracy on medium or long utterances(>3s), the\nperformance on short utterances (<=1s) is still far from satisfactory. We\npropose a BERT-based language identification system (BERT-LID) to improve\nlanguage identification performance, especially on short-duration speech\nsegments. We extend the original BERT model by taking the phonetic\nposteriorgrams (PPG) derived from the front-end phone recognizer as input. Then\nwe deployed the optimal deep classifier followed by it for language\nidentification. Our BERT-LID model can improve the baseline accuracy by about\n6.5% on long-segment identification and 19.9% on short-segment identification,\ndemonstrating our BERT-LID's effectiveness to language identification.", "journal": "ISCSLP (2022) 384-388"}
{"doi": "10.48550/arXiv.2203.03289", "date": "2022-03-07", "title": "$\u03bc$BERT: Mutation Testing using Pre-Trained Language Models", "authors": "Renzo Degiovanni, Mike Papadakis", "abstract": "We introduce $\\mu$BERT, a mutation testing tool that uses a pre-trained\nlanguage model (CodeBERT) to generate mutants. This is done by masking a token\nfrom the expression given as input and using CodeBERT to predict it. Thus, the\nmutants are generated by replacing the masked tokens with the predicted ones.\nWe evaluate $\\mu$BERT on 40 real faults from Defects4J and show that it can\ndetect 27 out of the 40 faults, while the baseline (PiTest) detects 26 of them.\nWe also show that $\\mu$BERT can be 2 times more cost-effective than PiTest,\nwhen the same number of mutants are analysed. Additionally, we evaluate the\nimpact of $\\mu$BERT's mutants when used by program assertion inference\ntechniques, and show that they can help in producing better specifications.\nFinally, we discuss about the quality and naturalness of some interesting\nmutants produced by $\\mu$BERT during our experimental evaluation.", "journal": ""}
{"doi": "10.48550/arXiv.2203.03550", "date": "2022-02-17", "title": "When BERT Meets Quantum Temporal Convolution Learning for Text Classification in Heterogeneous Computing", "authors": "Chao-Han Huck Yang, Jun Qi, Samuel Yen-Chi Chen, Yu Tsao, Pin-Yu Chen", "abstract": "The rapid development of quantum computing has demonstrated many unique\ncharacteristics of quantum advantages, such as richer feature representation\nand more secured protection on model parameters. This work proposes a vertical\nfederated learning architecture based on variational quantum circuits to\ndemonstrate the competitive performance of a quantum-enhanced pre-trained BERT\nmodel for text classification. In particular, our proposed hybrid\nclassical-quantum model consists of a novel random quantum temporal convolution\n(QTC) learning framework replacing some layers in the BERT-based decoder. Our\nexperiments on intent classification show that our proposed BERT-QTC model\nattains competitive experimental results in the Snips and ATIS spoken language\ndatasets. Particularly, the BERT-QTC boosts the performance of the existing\nquantum circuit-based language model in two text classification datasets by\n1.57% and 1.52% relative improvements. Furthermore, BERT-QTC can be feasibly\ndeployed on both existing commercial-accessible quantum computation hardware\nand CPU-based interface for ensuring data isolation.", "journal": ""}
{"doi": "10.48550/arXiv.2203.06482", "date": "2022-03-12", "title": "FiNER: Financial Numeric Entity Recognition for XBRL Tagging", "authors": "Lefteris Loukas, Manos Fergadiotis, Ilias Chalkidis, Eirini Spyropoulou, Prodromos Malakasiotis, Ion Androutsopoulos, Georgios Paliouras", "abstract": "Publicly traded companies are required to submit periodic reports with\neXtensive Business Reporting Language (XBRL) word-level tags. Manually tagging\nthe reports is tedious and costly. We, therefore, introduce XBRL tagging as a\nnew entity extraction task for the financial domain and release FiNER-139, a\ndataset of 1.1M sentences with gold XBRL tags. Unlike typical entity extraction\ndatasets, FiNER-139 uses a much larger label set of 139 entity types. Most\nannotated tokens are numeric, with the correct tag per token depending mostly\non context, rather than the token itself. We show that subword fragmentation of\nnumeric expressions harms BERT's performance, allowing word-level BILSTMs to\nperform better. To improve BERT's performance, we propose two simple and\neffective solutions that replace numeric expressions with pseudo-tokens\nreflecting original token shapes and numeric magnitudes. We also experiment\nwith FIN-BERT, an existing BERT model for the financial domain, and release our\nown BERT (SEC-BERT), pre-trained on financial filings, which performs best.\nThrough data and error analysis, we finally identify possible limitations to\ninspire future work on XBRL tagging.", "journal": ""}
{"doi": "10.48550/arXiv.2206.11724", "date": "2022-06-23", "title": "BERT Rankers are Brittle: a Study using Adversarial Document Perturbations", "authors": "Yumeng Wang, Lijun Lyu, Avishek Anand", "abstract": "Contextual ranking models based on BERT are now well established for a wide\nrange of passage and document ranking tasks. However, the robustness of\nBERT-based ranking models under adversarial inputs is under-explored. In this\npaper, we argue that BERT-rankers are not immune to adversarial attacks\ntargeting retrieved documents given a query. Firstly, we propose algorithms for\nadversarial perturbation of both highly relevant and non-relevant documents\nusing gradient-based optimization methods. The aim of our algorithms is to\nadd/replace a small number of tokens to a highly relevant or non-relevant\ndocument to cause a large rank demotion or promotion. Our experiments show that\na small number of tokens can already result in a large change in the rank of a\ndocument. Moreover, we find that BERT-rankers heavily rely on the document\nstart/head for relevance prediction, making the initial part of the document\nmore susceptible to adversarial attacks. More interestingly, we find a small\nset of recurring adversarial words that when added to documents result in\nsuccessful rank demotion/promotion of any relevant/non-relevant document\nrespectively. Finally, our adversarial tokens also show particular topic\npreferences within and across datasets, exposing potential biases from BERT\npre-training or downstream datasets.", "journal": ""}
{"doi": "10.48550/arXiv.2206.15195", "date": "2022-06-30", "title": "The Topological BERT: Transforming Attention into Topology for Natural Language Processing", "authors": "Ilan Perez, Raphael Reinauer", "abstract": "In recent years, the introduction of the Transformer models sparked a\nrevolution in natural language processing (NLP). BERT was one of the first text\nencoders using only the attention mechanism without any recurrent parts to\nachieve state-of-the-art results on many NLP tasks.\n  This paper introduces a text classifier using topological data analysis. We\nuse BERT's attention maps transformed into attention graphs as the only input\nto that classifier. The model can solve tasks such as distinguishing spam from\nham messages, recognizing whether a sentence is grammatically correct, or\nevaluating a movie review as negative or positive. It performs comparably to\nthe BERT baseline and outperforms it on some tasks.\n  Additionally, we propose a new method to reduce the number of BERT's\nattention heads considered by the topological classifier, which allows us to\nprune the number of heads from 144 down to as few as ten with no reduction in\nperformance. Our work also shows that the topological model displays higher\nrobustness against adversarial attacks than the original BERT model, which is\nmaintained during the pruning process. To the best of our knowledge, this work\nis the first to confront topological-based models with adversarial attacks in\nthe context of NLP.", "journal": ""}
{"doi": "10.48550/arXiv.2210.03970", "date": "2022-10-08", "title": "KG-MTT-BERT: Knowledge Graph Enhanced BERT for Multi-Type Medical Text Classification", "authors": "Yong He, Cheng Wang, Shun Zhang, Nan Li, Zhaorong Li, Zhenyu Zeng", "abstract": "Medical text learning has recently emerged as a promising area to improve\nhealthcare due to the wide adoption of electronic health record (EHR) systems.\nThe complexity of the medical text such as diverse length, mixed text types,\nand full of medical jargon, poses a great challenge for developing effective\ndeep learning models. BERT has presented state-of-the-art results in many NLP\ntasks, such as text classification and question answering. However, the\nstandalone BERT model cannot deal with the complexity of the medical text,\nespecially the lengthy clinical notes. Herein, we develop a new model called\nKG-MTT-BERT (Knowledge Graph Enhanced Multi-Type Text BERT) by extending the\nBERT model for long and multi-type text with the integration of the medical\nknowledge graph. Our model can outperform all baselines and other\nstate-of-the-art models in diagnosis-related group (DRG) classification, which\nrequires comprehensive medical text for accurate classification. We also\ndemonstrated that our model can effectively handle multi-type text and the\nintegration of medical knowledge graph can significantly improve the\nperformance.", "journal": ""}
{"doi": "10.48550/arXiv.2210.16168", "date": "2022-10-28", "title": "Feature Engineering vs BERT on Twitter Data", "authors": "Ryiaadh Gani, Lisa Chalaguine", "abstract": "In this paper, we compare the performances of traditional machine learning\nmodels using feature engineering and word vectors and the state-of-the-art\nlanguage model BERT using word embeddings on three datasets. We also consider\nthe time and cost efficiency of feature engineering compared to BERT. From our\nresults we conclude that the use of the BERT model was only worth the time and\ncost trade-off for one of the three datasets we used for comparison, where the\nBERT model significantly outperformed any kind of traditional classifier that\nuses feature vectors, instead of embeddings. Using the BERT model for the other\ndatasets only achieved an increase of 0.03 and 0.05 of accuracy and F1 score\nrespectively, which could be argued makes its use not worth the time and cost\nof GPU.", "journal": ""}
{"doi": "10.48550/arXiv.2211.05273", "date": "2022-11-10", "title": "BERT-Based Combination of Convolutional and Recurrent Neural Network for Indonesian Sentiment Analysis", "authors": "Hendri Murfi, Syamsyuriani, Theresia Gowandi, Gianinna Ardaneswari, Siti Nurrohmah", "abstract": "Sentiment analysis is the computational study of opinions and emotions\nex-pressed in text. Deep learning is a model that is currently producing\nstate-of-the-art in various application domains, including sentiment analysis.\nMany researchers are using a hybrid approach that combines different deep\nlearning models and has been shown to improve model performance. In sentiment\nanalysis, input in text data is first converted into a numerical\nrepresentation. The standard method used to obtain a text representation is the\nfine-tuned embedding method. However, this method does not pay attention to\neach word's context in the sentence. Therefore, the Bidirectional Encoder\nRepresentation from Transformer (BERT) model is used to obtain text\nrepresentations based on the context and position of words in sentences. This\nresearch extends the previous hybrid deep learning using BERT representation\nfor Indonesian sentiment analysis. Our simulation shows that the BERT\nrepresentation improves the accuracies of all hybrid architectures. The\nBERT-based LSTM-CNN also reaches slightly better accuracies than other\nBERT-based hybrid architectures.", "journal": ""}
{"doi": "10.48550/arXiv.2301.11069", "date": "2023-01-26", "title": "BERT-Embedding and Citation Network Analysis based Query Expansion Technique for Scholarly Search", "authors": "Shah Khalid, Shah Khusro, Aftab Alam, Abdul Wahid", "abstract": "The enormous growth of research publications has made it challenging for\nacademic search engines to bring the most relevant papers against the given\nsearch query. Numerous solutions have been proposed over the years to improve\nthe effectiveness of academic search, including exploiting query expansion and\ncitation analysis. Query expansion techniques mitigate the mismatch between the\nlanguage used in a query and indexed documents. However, these techniques can\nsuffer from introducing non-relevant information while expanding the original\nquery. Recently, contextualized model BERT to document retrieval has been quite\nsuccessful in query expansion. Motivated by such issues and inspired by the\nsuccess of BERT, this paper proposes a novel approach called QeBERT. QeBERT\nexploits BERT-based embedding and Citation Network Analysis (CNA) in query\nexpansion for improving scholarly search. Specifically, we use the\ncontext-aware BERT-embedding and CNA for query expansion in Pseudo-Relevance\nFeedback (PRF) fash-ion. Initial experimental results on the ACL dataset show\nthat BERT-embedding can provide a valuable augmentation to query expansion and\nimprove search relevance when combined with CNA.", "journal": ""}
{"doi": "10.48550/arXiv.2307.07258", "date": "2023-07-14", "title": "Improving BERT with Hybrid Pooling Network and Drop Mask", "authors": "Qian Chen, Wen Wang, Qinglin Zhang, Chong Deng, Ma Yukun, Siqi Zheng", "abstract": "Transformer-based pre-trained language models, such as BERT, achieve great\nsuccess in various natural language understanding tasks. Prior research found\nthat BERT captures a rich hierarchy of linguistic information at different\nlayers. However, the vanilla BERT uses the same self-attention mechanism for\neach layer to model the different contextual features. In this paper, we\npropose a HybridBERT model which combines self-attention and pooling networks\nto encode different contextual features in each layer. Additionally, we propose\na simple DropMask method to address the mismatch between pre-training and\nfine-tuning caused by excessive use of special mask tokens during Masked\nLanguage Modeling pre-training. Experiments show that HybridBERT outperforms\nBERT in pre-training with lower loss, faster training speed (8% relative),\nlower memory cost (13% relative), and also in transfer learning with 1.5%\nrelative higher accuracies on downstream tasks. Additionally, DropMask improves\naccuracies of BERT on downstream tasks across various masking rates.", "journal": ""}
{"doi": "10.48550/arXiv.2307.11764", "date": "2023-07-14", "title": "Sensi-BERT: Towards Sensitivity Driven Fine-Tuning for Parameter-Efficient BERT", "authors": "Souvik Kundu, Sharath Nittur Sridhar, Maciej Szankin, Sairam Sundaresan", "abstract": "Large pre-trained language models have recently gained significant traction\ndue to their improved performance on various down-stream tasks like text\nclassification and question answering, requiring only few epochs of\nfine-tuning. However, their large model sizes often prohibit their applications\non resource-constrained edge devices. Existing solutions of yielding\nparameter-efficient BERT models largely rely on compute-exhaustive training and\nfine-tuning. Moreover, they often rely on additional compute heavy models to\nmitigate the performance gap. In this paper, we present Sensi-BERT, a\nsensitivity driven efficient fine-tuning of BERT models that can take an\noff-the-shelf pre-trained BERT model and yield highly parameter-efficient\nmodels for downstream tasks. In particular, we perform sensitivity analysis to\nrank each individual parameter tensor, that then is used to trim them\naccordingly during fine-tuning for a given parameter or FLOPs budget. Our\nexperiments show the efficacy of Sensi-BERT across different downstream tasks\nincluding MNLI, QQP, QNLI, SST-2 and SQuAD, showing better performance at\nsimilar or smaller parameter budget compared to various alternatives.", "journal": ""}
{"doi": "10.48550/arXiv.2308.00108", "date": "2023-07-26", "title": "DPBERT: Efficient Inference for BERT based on Dynamic Planning", "authors": "Weixin Wu, Hankz Hankui Zhuo", "abstract": "Large-scale pre-trained language models such as BERT have contributed\nsignificantly to the development of NLP. However, those models require large\ncomputational resources, making it difficult to be applied to mobile devices\nwhere computing power is limited. In this paper we aim to address the weakness\nof existing input-adaptive inference methods which fail to take full advantage\nof the structure of BERT. We propose Dynamic Planning in BERT, a novel\nfine-tuning strategy that can accelerate the inference process of BERT through\nselecting a subsequence of transformer layers list of backbone as a\ncomputational path for an input sample. To do this, our approach adds a\nplanning module to the original BERT model to determine whether a layer is\nincluded or bypassed during inference. Experimental results on the GLUE\nbenchmark exhibit that our method reduces latency to 75\\% while maintaining\n98\\% accuracy, yielding a better accuracy-speed trade-off compared to\nstate-of-the-art input-adaptive methods.", "journal": ""}
{"doi": "10.48550/arXiv.2311.04799", "date": "2023-11-08", "title": "DACBERT: Leveraging Dependency Agreement for Cost-Efficient Bert Pretraining", "authors": "Martin Kuo, Jianyi Zhang, Yiran Chen", "abstract": "Building on the cost-efficient pretraining advancements brought about by\nCrammed BERT, we enhance its performance and interpretability further by\nintroducing a novel pretrained model Dependency Agreement Crammed BERT\n(DACBERT) and its two-stage pretraining framework - Dependency Agreement\nPretraining. This framework, grounded by linguistic theories, seamlessly weaves\nsyntax and semantic information into the pretraining process. The first stage\nemploys four dedicated submodels to capture representative dependency\nagreements at the chunk level, effectively converting these agreements into\nembeddings. The second stage uses these refined embeddings, in tandem with\nconventional BERT embeddings, to guide the pretraining of the rest of the\nmodel. Evaluated on the GLUE benchmark, our DACBERT demonstrates notable\nimprovement across various tasks, surpassing Crammed BERT by 3.13% in the RTE\ntask and by 2.26% in the MRPC task. Furthermore, our method boosts the average\nGLUE score by 0.83%, underscoring its significant potential. The pretraining\nprocess can be efficiently executed on a single GPU within a 24-hour cycle,\nnecessitating no supplementary computational resources or extending the\npretraining duration compared with the Crammed BERT. Extensive studies further\nilluminate our approach's instrumental role in bolstering the interpretability\nof pretrained language models for natural language understanding tasks.", "journal": ""}
{"doi": "10.48550/arXiv.2312.04463", "date": "2023-12-07", "title": "Leveraging Transformer-based Language Models to Automate Requirements Satisfaction Assessment", "authors": "Amrit Poudel, Jinfeng Lin, Jane Cleland-Huang", "abstract": "Requirements Satisfaction Assessment (RSA) evaluates whether the set of\ndesign elements linked to a single requirement provide sufficient coverage of\nthat requirement -- typically meaning that all concepts in the requirement are\naddressed by at least one of the design elements. RSA is an important software\nengineering activity for systems with any form of hierarchical decomposition --\nespecially safety or mission critical ones. In previous studies, researchers\nused basic Information Retrieval (IR) models to decompose requirements and\ndesign elements into chunks, and then evaluated the extent to which chunks of\ndesign elements covered all chunks in the requirement. However, results had low\naccuracy because many critical concepts that extend across the entirety of the\nsentence were not well represented when the sentence was parsed into\nindependent chunks. In this paper we leverage recent advances in natural\nlanguage processing to deliver significantly more accurate results. We propose\ntwo major architectures: Satisfaction BERT (Sat-BERT), and Dual-Satisfaction\nBERT (DSat-BERT), along with their multitask learning variants to improve\nsatisfaction assessments. We perform RSA on five different datasets and compare\nresults from our variants against the chunk-based legacy approach. All\nBERT-based models significantly outperformed the legacy baseline, and Sat-BERT\ndelivered the best results returning an average improvement of 124.75% in Mean\nAverage Precision.", "journal": ""}
{"doi": "10.48550/arXiv.2403.08217", "date": "2024-03-13", "title": "Research on the Application of Deep Learning-based BERT Model in Sentiment Analysis", "authors": "Yichao Wu, Zhengyu Jin, Chenxi Shi, Penghao Liang, Tong Zhan", "abstract": "This paper explores the application of deep learning techniques, particularly\nfocusing on BERT models, in sentiment analysis. It begins by introducing the\nfundamental concept of sentiment analysis and how deep learning methods are\nutilized in this domain. Subsequently, it delves into the architecture and\ncharacteristics of BERT models. Through detailed explanation, it elucidates the\napplication effects and optimization strategies of BERT models in sentiment\nanalysis, supported by experimental validation. The experimental findings\nindicate that BERT models exhibit robust performance in sentiment analysis\ntasks, with notable enhancements post fine-tuning. Lastly, the paper concludes\nby summarizing the potential applications of BERT models in sentiment analysis\nand suggests directions for future research and practical implementations.", "journal": ""}
{"doi": "10.48550/arXiv.2410.17957", "date": "2024-10-23", "title": "MCUBERT: Memory-Efficient BERT Inference on Commodity Microcontrollers", "authors": "Zebin Yang, Renze Chen, Taiqiang Wu, Ngai Wong, Yun Liang, Runsheng Wang, Ru Huang, Meng Li", "abstract": "In this paper, we propose MCUBERT to enable language models like BERT on tiny\nmicrocontroller units (MCUs) through network and scheduling co-optimization. We\nobserve the embedding table contributes to the major storage bottleneck for\ntiny BERT models. Hence, at the network level, we propose an MCU-aware\ntwo-stage neural architecture search algorithm based on clustered low-rank\napproximation for embedding compression. To reduce the inference memory\nrequirements, we further propose a novel fine-grained MCU-friendly scheduling\nstrategy. Through careful computation tiling and re-ordering as well as kernel\ndesign, we drastically increase the input sequence lengths supported on MCUs\nwithout any latency or accuracy penalty. MCUBERT reduces the parameter size of\nBERT-tiny and BERT-mini by 5.7$\\times$ and 3.0$\\times$ and the execution memory\nby 3.5$\\times$ and 4.3$\\times$, respectively. MCUBERT also achieves 1.5$\\times$\nlatency reduction. For the first time, MCUBERT enables lightweight BERT models\non commodity MCUs and processing more than 512 tokens with less than 256KB of\nmemory.", "journal": ""}
{"doi": "10.48550/arXiv.2411.17661", "date": "2024-11-26", "title": "Non-Contextual BERT or FastText? A Comparative Analysis", "authors": "Abhay Shanbhag, Suramya Jadhav, Amogh Thakurdesai, Ridhima Sinare, Raviraj Joshi", "abstract": "Natural Language Processing (NLP) for low-resource languages, which lack\nlarge annotated datasets, faces significant challenges due to limited\nhigh-quality data and linguistic resources. The selection of embeddings plays a\ncritical role in achieving strong performance in NLP tasks. While contextual\nBERT embeddings require a full forward pass, non-contextual BERT embeddings\nrely only on table lookup. Existing research has primarily focused on\ncontextual BERT embeddings, leaving non-contextual embeddings largely\nunexplored. In this study, we analyze the effectiveness of non-contextual\nembeddings from BERT models (MuRIL and MahaBERT) and FastText models (IndicFT\nand MahaFT) for tasks such as news classification, sentiment analysis, and hate\nspeech detection in one such low-resource language Marathi. We compare these\nembeddings with their contextual and compressed variants. Our findings indicate\nthat non-contextual BERT embeddings extracted from the model's first embedding\nlayer outperform FastText embeddings, presenting a promising alternative for\nlow-resource NLP.", "journal": ""}
{"doi": "10.48550/arXiv.2501.08053", "date": "2025-01-14", "title": "Exploring Narrative Clustering in Large Language Models: A Layerwise Analysis of BERT", "authors": "Awritrojit Banerjee, Achim Schilling, Patrick Krauss", "abstract": "This study investigates the internal mechanisms of BERT, a transformer-based\nlarge language model, with a focus on its ability to cluster narrative content\nand authorial style across its layers. Using a dataset of narratives developed\nvia GPT-4, featuring diverse semantic content and stylistic variations, we\nanalyze BERT's layerwise activations to uncover patterns of localized neural\nprocessing. Through dimensionality reduction techniques such as Principal\nComponent Analysis (PCA) and Multidimensional Scaling (MDS), we reveal that\nBERT exhibits strong clustering based on narrative content in its later layers,\nwith progressively compact and distinct clusters. While strong stylistic\nclustering might occur when narratives are rephrased into different text types\n(e.g., fables, sci-fi, kids' stories), minimal clustering is observed for\nauthorial style specific to individual writers. These findings highlight BERT's\nprioritization of semantic content over stylistic features, offering insights\ninto its representational capabilities and processing hierarchy. This study\ncontributes to understanding how transformer models like BERT encode linguistic\ninformation, paving the way for future interdisciplinary research in artificial\nintelligence and cognitive neuroscience.", "journal": ""}
{"doi": "10.48550/arXiv.2502.19593", "date": "2025-02-26", "title": "Improving Representation Learning of Complex Critical Care Data with ICU-BERT", "authors": "Ricardo Santos, Andr\u00e9 V. Carreiro, Xi Peng, Hugo Gamboa, Holger Fr\u00f6hlich", "abstract": "The multivariate, asynchronous nature of real-world clinical data, such as\nthat generated in Intensive Care Units (ICUs), challenges traditional AI-based\ndecision-support systems. These often assume data regularity and feature\nindependence and frequently rely on limited data scopes and manual feature\nengineering. The potential of generative AI technologies has not yet been fully\nexploited to analyze clinical data. We introduce ICU-BERT, a transformer-based\nmodel pre-trained on the MIMIC-IV database using a multi-task scheme to learn\nrobust representations of complex ICU data with minimal preprocessing. ICU-BERT\nemploys a multi-token input strategy, incorporating dense embeddings from a\nbiomedical Large Language Model to learn a generalizable representation of\ncomplex and multivariate ICU data. With an initial evaluation of five tasks and\nfour additional ICU datasets, ICU-BERT results indicate that ICU-BERT either\ncompares to or surpasses current performance benchmarks by leveraging\nfine-tuning. By integrating structured and unstructured data, ICU-BERT advances\nthe use of foundational models in medical informatics, offering an adaptable\nsolution for clinical decision support across diverse applications.", "journal": ""}
{"doi": "10.48550/arXiv.2111.08585", "date": "2021-11-10", "title": "CEHR-BERT: Incorporating temporal information from structured EHR data to improve prediction tasks", "authors": "Chao Pang, Xinzhuo Jiang, Krishna S Kalluri, Matthew Spotnitz, RuiJun Chen, Adler Perotte, Karthik Natarajan", "abstract": "Embedding algorithms are increasingly used to represent clinical concepts in\nhealthcare for improving machine learning tasks such as clinical phenotyping\nand disease prediction. Recent studies have adapted state-of-the-art\nbidirectional encoder representations from transformers (BERT) architecture to\nstructured electronic health records (EHR) data for the generation of\ncontextualized concept embeddings, yet do not fully incorporate temporal data\nacross multiple clinical domains. Therefore we developed a new BERT adaptation,\nCEHR-BERT, to incorporate temporal information using a hybrid approach by\naugmenting the input to BERT using artificial time tokens, incorporating time,\nage, and concept embeddings, and introducing a new second learning objective\nfor visit type. CEHR-BERT was trained on a subset of Columbia University Irving\nMedical Center-York Presbyterian Hospital's clinical data, which includes 2.4M\npatients, spanning over three decades, and tested using 4-fold cross-validation\non the following prediction tasks: hospitalization, death, new heart failure\n(HF) diagnosis, and HF readmission. Our experiments show that CEHR-BERT\noutperformed existing state-of-the-art clinical BERT adaptations and baseline\nmodels across all 4 prediction tasks in both ROC-AUC and PR-AUC. CEHR-BERT also\ndemonstrated strong transfer learning capability, as our model trained on only\n5% of data outperformed comparison models trained on the entire data set.\nAblation studies to better understand the contribution of each time component\nshowed incremental gains with every element, suggesting that CEHR-BERT's\nincorporation of artificial time tokens, time and age embeddings with concept\nembeddings, and the addition of the second learning objective represents a\npromising approach for future BERT-based clinical embeddings.", "journal": "Proceedings of Machine Learning for Health, PMLR 158:239-260, 2021"}
{"doi": "10.48550/arXiv.2007.15356", "date": "2020-07-30", "title": "What does BERT know about books, movies and music? Probing BERT for Conversational Recommendation", "authors": "Gustavo Penha, Claudia Hauff", "abstract": "Heavily pre-trained transformer models such as BERT have recently shown to be\nremarkably powerful at language modelling by achieving impressive results on\nnumerous downstream tasks. It has also been shown that they are able to\nimplicitly store factual knowledge in their parameters after pre-training.\nUnderstanding what the pre-training procedure of LMs actually learns is a\ncrucial step for using and improving them for Conversational Recommender\nSystems (CRS). We first study how much off-the-shelf pre-trained BERT \"knows\"\nabout recommendation items such as books, movies and music. In order to analyze\nthe knowledge stored in BERT's parameters, we use different probes that require\ndifferent types of knowledge to solve, namely content-based and\ncollaborative-based. Content-based knowledge is knowledge that requires the\nmodel to match the titles of items with their content information, such as\ntextual descriptions and genres. In contrast, collaborative-based knowledge\nrequires the model to match items with similar ones, according to community\ninteractions such as ratings. We resort to BERT's Masked Language Modelling\nhead to probe its knowledge about the genre of items, with cloze style prompts.\nIn addition, we employ BERT's Next Sentence Prediction head and\nrepresentations' similarity to compare relevant and non-relevant search and\nrecommendation query-document inputs to explore whether BERT can, without any\nfine-tuning, rank relevant items first. Finally, we study how BERT performs in\na conversational recommendation downstream task. Overall, our analyses and\nexperiments show that: (i) BERT has knowledge stored in its parameters about\nthe content of books, movies and music; (ii) it has more content-based\nknowledge than collaborative-based knowledge; and (iii) fails on conversational\nrecommendation when faced with adversarial data.", "journal": ""}
{"doi": "10.48550/arXiv.2003.02912", "date": "2020-03-05", "title": "What the [MASK]? Making Sense of Language-Specific BERT Models", "authors": "Debora Nozza, Federico Bianchi, Dirk Hovy", "abstract": "Recently, Natural Language Processing (NLP) has witnessed an impressive\nprogress in many areas, due to the advent of novel, pretrained contextual\nrepresentation models. In particular, Devlin et al. (2019) proposed a model,\ncalled BERT (Bidirectional Encoder Representations from Transformers), which\nenables researchers to obtain state-of-the art performance on numerous NLP\ntasks by fine-tuning the representations on their data set and task, without\nthe need for developing and training highly-specific architectures. The authors\nalso released multilingual BERT (mBERT), a model trained on a corpus of 104\nlanguages, which can serve as a universal language model. This model obtained\nimpressive results on a zero-shot cross-lingual natural inference task. Driven\nby the potential of BERT models, the NLP community has started to investigate\nand generate an abundant number of BERT models that are trained on a particular\nlanguage, and tested on a specific data domain and task. This allows us to\nevaluate the true potential of mBERT as a universal language model, by\ncomparing it to the performance of these more specific models. This paper\npresents the current state of the art in language-specific BERT models,\nproviding an overall picture with respect to different dimensions (i.e.\narchitectures, data domains, and tasks). Our aim is to provide an immediate and\nstraightforward overview of the commonalities and differences between\nLanguage-Specific (language-specific) BERT models and mBERT. We also provide an\ninteractive and constantly updated website that can be used to explore the\ninformation we have collected, at https://bertlang.unibocconi.it.", "journal": ""}
{"doi": "10.48550/arXiv.2201.04458", "date": "2022-01-12", "title": "Diagnosing BERT with Retrieval Heuristics", "authors": "Arthur C\u00e2mara, Claudia Hauff", "abstract": "Word embeddings, made widely popular in 2013 with the release of word2vec,\nhave become a mainstay of NLP engineering pipelines. Recently, with the release\nof BERT, word embeddings have moved from the term-based embedding space to the\ncontextual embedding space -- each term is no longer represented by a single\nlow-dimensional vector but instead each term and \\emph{its context} determine\nthe vector weights. BERT's setup and architecture have been shown to be general\nenough to be applicable to many natural language tasks. Importantly for\nInformation Retrieval (IR), in contrast to prior deep learning solutions to IR\nproblems which required significant tuning of neural net architectures and\ntraining regimes, \"vanilla BERT\" has been shown to outperform existing\nretrieval algorithms by a wide margin, including on tasks and corpora that have\nlong resisted retrieval effectiveness gains over traditional IR baselines (such\nas Robust04). In this paper, we employ the recently proposed axiomatic dataset\nanalysis technique -- that is, we create diagnostic datasets that each fulfil a\nretrieval heuristic (both term matching and semantic-based) -- to explore what\nBERT is able to learn. In contrast to our expectations, we find BERT, when\napplied to a recently released large-scale web corpus with ad-hoc topics, to\n\\emph{not} adhere to any of the explored axioms. At the same time, BERT\noutperforms the traditional query likelihood retrieval model by 40\\%. This\nmeans that the axiomatic approach to IR (and its extension of diagnostic\ndatasets created for retrieval heuristics) may in its current form not be\napplicable to large-scale corpora. Additional -- different -- axioms are\nneeded.", "journal": "Advances in Information Retrieval. 2020;12035:605-618. Published\n  2020 Mar 17"}
{"doi": "10.48550/arXiv.2202.12142", "date": "2022-02-24", "title": "Pretraining without Wordpieces: Learning Over a Vocabulary of Millions of Words", "authors": "Zhangyin Feng, Duyu Tang, Cong Zhou, Junwei Liao, Shuangzhi Wu, Xiaocheng Feng, Bing Qin, Yunbo Cao, Shuming Shi", "abstract": "The standard BERT adopts subword-based tokenization, which may break a word\ninto two or more wordpieces (e.g., converting \"lossless\" to \"loss\" and \"less\").\nThis will bring inconvenience in following situations: (1) what is the best way\nto obtain the contextual vector of a word that is divided into multiple\nwordpieces? (2) how to predict a word via cloze test without knowing the number\nof wordpieces in advance? In this work, we explore the possibility of\ndeveloping BERT-style pretrained model over a vocabulary of words instead of\nwordpieces. We call such word-level BERT model as WordBERT. We train models\nwith different vocabulary sizes, initialization configurations and languages.\nResults show that, compared to standard wordpiece-based BERT, WordBERT makes\nsignificant improvements on cloze test and machine reading comprehension. On\nmany other natural language understanding tasks, including POS tagging,\nchunking and NER, WordBERT consistently performs better than BERT. Model\nanalysis indicates that the major advantage of WordBERT over BERT lies in the\nunderstanding for low-frequency words and rare words. Furthermore, since the\npipeline is language-independent, we train WordBERT for Chinese language and\nobtain significant gains on five natural language understanding datasets.\nLastly, the analyse on inference speed illustrates WordBERT has comparable time\ncost to BERT in natural language understanding tasks.", "journal": ""}
{"doi": "10.48550/arXiv.2001.06286", "date": "2020-01-17", "title": "RobBERT: a Dutch RoBERTa-based Language Model", "authors": "Pieter Delobelle, Thomas Winters, Bettina Berendt", "abstract": "Pre-trained language models have been dominating the field of natural\nlanguage processing in recent years, and have led to significant performance\ngains for various complex natural language tasks. One of the most prominent\npre-trained language models is BERT, which was released as an English as well\nas a multilingual version. Although multilingual BERT performs well on many\ntasks, recent studies show that BERT models trained on a single language\nsignificantly outperform the multilingual version. Training a Dutch BERT model\nthus has a lot of potential for a wide range of Dutch NLP tasks. While previous\napproaches have used earlier implementations of BERT to train a Dutch version\nof BERT, we used RoBERTa, a robustly optimized BERT approach, to train a Dutch\nlanguage model called RobBERT. We measured its performance on various tasks as\nwell as the importance of the fine-tuning dataset size. We also evaluated the\nimportance of language-specific tokenizers and the model's fairness. We found\nthat RobBERT improves state-of-the-art results for various tasks, and\nespecially significantly outperforms other models when dealing with smaller\ndatasets. These results indicate that it is a powerful pre-trained model for a\nlarge variety of Dutch language tasks. The pre-trained and fine-tuned models\nare publicly available to support further downstream Dutch NLP applications.", "journal": ""}
{"doi": "10.48550/arXiv.2002.06275", "date": "2020-02-14", "title": "TwinBERT: Distilling Knowledge to Twin-Structured BERT Models for Efficient Retrieval", "authors": "Wenhao Lu, Jian Jiao, Ruofei Zhang", "abstract": "Pre-trained language models like BERT have achieved great success in a wide\nvariety of NLP tasks, while the superior performance comes with high demand in\ncomputational resources, which hinders the application in low-latency IR\nsystems. We present TwinBERT model for effective and efficient retrieval, which\nhas twin-structured BERT-like encoders to represent query and document\nrespectively and a crossing layer to combine the embeddings and produce a\nsimilarity score. Different from BERT, where the two input sentences are\nconcatenated and encoded together, TwinBERT decouples them during encoding and\nproduces the embeddings for query and document independently, which allows\ndocument embeddings to be pre-computed offline and cached in memory. Thereupon,\nthe computation left for run-time is from the query encoding and query-document\ncrossing only. This single change can save large amount of computation time and\nresources, and therefore significantly improve serving efficiency. Moreover, a\nfew well-designed network layers and training strategies are proposed to\nfurther reduce computational cost while at the same time keep the performance\nas remarkable as BERT model. Lastly, we develop two versions of TwinBERT for\nretrieval and relevance tasks correspondingly, and both of them achieve close\nor on-par performance to BERT-Base model.\n  The model was trained following the teacher-student framework and evaluated\nwith data from one of the major search engines. Experimental results showed\nthat the inference time was significantly reduced and was firstly controlled\naround 20ms on CPUs while at the same time the performance gain from fine-tuned\nBERT-Base model was mostly retained. Integration of the models into production\nsystems also demonstrated remarkable improvements on relevance metrics with\nnegligible influence on latency.", "journal": ""}
{"doi": "10.48550/arXiv.2002.10832", "date": "2020-02-25", "title": "What BERT Sees: Cross-Modal Transfer for Visual Question Generation", "authors": "Thomas Scialom, Patrick Bordes, Paul-Alexis Dray, Jacopo Staiano, Patrick Gallinari", "abstract": "Pre-trained language models have recently contributed to significant advances\nin NLP tasks. Recently, multi-modal versions of BERT have been developed, using\nheavy pre-training relying on vast corpora of aligned textual and image data,\nprimarily applied to classification tasks such as VQA. In this paper, we are\ninterested in evaluating the visual capabilities of BERT out-of-the-box, by\navoiding pre-training made on supplementary data. We choose to study Visual\nQuestion Generation, a task of great interest for grounded dialog, that enables\nto study the impact of each modality (as input can be visual and/or textual).\nMoreover, the generation aspect of the task requires an adaptation since BERT\nis primarily designed as an encoder. We introduce BERT-gen, a BERT-based\narchitecture for text generation, able to leverage on either mono- or multi-\nmodal representations. The results reported under different configurations\nindicate an innate capacity for BERT-gen to adapt to multi-modal data and text\ngeneration, even with few data available, avoiding expensive pre-training. The\nproposed model obtains substantial improvements over the state-of-the-art on\ntwo established VQG datasets.", "journal": ""}
{"doi": "10.48550/arXiv.2005.12833", "date": "2020-05-22", "title": "Med-BERT: pre-trained contextualized embeddings on large-scale structured electronic health records for disease prediction", "authors": "Laila Rasmy, Yang Xiang, Ziqian Xie, Cui Tao, Degui Zhi", "abstract": "Deep learning (DL) based predictive models from electronic health records\n(EHR) deliver impressive performance in many clinical tasks. Large training\ncohorts, however, are often required to achieve high accuracy, hindering the\nadoption of DL-based models in scenarios with limited training data size.\nRecently, bidirectional encoder representations from transformers (BERT) and\nrelated models have achieved tremendous successes in the natural language\nprocessing domain. The pre-training of BERT on a very large training corpus\ngenerates contextualized embeddings that can boost the performance of models\ntrained on smaller datasets. We propose Med-BERT, which adapts the BERT\nframework for pre-training contextualized embedding models on structured\ndiagnosis data from 28,490,650 patients EHR dataset. Fine-tuning experiments\nare conducted on two disease-prediction tasks: (1) prediction of heart failure\nin patients with diabetes and (2) prediction of pancreatic cancer from two\nclinical databases. Med-BERT substantially improves prediction accuracy,\nboosting the area under receiver operating characteristics curve (AUC) by\n2.02-7.12%. In particular, pre-trained Med-BERT substantially improves the\nperformance of tasks with very small fine-tuning training sets (300-500\nsamples) boosting the AUC by more than 20% or equivalent to the AUC of 10 times\nlarger training set. We believe that Med-BERT will benefit disease-prediction\nstudies with small local training datasets, reduce data collection expenses,\nand accelerate the pace of artificial intelligence aided healthcare.", "journal": ""}
{"doi": "10.48550/arXiv.2007.01127", "date": "2020-07-02", "title": "Bidirectional Encoder Representations from Transformers (BERT): A sentiment analysis odyssey", "authors": "Shivaji Alaparthi, Manit Mishra", "abstract": "The purpose of the study is to investigate the relative effectiveness of four\ndifferent sentiment analysis techniques: (1) unsupervised lexicon-based model\nusing Sent WordNet; (2) traditional supervised machine learning model using\nlogistic regression; (3) supervised deep learning model using Long Short-Term\nMemory (LSTM); and, (4) advanced supervised deep learning models using\nBidirectional Encoder Representations from Transformers (BERT). We use publicly\navailable labeled corpora of 50,000 movie reviews originally posted on internet\nmovie database (IMDB) for analysis using Sent WordNet lexicon, logistic\nregression, LSTM, and BERT. The first three models were run on CPU based system\nwhereas BERT was run on GPU based system. The sentiment classification\nperformance was evaluated based on accuracy, precision, recall, and F1 score.\nThe study puts forth two key insights: (1) relative efficacy of four highly\nadvanced and widely used sentiment analysis techniques; (2) undisputed\nsuperiority of pre-trained advanced supervised deep learning BERT model in\nsentiment analysis from text data. This study provides professionals in\nanalytics industry and academicians working on text analysis key insight\nregarding comparative classification performance evaluation of key sentiment\nanalysis techniques, including the recently developed BERT. This is the first\nresearch endeavor to compare the advanced pre-trained supervised deep learning\nmodel of BERT vis-\\`a-vis other sentiment analysis models of LSTM, logistic\nregression, and Sent WordNet.", "journal": ""}
{"doi": "10.48550/arXiv.2009.05959", "date": "2020-09-13", "title": "BoostingBERT:Integrating Multi-Class Boosting into BERT for NLP Tasks", "authors": "Tongwen Huang, Qingyun She, Junlin Zhang", "abstract": "As a pre-trained Transformer model, BERT (Bidirectional Encoder\nRepresentations from Transformers) has achieved ground-breaking performance on\nmultiple NLP tasks. On the other hand, Boosting is a popular ensemble learning\ntechnique which combines many base classifiers and has been demonstrated to\nyield better generalization performance in many machine learning tasks. Some\nworks have indicated that ensemble of BERT can further improve the application\nperformance. However, current ensemble approaches focus on bagging or stacking\nand there has not been much effort on exploring the boosting. In this work, we\nproposed a novel Boosting BERT model to integrate multi-class boosting into the\nBERT. Our proposed model uses the pre-trained Transformer as the base\nclassifier to choose harder training sets to fine-tune and gains the benefits\nof both the pre-training language knowledge and boosting ensemble in NLP tasks.\nWe evaluate the proposed model on the GLUE dataset and 3 popular Chinese NLU\nbenchmarks. Experimental results demonstrate that our proposed model\nsignificantly outperforms BERT on all datasets and proves its effectiveness in\nmany NLP tasks. Replacing the BERT base with RoBERTa as base classifier,\nBoostingBERT achieves new state-of-the-art results in several NLP Tasks. We\nalso use knowledge distillation within the \"teacher-student\" framework to\nreduce the computational overhead and model storage of BoostingBERT while\nkeeping its performance for practical application.", "journal": ""}
{"doi": "10.48550/arXiv.2104.07252", "date": "2021-04-15", "title": "Emotion Dynamics Modeling via BERT", "authors": "Haiqin Yang, Jianping Shen", "abstract": "Emotion dynamics modeling is a significant task in emotion recognition in\nconversation. It aims to predict conversational emotions when building\nempathetic dialogue systems. Existing studies mainly develop models based on\nRecurrent Neural Networks (RNNs). They cannot benefit from the power of the\nrecently-developed pre-training strategies for better token representation\nlearning in conversations. More seriously, it is hard to distinguish the\ndependency of interlocutors and the emotional influence among interlocutors by\nsimply assembling the features on top of RNNs. In this paper, we develop a\nseries of BERT-based models to specifically capture the inter-interlocutor and\nintra-interlocutor dependencies of the conversational emotion dynamics.\nConcretely, we first substitute BERT for RNNs to enrich the token\nrepresentations. Then, a Flat-structured BERT (F-BERT) is applied to link up\nutterances in a conversation directly, and a Hierarchically-structured BERT\n(H-BERT) is employed to distinguish the interlocutors when linking up\nutterances. More importantly, a Spatial-Temporal-structured BERT, namely\nST-BERT, is proposed to further determine the emotional influence among\ninterlocutors. Finally, we conduct extensive experiments on two popular emotion\nrecognition in conversation benchmark datasets and demonstrate that our\nproposed models can attain around 5\\% and 10\\% improvement over the\nstate-of-the-art baselines, respectively.", "journal": ""}
{"doi": "10.48550/arXiv.2106.12744", "date": "2021-06-24", "title": "An Automated Knowledge Mining and Document Classification System with Multi-model Transfer Learning", "authors": "Jia Wei Chong, Zhiyuan Chen, Mei Shin Oh", "abstract": "Service manual documents are crucial to the engineering company as they\nprovide guidelines and knowledge to service engineers. However, it has become\ninconvenient and inefficient for service engineers to retrieve specific\nknowledge from documents due to the complexity of resources. In this research,\nwe propose an automated knowledge mining and document classification system\nwith novel multi-model transfer learning approaches. Particularly, the\nclassification performance of the system has been improved with three effective\ntechniques: fine-tuning, pruning, and multi-model method. The fine-tuning\ntechnique optimizes a pre-trained BERT model by adding a feed-forward neural\nnetwork layer and the pruning technique is used to retrain the BERT model with\nnew data. The multi-model method initializes and trains multiple BERT models to\novercome the randomness of data ordering during the fine-tuning process. In the\nfirst iteration of the training process, multiple BERT models are being trained\nsimultaneously. The best model is then selected for the next phase of the\ntraining process with another two iterations and the training processes for\nother BERT models will be terminated. The performance of the proposed system\nhas been evaluated by comparing with two robust baseline methods, BERT and\nBERT-CNN. Experimental results on a widely used Corpus of Linguistic\nAcceptability (CoLA) dataset have shown that the proposed techniques perform\nbetter than these baseline methods in terms of accuracy and MCC score.", "journal": ""}
{"doi": "10.48550/arXiv.2112.14169", "date": "2021-12-28", "title": "Fast Changeset-based Bug Localization with BERT", "authors": "Agnieszka Ciborowska, Kostadin Damevski", "abstract": "Automatically localizing software bugs to the changesets that induced them\nhas the potential to improve software developer efficiency and to positively\naffect software quality. To facilitate this automation, a bug report has to be\neffectively matched with source code changes, even when a significant lexical\ngap exists between natural language used to describe the bug and identifier\nnaming practices used by developers. To bridge this gap, we need techniques\nthat are able to capture software engineering-specific and project-specific\nsemantics in order to detect relatedness between the two types of documents\nthat goes beyond exact term matching. Popular transformer-based deep learning\narchitectures, such as BERT, excel at leveraging contextual information, hence\nappear to be a suitable candidate for the task. However, BERT-like models are\ncomputationally expensive, which precludes them from being used in an\nenvironment where response time is important. In this paper, we describe how\nBERT can be made fast enough to be applicable to changeset-based bug\nlocalization. We also explore several design decisions in using BERT for this\npurpose, including how best to encode changesets and how to match bug reports\nto individual changes for improved accuracy. We compare the accuracy and\nperformance of our model to a non-contextual baseline (i.e., vector space\nmodel) and BERT-based architectures previously used in software engineering.\nOur evaluation results demonstrate advantages in using the proposed BERT model\ncompared to the baselines, especially for bug reports that lack any hints about\nrelated code elements.", "journal": ""}
{"doi": "10.48550/arXiv.2206.04184", "date": "2022-06-08", "title": "Abstraction not Memory: BERT and the English Article System", "authors": "Harish Tayyar Madabushi, Dagmar Divjak, Petar Milin", "abstract": "Article prediction is a task that has long defied accurate linguistic\ndescription. As such, this task is ideally suited to evaluate models on their\nability to emulate native-speaker intuition. To this end, we compare the\nperformance of native English speakers and pre-trained models on the task of\narticle prediction set up as a three way choice (a/an, the, zero). Our\nexperiments with BERT show that BERT outperforms humans on this task across all\narticles. In particular, BERT is far superior to humans at detecting the zero\narticle, possibly because we insert them using rules that the deep neural model\ncan easily pick up. More interestingly, we find that BERT tends to agree more\nwith annotators than with the corpus when inter-annotator agreement is high but\nswitches to agreeing more with the corpus as inter-annotator agreement drops.\nWe contend that this alignment with annotators, despite being trained on the\ncorpus, suggests that BERT is not memorising article use, but captures a high\nlevel generalisation of article use akin to human intuition.", "journal": ""}
{"doi": "10.48550/arXiv.2207.07116", "date": "2022-07-14", "title": "Bootstrapped Masked Autoencoders for Vision BERT Pretraining", "authors": "Xiaoyi Dong, Jianmin Bao, Ting Zhang, Dongdong Chen, Weiming Zhang, Lu Yuan, Dong Chen, Fang Wen, Nenghai Yu", "abstract": "We propose bootstrapped masked autoencoders (BootMAE), a new approach for\nvision BERT pretraining. BootMAE improves the original masked autoencoders\n(MAE) with two core designs: 1) momentum encoder that provides online feature\nas extra BERT prediction targets; 2) target-aware decoder that tries to reduce\nthe pressure on the encoder to memorize target-specific information in BERT\npretraining. The first design is motivated by the observation that using a\npretrained MAE to extract the features as the BERT prediction target for masked\ntokens can achieve better pretraining performance. Therefore, we add a momentum\nencoder in parallel with the original MAE encoder, which bootstraps the\npretraining performance by using its own representation as the BERT prediction\ntarget. In the second design, we introduce target-specific information (e.g.,\npixel values of unmasked patches) from the encoder directly to the decoder to\nreduce the pressure on the encoder of memorizing the target-specific\ninformation. Thus, the encoder focuses on semantic modeling, which is the goal\nof BERT pretraining, and does not need to waste its capacity in memorizing the\ninformation of unmasked tokens related to the prediction target. Through\nextensive experiments, our BootMAE achieves $84.2\\%$ Top-1 accuracy on\nImageNet-1K with ViT-B backbone, outperforming MAE by $+0.8\\%$ under the same\npre-training epochs. BootMAE also gets $+1.0$ mIoU improvements on semantic\nsegmentation on ADE20K and $+1.3$ box AP, $+1.4$ mask AP improvement on object\ndetection and segmentation on COCO dataset. Code is released at\nhttps://github.com/LightDXY/BootMAE.", "journal": ""}
{"doi": "10.48550/arXiv.2207.12376", "date": "2022-07-25", "title": "Fine-Tuning BERT for Automatic ADME Semantic Labeling in FDA Drug Labeling to Enhance Product-Specific Guidance Assessment", "authors": "Yiwen Shi, Jing Wang, Ping Ren, Taha ValizadehAslani, Yi Zhang, Meng Hu, Hualou Liang", "abstract": "Product-specific guidances (PSGs) recommended by the United States Food and\nDrug Administration (FDA) are instrumental to promote and guide generic drug\nproduct development. To assess a PSG, the FDA assessor needs to take extensive\ntime and effort to manually retrieve supportive drug information of absorption,\ndistribution, metabolism, and excretion (ADME) from the reference listed drug\nlabeling. In this work, we leveraged the state-of-the-art pre-trained language\nmodels to automatically label the ADME paragraphs in the pharmacokinetics\nsection from the FDA-approved drug labeling to facilitate PSG assessment. We\napplied a transfer learning approach by fine-tuning the pre-trained\nBidirectional Encoder Representations from Transformers (BERT) model to develop\na novel application of ADME semantic labeling, which can automatically retrieve\nADME paragraphs from drug labeling instead of manual work. We demonstrated that\nfine-tuning the pre-trained BERT model can outperform the conventional machine\nlearning techniques, achieving up to 11.6% absolute F1 improvement. To our\nknowledge, we were the first to successfully apply BERT to solve the ADME\nsemantic labeling task. We further assessed the relative contribution of\npre-training and fine-tuning to the overall performance of the BERT model in\nthe ADME semantic labeling task using a series of analysis methods such as\nattention similarity and layer-based ablations. Our analysis revealed that the\ninformation learned via fine-tuning is focused on task-specific knowledge in\nthe top layers of the BERT, whereas the benefit from the pre-trained BERT model\nis from the bottom layers.", "journal": ""}
{"doi": "10.48550/arXiv.2207.13226", "date": "2022-07-27", "title": "Boosting Point-BERT by Multi-choice Tokens", "authors": "Kexue Fu, Mingzhi Yuan, Manning Wang", "abstract": "Masked language modeling (MLM) has become one of the most successful\nself-supervised pre-training task. Inspired by its success, Point-BERT, as a\npioneer work in point cloud, proposed masked point modeling (MPM) to pre-train\npoint transformer on large scale unanotated dataset. Despite its great\nperformance, we find the inherent difference between language and point cloud\ntends to cause ambiguous tokenization for point cloud. For point cloud, there\ndoesn't exist a gold standard for point cloud tokenization. Point-BERT use a\ndiscrete Variational AutoEncoder (dVAE) as tokenizer, but it might generate\ndifferent token ids for semantically-similar patches and generate the same\ntoken ids for semantically-dissimilar patches. To tackle above problem, we\npropose our McP-BERT, a pre-training framework with multi-choice tokens.\nSpecifically, we ease the previous single-choice constraint on patch token ids\nin Point-BERT, and provide multi-choice token ids for each patch as\nsupervision. Moreover, we utilitze the high-level semantics learned by\ntransformer to further refine our supervision signals. Extensive experiments on\npoint cloud classification, few-shot classification and part segmentation tasks\ndemonstrate the superiority of our method, e.g., the pre-trained transformer\nachieves 94.1% accuracy on ModelNet40, 84.28% accuracy on the hardest setting\nof ScanObjectNN and new state-of-the-art performance on few-shot learning. We\nalso demonstrate that our method not only improves the performance of\nPoint-BERT on all downstream tasks, but also incurs almost no extra\ncomputational overhead. The code will be released in\nhttps://github.com/fukexue/McP-BERT.", "journal": ""}
{"doi": "10.48550/arXiv.2209.06344", "date": "2022-09-13", "title": "CNN-Trans-Enc: A CNN-Enhanced Transformer-Encoder On Top Of Static BERT representations for Document Classification", "authors": "Charaf Eddine Benarab, Shenglin Gui", "abstract": "BERT achieves remarkable results in text classification tasks, it is yet not\nfully exploited, since only the last layer is used as a representation output\nfor downstream classifiers. The most recent studies on the nature of linguistic\nfeatures learned by BERT, suggest that different layers focus on different\nkinds of linguistic features. We propose a CNN-Enhanced Transformer-Encoder\nmodel which is trained on top of fixed BERT $[CLS]$ representations from all\nlayers, employing Convolutional Neural Networks to generate QKV feature maps\ninside the Transformer-Encoder, instead of linear projections of the input into\nthe embedding space. CNN-Trans-Enc is relatively small as a downstream\nclassifier and doesn't require any fine-tuning of BERT, as it ensures an\noptimal use of the $[CLS]$ representations from all layers, leveraging\ndifferent linguistic features with more meaningful, and generalizable QKV\nrepresentations of the input. Using BERT with CNN-Trans-Enc keeps $98.9\\%$ and\n$94.8\\%$ of current state-of-the-art performance on the IMDB and SST-5 datasets\nrespectably, while obtaining new state-of-the-art on YELP-5 with $82.23$\n($8.9\\%$ improvement), and on Amazon-Polarity with $0.98\\%$ ($0.2\\%$\nimprovement) (K-fold Cross Validation on a 1M sample subset from both\ndatasets). On the AG news dataset CNN-Trans-Enc achieves $99.94\\%$ of the\ncurrent state-of-the-art, and achieves a new top performance with an average\naccuracy of $99.51\\%$ on DBPedia-14.\n  Index terms: Text Classification, Natural Language Processing, Convolutional\nNeural Networks, Transformers, BERT", "journal": ""}
{"doi": "10.48550/arXiv.2209.09815", "date": "2022-09-20", "title": "Towards Fine-tuning Pre-trained Language Models with Integer Forward and Backward Propagation", "authors": "Mohammadreza Tayaranian, Alireza Ghaffari, Marzieh S. Tahaei, Mehdi Rezagholizadeh, Masoud Asgharian, Vahid Partovi Nia", "abstract": "The large number of parameters of some prominent language models, such as\nBERT, makes their fine-tuning on downstream tasks computationally intensive and\nenergy hungry. Previously researchers were focused on lower bit-width integer\ndata types for the forward propagation of language models to save memory and\ncomputation. As for the backward propagation, however, only 16-bit\nfloating-point data type has been used for the fine-tuning of BERT. In this\nwork, we use integer arithmetic for both forward and back propagation in the\nfine-tuning of BERT. We study the effects of varying the integer bit-width on\nthe model's metric performance. Our integer fine-tuning uses integer arithmetic\nto perform forward propagation and gradient computation of linear, layer-norm,\nand embedding layers of BERT. We fine-tune BERT using our integer training\nmethod on SQuAD v1.1 and SQuAD v2., and GLUE benchmark. We demonstrate that\nmetric performance of fine-tuning 16-bit integer BERT matches both 16-bit and\n32-bit floating-point baselines. Furthermore, using the faster and more memory\nefficient 8-bit integer data type, integer fine-tuning of BERT loses an average\nof 3.1 points compared to the FP32 baseline.", "journal": ""}
{"doi": "10.48550/arXiv.2210.16621", "date": "2022-10-29", "title": "Empirical Evaluation of Post-Training Quantization Methods for Language Tasks", "authors": "Ting Hu, Christoph Meinel, Haojin Yang", "abstract": "Transformer-based architectures like BERT have achieved great success in a\nwide range of Natural Language tasks. Despite their decent performance, the\nmodels still have numerous parameters and high computational complexity,\nimpeding their deployment in resource-constrained environments. Post-Training\nQuantization (PTQ), which enables low-bit computations without extra training,\ncould be a promising tool. In this work, we conduct an empirical evaluation of\nthree PTQ methods on BERT-Base and BERT-Large: Linear Quantization (LQ),\nAnalytical Clipping for Integer Quantization (ACIQ), and Outlier Channel\nSplitting (OCS). OCS theoretically surpasses the others in minimizing the Mean\nSquare quantization Error and avoiding distorting the weights' outliers. That\nis consistent with the evaluation results of most language tasks of GLUE\nbenchmark and a reading comprehension task, SQuAD. Moreover, low-bit quantized\nBERT models could outperform the corresponding 32-bit baselines on several\nsmall language tasks, which we attribute to the alleviation of\nover-parameterization. We further explore the limit of quantization bit and\nshow that OCS could quantize BERT-Base and BERT-Large to 3-bits and retain 98%\nand 96% of the performance on the GLUE benchmark accordingly. Moreover, we\nconduct quantization on the whole BERT family, i.e., BERT models in different\nconfigurations, and comprehensively evaluate their performance on the GLUE\nbenchmark and SQuAD, hoping to provide valuable guidelines for their deployment\nin various computation environments.", "journal": ""}
{"doi": "10.48550/arXiv.2212.06042", "date": "2022-11-07", "title": "AD-BERT: Using Pre-trained contextualized embeddings to Predict the Progression from Mild Cognitive Impairment to Alzheimer's Disease", "authors": "Chengsheng Mao, Jie Xu, Luke Rasmussen, Yikuan Li, Prakash Adekkanattu, Jennifer Pacheco, Borna Bonakdarpour, Robert Vassar, Guoqian Jiang, Fei Wang, Jyotishman Pathak, Yuan Luo", "abstract": "Objective: We develop a deep learning framework based on the pre-trained\nBidirectional Encoder Representations from Transformers (BERT) model using\nunstructured clinical notes from electronic health records (EHRs) to predict\nthe risk of disease progression from Mild Cognitive Impairment (MCI) to\nAlzheimer's Disease (AD). Materials and Methods: We identified 3657 patients\ndiagnosed with MCI together with their progress notes from Northwestern\nMedicine Enterprise Data Warehouse (NMEDW) between 2000-2020. The progress\nnotes no later than the first MCI diagnosis were used for the prediction. We\nfirst preprocessed the notes by deidentification, cleaning and splitting, and\nthen pretrained a BERT model for AD (AD-BERT) based on the publicly available\nBio+Clinical BERT on the preprocessed notes. The embeddings of all the sections\nof a patient's notes processed by AD-BERT were combined by MaxPooling to\ncompute the probability of MCI-to-AD progression. For replication, we conducted\na similar set of experiments on 2563 MCI patients identified at Weill Cornell\nMedicine (WCM) during the same timeframe. Results: Compared with the 7 baseline\nmodels, the AD-BERT model achieved the best performance on both datasets, with\nArea Under receiver operating characteristic Curve (AUC) of 0.8170 and F1 score\nof 0.4178 on NMEDW dataset and AUC of 0.8830 and F1 score of 0.6836 on WCM\ndataset. Conclusion: We developed a deep learning framework using BERT models\nwhich provide an effective solution for prediction of MCI-to-AD progression\nusing clinical note analysis.", "journal": ""}
{"doi": "10.48550/arXiv.2303.05388", "date": "2023-03-07", "title": "German BERT Model for Legal Named Entity Recognition", "authors": "Harshil Darji, Jelena Mitrovi\u0107, Michael Granitzer", "abstract": "The use of BERT, one of the most popular language models, has led to\nimprovements in many Natural Language Processing (NLP) tasks. One such task is\nNamed Entity Recognition (NER) i.e. automatic identification of named entities\nsuch as location, person, organization, etc. from a given text. It is also an\nimportant base step for many NLP tasks such as information extraction and\nargumentation mining. Even though there is much research done on NER using BERT\nand other popular language models, the same is not explored in detail when it\ncomes to Legal NLP or Legal Tech. Legal NLP applies various NLP techniques such\nas sentence similarity or NER specifically on legal data. There are only a\nhandful of models for NER tasks using BERT language models, however, none of\nthese are aimed at legal documents in German. In this paper, we fine-tune a\npopular BERT language model trained on German data (German BERT) on a Legal\nEntity Recognition (LER) dataset. To make sure our model is not overfitting, we\nperformed a stratified 10-fold cross-validation. The results we achieve by\nfine-tuning German BERT on the LER dataset outperform the BiLSTM-CRF+ model\nused by the authors of the same LER dataset. Finally, we make the model openly\navailable via HuggingFace.", "journal": "Proceedings of the 15th International Conference on Agents and\n  Artificial Intelligence - Volume 3: ICAART (2023) 723-728"}
{"doi": "10.48550/arXiv.2312.17482", "date": "2023-12-29", "title": "MosaicBERT: A Bidirectional Encoder Optimized for Fast Pretraining", "authors": "Jacob Portes, Alex Trott, Sam Havens, Daniel King, Abhinav Venigalla, Moin Nadeem, Nikhil Sardana, Daya Khudia, Jonathan Frankle", "abstract": "Although BERT-style encoder models are heavily used in NLP research, many\nresearchers do not pretrain their own BERTs from scratch due to the high cost\nof training. In the past half-decade since BERT first rose to prominence, many\nadvances have been made with other transformer architectures and training\nconfigurations that have yet to be systematically incorporated into BERT. Here,\nwe introduce MosaicBERT, a BERT-style encoder architecture and training recipe\nthat is empirically optimized for fast pretraining. This efficient architecture\nincorporates FlashAttention, Attention with Linear Biases (ALiBi), Gated Linear\nUnits (GLU), a module to dynamically remove padded tokens, and low precision\nLayerNorm into the classic transformer encoder block. The training recipe\nincludes a 30% masking ratio for the Masked Language Modeling (MLM) objective,\nbfloat16 precision, and vocabulary size optimized for GPU throughput, in\naddition to best-practices from RoBERTa and other encoder models. When\npretrained from scratch on the C4 dataset, this base model achieves a\ndownstream average GLUE (dev) score of 79.6 in 1.13 hours on 8 A100 80 GB GPUs\nat a cost of roughly $20. We plot extensive accuracy vs. pretraining speed\nPareto curves and show that MosaicBERT base and large are consistently Pareto\noptimal when compared to a competitive BERT base and large. This empirical\nspeed up in pretraining enables researchers and engineers to pretrain custom\nBERT-style models at low cost instead of finetune on existing generic models.\nWe open source our model weights and code.", "journal": "NeurIPS 2023"}
{"doi": "10.48550/arXiv.2402.03477", "date": "2024-02-05", "title": "Arabic Synonym BERT-based Adversarial Examples for Text Classification", "authors": "Norah Alshahrani, Saied Alshahrani, Esma Wali, Jeanna Matthews", "abstract": "Text classification systems have been proven vulnerable to adversarial text\nexamples, modified versions of the original text examples that are often\nunnoticed by human eyes, yet can force text classification models to alter\ntheir classification. Often, research works quantifying the impact of\nadversarial text attacks have been applied only to models trained in English.\nIn this paper, we introduce the first word-level study of adversarial attacks\nin Arabic. Specifically, we use a synonym (word-level) attack using a Masked\nLanguage Modeling (MLM) task with a BERT model in a black-box setting to assess\nthe robustness of the state-of-the-art text classification models to\nadversarial attacks in Arabic. To evaluate the grammatical and semantic\nsimilarities of the newly produced adversarial examples using our synonym\nBERT-based attack, we invite four human evaluators to assess and compare the\nproduced adversarial examples with their original examples. We also study the\ntransferability of these newly produced Arabic adversarial examples to various\nmodels and investigate the effectiveness of defense mechanisms against these\nadversarial examples on the BERT models. We find that fine-tuned BERT models\nwere more susceptible to our synonym attacks than the other Deep Neural\nNetworks (DNN) models like WordCNN and WordLSTM we trained. We also find that\nfine-tuned BERT models were more susceptible to transferred attacks. We,\nlastly, find that fine-tuned BERT models successfully regain at least 2% in\naccuracy after applying adversarial training as an initial defense mechanism.", "journal": ""}
{"doi": "10.48550/arXiv.2405.12990", "date": "2024-04-24", "title": "BERT vs GPT for financial engineering", "authors": "Edward Sharkey, Philip Treleaven", "abstract": "The paper benchmarks several Transformer models [4], to show how these models\ncan judge sentiment from a news event. This signal can then be used for\ndownstream modelling and signal identification for commodity trading. We find\nthat fine-tuned BERT models outperform fine-tuned or vanilla GPT models on this\ntask. Transformer models have revolutionized the field of natural language\nprocessing (NLP) in recent years, achieving state-of-the-art results on various\ntasks such as machine translation, text summarization, question answering, and\nnatural language generation. Among the most prominent transformer models are\nBidirectional Encoder Representations from Transformers (BERT) and Generative\nPre-trained Transformer (GPT), which differ in their architectures and\nobjectives.\n  A CopBERT model training data and process overview is provided. The CopBERT\nmodel outperforms similar domain specific BERT trained models such as FinBERT.\nThe below confusion matrices show the performance on CopBERT & CopGPT\nrespectively. We see a ~10 percent increase in f1_score when compare CopBERT vs\nGPT4 and 16 percent increase vs CopGPT. Whilst GPT4 is dominant It highlights\nthe importance of considering alternatives to GPT models for financial\nengineering tasks, given risks of hallucinations, and challenges with\ninterpretability. We unsurprisingly see the larger LLMs outperform the BERT\nmodels, with predictive power. In summary BERT is partially the new XGboost,\nwhat it lacks in predictive power it provides with higher levels of\ninterpretability. Concluding that BERT models might not be the next XGboost\n[2], but represent an interesting alternative for financial engineering tasks,\nthat require a blend of interpretability and accuracy.", "journal": ""}
{"doi": "10.48550/arXiv.2407.00648", "date": "2024-06-30", "title": "LegalTurk Optimized BERT for Multi-Label Text Classification and NER", "authors": "Farnaz Zeidi, Mehmet Fatih Amasyali, \u00c7i\u011fdem Erol", "abstract": "The introduction of the Transformer neural network, along with techniques\nlike self-supervised pre-training and transfer learning, has paved the way for\nadvanced models like BERT. Despite BERT's impressive performance, opportunities\nfor further enhancement exist. To our knowledge, most efforts are focusing on\nimproving BERT's performance in English and in general domains, with no study\nspecifically addressing the legal Turkish domain. Our study is primarily\ndedicated to enhancing the BERT model within the legal Turkish domain through\nmodifications in the pre-training phase. In this work, we introduce our\ninnovative modified pre-training approach by combining diverse masking\nstrategies. In the fine-tuning task, we focus on two essential downstream tasks\nin the legal domain: name entity recognition and multi-label text\nclassification. To evaluate our modified pre-training approach, we fine-tuned\nall customized models alongside the original BERT models to compare their\nperformance. Our modified approach demonstrated significant improvements in\nboth NER and multi-label text classification tasks compared to the original\nBERT model. Finally, to showcase the impact of our proposed models, we trained\nour best models with different corpus sizes and compared them with BERTurk\nmodels. The experimental results demonstrate that our innovative approach,\ndespite being pre-trained on a smaller corpus, competes with BERTurk.", "journal": ""}
{"doi": "10.48550/arXiv.2501.02044", "date": "2025-01-03", "title": "Advancing Pancreatic Cancer Prediction with a Next Visit Token Prediction Head on top of Med-BERT", "authors": "Jianping He, Laila Rasmy, Degui Zhi, Cui Tao", "abstract": "Background: Recently, numerous foundation models pretrained on extensive data\nhave demonstrated efficacy in disease prediction using Electronic Health\nRecords (EHRs). However, there remains some unanswered questions on how to best\nutilize such models especially with very small fine-tuning cohorts. Methods: We\nutilized Med-BERT, an EHR-specific foundation model, and reformulated the\ndisease binary prediction task into a token prediction task and a next visit\nmask token prediction task to align with Med-BERT's pretraining task format in\norder to improve the accuracy of pancreatic cancer (PaCa) prediction in both\nfew-shot and fully supervised settings. Results: The reformulation of the task\ninto a token prediction task, referred to as Med-BERT-Sum, demonstrates\nslightly superior performance in both few-shot scenarios and larger data\nsamples. Furthermore, reformulating the prediction task as a Next Visit Mask\nToken Prediction task (Med-BERT-Mask) significantly outperforms the\nconventional Binary Classification (BC) prediction task (Med-BERT-BC) by 3% to\n7% in few-shot scenarios with data sizes ranging from 10 to 500 samples. These\nfindings highlight that aligning the downstream task with Med-BERT's\npretraining objectives substantially enhances the model's predictive\ncapabilities, thereby improving its effectiveness in predicting both rare and\ncommon diseases. Conclusion: Reformatting disease prediction tasks to align\nwith the pretraining of foundation models enhances prediction accuracy, leading\nto earlier detection and timely intervention. This approach improves treatment\neffectiveness, survival rates, and overall patient outcomes for PaCa and\npotentially other cancers.", "journal": ""}
{"doi": "10.48550/arXiv.2105.14444", "date": "2021-05-30", "title": "NAS-BERT: Task-Agnostic and Adaptive-Size BERT Compression with Neural Architecture Search", "authors": "Jin Xu, Xu Tan, Renqian Luo, Kaitao Song, Jian Li, Tao Qin, Tie-Yan Liu", "abstract": "While pre-trained language models (e.g., BERT) have achieved impressive\nresults on different natural language processing tasks, they have large numbers\nof parameters and suffer from big computational and memory costs, which make\nthem difficult for real-world deployment. Therefore, model compression is\nnecessary to reduce the computation and memory cost of pre-trained models. In\nthis work, we aim to compress BERT and address the following two challenging\npractical issues: (1) The compression algorithm should be able to output\nmultiple compressed models with different sizes and latencies, in order to\nsupport devices with different memory and latency limitations; (2) The\nalgorithm should be downstream task agnostic, so that the compressed models are\ngenerally applicable for different downstream tasks. We leverage techniques in\nneural architecture search (NAS) and propose NAS-BERT, an efficient method for\nBERT compression. NAS-BERT trains a big supernet on a search space containing a\nvariety of architectures and outputs multiple compressed models with adaptive\nsizes and latency. Furthermore, the training of NAS-BERT is conducted on\nstandard self-supervised pre-training tasks (e.g., masked language model) and\ndoes not depend on specific downstream tasks. Thus, the compressed models can\nbe used across various downstream tasks. The technical challenge of NAS-BERT is\nthat training a big supernet on the pre-training task is extremely costly. We\nemploy several techniques including block-wise search, search space pruning,\nand performance approximation to improve search efficiency and accuracy.\nExtensive experiments on GLUE and SQuAD benchmark datasets demonstrate that\nNAS-BERT can find lightweight models with better accuracy than previous\napproaches, and can be directly applied to different downstream tasks with\nadaptive model sizes for different requirements of memory or latency.", "journal": ""}
{"doi": "10.48550/arXiv.2005.07202", "date": "2020-05-14", "title": "Pre-training technique to localize medical BERT and enhance biomedical BERT", "authors": "Shoya Wada, Toshihiro Takeda, Shiro Manabe, Shozo Konishi, Jun Kamohara, Yasushi Matsumura", "abstract": "Pre-training large-scale neural language models on raw texts has made a\nsignificant contribution to improving transfer learning in natural language\nprocessing (NLP). With the introduction of transformer-based language models,\nsuch as bidirectional encoder representations from transformers (BERT), the\nperformance of information extraction from a free text by NLP has significantly\nimproved for both the general domain and medical domain; however, it is\ndifficult to train specific BERT models that perform well for domains in which\nthere are few publicly available databases of high quality and large size. We\nhypothesized that this problem can be addressed by up-sampling a\ndomain-specific corpus and using it for pre-training with a larger corpus in a\nbalanced manner. Our proposed method consists of a single intervention with one\noption: simultaneous pre-training after up-sampling and amplified vocabulary.\nWe conducted three experiments and evaluated the resulting products. We\nconfirmed that our Japanese medical BERT outperformed conventional baselines\nand the other BERT models in terms of the medical document classification task\nand that our English BERT pre-trained using both the general and medical-domain\ncorpora performed sufficiently well for practical use in terms of the\nbiomedical language understanding evaluation (BLUE) benchmark. Moreover, our\nenhanced biomedical BERT model, in which clinical notes were not used during\npre-training, showed that both the clinical and biomedical scores of the BLUE\nbenchmark were 0.3 points above that of the ablation model trained without our\nproposed method. Well-balanced pre-training by up-sampling instances derived\nfrom a corpus appropriate for the target task allows us to construct a\nhigh-performance BERT model.", "journal": ""}
{"doi": "10.48550/arXiv.2012.11405", "date": "2020-12-21", "title": "Cross-domain Retrieval in the Legal and Patent Domains: a Reproducibility Study", "authors": "Sophia Althammer, Sebastian Hofst\u00e4tter, Allan Hanbury", "abstract": "Domain specific search has always been a challenging information retrieval\ntask due to several challenges such as the domain specific language, the unique\ntask setting, as well as the lack of accessible queries and corresponding\nrelevance judgements. In the last years, pretrained language models, such as\nBERT, revolutionized web and news search. Naturally, the community aims to\nadapt these advancements to cross-domain transfer of retrieval models for\ndomain specific search. In the context of legal document retrieval, Shao et al.\npropose the BERT-PLI framework by modeling the Paragraph Level Interactions\nwith the language model BERT. In this paper we reproduce the original\nexperiments, we clarify pre-processing steps, add missing scripts for framework\nsteps and investigate different evaluation approaches, however we are not able\nto reproduce the evaluation results. Contrary to the original paper, we\ndemonstrate that the domain specific paragraph-level modelling does not appear\nto help the performance of the BERT-PLI model compared to paragraph-level\nmodelling with the original BERT. In addition to our legal search\nreproducibility study, we investigate BERT-PLI for document retrieval in the\npatent domain. We find that the BERT-PLI model does not yet achieve performance\nimprovements for patent document retrieval compared to the BM25 baseline.\nFurthermore, we evaluate the BERT-PLI model for cross-domain retrieval between\nthe legal and patent domain on individual components, both on a paragraph and\ndocument-level. We find that the transfer of the BERT-PLI model on the\nparagraph-level leads to comparable results between both domains as well as\nfirst promising results for the cross-domain transfer on the document-level.\nFor reproducibility and transparency as well as to benefit the community we\nmake our source code and the trained models publicly available.", "journal": ""}
{"doi": "10.48550/arXiv.1810.04805", "date": "2018-10-11", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", "authors": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova", "abstract": "We introduce a new language representation model called BERT, which stands\nfor Bidirectional Encoder Representations from Transformers. Unlike recent\nlanguage representation models, BERT is designed to pre-train deep\nbidirectional representations from unlabeled text by jointly conditioning on\nboth left and right context in all layers. As a result, the pre-trained BERT\nmodel can be fine-tuned with just one additional output layer to create\nstate-of-the-art models for a wide range of tasks, such as question answering\nand language inference, without substantial task-specific architecture\nmodifications.\n  BERT is conceptually simple and empirically powerful. It obtains new\nstate-of-the-art results on eleven natural language processing tasks, including\npushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI\naccuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering\nTest F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1\n(5.1 point absolute improvement).", "journal": ""}
{"doi": "10.48550/arXiv.1905.07504", "date": "2019-05-17", "title": "Story Ending Prediction by Transferable BERT", "authors": "Zhongyang Li, Xiao Ding, Ting Liu", "abstract": "Recent advances, such as GPT and BERT, have shown success in incorporating a\npre-trained transformer language model and fine-tuning operation to improve\ndownstream NLP systems. However, this framework still has some fundamental\nproblems in effectively incorporating supervised knowledge from other related\ntasks. In this study, we investigate a transferable BERT (TransBERT) training\nframework, which can transfer not only general language knowledge from\nlarge-scale unlabeled data but also specific kinds of knowledge from various\nsemantically related supervised tasks, for a target task. Particularly, we\npropose utilizing three kinds of transfer tasks, including natural language\ninference, sentiment classification, and next action prediction, to further\ntrain BERT based on a pre-trained model. This enables the model to get a better\ninitialization for the target task. We take story ending prediction as the\ntarget task to conduct experiments. The final result, an accuracy of 91.8%,\ndramatically outperforms previous state-of-the-art baseline methods. Several\ncomparative experiments give some helpful suggestions on how to select transfer\ntasks. Error analysis shows what are the strength and weakness of BERT-based\nmodels for story ending prediction.", "journal": ""}
{"doi": "10.48550/arXiv.1905.08868", "date": "2019-05-21", "title": "Look Again at the Syntax: Relational Graph Convolutional Network for Gendered Ambiguous Pronoun Resolution", "authors": "Yinchuan Xu, Junlin Yang", "abstract": "Gender bias has been found in existing coreference resolvers. In order to\neliminate gender bias, a gender-balanced dataset Gendered Ambiguous Pronouns\n(GAP) has been released and the best baseline model achieves only 66.9% F1.\nBidirectional Encoder Representations from Transformers (BERT) has broken\nseveral NLP task records and can be used on GAP dataset. However, fine-tune\nBERT on a specific task is computationally expensive. In this paper, we propose\nan end-to-end resolver by combining pre-trained BERT with Relational Graph\nConvolutional Network (R-GCN). R-GCN is used for digesting structural syntactic\ninformation and learning better task-specific embeddings. Empirical results\ndemonstrate that, under explicit syntactic supervision and without the need to\nfine tune BERT, R-GCN's embeddings outperform the original BERT embeddings on\nthe coreference task. Our work significantly improves the snippet-context\nbaseline F1 score on GAP dataset from 66.9% to 80.3%. We participated in the\n2019 GAP Coreference Shared Task, and our codes are available online.", "journal": ""}
{"doi": "10.48550/arXiv.1908.03548", "date": "2019-08-09", "title": "BERT-based Ranking for Biomedical Entity Normalization", "authors": "Zongcheng Ji, Qiang Wei, Hua Xu", "abstract": "Developing high-performance entity normalization algorithms that can\nalleviate the term variation problem is of great interest to the biomedical\ncommunity. Although deep learning-based methods have been successfully applied\nto biomedical entity normalization, they often depend on traditional\ncontext-independent word embeddings. Bidirectional Encoder Representations from\nTransformers (BERT), BERT for Biomedical Text Mining (BioBERT) and BERT for\nClinical Text Mining (ClinicalBERT) were recently introduced to pre-train\ncontextualized word representation models using bidirectional Transformers,\nadvancing the state-of-the-art for many natural language processing tasks. In\nthis study, we proposed an entity normalization architecture by fine-tuning the\npre-trained BERT / BioBERT / ClinicalBERT models and conducted extensive\nexperiments to evaluate the effectiveness of the pre-trained models for\nbiomedical entity normalization using three different types of datasets. Our\nexperimental results show that the best fine-tuned models consistently\noutperformed previous methods and advanced the state-of-the-art for biomedical\nentity normalization, with up to 1.17% increase in accuracy.", "journal": ""}
{"doi": "10.48550/arXiv.1908.04943", "date": "2019-08-14", "title": "Establishing Strong Baselines for the New Decade: Sequence Tagging, Syntactic and Semantic Parsing with BERT", "authors": "Han He, Jinho D. Choi", "abstract": "This paper presents new state-of-the-art models for three tasks,\npart-of-speech tagging, syntactic parsing, and semantic parsing, using the\ncutting-edge contextualized embedding framework known as BERT. For each task,\nwe first replicate and simplify the current state-of-the-art approach to\nenhance its model efficiency. We then evaluate our simplified approaches on\nthose three tasks using token embeddings generated by BERT. 12 datasets in both\nEnglish and Chinese are used for our experiments. The BERT models outperform\nthe previously best-performing models by 2.5% on average (7.5% for the most\nsignificant case). Moreover, an in-depth analysis on the impact of BERT\nembeddings is provided using self-attention, which helps understanding in this\nrich yet representation. All models and source codes are available in public so\nthat researchers can improve upon and utilize them to establish strong\nbaselines for the next decade.", "journal": ""}
{"doi": "10.48550/arXiv.1908.05908", "date": "2019-08-16", "title": "BERT-Based Multi-Head Selection for Joint Entity-Relation Extraction", "authors": "Weipeng Huang, Xingyi Cheng, Taifeng Wang, Wei Chu", "abstract": "In this paper, we report our method for the Information Extraction task in\n2019 Language and Intelligence Challenge. We incorporate BERT into the\nmulti-head selection framework for joint entity-relation extraction. This model\nextends existing approaches from three perspectives. First, BERT is adopted as\na feature extraction layer at the bottom of the multi-head selection framework.\nWe further optimize BERT by introducing a semantic-enhanced task during BERT\npre-training. Second, we introduce a large-scale Baidu Baike corpus for entity\nrecognition pre-training, which is of weekly supervised learning since there is\nno actual named entity label. Third, soft label embedding is proposed to\neffectively transmit information between entity recognition and relation\nextraction. Combining these three contributions, we enhance the information\nextracting ability of the multi-head selection model and achieve F1-score 0.876\non testset-1 with a single model. By ensembling four variants of our model, we\nfinally achieve F1 score 0.892 (1st place) on testset-1 and F1 score 0.8924\n(2nd place) on testset-2.", "journal": ""}
{"doi": "10.48550/arXiv.1908.08593", "date": "2019-08-21", "title": "Revealing the Dark Secrets of BERT", "authors": "Olga Kovaleva, Alexey Romanov, Anna Rogers, Anna Rumshisky", "abstract": "BERT-based architectures currently give state-of-the-art performance on many\nNLP tasks, but little is known about the exact mechanisms that contribute to\nits success. In the current work, we focus on the interpretation of\nself-attention, which is one of the fundamental underlying components of BERT.\nUsing a subset of GLUE tasks and a set of handcrafted features-of-interest, we\npropose the methodology and carry out a qualitative and quantitative analysis\nof the information encoded by the individual BERT's heads. Our findings suggest\nthat there is a limited set of attention patterns that are repeated across\ndifferent heads, indicating the overall model overparametrization. While\ndifferent heads consistently use the same attention patterns, they have varying\nimpact on performance across different tasks. We show that manually disabling\nattention in certain heads leads to a performance improvement over the regular\nfine-tuned BERT models.", "journal": ""}
{"doi": "10.48550/arXiv.1908.11860", "date": "2019-08-30", "title": "Adapt or Get Left Behind: Domain Adaptation through BERT Language Model Finetuning for Aspect-Target Sentiment Classification", "authors": "Alexander Rietzler, Sebastian Stabinger, Paul Opitz, Stefan Engl", "abstract": "Aspect-Target Sentiment Classification (ATSC) is a subtask of Aspect-Based\nSentiment Analysis (ABSA), which has many applications e.g. in e-commerce,\nwhere data and insights from reviews can be leveraged to create value for\nbusinesses and customers. Recently, deep transfer-learning methods have been\napplied successfully to a myriad of Natural Language Processing (NLP) tasks,\nincluding ATSC. Building on top of the prominent BERT language model, we\napproach ATSC using a two-step procedure: self-supervised domain-specific BERT\nlanguage model finetuning, followed by supervised task-specific finetuning. Our\nfindings on how to best exploit domain-specific language model finetuning\nenable us to produce new state-of-the-art performance on the SemEval 2014 Task\n4 restaurants dataset. In addition, to explore the real-world robustness of our\nmodels, we perform cross-domain evaluation. We show that a cross-domain adapted\nBERT language model performs significantly better than strong baseline models\nlike vanilla BERT-base and XLNet-base. Finally, we conduct a case study to\ninterpret model prediction errors.", "journal": ""}
{"doi": "10.48550/arXiv.1909.10649", "date": "2019-09-23", "title": "Portuguese Named Entity Recognition using BERT-CRF", "authors": "F\u00e1bio Souza, Rodrigo Nogueira, Roberto Lotufo", "abstract": "Recent advances in language representation using neural networks have made it\nviable to transfer the learned internal states of a trained model to downstream\nnatural language processing tasks, such as named entity recognition (NER) and\nquestion answering. It has been shown that the leverage of pre-trained language\nmodels improves the overall performance on many tasks and is highly beneficial\nwhen labeled data is scarce. In this work, we train Portuguese BERT models and\nemploy a BERT-CRF architecture to the NER task on the Portuguese language,\ncombining the transfer capabilities of BERT with the structured predictions of\nCRF. We explore feature-based and fine-tuning training strategies for the BERT\nmodel. Our fine-tuning approach obtains new state-of-the-art results on the\nHAREM I dataset, improving the F1-score by 1 point on the selective scenario (5\nNE classes) and by 4 points on the total scenario (10 NE classes).", "journal": ""}
{"doi": "10.48550/arXiv.1909.12744", "date": "2019-09-27", "title": "On the use of BERT for Neural Machine Translation", "authors": "St\u00e9phane Clinchant, Kweon Woo Jung, Vassilina Nikoulina", "abstract": "Exploiting large pretrained models for various NMT tasks have gained a lot of\nvisibility recently. In this work we study how BERT pretrained models could be\nexploited for supervised Neural Machine Translation. We compare various ways to\nintegrate pretrained BERT model with NMT model and study the impact of the\nmonolingual data used for BERT training on the final translation quality. We\nuse WMT-14 English-German, IWSLT15 English-German and IWSLT14 English-Russian\ndatasets for these experiments. In addition to standard task test set\nevaluation, we perform evaluation on out-of-domain test sets and noise injected\ntest sets, in order to assess how BERT pretrained representations affect model\nrobustness.", "journal": ""}
{"doi": "10.48550/arXiv.2006.05987", "date": "2020-06-10", "title": "Revisiting Few-sample BERT Fine-tuning", "authors": "Tianyi Zhang, Felix Wu, Arzoo Katiyar, Kilian Q. Weinberger, Yoav Artzi", "abstract": "This paper is a study of fine-tuning of BERT contextual representations, with\nfocus on commonly observed instabilities in few-sample scenarios. We identify\nseveral factors that cause this instability: the common use of a non-standard\noptimization method with biased gradient estimation; the limited applicability\nof significant parts of the BERT network for down-stream tasks; and the\nprevalent practice of using a pre-determined, and small number of training\niterations. We empirically test the impact of these factors, and identify\nalternative practices that resolve the commonly observed instability of the\nprocess. In light of these observations, we re-visit recently proposed methods\nto improve few-sample fine-tuning with BERT and re-evaluate their\neffectiveness. Generally, we observe the impact of these methods diminishes\nsignificantly with our modified process.", "journal": ""}
{"doi": "10.48550/arXiv.1910.01157", "date": "2019-10-02", "title": "Cracking the Contextual Commonsense Code: Understanding Commonsense Reasoning Aptitude of Deep Contextual Representations", "authors": "Jeff Da, Jungo Kasai", "abstract": "Pretrained deep contextual representations have advanced the state-of-the-art\non various commonsense NLP tasks, but we lack a concrete understanding of the\ncapability of these models. Thus, we investigate and challenge several aspects\nof BERT's commonsense representation abilities. First, we probe BERT's ability\nto classify various object attributes, demonstrating that BERT shows a strong\nability in encoding various commonsense features in its embedding space, but is\nstill deficient in many areas. Next, we show that, by augmenting BERT's\npretraining data with additional data related to the deficient attributes, we\nare able to improve performance on a downstream commonsense reasoning task\nwhile using a minimal amount of data. Finally, we develop a method of\nfine-tuning knowledge graphs embeddings alongside BERT and show the continued\nimportance of explicit knowledge graphs.", "journal": ""}
{"doi": "10.48550/arXiv.1910.02655", "date": "2019-10-07", "title": "BERT for Evidence Retrieval and Claim Verification", "authors": "Amir Soleimani, Christof Monz, Marcel Worring", "abstract": "Motivated by the promising performance of pre-trained language models, we\ninvestigate BERT in an evidence retrieval and claim verification pipeline for\nthe FEVER fact extraction and verification challenge. To this end, we propose\nto use two BERT models, one for retrieving potential evidence sentences\nsupporting or rejecting claims, and another for verifying claims based on the\npredicted evidence sets. To train the BERT retrieval system, we use pointwise\nand pairwise loss functions, and examine the effect of hard negative mining. A\nsecond BERT model is trained to classify the samples as supported, refuted, and\nnot enough information. Our system achieves a new state of the art recall of\n87.1 for retrieving top five sentences out of the FEVER documents consisting of\n50K Wikipedia pages, and scores second in the official leaderboard with the\nFEVER score of 69.7.", "journal": ""}
{"doi": "10.48550/arXiv.2003.03106", "date": "2020-03-06", "title": "Sensitive Data Detection and Classification in Spanish Clinical Text: Experiments with BERT", "authors": "Aitor Garc\u00eda-Pablos, Naiara Perez, Montse Cuadros", "abstract": "Massive digital data processing provides a wide range of opportunities and\nbenefits, but at the cost of endangering personal data privacy. Anonymisation\nconsists in removing or replacing sensitive information from data, enabling its\nexploitation for different purposes while preserving the privacy of\nindividuals. Over the years, a lot of automatic anonymisation systems have been\nproposed; however, depending on the type of data, the target language or the\navailability of training documents, the task remains challenging still. The\nemergence of novel deep-learning models during the last two years has brought\nlarge improvements to the state of the art in the field of Natural Language\nProcessing. These advancements have been most noticeably led by BERT, a model\nproposed by Google in 2018, and the shared language models pre-trained on\nmillions of documents. In this paper, we use a BERT-based sequence labelling\nmodel to conduct a series of anonymisation experiments on several clinical\ndatasets in Spanish. We also compare BERT to other algorithms. The experiments\nshow that a simple BERT-based model with general-domain pre-training obtains\nhighly competitive results without any domain specific feature engineering.", "journal": ""}
{"doi": "10.48550/arXiv.2201.04337", "date": "2022-01-12", "title": "PromptBERT: Improving BERT Sentence Embeddings with Prompts", "authors": "Ting Jiang, Jian Jiao, Shaohan Huang, Zihan Zhang, Deqing Wang, Fuzhen Zhuang, Furu Wei, Haizhen Huang, Denvy Deng, Qi Zhang", "abstract": "We propose PromptBERT, a novel contrastive learning method for learning\nbetter sentence representation. We firstly analyze the drawback of current\nsentence embedding from original BERT and find that it is mainly due to the\nstatic token embedding bias and ineffective BERT layers. Then we propose the\nfirst prompt-based sentence embeddings method and discuss two prompt\nrepresenting methods and three prompt searching methods to make BERT achieve\nbetter sentence embeddings. Moreover, we propose a novel unsupervised training\nobjective by the technology of template denoising, which substantially shortens\nthe performance gap between the supervised and unsupervised settings. Extensive\nexperiments show the effectiveness of our method. Compared to SimCSE,\nPromptBert achieves 2.29 and 2.58 points of improvement based on BERT and\nRoBERTa in the unsupervised setting.", "journal": ""}
{"doi": "10.48550/arXiv.2202.12210", "date": "2022-02-24", "title": "BERTVision -- A Parameter-Efficient Approach for Question Answering", "authors": "Siduo Jiang, Cristopher Benge, William Casey King", "abstract": "We present a highly parameter efficient approach for Question Answering that\nsignificantly reduces the need for extended BERT fine-tuning. Our method uses\ninformation from the hidden state activations of each BERT transformer layer,\nwhich is discarded during typical BERT inference. Our best model achieves\nmaximal BERT performance at a fraction of the training time and GPU or TPU\nexpense. Performance is further improved by ensembling our model with BERTs\npredictions. Furthermore, we find that near optimal performance can be achieved\nfor QA span annotation using less training data. Our experiments show that this\napproach works well not only for span annotation, but also for classification,\nsuggesting that it may be extensible to a wider range of tasks.", "journal": ""}
{"doi": "10.48550/arXiv.2002.02925", "date": "2020-02-07", "title": "BERT-of-Theseus: Compressing BERT by Progressive Module Replacing", "authors": "Canwen Xu, Wangchunshu Zhou, Tao Ge, Furu Wei, Ming Zhou", "abstract": "In this paper, we propose a novel model compression approach to effectively\ncompress BERT by progressive module replacing. Our approach first divides the\noriginal BERT into several modules and builds their compact substitutes. Then,\nwe randomly replace the original modules with their substitutes to train the\ncompact modules to mimic the behavior of the original modules. We progressively\nincrease the probability of replacement through the training. In this way, our\napproach brings a deeper level of interaction between the original and compact\nmodels. Compared to the previous knowledge distillation approaches for BERT\ncompression, our approach does not introduce any additional loss function. Our\napproach outperforms existing knowledge distillation approaches on GLUE\nbenchmark, showing a new perspective of model compression.", "journal": ""}
{"doi": "10.48550/arXiv.2002.06652", "date": "2020-02-16", "title": "SBERT-WK: A Sentence Embedding Method by Dissecting BERT-based Word Models", "authors": "Bin Wang, C. -C. Jay Kuo", "abstract": "Sentence embedding is an important research topic in natural language\nprocessing (NLP) since it can transfer knowledge to downstream tasks.\nMeanwhile, a contextualized word representation, called BERT, achieves the\nstate-of-the-art performance in quite a few NLP tasks. Yet, it is an open\nproblem to generate a high quality sentence representation from BERT-based word\nmodels. It was shown in previous study that different layers of BERT capture\ndifferent linguistic properties. This allows us to fusion information across\nlayers to find better sentence representation. In this work, we study the\nlayer-wise pattern of the word representation of deep contextualized models.\nThen, we propose a new sentence embedding method by dissecting BERT-based word\nmodels through geometric analysis of the space spanned by the word\nrepresentation. It is called the SBERT-WK method. No further training is\nrequired in SBERT-WK. We evaluate SBERT-WK on semantic textual similarity and\ndownstream supervised tasks. Furthermore, ten sentence-level probing tasks are\npresented for detailed linguistic analysis. Experiments show that SBERT-WK\nachieves the state-of-the-art performance. Our codes are publicly available.", "journal": ""}
{"doi": "10.48550/arXiv.2004.03742", "date": "2020-04-07", "title": "Towards Evaluating the Robustness of Chinese BERT Classifiers", "authors": "Boxin Wang, Boyuan Pan, Xin Li, Bo Li", "abstract": "Recent advances in large-scale language representation models such as BERT\nhave improved the state-of-the-art performances in many NLP tasks. Meanwhile,\ncharacter-level Chinese NLP models, including BERT for Chinese, have also\ndemonstrated that they can outperform the existing models. In this paper, we\nshow that, however, such BERT-based models are vulnerable under character-level\nadversarial attacks. We propose a novel Chinese char-level attack method\nagainst BERT-based classifiers. Essentially, we generate \"small\" perturbation\non the character level in the embedding space and guide the character\nsubstitution procedure. Extensive experiments show that the classification\naccuracy on a Chinese news dataset drops from 91.8% to 0% by manipulating less\nthan 2 characters on average based on the proposed attack. Human evaluations\nalso confirm that our generated Chinese adversarial examples barely affect\nhuman performance on these NLP tasks.", "journal": ""}
{"doi": "10.48550/arXiv.2004.03808", "date": "2020-04-08", "title": "Improving BERT with Self-Supervised Attention", "authors": "Yiren Chen, Xiaoyu Kou, Jiangang Bai, Yunhai Tong", "abstract": "One of the most popular paradigms of applying large pre-trained NLP models\nsuch as BERT is to fine-tune it on a smaller dataset. However, one challenge\nremains as the fine-tuned model often overfits on smaller datasets. A symptom\nof this phenomenon is that irrelevant or misleading words in the sentence,\nwhich are easy to understand for human beings, can substantially degrade the\nperformance of these finetuned BERT models. In this paper, we propose a novel\ntechnique, called Self-Supervised Attention (SSA) to help facilitate this\ngeneralization challenge. Specifically, SSA automatically generates weak,\ntoken-level attention labels iteratively by probing the fine-tuned model from\nthe previous iteration. We investigate two different ways of integrating SSA\ninto BERT and propose a hybrid approach to combine their benefits. Empirically,\nthrough a variety of public datasets, we illustrate significant performance\nimprovement using our SSA-enhanced BERT model.", "journal": ""}
{"doi": "10.48550/arXiv.2004.04124", "date": "2020-04-08", "title": "LadaBERT: Lightweight Adaptation of BERT through Hybrid Model Compression", "authors": "Yihuan Mao, Yujing Wang, Chufan Wu, Chen Zhang, Yang Wang, Yaming Yang, Quanlu Zhang, Yunhai Tong, Jing Bai", "abstract": "BERT is a cutting-edge language representation model pre-trained by a large\ncorpus, which achieves superior performances on various natural language\nunderstanding tasks. However, a major blocking issue of applying BERT to online\nservices is that it is memory-intensive and leads to unsatisfactory latency of\nuser requests, raising the necessity of model compression. Existing solutions\nleverage the knowledge distillation framework to learn a smaller model that\nimitates the behaviors of BERT. However, the training procedure of knowledge\ndistillation is expensive itself as it requires sufficient training data to\nimitate the teacher model. In this paper, we address this issue by proposing a\nhybrid solution named LadaBERT (Lightweight adaptation of BERT through hybrid\nmodel compression), which combines the advantages of different model\ncompression methods, including weight pruning, matrix factorization and\nknowledge distillation. LadaBERT achieves state-of-the-art accuracy on various\npublic datasets while the training overheads can be reduced by an order of\nmagnitude.", "journal": ""}
{"doi": "10.48550/arXiv.2004.07093", "date": "2020-04-15", "title": "lamBERT: Language and Action Learning Using Multimodal BERT", "authors": "Kazuki Miyazawa, Tatsuya Aoki, Takato Horii, Takayuki Nagai", "abstract": "Recently, the bidirectional encoder representations from transformers (BERT)\nmodel has attracted much attention in the field of natural language processing,\nowing to its high performance in language understanding-related tasks. The BERT\nmodel learns language representation that can be adapted to various tasks via\npre-training using a large corpus in an unsupervised manner. This study\nproposes the language and action learning using multimodal BERT (lamBERT) model\nthat enables the learning of language and actions by 1) extending the BERT\nmodel to multimodal representation and 2) integrating it with reinforcement\nlearning. To verify the proposed model, an experiment is conducted in a grid\nenvironment that requires language understanding for the agent to act properly.\nAs a result, the lamBERT model obtained higher rewards in multitask settings\nand transfer settings when compared to other models, such as the convolutional\nneural network-based model and the lamBERT model without pre-training.", "journal": ""}
{"doi": "10.48550/arXiv.2004.08097", "date": "2020-04-17", "title": "Fast and Accurate Deep Bidirectional Language Representations for Unsupervised Learning", "authors": "Joongbo Shin, Yoonhyung Lee, Seunghyun Yoon, Kyomin Jung", "abstract": "Even though BERT achieves successful performance improvements in various\nsupervised learning tasks, applying BERT for unsupervised tasks still holds a\nlimitation that it requires repetitive inference for computing contextual\nlanguage representations. To resolve the limitation, we propose a novel deep\nbidirectional language model called Transformer-based Text Autoencoder (T-TA).\nThe T-TA computes contextual language representations without repetition and\nhas benefits of the deep bidirectional architecture like BERT. In run-time\nexperiments on CPU environments, the proposed T-TA performs over six times\nfaster than the BERT-based model in the reranking task and twelve times faster\nin the semantic similarity task. Furthermore, the T-TA shows competitive or\neven better accuracies than those of BERT on the above tasks.", "journal": ""}
{"doi": "10.48550/arXiv.2004.09205", "date": "2020-04-20", "title": "A Study of Cross-Lingual Ability and Language-specific Information in Multilingual BERT", "authors": "Chi-Liang Liu, Tsung-Yuan Hsu, Yung-Sung Chuang, Hung-Yi Lee", "abstract": "Recently, multilingual BERT works remarkably well on cross-lingual transfer\ntasks, superior to static non-contextualized word embeddings. In this work, we\nprovide an in-depth experimental study to supplement the existing literature of\ncross-lingual ability. We compare the cross-lingual ability of\nnon-contextualized and contextualized representation model with the same data.\nWe found that datasize and context window size are crucial factors to the\ntransferability. We also observe the language-specific information in\nmultilingual BERT. By manipulating the latent representations, we can control\nthe output languages of multilingual BERT, and achieve unsupervised token\ntranslation. We further show that based on the observation, there is a\ncomputationally cheap but effective approach to improve the cross-lingual\nability of multilingual BERT.", "journal": ""}
{"doi": "10.48550/arXiv.2004.09984", "date": "2020-04-21", "title": "BERT-ATTACK: Adversarial Attack Against BERT Using BERT", "authors": "Linyang Li, Ruotian Ma, Qipeng Guo, Xiangyang Xue, Xipeng Qiu", "abstract": "Adversarial attacks for discrete data (such as texts) have been proved\nsignificantly more challenging than continuous data (such as images) since it\nis difficult to generate adversarial samples with gradient-based methods.\nCurrent successful attack methods for texts usually adopt heuristic replacement\nstrategies on the character or word level, which remains challenging to find\nthe optimal solution in the massive space of possible combinations of\nreplacements while preserving semantic consistency and language fluency. In\nthis paper, we propose \\textbf{BERT-Attack}, a high-quality and effective\nmethod to generate adversarial samples using pre-trained masked language models\nexemplified by BERT. We turn BERT against its fine-tuned models and other deep\nneural models in downstream tasks so that we can successfully mislead the\ntarget models to predict incorrectly. Our method outperforms state-of-the-art\nattack strategies in both success rate and perturb percentage, while the\ngenerated adversarial samples are fluent and semantically preserved. Also, the\ncost of calculation is low, thus possible for large-scale generations. The code\nis available at https://github.com/LinyangLee/BERT-Attack.", "journal": ""}
{"doi": "10.48550/arXiv.2004.13005", "date": "2020-04-24", "title": "Cross-lingual Information Retrieval with BERT", "authors": "Zhuolin Jiang, Amro El-Jaroudi, William Hartmann, Damianos Karakos, Lingjun Zhao", "abstract": "Multiple neural language models have been developed recently, e.g., BERT and\nXLNet, and achieved impressive results in various NLP tasks including sentence\nclassification, question answering and document ranking. In this paper, we\nexplore the use of the popular bidirectional language model, BERT, to model and\nlearn the relevance between English queries and foreign-language documents in\nthe task of cross-lingual information retrieval. A deep relevance matching\nmodel based on BERT is introduced and trained by finetuning a pretrained\nmultilingual BERT model with weak supervision, using home-made CLIR training\ndata derived from parallel corpora. Experimental results of the retrieval of\nLithuanian documents against short English queries show that our model is\neffective and outperforms the competitive baseline approaches.", "journal": ""}
{"doi": "10.48550/arXiv.2004.14620", "date": "2020-04-30", "title": "Universal Dependencies according to BERT: both more specific and more general", "authors": "Tomasz Limisiewicz, Rudolf Rosa, David Mare\u010dek", "abstract": "This work focuses on analyzing the form and extent of syntactic abstraction\ncaptured by BERT by extracting labeled dependency trees from self-attentions.\n  Previous work showed that individual BERT heads tend to encode particular\ndependency relation types. We extend these findings by explicitly comparing\nBERT relations to Universal Dependencies (UD) annotations, showing that they\noften do not match one-to-one.\n  We suggest a method for relation identification and syntactic tree\nconstruction. Our approach produces significantly more consistent dependency\ntrees than previous work, showing that it better explains the syntactic\nabstractions in BERT. At the same time, it can be successfully applied with\nonly a minimal amount of supervision and generalizes well across languages.", "journal": "Findings of the Association for Computational Linguistics: EMNLP\n  2020"}
{"doi": "10.48550/arXiv.2005.00396", "date": "2020-05-01", "title": "Identifying Necessary Elements for BERT's Multilinguality", "authors": "Philipp Dufter, Hinrich Sch\u00fctze", "abstract": "It has been shown that multilingual BERT (mBERT) yields high quality\nmultilingual representations and enables effective zero-shot transfer. This is\nsurprising given that mBERT does not use any crosslingual signal during\ntraining. While recent literature has studied this phenomenon, the reasons for\nthe multilinguality are still somewhat obscure. We aim to identify\narchitectural properties of BERT and linguistic properties of languages that\nare necessary for BERT to become multilingual. To allow for fast\nexperimentation we propose an efficient setup with small BERT models trained on\na mix of synthetic and natural data. Overall, we identify four architectural\nand two linguistic elements that influence multilinguality. Based on our\ninsights, we experiment with a multilingual pretraining setup that modifies the\nmasking strategy using VecMap, i.e., unsupervised embedding alignment.\nExperiments on XNLI with three languages indicate that our findings transfer\nfrom our small setup to larger scale settings.", "journal": ""}
{"doi": "10.48550/arXiv.2005.03848", "date": "2020-05-08", "title": "Distilling Knowledge from Pre-trained Language Models via Text Smoothing", "authors": "Xing Wu, Yibing Liu, Xiangyang Zhou, Dianhai Yu", "abstract": "This paper studies compressing pre-trained language models, like BERT (Devlin\net al.,2019), via teacher-student knowledge distillation. Previous works\nusually force the student model to strictly mimic the smoothed labels predicted\nby the teacher BERT. As an alternative, we propose a new method for BERT\ndistillation, i.e., asking the teacher to generate smoothed word ids, rather\nthan labels, for teaching the student model in knowledge distillation. We call\nthis kind of methodTextSmoothing. Practically, we use the softmax prediction of\nthe Masked Language Model(MLM) in BERT to generate word distributions for given\ntexts and smooth those input texts using that predicted soft word ids. We\nassume that both the smoothed labels and the smoothed texts can implicitly\naugment the input corpus, while text smoothing is intuitively more efficient\nsince it can generate more instances in one neural network forward\nstep.Experimental results on GLUE and SQuAD demonstrate that our solution can\nachieve competitive results compared with existing BERT distillation methods.", "journal": ""}
{"doi": "10.48550/arXiv.2005.06628", "date": "2020-05-09", "title": "schuBERT: Optimizing Elements of BERT", "authors": "Ashish Khetan, Zohar Karnin", "abstract": "Transformers \\citep{vaswani2017attention} have gradually become a key\ncomponent for many state-of-the-art natural language representation models. A\nrecent Transformer based model- BERT \\citep{devlin2018bert} achieved\nstate-of-the-art results on various natural language processing tasks,\nincluding GLUE, SQuAD v1.1, and SQuAD v2.0. This model however is\ncomputationally prohibitive and has a huge number of parameters. In this work\nwe revisit the architecture choices of BERT in efforts to obtain a lighter\nmodel. We focus on reducing the number of parameters yet our methods can be\napplied towards other objectives such FLOPs or latency. We show that much\nefficient light BERT models can be obtained by reducing algorithmically chosen\ncorrect architecture design dimensions rather than reducing the number of\nTransformer encoder layers. In particular, our schuBERT gives $6.6\\%$ higher\naverage accuracy on GLUE and SQuAD datasets as compared to BERT with three\nencoder layers while having the same number of parameters.", "journal": ""}
{"doi": "10.48550/arXiv.2005.07503", "date": "2020-05-15", "title": "COVID-Twitter-BERT: A Natural Language Processing Model to Analyse COVID-19 Content on Twitter", "authors": "Martin M\u00fcller, Marcel Salath\u00e9, Per E Kummervold", "abstract": "In this work, we release COVID-Twitter-BERT (CT-BERT), a transformer-based\nmodel, pretrained on a large corpus of Twitter messages on the topic of\nCOVID-19. Our model shows a 10-30% marginal improvement compared to its base\nmodel, BERT-Large, on five different classification datasets. The largest\nimprovements are on the target domain. Pretrained transformer models, such as\nCT-BERT, are trained on a specific target domain and can be used for a wide\nvariety of natural language processing tasks, including classification,\nquestion-answering and chatbots. CT-BERT is optimised to be used on COVID-19\ncontent, in particular social media posts from Twitter.", "journal": ""}
{"doi": "10.48550/arXiv.2005.09159", "date": "2020-05-19", "title": "Sketch-BERT: Learning Sketch Bidirectional Encoder Representation from Transformers by Self-supervised Learning of Sketch Gestalt", "authors": "Hangyu Lin, Yanwei Fu, Yu-Gang Jiang, Xiangyang Xue", "abstract": "Previous researches of sketches often considered sketches in pixel format and\nleveraged CNN based models in the sketch understanding. Fundamentally, a sketch\nis stored as a sequence of data points, a vector format representation, rather\nthan the photo-realistic image of pixels. SketchRNN studied a generative neural\nrepresentation for sketches of vector format by Long Short Term Memory networks\n(LSTM). Unfortunately, the representation learned by SketchRNN is primarily for\nthe generation tasks, rather than the other tasks of recognition and retrieval\nof sketches. To this end and inspired by the recent BERT model, we present a\nmodel of learning Sketch Bidirectional Encoder Representation from Transformer\n(Sketch-BERT). We generalize BERT to sketch domain, with the novel proposed\ncomponents and pre-training algorithms, including the newly designed sketch\nembedding networks, and the self-supervised learning of sketch gestalt.\nParticularly, towards the pre-training task, we present a novel Sketch Gestalt\nModel (SGM) to help train the Sketch-BERT. Experimentally, we show that the\nlearned representation of Sketch-BERT can help and improve the performance of\nthe downstream tasks of sketch recognition, sketch retrieval, and sketch\ngestalt.", "journal": ""}
{"doi": "10.48550/arXiv.2005.09207", "date": "2020-05-19", "title": "Table Search Using a Deep Contextualized Language Model", "authors": "Zhiyu Chen, Mohamed Trabelsi, Jeff Heflin, Yinan Xu, Brian D. Davison", "abstract": "Pretrained contextualized language models such as BERT have achieved\nimpressive results on various natural language processing benchmarks.\nBenefiting from multiple pretraining tasks and large scale training corpora,\npretrained models can capture complex syntactic word relations. In this paper,\nwe use the deep contextualized language model BERT for the task of ad hoc table\nretrieval. We investigate how to encode table content considering the table\nstructure and input length limit of BERT. We also propose an approach that\nincorporates features from prior literature on table retrieval and jointly\ntrains them with BERT. In experiments on public datasets, we show that our best\napproach can outperform the previous state-of-the-art method and BERT baselines\nwith a large margin under different evaluation metrics.", "journal": ""}
{"doi": "10.48550/arXiv.2005.12142", "date": "2020-05-25", "title": "An Audio-enriched BERT-based Framework for Spoken Multiple-choice Question Answering", "authors": "Chia-Chih Kuo, Shang-Bao Luo, Kuan-Yu Chen", "abstract": "In a spoken multiple-choice question answering (SMCQA) task, given a passage,\na question, and multiple choices all in the form of speech, the machine needs\nto pick the correct choice to answer the question. While the audio could\ncontain useful cues for SMCQA, usually only the auto-transcribed text is\nutilized in system development. Thanks to the large-scaled pre-trained language\nrepresentation models, such as the bidirectional encoder representations from\ntransformers (BERT), systems with only auto-transcribed text can still achieve\na certain level of performance. However, previous studies have evidenced that\nacoustic-level statistics can offset text inaccuracies caused by the automatic\nspeech recognition systems or representation inadequacy lurking in word\nembedding generators, thereby making the SMCQA system robust. Along the line of\nresearch, this study concentrates on designing a BERT-based SMCQA framework,\nwhich not only inherits the advantages of contextualized language\nrepresentations learned by BERT, but integrates the complementary\nacoustic-level information distilled from audio with the text-level\ninformation. Consequently, an audio-enriched BERT-based SMCQA framework is\nproposed. A series of experiments demonstrates remarkable improvements in\naccuracy over selected baselines and SOTA systems on a published Chinese SMCQA\ndataset.", "journal": ""}
{"doi": "10.48550/arXiv.2008.02496", "date": "2020-08-06", "title": "ConvBERT: Improving BERT with Span-based Dynamic Convolution", "authors": "Zihang Jiang, Weihao Yu, Daquan Zhou, Yunpeng Chen, Jiashi Feng, Shuicheng Yan", "abstract": "Pre-trained language models like BERT and its variants have recently achieved\nimpressive performance in various natural language understanding tasks.\nHowever, BERT heavily relies on the global self-attention block and thus\nsuffers large memory footprint and computation cost. Although all its attention\nheads query on the whole input sequence for generating the attention map from a\nglobal perspective, we observe some heads only need to learn local\ndependencies, which means the existence of computation redundancy. We therefore\npropose a novel span-based dynamic convolution to replace these self-attention\nheads to directly model local dependencies. The novel convolution heads,\ntogether with the rest self-attention heads, form a new mixed attention block\nthat is more efficient at both global and local context learning. We equip BERT\nwith this mixed attention design and build a ConvBERT model. Experiments have\nshown that ConvBERT significantly outperforms BERT and its variants in various\ndownstream tasks, with lower training cost and fewer model parameters.\nRemarkably, ConvBERTbase model achieves 86.4 GLUE score, 0.7 higher than\nELECTRAbase, while using less than 1/4 training cost. Code and pre-trained\nmodels will be released.", "journal": ""}
{"doi": "10.48550/arXiv.2008.03822", "date": "2020-08-09", "title": "Distilling the Knowledge of BERT for Sequence-to-Sequence ASR", "authors": "Hayato Futami, Hirofumi Inaguma, Sei Ueno, Masato Mimura, Shinsuke Sakai, Tatsuya Kawahara", "abstract": "Attention-based sequence-to-sequence (seq2seq) models have achieved promising\nresults in automatic speech recognition (ASR). However, as these models decode\nin a left-to-right way, they do not have access to context on the right. We\nleverage both left and right context by applying BERT as an external language\nmodel to seq2seq ASR through knowledge distillation. In our proposed method,\nBERT generates soft labels to guide the training of seq2seq ASR. Furthermore,\nwe leverage context beyond the current utterance as input to BERT. Experimental\nevaluations show that our method significantly improves the ASR performance\nfrom the seq2seq baseline on the Corpus of Spontaneous Japanese (CSJ).\nKnowledge distillation from BERT outperforms that from a transformer LM that\nonly looks at left context. We also show the effectiveness of leveraging\ncontext beyond the current utterance. Our method outperforms other LM\napplication approaches such as n-best rescoring and shallow fusion, while it\ndoes not require extra inference cost.", "journal": ""}
{"doi": "10.48550/arXiv.2008.03945", "date": "2020-08-10", "title": "On Commonsense Cues in BERT for Solving Commonsense Tasks", "authors": "Leyang Cui, Sijie Cheng, Yu Wu, Yue Zhang", "abstract": "BERT has been used for solving commonsense tasks such as CommonsenseQA. While\nprior research has found that BERT does contain commonsense information to some\nextent, there has been work showing that pre-trained models can rely on\nspurious associations (e.g., data bias) rather than key cues in solving\nsentiment classification and other problems. We quantitatively investigate the\npresence of structural commonsense cues in BERT when solving commonsense tasks,\nand the importance of such cues for the model prediction. Using two different\nmeasures, we find that BERT does use relevant knowledge for solving the task,\nand the presence of commonsense knowledge is positively correlated to the model\naccuracy.", "journal": ""}
{"doi": "10.48550/arXiv.2009.12812", "date": "2020-09-27", "title": "TernaryBERT: Distillation-aware Ultra-low Bit BERT", "authors": "Wei Zhang, Lu Hou, Yichun Yin, Lifeng Shang, Xiao Chen, Xin Jiang, Qun Liu", "abstract": "Transformer-based pre-training models like BERT have achieved remarkable\nperformance in many natural language processing tasks.However, these models are\nboth computation and memory expensive, hindering their deployment to\nresource-constrained devices. In this work, we propose TernaryBERT, which\nternarizes the weights in a fine-tuned BERT model. Specifically, we use both\napproximation-based and loss-aware ternarization methods and empirically\ninvestigate the ternarization granularity of different parts of BERT. Moreover,\nto reduce the accuracy degradation caused by the lower capacity of low bits, we\nleverage the knowledge distillation technique in the training process.\nExperiments on the GLUE benchmark and SQuAD show that our proposed TernaryBERT\noutperforms the other BERT quantization methods, and even achieves comparable\nperformance as the full-precision model while being 14.9x smaller.", "journal": ""}
{"doi": "10.48550/arXiv.2009.14409", "date": "2020-09-30", "title": "AUBER: Automated BERT Regularization", "authors": "Hyun Dong Lee, Seongmin Lee, U Kang", "abstract": "How can we effectively regularize BERT? Although BERT proves its\neffectiveness in various downstream natural language processing tasks, it often\noverfits when there are only a small number of training instances. A promising\ndirection to regularize BERT is based on pruning its attention heads based on a\nproxy score for head importance. However, heuristic-based methods are usually\nsuboptimal since they predetermine the order by which attention heads are\npruned. In order to overcome such a limitation, we propose AUBER, an effective\nregularization method that leverages reinforcement learning to automatically\nprune attention heads from BERT. Instead of depending on heuristics or\nrule-based policies, AUBER learns a pruning policy that determines which\nattention heads should or should not be pruned for regularization. Experimental\nresults show that AUBER outperforms existing pruning methods by achieving up to\n10% better accuracy. In addition, our ablation study empirically demonstrates\nthe effectiveness of our design choices for AUBER.", "journal": ""}
{"doi": "10.48550/arXiv.2010.03746", "date": "2020-10-08", "title": "Infusing Disease Knowledge into BERT for Health Question Answering, Medical Inference and Disease Name Recognition", "authors": "Yun He, Ziwei Zhu, Yin Zhang, Qin Chen, James Caverlee", "abstract": "Knowledge of a disease includes information of various aspects of the\ndisease, such as signs and symptoms, diagnosis and treatment. This disease\nknowledge is critical for many health-related and biomedical tasks, including\nconsumer health question answering, medical language inference and disease name\nrecognition. While pre-trained language models like BERT have shown success in\ncapturing syntactic, semantic, and world knowledge from text, we find they can\nbe further complemented by specific information like knowledge of symptoms,\ndiagnoses, treatments, and other disease aspects. Hence, we integrate BERT with\ndisease knowledge for improving these important tasks. Specifically, we propose\na new disease knowledge infusion training procedure and evaluate it on a suite\nof BERT models including BERT, BioBERT, SciBERT, ClinicalBERT, BlueBERT, and\nALBERT. Experiments over the three tasks show that these models can be enhanced\nin nearly all cases, demonstrating the viability of disease knowledge infusion.\nFor example, accuracy of BioBERT on consumer health question answering is\nimproved from 68.29% to 72.09%, while new SOTA results are observed in two\ndatasets. We make our data and code freely available.", "journal": ""}
{"doi": "10.48550/arXiv.2010.09313", "date": "2020-10-19", "title": "BERTnesia: Investigating the capture and forgetting of knowledge in BERT", "authors": "Jonas Wallat, Jaspreet Singh, Avishek Anand", "abstract": "Probing complex language models has recently revealed several insights into\nlinguistic and semantic patterns found in the learned representations. In this\npaper, we probe BERT specifically to understand and measure the relational\nknowledge it captures. We utilize knowledge base completion tasks to probe\nevery layer of pre-trained as well as fine-tuned BERT (ranking, question\nanswering, NER). Our findings show that knowledge is not just contained in\nBERT's final layers. Intermediate layers contribute a significant amount\n(17-60%) to the total knowledge found. Probing intermediate layers also reveals\nhow different types of knowledge emerge at varying rates. When BERT is\nfine-tuned, relational knowledge is forgotten but the extent of forgetting is\nimpacted by the fine-tuning objective but not the size of the dataset. We found\nthat ranking models forget the least and retain more knowledge in their final\nlayer. We release our code on github to repeat the experiments.", "journal": ""}
{"doi": "10.48550/arXiv.2010.11639", "date": "2020-10-22", "title": "Towards Fully Bilingual Deep Language Modeling", "authors": "Li-Hsin Chang, Sampo Pyysalo, Jenna Kanerva, Filip Ginter", "abstract": "Language models based on deep neural networks have facilitated great advances\nin natural language processing and understanding tasks in recent years. While\nmodels covering a large number of languages have been introduced, their\nmultilinguality has come at a cost in terms of monolingual performance, and the\nbest-performing models at most tasks not involving cross-lingual transfer\nremain monolingual. In this paper, we consider the question of whether it is\npossible to pre-train a bilingual model for two remotely related languages\nwithout compromising performance at either language. We collect pre-training\ndata, create a Finnish-English bilingual BERT model and evaluate its\nperformance on datasets used to evaluate the corresponding monolingual models.\nOur bilingual model performs on par with Google's original English BERT on GLUE\nand nearly matches the performance of monolingual Finnish BERT on a range of\nFinnish NLP tasks, clearly outperforming multilingual BERT. We find that when\nthe model vocabulary size is increased, the BERT-Base architecture has\nsufficient capacity to learn two remotely related languages to a level where it\nachieves comparable performance with monolingual models, demonstrating the\nfeasibility of training fully bilingual deep language models. The model and all\ntools involved in its creation are freely available at\nhttps://github.com/TurkuNLP/biBERT", "journal": ""}
{"doi": "10.48550/arXiv.2010.14042", "date": "2020-10-27", "title": "To BERT or Not to BERT: Comparing Task-specific and Task-agnostic Semi-Supervised Approaches for Sequence Tagging", "authors": "Kasturi Bhattacharjee, Miguel Ballesteros, Rishita Anubhai, Smaranda Muresan, Jie Ma, Faisal Ladhak, Yaser Al-Onaizan", "abstract": "Leveraging large amounts of unlabeled data using Transformer-like\narchitectures, like BERT, has gained popularity in recent times owing to their\neffectiveness in learning general representations that can then be further\nfine-tuned for downstream tasks to much success. However, training these models\ncan be costly both from an economic and environmental standpoint. In this work,\nwe investigate how to effectively use unlabeled data: by exploring the\ntask-specific semi-supervised approach, Cross-View Training (CVT) and comparing\nit with task-agnostic BERT in multiple settings that include domain and task\nrelevant English data. CVT uses a much lighter model architecture and we show\nthat it achieves similar performance to BERT on a set of sequence tagging\ntasks, with lesser financial and environmental impact.", "journal": ""}
{"doi": "10.48550/arXiv.2010.14095", "date": "2020-10-27", "title": "MMFT-BERT: Multimodal Fusion Transformer with BERT Encodings for Visual Question Answering", "authors": "Aisha Urooj Khan, Amir Mazaheri, Niels da Vitoria Lobo, Mubarak Shah", "abstract": "We present MMFT-BERT(MultiModal Fusion Transformer with BERT encodings), to\nsolve Visual Question Answering (VQA) ensuring individual and combined\nprocessing of multiple input modalities. Our approach benefits from processing\nmultimodal data (video and text) adopting the BERT encodings individually and\nusing a novel transformer-based fusion method to fuse them together. Our method\ndecomposes the different sources of modalities, into different BERT instances\nwith similar architectures, but variable weights. This achieves SOTA results on\nthe TVQA dataset. Additionally, we provide TVQA-Visual, an isolated diagnostic\nsubset of TVQA, which strictly requires the knowledge of visual (V) modality\nbased on a human annotator's judgment. This set of questions helps us to study\nthe model's behavior and the challenges TVQA poses to prevent the achievement\nof super human performance. Extensive experiments show the effectiveness and\nsuperiority of our method.", "journal": ""}
{"doi": "10.48550/arXiv.2011.02417", "date": "2020-11-04", "title": "Investigating Novel Verb Learning in BERT: Selectional Preference Classes and Alternation-Based Syntactic Generalization", "authors": "Tristan Thrush, Ethan Wilcox, Roger Levy", "abstract": "Previous studies investigating the syntactic abilities of deep learning\nmodels have not targeted the relationship between the strength of the\ngrammatical generalization and the amount of evidence to which the model is\nexposed during training. We address this issue by deploying a novel\nword-learning paradigm to test BERT's few-shot learning capabilities for two\naspects of English verbs: alternations and classes of selectional preferences.\nFor the former, we fine-tune BERT on a single frame in a verbal-alternation\npair and ask whether the model expects the novel verb to occur in its sister\nframe. For the latter, we fine-tune BERT on an incomplete selectional network\nof verbal objects and ask whether it expects unattested but plausible\nverb/object pairs. We find that BERT makes robust grammatical generalizations\nafter just one or two instances of a novel word in fine-tuning. For the verbal\nalternation tests, we find that the model displays behavior that is consistent\nwith a transitivity bias: verbs seen few times are expected to take direct\nobjects, but verbs seen with direct objects are not expected to occur\nintransitively.", "journal": ""}
{"doi": "10.48550/arXiv.2011.04134", "date": "2020-11-09", "title": "CxGBERT: BERT meets Construction Grammar", "authors": "Harish Tayyar Madabushi, Laurence Romain, Dagmar Divjak, Petar Milin", "abstract": "While lexico-semantic elements no doubt capture a large amount of linguistic\ninformation, it has been argued that they do not capture all information\ncontained in text. This assumption is central to constructionist approaches to\nlanguage which argue that language consists of constructions, learned pairings\nof a form and a function or meaning that are either frequent or have a meaning\nthat cannot be predicted from its component parts. BERT's training objectives\ngive it access to a tremendous amount of lexico-semantic information, and while\nBERTology has shown that BERT captures certain important linguistic dimensions,\nthere have been no studies exploring the extent to which BERT might have access\nto constructional information. In this work we design several probes and\nconduct extensive experiments to answer this question. Our results allow us to\nconclude that BERT does indeed have access to a significant amount of\ninformation, much of which linguists typically call constructional information.\nThe impact of this observation is potentially far-reaching as it provides\ninsights into what deep learning methods learn from text, while also showing\nthat information contained in constructions is redundantly encoded in\nlexico-semantics.", "journal": ""}
{"doi": "10.48550/arXiv.2011.13922", "date": "2020-11-26", "title": "A Recurrent Vision-and-Language BERT for Navigation", "authors": "Yicong Hong, Qi Wu, Yuankai Qi, Cristian Rodriguez-Opazo, Stephen Gould", "abstract": "Accuracy of many visiolinguistic tasks has benefited significantly from the\napplication of vision-and-language(V&L) BERT. However, its application for the\ntask of vision-and-language navigation (VLN) remains limited. One reason for\nthis is the difficulty adapting the BERT architecture to the partially\nobservable Markov decision process present in VLN, requiring history-dependent\nattention and decision making. In this paper we propose a recurrent BERT model\nthat is time-aware for use in VLN. Specifically, we equip the BERT model with a\nrecurrent function that maintains cross-modal state information for the agent.\nThrough extensive experiments on R2R and REVERIE we demonstrate that our model\ncan replace more complex encoder-decoder models to achieve state-of-the-art\nresults. Moreover, our approach can be generalised to other transformer-based\narchitectures, supports pre-training, and is capable of solving navigation and\nreferring expression tasks simultaneously.", "journal": ""}
{"doi": "10.48550/arXiv.2012.04539", "date": "2020-12-07", "title": "Dartmouth CS at WNUT-2020 Task 2: Informative COVID-19 Tweet Classification Using BERT", "authors": "Dylan Whang, Soroush Vosoughi", "abstract": "We describe the systems developed for the WNUT-2020 shared task 2,\nidentification of informative COVID-19 English Tweets. BERT is a highly\nperformant model for Natural Language Processing tasks. We increased BERT's\nperformance in this classification task by fine-tuning BERT and concatenating\nits embeddings with Tweet-specific features and training a Support Vector\nMachine (SVM) for classification (henceforth called BERT+). We compared its\nperformance to a suite of machine learning models. We used a Twitter specific\ndata cleaning pipeline and word-level TF-IDF to extract features for the\nnon-BERT models. BERT+ was the top performing model with an F1-score of 0.8713.", "journal": ""}
{"doi": "10.48550/arXiv.2101.05938", "date": "2021-01-15", "title": "KDLSQ-BERT: A Quantized Bert Combining Knowledge Distillation with Learned Step Size Quantization", "authors": "Jing Jin, Cai Liang, Tiancheng Wu, Liqin Zou, Zhiliang Gan", "abstract": "Recently, transformer-based language models such as BERT have shown\ntremendous performance improvement for a range of natural language processing\ntasks. However, these language models usually are computation expensive and\nmemory intensive during inference. As a result, it is difficult to deploy them\non resource-restricted devices. To improve the inference performance, as well\nas reduce the model size while maintaining the model accuracy, we propose a\nnovel quantization method named KDLSQ-BERT that combines knowledge distillation\n(KD) with learned step size quantization (LSQ) for language model quantization.\nThe main idea of our method is that the KD technique is leveraged to transfer\nthe knowledge from a \"teacher\" model to a \"student\" model when exploiting LSQ\nto quantize that \"student\" model during the quantization training process.\nExtensive experiment results on GLUE benchmark and SQuAD demonstrate that our\nproposed KDLSQ-BERT not only performs effectively when doing different bit\n(e.g. 2-bit $\\sim$ 8-bit) quantization, but also outperforms the existing BERT\nquantization methods, and even achieves comparable performance as the\nfull-precision base-line model while obtaining 14.9x compression ratio. Our\ncode will be public available.", "journal": ""}
{"doi": "10.48550/arXiv.2102.12330", "date": "2021-02-24", "title": "Re-Evaluating GermEval17 Using German Pre-Trained Language Models", "authors": "M. A\u00dfenmacher, A. Corvonato, C. Heumann", "abstract": "The lack of a commonly used benchmark data set (collection) such as\n(Super-)GLUE (Wang et al., 2018, 2019) for the evaluation of non-English\npre-trained language models is a severe shortcoming of current English-centric\nNLP-research. It concentrates a large part of the research on English,\nneglecting the uncertainty when transferring conclusions found for the English\nlanguage to other languages. We evaluate the performance of the German and\nmultilingual BERT-based models currently available via the huggingface\ntransformers library on the four tasks of the GermEval17 workshop. We compare\nthem to pre-BERT architectures (Wojatzki et al., 2017; Schmitt et al., 2018;\nAttia et al., 2018) as well as to an ELMo-based architecture (Biesialska et\nal., 2020) and a BERT-based approach (Guhr et al., 2020). The observed\nimprovements are put in relation to those for similar tasks and similar models\n(pre-BERT vs. BERT-based) for the English language in order to draw tentative\nconclusions about whether the observed improvements are transferable to German\nor potentially other related languages.", "journal": ""}
{"doi": "10.48550/arXiv.2103.16110", "date": "2021-03-30", "title": "Kaleido-BERT: Vision-Language Pre-training on Fashion Domain", "authors": "Mingchen Zhuge, Dehong Gao, Deng-Ping Fan, Linbo Jin, Ben Chen, Haoming Zhou, Minghui Qiu, Ling Shao", "abstract": "We present a new vision-language (VL) pre-training model dubbed Kaleido-BERT,\nwhich introduces a novel kaleido strategy for fashion cross-modality\nrepresentations from transformers. In contrast to random masking strategy of\nrecent VL models, we design alignment guided masking to jointly focus more on\nimage-text semantic relations. To this end, we carry out five novel tasks,\ni.e., rotation, jigsaw, camouflage, grey-to-color, and blank-to-color for\nself-supervised VL pre-training at patches of different scale. Kaleido-BERT is\nconceptually simple and easy to extend to the existing BERT framework, it\nattains new state-of-the-art results by large margins on four downstream tasks,\nincluding text retrieval (R@1: 4.03% absolute improvement), image retrieval\n(R@1: 7.13% abs imv.), category recognition (ACC: 3.28% abs imv.), and fashion\ncaptioning (Bleu4: 1.2 abs imv.). We validate the efficiency of Kaleido-BERT on\na wide range of e-commerical websites, demonstrating its broader potential in\nreal-world applications.", "journal": ""}
{"doi": "10.48550/arXiv.2104.01477", "date": "2021-04-03", "title": "Exploring the Role of BERT Token Representations to Explain Sentence Probing Results", "authors": "Hosein Mohebbi, Ali Modarressi, Mohammad Taher Pilehvar", "abstract": "Several studies have been carried out on revealing linguistic features\ncaptured by BERT. This is usually achieved by training a diagnostic classifier\non the representations obtained from different layers of BERT. The subsequent\nclassification accuracy is then interpreted as the ability of the model in\nencoding the corresponding linguistic property. Despite providing insights,\nthese studies have left out the potential role of token representations. In\nthis paper, we provide a more in-depth analysis on the representation space of\nBERT in search for distinct and meaningful subspaces that can explain the\nreasons behind these probing results. Based on a set of probing tasks and with\nthe help of attribution methods we show that BERT tends to encode meaningful\nknowledge in specific token representations (which are often ignored in\nstandard classification setups), allowing the model to detect syntactic and\nsemantic abnormalities, and to distinctively separate grammatical number and\ntense subspaces.", "journal": ""}
{"doi": "10.48550/arXiv.2104.04697", "date": "2021-04-10", "title": "ZS-BERT: Towards Zero-Shot Relation Extraction with Attribute Representation Learning", "authors": "Chih-Yao Chen, Cheng-Te Li", "abstract": "While relation extraction is an essential task in knowledge acquisition and\nrepresentation, and new-generated relations are common in the real world, less\neffort is made to predict unseen relations that cannot be observed at the\ntraining stage. In this paper, we formulate the zero-shot relation extraction\nproblem by incorporating the text description of seen and unseen relations. We\npropose a novel multi-task learning model, zero-shot BERT (ZS-BERT), to\ndirectly predict unseen relations without hand-crafted attribute labeling and\nmultiple pairwise classifications. Given training instances consisting of input\nsentences and the descriptions of their relations, ZS-BERT learns two functions\nthat project sentences and relation descriptions into an embedding space by\njointly minimizing the distances between them and classifying seen relations.\nBy generating the embeddings of unseen relations and new-coming sentences based\non such two functions, we use nearest neighbor search to obtain the prediction\nof unseen relations. Experiments conducted on two well-known datasets exhibit\nthat ZS-BERT can outperform existing methods by at least 13.54\\% improvement on\nF1 score.", "journal": ""}
{"doi": "10.48550/arXiv.2104.04950", "date": "2021-04-11", "title": "Innovative Bert-based Reranking Language Models for Speech Recognition", "authors": "Shih-Hsuan Chiu, Berlin Chen", "abstract": "More recently, Bidirectional Encoder Representations from Transformers (BERT)\nwas proposed and has achieved impressive success on many natural language\nprocessing (NLP) tasks such as question answering and language understanding,\ndue mainly to its effective pre-training then fine-tuning paradigm as well as\nstrong local contextual modeling ability. In view of the above, this paper\npresents a novel instantiation of the BERT-based contextualized language models\n(LMs) for use in reranking of N-best hypotheses produced by automatic speech\nrecognition (ASR). To this end, we frame N-best hypothesis reranking with BERT\nas a prediction problem, which aims to predict the oracle hypothesis that has\nthe lowest word error rate (WER) given the N-best hypotheses (denoted by\nPBERT). In particular, we also explore to capitalize on task-specific global\ntopic information in an unsupervised manner to assist PBERT in N-best\nhypothesis reranking (denoted by TPBERT). Extensive experiments conducted on\nthe AMI benchmark corpus demonstrate the effectiveness and feasibility of our\nmethods in comparison to the conventional autoregressive models like the\nrecurrent neural network (RNN) and a recently proposed method that employed\nBERT to compute pseudo-log-likelihood (PLL) scores for N-best hypothesis\nreranking.", "journal": ""}
{"doi": "10.48550/arXiv.2108.01589", "date": "2021-08-03", "title": "ExBERT: An External Knowledge Enhanced BERT for Natural Language Inference", "authors": "Amit Gajbhiye, Noura Al Moubayed, Steven Bradley", "abstract": "Neural language representation models such as BERT, pre-trained on\nlarge-scale unstructured corpora lack explicit grounding to real-world\ncommonsense knowledge and are often unable to remember facts required for\nreasoning and inference. Natural Language Inference (NLI) is a challenging\nreasoning task that relies on common human understanding of language and\nreal-world commonsense knowledge. We introduce a new model for NLI called\nExternal Knowledge Enhanced BERT (ExBERT), to enrich the contextual\nrepresentation with real-world commonsense knowledge from external knowledge\nsources and enhance BERT's language understanding and reasoning capabilities.\nExBERT takes full advantage of contextual word representations obtained from\nBERT and employs them to retrieve relevant external knowledge from knowledge\ngraphs and to encode the retrieved external knowledge. Our model adaptively\nincorporates the external knowledge context required for reasoning over the\ninputs. Extensive experiments on the challenging SciTail and SNLI benchmarks\ndemonstrate the effectiveness of ExBERT: in comparison to the previous\nstate-of-the-art, we obtain an accuracy of 95.9% on SciTail and 91.5% on SNLI.", "journal": ""}
{"doi": "10.48550/arXiv.2108.07789", "date": "2021-07-29", "title": "Adapting GPT, GPT-2 and BERT Language Models for Speech Recognition", "authors": "Xianrui Zheng, Chao Zhang, Philip C. Woodland", "abstract": "Language models (LMs) pre-trained on massive amounts of text, in particular\nbidirectional encoder representations from Transformers (BERT), generative\npre-training (GPT), and GPT-2, have become a key technology for many natural\nlanguage processing tasks. In this paper, we present results using fine-tuned\nGPT, GPT-2, and their combination for automatic speech recognition (ASR).\nUnlike unidirectional LM GPT and GPT-2, BERT is bidirectional whose direct\nproduct of the output probabilities is no longer a valid language prior\nprobability. A conversion method is proposed to compute the correct language\nprior probability based on bidirectional LM outputs in a mathematically exact\nway. Experimental results on the widely used AMI and Switchboard ASR tasks\nshowed that the combination of the fine-tuned GPT and GPT-2 outperformed the\ncombination of three neural LMs with different architectures trained from\nscratch on the in-domain text by up to a 12% relative word error rate reduction\n(WERR). Furthermore, on the AMI corpus, the proposed conversion for language\nprior probabilities enables BERT to obtain an extra 3% relative WERR, and the\ncombination of BERT, GPT and GPT-2 results in further improvements.", "journal": ""}
{"doi": "10.48550/arXiv.2108.13602", "date": "2021-08-31", "title": "How Does Adversarial Fine-Tuning Benefit BERT?", "authors": "Javid Ebrahimi, Hao Yang, Wei Zhang", "abstract": "Adversarial training (AT) is one of the most reliable methods for defending\nagainst adversarial attacks in machine learning. Variants of this method have\nbeen used as regularization mechanisms to achieve SOTA results on NLP\nbenchmarks, and they have been found to be useful for transfer learning and\ncontinual learning. We search for the reasons for the effectiveness of AT by\ncontrasting vanilla and adversarially fine-tuned BERT models. We identify\npartial preservation of BERT's syntactic abilities during fine-tuning as the\nkey to the success of AT. We observe that adversarially fine-tuned models\nremain more faithful to BERT's language modeling behavior and are more\nsensitive to the word order. As concrete examples of syntactic abilities, an\nadversarially fine-tuned model could have an advantage of up to 38% on anaphora\nagreement and up to 11% on dependency parsing. Our analysis demonstrates that\nvanilla fine-tuning oversimplifies the sentence representation by focusing\nheavily on a small subset of words. AT, however, moderates the effect of these\ninfluential words and encourages representational diversity. This allows for a\nmore hierarchical representation of a sentence and leads to the mitigation of\nBERT's loss of syntactic abilities.", "journal": ""}
{"doi": "10.48550/arXiv.2109.04810", "date": "2021-09-10", "title": "Mixture-of-Partitions: Infusing Large Biomedical Knowledge Graphs into BERT", "authors": "Zaiqiao Meng, Fangyu Liu, Thomas Hikaru Clark, Ehsan Shareghi, Nigel Collier", "abstract": "Infusing factual knowledge into pre-trained models is fundamental for many\nknowledge-intensive tasks. In this paper, we proposed Mixture-of-Partitions\n(MoP), an infusion approach that can handle a very large knowledge graph (KG)\nby partitioning it into smaller sub-graphs and infusing their specific\nknowledge into various BERT models using lightweight adapters. To leverage the\noverall factual knowledge for a target task, these sub-graph adapters are\nfurther fine-tuned along with the underlying BERT through a mixture layer. We\nevaluate our MoP with three biomedical BERTs (SciBERT, BioBERT, PubmedBERT) on\nsix downstream tasks (inc. NLI, QA, Classification), and the results show that\nour MoP consistently enhances the underlying BERTs in task performance, and\nachieves new SOTA performances on five evaluated datasets.", "journal": ""}
{"doi": "10.48550/arXiv.2109.06306", "date": "2021-09-13", "title": "BERT for Target Apps Selection: Analyzing the Diversity and Performance of BERT in Unified Mobile Search", "authors": "Negin Ghasemi, Mohammad Aliannejadi, Djoerd Hiemstra", "abstract": "A unified mobile search framework aims to identify the mobile apps that can\nsatisfy a user's information need and route the user's query to them. Previous\nwork has shown that resource descriptions for mobile apps are sparse as they\nrely on the app's previous queries. This problem puts certain apps in dominance\nand leaves out the resource-scarce apps from the top ranks. In this case, we\nneed a ranker that goes beyond simple lexical matching. Therefore, our goal is\nto study the extent of a BERT-based ranker's ability to improve the quality and\ndiversity of app selection. To this end, we compare the results of the\nBERT-based ranker with other information retrieval models, focusing on the\nanalysis of selected apps diversification. Our analysis shows that the\nBERT-based ranker selects more diverse apps while improving the quality of\nbaseline results by selecting the relevant apps such as Facebook and Contacts\nfor more personal queries and decreasing the bias towards the dominant\nresources such as the Google Search app.", "journal": ""}
{"doi": "10.48550/arXiv.2109.07020", "date": "2021-09-14", "title": "Frequency Effects on Syntactic Rule Learning in Transformers", "authors": "Jason Wei, Dan Garrette, Tal Linzen, Ellie Pavlick", "abstract": "Pre-trained language models perform well on a variety of linguistic tasks\nthat require symbolic reasoning, raising the question of whether such models\nimplicitly represent abstract symbols and rules. We investigate this question\nusing the case study of BERT's performance on English subject-verb agreement.\nUnlike prior work, we train multiple instances of BERT from scratch, allowing\nus to perform a series of controlled interventions at pre-training time. We\nshow that BERT often generalizes well to subject-verb pairs that never occurred\nin training, suggesting a degree of rule-governed behavior. We also find,\nhowever, that performance is heavily influenced by word frequency, with\nexperiments showing that both the absolute frequency of a verb form, as well as\nthe frequency relative to the alternate inflection, are causally implicated in\nthe predictions BERT makes at inference time. Closer analysis of these\nfrequency effects reveals that BERT's behavior is consistent with a system that\ncorrectly applies the SVA rule in general but struggles to overcome strong\ntraining priors and to estimate agreement features (singular vs. plural) on\ninfrequent lexical items.", "journal": ""}
{"doi": "10.48550/arXiv.2109.11745", "date": "2021-09-24", "title": "DACT-BERT: Differentiable Adaptive Computation Time for an Efficient BERT Inference", "authors": "Crist\u00f3bal Eyzaguirre, Felipe del R\u00edo, Vladimir Araujo, \u00c1lvaro Soto", "abstract": "Large-scale pre-trained language models have shown remarkable results in\ndiverse NLP applications. Unfortunately, these performance gains have been\naccompanied by a significant increase in computation time and model size,\nstressing the need to develop new or complementary strategies to increase the\nefficiency of these models. In this paper we propose DACT-BERT, a\ndifferentiable adaptive computation time strategy for BERT-like models.\nDACT-BERT adds an adaptive computational mechanism to BERT's regular processing\npipeline, which controls the number of Transformer blocks that need to be\nexecuted at inference time. By doing this, the model learns to combine the most\nappropriate intermediate representations for the task at hand. Our experiments\ndemonstrate that our approach, when compared to the baselines, excels on a\nreduced computational regime and is competitive in other less restrictive ones.", "journal": ""}
{"doi": "10.48550/arXiv.2112.06736", "date": "2021-12-13", "title": "Roof-Transformer: Divided and Joined Understanding with Knowledge Enhancement", "authors": "Wei-Lin Liao, Cheng-En Su, Wei-Yun Ma", "abstract": "Recent work on enhancing BERT-based language representation models with\nknowledge graphs (KGs) and knowledge bases (KBs) has yielded promising results\non multiple NLP tasks. State-of-the-art approaches typically integrate the\noriginal input sentences with KG triples and feed the combined representation\ninto a BERT model. However, as the sequence length of a BERT model is limited,\nsuch a framework supports little knowledge other than the original input\nsentences and is thus forced to discard some knowledge. This problem is\nespecially severe for downstream tasks for which the input is a long paragraph\nor even a document, such as QA or reading comprehension tasks. We address this\nproblem with Roof-Transformer, a model with two underlying BERTs and a fusion\nlayer on top. One underlying BERT encodes the knowledge resources and the other\none encodes the original input sentences, and the fusion layer integrates the\ntwo resultant encodings. Experimental results on a QA task and the GLUE\nbenchmark attest the effectiveness of the proposed model.", "journal": ""}
{"doi": "10.48550/arXiv.2205.10036", "date": "2022-05-20", "title": "Exploring Extreme Parameter Compression for Pre-trained Language Models", "authors": "Yuxin Ren, Benyou Wang, Lifeng Shang, Xin Jiang, Qun Liu", "abstract": "Recent work explored the potential of large-scale Transformer-based\npre-trained models, especially Pre-trained Language Models (PLMs) in natural\nlanguage processing. This raises many concerns from various perspectives, e.g.,\nfinancial costs and carbon emissions. Compressing PLMs like BERT with\nnegligible performance loss for faster inference and cheaper deployment has\nattracted much attention. In this work, we aim to explore larger compression\nratios for PLMs, among which tensor decomposition is a potential but\nunder-investigated one. Two decomposition and reconstruction protocols are\nfurther proposed to improve the effectiveness and efficiency during\ncompression. Our compressed BERT with ${1}/{7}$ parameters in Transformer\nlayers performs on-par with, sometimes slightly better than the original BERT\nin GLUE benchmark. A tiny version achieves $96.7\\%$ performance of BERT-base\nwith $ {1}/{48} $ encoder parameters (i.e., less than 2M parameters excluding\nthe embedding layer) and $2.7 \\times$ faster on inference. To show that the\nproposed method is orthogonal to existing compression methods like knowledge\ndistillation, we also explore the benefit of the proposed method on a distilled\nBERT.", "journal": ""}
{"doi": "10.48550/arXiv.2207.03037", "date": "2022-07-07", "title": "Sensitivity Analysis on Transferred Neural Architectures of BERT and GPT-2 for Financial Sentiment Analysis", "authors": "Tracy Qian, Andy Xie, Camille Bruckmann", "abstract": "The explosion in novel NLP word embedding and deep learning techniques has\ninduced significant endeavors into potential applications. One of these\ndirections is in the financial sector. Although there is a lot of work done in\nstate-of-the-art models like GPT and BERT, there are relatively few works on\nhow well these methods perform through fine-tuning after being pre-trained, as\nwell as info on how sensitive their parameters are. We investigate the\nperformance and sensitivity of transferred neural architectures from\npre-trained GPT-2 and BERT models. We test the fine-tuning performance based on\nfreezing transformer layers, batch size, and learning rate. We find the\nparameters of BERT are hypersensitive to stochasticity in fine-tuning and that\nGPT-2 is more stable in such practice. It is also clear that the earlier layers\nof GPT-2 and BERT contain essential word pattern information that should be\nmaintained.", "journal": ""}
{"doi": "10.48550/arXiv.2209.02030", "date": "2022-09-05", "title": "Distilling the Knowledge of BERT for CTC-based ASR", "authors": "Hayato Futami, Hirofumi Inaguma, Masato Mimura, Shinsuke Sakai, Tatsuya Kawahara", "abstract": "Connectionist temporal classification (CTC) -based models are attractive\nbecause of their fast inference in automatic speech recognition (ASR). Language\nmodel (LM) integration approaches such as shallow fusion and rescoring can\nimprove the recognition accuracy of CTC-based ASR by taking advantage of the\nknowledge in text corpora. However, they significantly slow down the inference\nof CTC. In this study, we propose to distill the knowledge of BERT for\nCTC-based ASR, extending our previous study for attention-based ASR. CTC-based\nASR learns the knowledge of BERT during training and does not use BERT during\ntesting, which maintains the fast inference of CTC. Different from\nattention-based models, CTC-based models make frame-level predictions, so they\nneed to be aligned with token-level predictions of BERT for distillation. We\npropose to obtain alignments by calculating the most plausible CTC paths.\nExperimental evaluations on the Corpus of Spontaneous Japanese (CSJ) and\nTED-LIUM2 show that our method improves the performance of CTC-based ASR\nwithout the cost of inference speed.", "journal": ""}
{"doi": "10.48550/arXiv.2209.05286", "date": "2022-09-12", "title": "DECK: Behavioral Tests to Improve Interpretability and Generalizability of BERT Models Detecting Depression from Text", "authors": "Jekaterina Novikova, Ksenia Shkaruta", "abstract": "Models that accurately detect depression from text are important tools for\naddressing the post-pandemic mental health crisis. BERT-based classifiers'\npromising performance and the off-the-shelf availability make them great\ncandidates for this task. However, these models are known to suffer from\nperformance inconsistencies and poor generalization. In this paper, we\nintroduce the DECK (DEpression ChecKlist), depression-specific model\nbehavioural tests that allow better interpretability and improve\ngeneralizability of BERT classifiers in depression domain. We create 23 tests\nto evaluate BERT, RoBERTa and ALBERT depression classifiers on three datasets,\ntwo Twitter-based and one clinical interview-based. Our evaluation shows that\nthese models: 1) are robust to certain gender-sensitive variations in text; 2)\nrely on the important depressive language marker of the increased use of first\nperson pronouns; 3) fail to detect some other depression symptoms like suicidal\nideation. We also demonstrate that DECK tests can be used to incorporate\nsymptom-specific information in the training data and consistently improve\ngeneralizability of all three BERT models, with an out-of-distribution F1-score\nincrease of up to 53.93%.", "journal": ""}
{"doi": "10.48550/arXiv.2211.11418", "date": "2022-11-21", "title": "L3Cube-HindBERT and DevBERT: Pre-Trained BERT Transformer models for Devanagari based Hindi and Marathi Languages", "authors": "Raviraj Joshi", "abstract": "The monolingual Hindi BERT models currently available on the model hub do not\nperform better than the multi-lingual models on downstream tasks. We present\nL3Cube-HindBERT, a Hindi BERT model pre-trained on Hindi monolingual corpus.\nFurther, since Indic languages, Hindi and Marathi share the Devanagari script,\nwe train a single model for both languages. We release DevBERT, a Devanagari\nBERT model trained on both Marathi and Hindi monolingual datasets. We evaluate\nthese models on downstream Hindi and Marathi text classification and named\nentity recognition tasks. The HindBERT and DevBERT-based models show\nsignificant improvements over multi-lingual MuRIL, IndicBERT, and XLM-R. Based\non these observations we also release monolingual BERT models for other Indic\nlanguages Kannada, Telugu, Malayalam, Tamil, Gujarati, Assamese, Odia, Bengali,\nand Punjabi. These models are shared at https://huggingface.co/l3cube-pune .", "journal": ""}
{"doi": "10.48550/arXiv.2212.02168", "date": "2022-12-05", "title": "Video Games as a Corpus: Sentiment Analysis using Fallout New Vegas Dialog", "authors": "Mika H\u00e4m\u00e4l\u00e4inen, Khalid Alnajjar, Thierry Poibeau", "abstract": "We present a method for extracting a multilingual sentiment annotated dialog\ndata set from Fallout New Vegas. The game developers have preannotated every\nline of dialog in the game in one of the 8 different sentiments: \\textit{anger,\ndisgust, fear, happy, neutral, pained, sad } and \\textit{surprised}. The game\nhas been translated into English, Spanish, German, French and Italian. We\nconduct experiments on multilingual, multilabel sentiment analysis on the\nextracted data set using multilingual BERT, XLMRoBERTa and language specific\nBERT models. In our experiments, multilingual BERT outperformed XLMRoBERTa for\nmost of the languages, also language specific models were slightly better than\nmultilingual BERT for most of the languages. The best overall accuracy was 54\\%\nand it was achieved by using multilingual BERT on Spanish data. The extracted\ndata set presents a challenging task for sentiment analysis. We have released\nthe data, including the testing and training splits, openly on Zenodo. The data\nset has been shuffled for copyright reasons.", "journal": ""}
{"doi": "10.48550/arXiv.2301.06056", "date": "2023-01-15", "title": "Improving Noise Robustness for Spoken Content Retrieval using Semi-supervised ASR and N-best Transcripts for BERT-based Ranking Models", "authors": "Yasufumi Moriya, Gareth. J. F. Jones", "abstract": "BERT-based re-ranking and dense retrieval (DR) systems have been shown to\nimprove search effectiveness for spoken content retrieval (SCR). However, both\nmethods can still show a reduction in effectiveness when using ASR transcripts\nin comparison to accurate manual transcripts. We find that a known-item search\ntask on the How2 dataset of spoken instruction videos shows a reduction in mean\nreciprocal rank (MRR) scores of 10-14%. As a potential method to reduce this\ndisparity, we investigate the use of semi-supervised ASR transcripts and N-best\nASR transcripts to mitigate ASR errors for spoken search using BERT-based\nranking. Semi-supervised ASR transcripts brought 2-5.5% MRR improvements over\nstandard ASR transcripts and our N-best early fusion methods for BERT DR\nsystems improved MRR by 3-4%. Combining semi-supervised transcripts with N-best\nearly fusion for BERT DR reduced the MRR gap in search effectiveness between\nmanual and ASR transcripts by more than 50% from 14.32% to 6.58%.", "journal": ""}
{"doi": "10.48550/arXiv.2302.07232", "date": "2023-02-14", "title": "A Psycholinguistic Analysis of BERT's Representations of Compounds", "authors": "Lars Buijtelaar, Sandro Pezzelle", "abstract": "This work studies the semantic representations learned by BERT for compounds,\nthat is, expressions such as sunlight or bodyguard. We build on recent studies\nthat explore semantic information in Transformers at the word level and test\nwhether BERT aligns with human semantic intuitions when dealing with\nexpressions (e.g., sunlight) whose overall meaning depends -- to a various\nextent -- on the semantics of the constituent words (sun, light). We leverage a\ndataset that includes human judgments on two psycholinguistic measures of\ncompound semantic analysis: lexeme meaning dominance (LMD; quantifying the\nweight of each constituent toward the compound meaning) and semantic\ntransparency (ST; evaluating the extent to which the compound meaning is\nrecoverable from the constituents' semantics). We show that BERT-based measures\nmoderately align with human intuitions, especially when using contextualized\nrepresentations, and that LMD is overall more predictable than ST. Contrary to\nthe results reported for 'standard' words, higher, more contextualized layers\nare the best at representing compound meaning. These findings shed new light on\nthe abilities of BERT in dealing with fine-grained semantic phenomena.\nMoreover, they can provide insights into how speakers represent compounds.", "journal": ""}
{"doi": "10.48550/arXiv.2303.01081", "date": "2023-03-02", "title": "Can BERT Refrain from Forgetting on Sequential Tasks? A Probing Study", "authors": "Mingxu Tao, Yansong Feng, Dongyan Zhao", "abstract": "Large pre-trained language models help to achieve state of the art on a\nvariety of natural language processing (NLP) tasks, nevertheless, they still\nsuffer from forgetting when incrementally learning a sequence of tasks. To\nalleviate this problem, recent works enhance existing models by sparse\nexperience replay and local adaption, which yield satisfactory performance.\nHowever, in this paper we find that pre-trained language models like BERT have\na potential ability to learn sequentially, even without any sparse memory\nreplay. To verify the ability of BERT to maintain old knowledge, we adopt and\nre-finetune single-layer probe networks with the parameters of BERT fixed. We\ninvestigate the models on two types of NLP tasks, text classification and\nextractive question answering. Our experiments reveal that BERT can actually\ngenerate high quality representations for previously learned tasks in a long\nterm, under extremely sparse replay or even no replay. We further introduce a\nseries of novel methods to interpret the mechanism of forgetting and how memory\nrehearsal plays a significant role in task incremental learning, which bridges\nthe gap between our new discovery and previous studies about catastrophic\nforgetting.", "journal": ""}
{"doi": "10.48550/arXiv.2303.08599", "date": "2023-03-15", "title": "Efficient Uncertainty Estimation with Gaussian Process for Reliable Dialog Response Retrieval", "authors": "Tong Ye, Zhitao Li, Jianzong Wang, Ning Cheng, Jing Xiao", "abstract": "Deep neural networks have achieved remarkable performance in retrieval-based\ndialogue systems, but they are shown to be ill calibrated. Though basic\ncalibration methods like Monte Carlo Dropout and Ensemble can calibrate well,\nthese methods are time-consuming in the training or inference stages. To tackle\nthese challenges, we propose an efficient uncertainty calibration framework\nGPF-BERT for BERT-based conversational search, which employs a Gaussian Process\nlayer and the focal loss on top of the BERT architecture to achieve a\nhigh-quality neural ranker. Extensive experiments are conducted to verify the\neffectiveness of our method. In comparison with basic calibration methods,\nGPF-BERT achieves the lowest empirical calibration error (ECE) in three\nin-domain datasets and the distributional shift tasks, while yielding the\nhighest $R_{10}@1$ and MAP performance on most cases. In terms of time\nconsumption, our GPF-BERT has an 8$\\times$ speedup.", "journal": ""}
{"doi": "10.48550/arXiv.2305.15096", "date": "2023-05-24", "title": "Dynamic Masking Rate Schedules for MLM Pretraining", "authors": "Zachary Ankner, Naomi Saphra, Davis Blalock, Jonathan Frankle, Matthew L. Leavitt", "abstract": "Most works on transformers trained with the Masked Language Modeling (MLM)\nobjective use the original BERT model's fixed masking rate of 15%. We propose\nto instead dynamically schedule the masking rate throughout training. We find\nthat linearly decreasing the masking rate over the course of pretraining\nimproves average GLUE accuracy by up to 0.46% and 0.25% in BERT-base and\nBERT-large, respectively, compared to fixed rate baselines. These gains come\nfrom exposure to both high and low masking rate regimes, providing benefits\nfrom both settings. Our results demonstrate that masking rate scheduling is a\nsimple way to improve the quality of masked language models, achieving up to a\n1.89x speedup in pretraining for BERT-base as well as a Pareto improvement for\nBERT-large.", "journal": ""}
{"doi": "10.48550/arXiv.2306.15298", "date": "2023-06-27", "title": "Gender Bias in BERT -- Measuring and Analysing Biases through Sentiment Rating in a Realistic Downstream Classification Task", "authors": "Sophie Jentzsch, Cigdem Turan", "abstract": "Pretrained language models are publicly available and constantly finetuned\nfor various real-life applications. As they become capable of grasping complex\ncontextual information, harmful biases are likely increasingly intertwined with\nthose models. This paper analyses gender bias in BERT models with two main\ncontributions: First, a novel bias measure is introduced, defining biases as\nthe difference in sentiment valuation of female and male sample versions.\nSecond, we comprehensively analyse BERT's biases on the example of a realistic\nIMDB movie classifier. By systematically varying elements of the training\npipeline, we can conclude regarding their impact on the final model bias. Seven\ndifferent public BERT models in nine training conditions, i.e. 63 models in\ntotal, are compared. Almost all conditions yield significant gender biases.\nResults indicate that reflected biases stem from public BERT models rather than\ntask-specific data, emphasising the weight of responsible usage.", "journal": "Proceedings of the 4th Workshop on Gender Bias in Natural Language\n  Processing (GeBNLP) (2022); Pages 184-199"}
{"doi": "10.48550/arXiv.2307.15331", "date": "2023-07-28", "title": "Tutorials on Stance Detection using Pre-trained Language Models: Fine-tuning BERT and Prompting Large Language Models", "authors": "Yun-Shiuan Chuang", "abstract": "This paper presents two self-contained tutorials on stance detection in\nTwitter data using BERT fine-tuning and prompting large language models (LLMs).\nThe first tutorial explains BERT architecture and tokenization, guiding users\nthrough training, tuning, and evaluating standard and domain-specific BERT\nmodels with HuggingFace transformers. The second focuses on constructing\nprompts and few-shot examples to elicit stances from ChatGPT and open-source\nFLAN-T5 without fine-tuning. Various prompting strategies are implemented and\nevaluated using confusion matrices and macro F1 scores. The tutorials provide\ncode, visualizations, and insights revealing the strengths of few-shot ChatGPT\nand FLAN-T5 which outperform fine-tuned BERTs. By covering both model\nfine-tuning and prompting-based techniques in an accessible, hands-on manner,\nthese tutorials enable learners to gain applied experience with cutting-edge\nmethods for stance detection.", "journal": ""}
{"doi": "10.48550/arXiv.2309.01196", "date": "2023-09-03", "title": "A Visual Interpretation-Based Self-Improved Classification System Using Virtual Adversarial Training", "authors": "Shuai Jiang, Sayaka Kamei, Chen Li, Shengzhe Hou, Yasuhiko Morimoto", "abstract": "The successful application of large pre-trained models such as BERT in\nnatural language processing has attracted more attention from researchers.\nSince the BERT typically acts as an end-to-end black box, classification\nsystems based on it usually have difficulty in interpretation and low\nrobustness. This paper proposes a visual interpretation-based self-improving\nclassification model with a combination of virtual adversarial training (VAT)\nand BERT models to address the above problems. Specifically, a fine-tuned BERT\nmodel is used as a classifier to classify the sentiment of the text. Then, the\npredicted sentiment classification labels are used as part of the input of\nanother BERT for spam classification via a semi-supervised training manner\nusing VAT. Additionally, visualization techniques, including visualizing the\nimportance of words and normalizing the attention head matrix, are employed to\nanalyze the relevance of each component to classification accuracy. Moreover,\nbrand-new features will be found in the visual analysis, and classification\nperformance will be improved. Experimental results on Twitter's tweet dataset\ndemonstrate the effectiveness of the proposed model on the classification task.\nFurthermore, the ablation study results illustrate the effect of different\ncomponents of the proposed model on the classification results.", "journal": ""}
{"doi": "10.48550/arXiv.2310.20144", "date": "2023-10-31", "title": "EELBERT: Tiny Models through Dynamic Embeddings", "authors": "Gabrielle Cohn, Rishika Agarwal, Deepanshu Gupta, Siddharth Patwardhan", "abstract": "We introduce EELBERT, an approach for compression of transformer-based models\n(e.g., BERT), with minimal impact on the accuracy of downstream tasks. This is\nachieved by replacing the input embedding layer of the model with dynamic, i.e.\non-the-fly, embedding computations. Since the input embedding layer accounts\nfor a significant fraction of the model size, especially for the smaller BERT\nvariants, replacing this layer with an embedding computation function helps us\nreduce the model size significantly. Empirical evaluation on the GLUE benchmark\nshows that our BERT variants (EELBERT) suffer minimal regression compared to\nthe traditional BERT models. Through this approach, we are able to develop our\nsmallest model UNO-EELBERT, which achieves a GLUE score within 4% of fully\ntrained BERT-tiny, while being 15x smaller (1.2 MB) in size.", "journal": ""}
{"doi": "10.48550/arXiv.2312.03194", "date": "2023-12-06", "title": "Corporate Bankruptcy Prediction with Domain-Adapted BERT", "authors": "Alex Kim, Sangwon Yoon", "abstract": "This study performs BERT-based analysis, which is a representative\ncontextualized language model, on corporate disclosure data to predict\nimpending bankruptcies. Prior literature on bankruptcy prediction mainly\nfocuses on developing more sophisticated prediction methodologies with\nfinancial variables. However, in our study, we focus on improving the quality\nof input dataset. Specifically, we employ BERT model to perform sentiment\nanalysis on MD&A disclosures. We show that BERT outperforms dictionary-based\npredictions and Word2Vec-based predictions in terms of adjusted R-square in\nlogistic regression, k-nearest neighbor (kNN-5), and linear kernel support\nvector machine (SVM). Further, instead of pre-training the BERT model from\nscratch, we apply self-learning with confidence-based filtering to corporate\ndisclosure data (10-K). We achieve the accuracy rate of 91.56% and demonstrate\nthat the domain adaptation procedure brings a significant improvement in\nprediction accuracy.", "journal": "Proceedings of the Third Workshop on Economics and Natural\n  Language Processing, 2021, 26--36"}
{"doi": "10.48550/arXiv.2405.15039", "date": "2024-05-23", "title": "CEEBERT: Cross-Domain Inference in Early Exit BERT", "authors": "Divya Jyoti Bajpai, Manjesh Kumar Hanawal", "abstract": "Pre-trained Language Models (PLMs), like BERT, with self-supervision\nobjectives exhibit remarkable performance and generalization across various\ntasks. However, they suffer in inference latency due to their large size. To\naddress this issue, side branches are attached at intermediate layers, enabling\nearly inference of samples without requiring them to pass through all layers.\nHowever, the challenge is to decide which layer to infer and exit each sample\nso that the accuracy and latency are balanced. Moreover, the distribution of\nthe samples to be inferred may differ from that used for training necessitating\ncross-domain adaptation. We propose an online learning algorithm named\nCross-Domain Inference in Early Exit BERT (CeeBERT) that dynamically determines\nearly exits of samples based on the level of confidence at each exit point.\nCeeBERT learns optimal thresholds from domain-specific confidence observed at\nintermediate layers on the fly, eliminating the need for labeled data.\nExperimental results on five distinct datasets with BERT and ALBERT models\ndemonstrate CeeBERT's ability to improve latency by reducing unnecessary\ncomputations with minimal drop in performance. By adapting to the threshold\nvalues, CeeBERT can speed up the BERT/ALBERT models by $2\\times$ - $3.5\\times$\nwith minimal drop in accuracy.", "journal": ""}
{"doi": "10.48550/arXiv.2406.09967", "date": "2024-06-14", "title": "Bag of Lies: Robustness in Continuous Pre-training BERT", "authors": "Ine Gevers, Walter Daelemans", "abstract": "This study aims to acquire more insights into the continuous pre-training\nphase of BERT regarding entity knowledge, using the COVID-19 pandemic as a case\nstudy. Since the pandemic emerged after the last update of BERT's pre-training\ndata, the model has little to no entity knowledge about COVID-19. Using\ncontinuous pre-training, we control what entity knowledge is available to the\nmodel. We compare the baseline BERT model with the further pre-trained variants\non the fact-checking benchmark Check-COVID. To test the robustness of\ncontinuous pre-training, we experiment with several adversarial methods to\nmanipulate the input data, such as training on misinformation and shuffling the\nword order until the input becomes nonsensical. Surprisingly, our findings\nreveal that these methods do not degrade, and sometimes even improve, the\nmodel's downstream performance. This suggests that continuous pre-training of\nBERT is robust against misinformation. Furthermore, we are releasing a new\ndataset, consisting of original texts from academic publications in the\nLitCovid repository and their AI-generated false counterparts.", "journal": ""}
{"doi": "10.48550/arXiv.2406.17282", "date": "2024-06-25", "title": "SetBERT: Enhancing Retrieval Performance for Boolean Logic and Set Operation Queries", "authors": "Quan Mai, Susan Gauch, Douglas Adams", "abstract": "We introduce SetBERT, a fine-tuned BERT-based model designed to enhance query\nembeddings for set operations and Boolean logic queries, such as Intersection\n(AND), Difference (NOT), and Union (OR). SetBERT significantly improves\nretrieval performance for logic-structured queries, an area where both\ntraditional and neural retrieval methods typically underperform. We propose an\ninnovative use of inversed-contrastive loss, focusing on identifying the\nnegative sentence, and fine-tuning BERT with a dataset generated via prompt\nGPT. Furthermore, we demonstrate that, unlike other BERT-based models,\nfine-tuning with triplet loss actually degrades performance for this specific\ntask. Our experiments reveal that SetBERT-base not only significantly\noutperforms BERT-base (up to a 63% improvement in Recall) but also achieves\nperformance comparable to the much larger BERT-large model, despite being only\none-third the size.", "journal": ""}
{"doi": "10.48550/arXiv.2410.00022", "date": "2024-09-16", "title": "TREB: a BERT attempt for imputing tabular data imputation", "authors": "Shuyue Wang, Wenjun Zhou, Han drk-m-s Jiang, Shuo Wang, Ren Zheng", "abstract": "TREB, a novel tabular imputation framework utilizing BERT, introduces a\ngroundbreaking approach for handling missing values in tabular data. Unlike\ntraditional methods that often overlook the specific demands of imputation,\nTREB leverages the robust capabilities of BERT to address this critical task.\nWhile many BERT-based approaches for tabular data have emerged, they frequently\nunder-utilize the language model's full potential. To rectify this, TREB\nemploys a BERT-based model fine-tuned specifically for the task of imputing\nreal-valued continuous numbers in tabular datasets. The paper comprehensively\naddresses the unique challenges posed by tabular data imputation, emphasizing\nthe importance of context-based interconnections. The effectiveness of TREB is\nvalidated through rigorous evaluation using the California Housing dataset. The\nresults demonstrate its ability to preserve feature interrelationships and\naccurately impute missing values. Moreover, the authors shed light on the\ncomputational efficiency and environmental impact of TREB, quantifying the\nfloating-point operations (FLOPs) and carbon footprint associated with its\ntraining and deployment.", "journal": ""}
{"doi": "10.48550/arXiv.2410.20792", "date": "2024-10-28", "title": "Deep Learning for Medical Text Processing: BERT Model Fine-Tuning and Comparative Study", "authors": "Jiacheng Hu, Yiru Cang, Guiran Liu, Meiqi Wang, Weijie He, Runyuan Bao", "abstract": "This paper proposes a medical literature summary generation method based on\nthe BERT model to address the challenges brought by the current explosion of\nmedical information. By fine-tuning and optimizing the BERT model, we develop\nan efficient summary generation system that can quickly extract key information\nfrom medical literature and generate coherent, accurate summaries. In the\nexperiment, we compared various models, including Seq-Seq, Attention,\nTransformer, and BERT, and demonstrated that the improved BERT model offers\nsignificant advantages in the Rouge and Recall metrics. Furthermore, the\nresults of this study highlight the potential of knowledge distillation\ntechniques to further enhance model performance. The system has demonstrated\nstrong versatility and efficiency in practical applications, offering a\nreliable tool for the rapid screening and analysis of medical literature.", "journal": ""}
{"doi": "10.48550/arXiv.2411.12703", "date": "2024-11-19", "title": "Strengthening Fake News Detection: Leveraging SVM and Sophisticated Text Vectorization Techniques. Defying BERT?", "authors": "Ahmed Akib Jawad Karim, Kazi Hafiz Md Asad, Aznur Azam", "abstract": "The rapid spread of misinformation, particularly through online platforms,\nunderscores the urgent need for reliable detection systems. This study explores\nthe utilization of machine learning and natural language processing,\nspecifically Support Vector Machines (SVM) and BERT, to detect news that are\nfake. We employ three distinct text vectorization methods for SVM: Term\nFrequency Inverse Document Frequency (TF-IDF), Word2Vec, and Bag of Words (BoW)\nevaluating their effectiveness in distinguishing between genuine and fake news.\nAdditionally, we compare these methods against the transformer large language\nmodel, BERT. Our comprehensive approach includes detailed preprocessing steps,\nrigorous model implementation, and thorough evaluation to determine the most\neffective techniques. The results demonstrate that while BERT achieves superior\naccuracy with 99.98% and an F1-score of 0.9998, the SVM model with a linear\nkernel and BoW vectorization also performs exceptionally well, achieving 99.81%\naccuracy and an F1-score of 0.9980. These findings highlight that, despite\nBERT's superior performance, SVM models with BoW and TF-IDF vectorization\nmethods come remarkably close, offering highly competitive performance with the\nadvantage of lower computational requirements.", "journal": ""}
{"doi": "10.48550/arXiv.2411.14254", "date": "2024-11-21", "title": "BERT-Based Approach for Automating Course Articulation Matrix Construction with Explainable AI", "authors": "Natenaile Asmamaw Shiferaw, Simpenzwe Honore Leandre, Aman Sinha, Dillip Rout", "abstract": "Course Outcome (CO) and Program Outcome (PO)/Program-Specific Outcome (PSO)\nalignment is a crucial task for ensuring curriculum coherence and assessing\neducational effectiveness. The construction of a Course Articulation Matrix\n(CAM), which quantifies the relationship between COs and POs/PSOs, typically\ninvolves assigning numerical values (0, 1, 2, 3) to represent the degree of\nalignment. In this study, We experiment with four models from the BERT family:\nBERT Base, DistilBERT, ALBERT, and RoBERTa, and use multiclass classification\nto assess the alignment between CO and PO/PSO pairs. We first evaluate\ntraditional machine learning classifiers, such as Decision Tree, Random Forest,\nand XGBoost, and then apply transfer learning to evaluate the performance of\nthe pretrained BERT models. To enhance model interpretability, we apply\nExplainable AI technique, specifically Local Interpretable Model-agnostic\nExplanations (LIME), to provide transparency into the decision-making process.\nOur system achieves accuracy, precision, recall, and F1-score values of 98.66%,\n98.67%, 98.66%, and 98.66%, respectively. This work demonstrates the potential\nof utilizing transfer learning with BERT-based models for the automated\ngeneration of CAMs, offering high performance and interpretability in\neducational outcome assessment.", "journal": ""}
{"doi": "10.48550/arXiv.2411.14877", "date": "2024-11-22", "title": "Astro-HEP-BERT: A bidirectional language model for studying the meanings of concepts in astrophysics and high energy physics", "authors": "Arno Simons", "abstract": "I present Astro-HEP-BERT, a transformer-based language model specifically\ndesigned for generating contextualized word embeddings (CWEs) to study the\nmeanings of concepts in astrophysics and high-energy physics. Built on a\ngeneral pretrained BERT model, Astro-HEP-BERT underwent further training over\nthree epochs using the Astro-HEP Corpus, a dataset I curated from 21.84 million\nparagraphs extracted from more than 600,000 scholarly articles on arXiv, all\nbelonging to at least one of these two scientific domains. The project\ndemonstrates both the effectiveness and feasibility of adapting a bidirectional\ntransformer for applications in the history, philosophy, and sociology of\nscience (HPSS). The entire training process was conducted using freely\navailable code, pretrained weights, and text inputs, completed on a single\nMacBook Pro Laptop (M2/96GB). Preliminary evaluations indicate that\nAstro-HEP-BERT's CWEs perform comparably to domain-adapted BERT models trained\nfrom scratch on larger datasets for domain-specific word sense disambiguation\nand induction and related semantic change analyses. This suggests that\nretraining general language models for specific scientific domains can be a\ncost-effective and efficient strategy for HPSS researchers, enabling high\nperformance without the need for extensive training from scratch.", "journal": ""}
{"doi": "10.48550/arXiv.2502.03984", "date": "2025-02-06", "title": "PGB: One-Shot Pruning for BERT via Weight Grouping and Permutation", "authors": "Hyemin Lim, Jaeyeon Lee, Dong-Wan Choi", "abstract": "Large pretrained language models such as BERT suffer from slow inference and\nhigh memory usage, due to their huge size. Recent approaches to compressing\nBERT rely on iterative pruning and knowledge distillation, which, however, are\noften too complicated and computationally intensive. This paper proposes a\nnovel semi-structured one-shot pruning method for BERT, called\n$\\textit{Permutation and Grouping for BERT}$ (PGB), which achieves high\ncompression efficiency and sparsity while preserving accuracy. To this end, PGB\nidentifies important groups of individual weights by permutation and prunes all\nother weights as a structure in both multi-head attention and feed-forward\nlayers. Furthermore, if no important group is formed in a particular layer, PGB\ndrops the entire layer to produce an even more compact model. Our experimental\nresults on BERT$_{\\text{BASE}}$ demonstrate that PGB outperforms the\nstate-of-the-art structured pruning methods in terms of computational cost and\naccuracy preservation.", "journal": ""}
{"doi": "10.48550/arXiv.2502.12033", "date": "2025-02-17", "title": "The geometry of BERT", "authors": "Matteo Bonino, Giorgia Ghione, Giansalvo Cirrincione", "abstract": "Transformer neural networks, particularly Bidirectional Encoder\nRepresentations from Transformers (BERT), have shown remarkable performance\nacross various tasks such as classification, text summarization, and question\nanswering. However, their internal mechanisms remain mathematically obscure,\nhighlighting the need for greater explainability and interpretability. In this\ndirection, this paper investigates the internal mechanisms of BERT proposing a\nnovel perspective on the attention mechanism of BERT from a theoretical\nperspective. The analysis encompasses both local and global network behavior.\nAt the local level, the concept of directionality of subspace selection as well\nas a comprehensive study of the patterns emerging from the self-attention\nmatrix are presented. Additionally, this work explores the semantic content of\nthe information stream through data distribution analysis and global\nstatistical measures including the novel concept of cone index. A case study on\nthe classification of SARS-CoV-2 variants using RNA which resulted in a very\nhigh accuracy has been selected in order to observe these concepts in an\napplication. The insights gained from this analysis contribute to a deeper\nunderstanding of BERT's classification process, offering potential avenues for\nfuture architectural improvements in Transformer models and further analysis in\nthe training process.", "journal": ""}
{"doi": "10.48550/arXiv.2502.16312", "date": "2025-02-22", "title": "Iterative Auto-Annotation for Scientific Named Entity Recognition Using BERT-Based Models", "authors": "Kartik Gupta", "abstract": "This paper presents an iterative approach to performing Scientific Named\nEntity Recognition (SciNER) using BERT-based models. We leverage transfer\nlearning to fine-tune pretrained models with a small but high-quality set of\nmanually annotated data. The process is iteratively refined by using the\nfine-tuned model to auto-annotate a larger dataset, followed by additional\nrounds of fine-tuning. We evaluated two models, dslim/bert-large-NER and\nbert-largecased, and found that bert-large-cased consistently outperformed the\nformer. Our approach demonstrated significant improvements in prediction\naccuracy and F1 scores, especially for less common entity classes. Future work\ncould include pertaining with unlabeled data, exploring more powerful encoders\nlike RoBERTa, and expanding the scope of manual annotations. This methodology\nhas broader applications in NLP tasks where access to labeled data is limited.", "journal": ""}
{"doi": "10.48550/arXiv.2504.21037", "date": "2025-04-28", "title": "Security Bug Report Prediction Within and Across Projects: A Comparative Study of BERT and Random Forest", "authors": "Farnaz Soltaniani, Mohammad Ghafari, Mohammed Sayagh", "abstract": "Early detection of security bug reports (SBRs) is crucial for preventing\nvulnerabilities and ensuring system reliability. While machine learning models\nhave been developed for SBR prediction, their predictive performance still has\nroom for improvement. In this study, we conduct a comprehensive comparison\nbetween BERT and Random Forest (RF), a competitive baseline for predicting\nSBRs. The results show that RF outperforms BERT with a 34% higher average\nG-measure for within-project predictions. Adding only SBRs from various\nprojects improves both models' average performance. However, including both\nsecurity and nonsecurity bug reports significantly reduces RF's average\nperformance to 46%, while boosts BERT to its best average performance of 66%,\nsurpassing RF. In cross-project SBR prediction, BERT achieves a remarkable 62%\nG-measure, which is substantially higher than RF.", "journal": ""}
{"doi": "10.48550/arXiv.2505.15696", "date": "2025-05-21", "title": "MaxPoolBERT: Enhancing BERT Classification via Layer- and Token-Wise Aggregation", "authors": "Maike Behrendt, Stefan Sylvius Wagner, Stefan Harmeling", "abstract": "The [CLS] token in BERT is commonly used as a fixed-length representation for\nclassification tasks, yet prior work has shown that both other tokens and\nintermediate layers encode valuable contextual information. In this work, we\npropose MaxPoolBERT, a lightweight extension to BERT that refines the [CLS]\nrepresentation by aggregating information across layers and tokens.\nSpecifically, we explore three modifications: (i) max-pooling the [CLS] token\nacross multiple layers, (ii) enabling the [CLS] token to attend over the entire\nfinal layer using an additional multi-head attention (MHA) layer, and (iii)\ncombining max-pooling across the full sequence with MHA. Our approach enhances\nBERT's classification accuracy (especially on low-resource tasks) without\nrequiring pre-training or significantly increasing model size. Experiments on\nthe GLUE benchmark show that MaxPoolBERT consistently achieves a better\nperformance on the standard BERT-base model.", "journal": ""}
{"doi": "10.48550/arXiv.2505.18215", "date": "2025-05-23", "title": "Do BERT-Like Bidirectional Models Still Perform Better on Text Classification in the Era of LLMs?", "authors": "Junyan Zhang, Yiming Huang, Shuliang Liu, Yubo Gao, Xuming Hu", "abstract": "The rapid adoption of LLMs has overshadowed the potential advantages of\ntraditional BERT-like models in text classification. This study challenges the\nprevailing \"LLM-centric\" trend by systematically comparing three category\nmethods, i.e., BERT-like models fine-tuning, LLM internal state utilization,\nand zero-shot inference across six high-difficulty datasets. Our findings\nreveal that BERT-like models often outperform LLMs. We further categorize\ndatasets into three types, perform PCA and probing experiments, and identify\ntask-specific model strengths: BERT-like models excel in pattern-driven tasks,\nwhile LLMs dominate those requiring deep semantics or world knowledge. Based on\nthis, we propose TaMAS, a fine-grained task selection strategy, advocating for\na nuanced, task-driven approach over a one-size-fits-all reliance on LLMs.", "journal": ""}
{"doi": "10.48550/arXiv.2506.11485", "date": "2025-06-13", "title": "Relational Schemata in BERT Are Inducible, Not Emergent: A Study of Performance vs. Competence in Language Models", "authors": "Cole Gawin", "abstract": "While large language models like BERT demonstrate strong empirical\nperformance on semantic tasks, whether this reflects true conceptual competence\nor surface-level statistical association remains unclear. I investigate whether\nBERT encodes abstract relational schemata by examining internal representations\nof concept pairs across taxonomic, mereological, and functional relations. I\ncompare BERT's relational classification performance with representational\nstructure in [CLS] token embeddings. Results reveal that pretrained BERT\nenables high classification accuracy, indicating latent relational signals.\nHowever, concept pairs organize by relation type in high-dimensional embedding\nspace only after fine-tuning on supervised relation classification tasks. This\nindicates relational schemata are not emergent from pretraining alone but can\nbe induced via task scaffolding. These findings demonstrate that behavioral\nperformance does not necessarily imply structured conceptual understanding,\nthough models can acquire inductive biases for grounded relational abstraction\nthrough appropriate training.", "journal": ""}
{"doi": "10.48550/arXiv.2506.18602", "date": "2025-06-23", "title": "Semantic similarity estimation for domain specific data using BERT and other techniques", "authors": "R. Prashanth", "abstract": "Estimation of semantic similarity is an important research problem both in\nnatural language processing and the natural language understanding, and that\nhas tremendous application on various downstream tasks such as question\nanswering, semantic search, information retrieval, document clustering,\nword-sense disambiguation and machine translation. In this work, we carry out\nthe estimation of semantic similarity using different state-of-the-art\ntechniques including the USE (Universal Sentence Encoder), InferSent and the\nmost recent BERT, or Bidirectional Encoder Representations from Transformers,\nmodels. We use two question pairs datasets for the analysis, one is a domain\nspecific in-house dataset and the other is a public dataset which is the\nQuora's question pairs dataset. We observe that the BERT model gave much\nsuperior performance as compared to the other methods. This should be because\nof the fine-tuning procedure that is involved in its training process, allowing\nit to learn patterns based on the training data that is used. This works\ndemonstrates the applicability of BERT on domain specific datasets. We infer\nfrom the analysis that BERT is the best technique to use in the case of domain\nspecific data.", "journal": ""}
{"doi": "10.48550/arXiv.1903.10972", "date": "2019-03-26", "title": "Simple Applications of BERT for Ad Hoc Document Retrieval", "authors": "Wei Yang, Haotian Zhang, Jimmy Lin", "abstract": "Following recent successes in applying BERT to question answering, we explore\nsimple applications to ad hoc document retrieval. This required confronting the\nchallenge posed by documents that are typically longer than the length of input\nBERT was designed to handle. We address this issue by applying inference on\nsentences individually, and then aggregating sentence scores to produce\ndocument scores. Experiments on TREC microblog and newswire test collections\nshow that our approach is simple yet effective, as we report the highest\naverage precision on these datasets by neural approaches that we are aware of.", "journal": ""}
{"doi": "10.48550/arXiv.1909.03193", "date": "2019-09-07", "title": "KG-BERT: BERT for Knowledge Graph Completion", "authors": "Liang Yao, Chengsheng Mao, Yuan Luo", "abstract": "Knowledge graphs are important resources for many artificial intelligence\ntasks but often suffer from incompleteness. In this work, we propose to use\npre-trained language models for knowledge graph completion. We treat triples in\nknowledge graphs as textual sequences and propose a novel framework named\nKnowledge Graph Bidirectional Encoder Representations from Transformer\n(KG-BERT) to model these triples. Our method takes entity and relation\ndescriptions of a triple as input and computes scoring function of the triple\nwith the KG-BERT language model. Experimental results on multiple benchmark\nknowledge graphs show that our method can achieve state-of-the-art performance\nin triple classification, link prediction and relation prediction tasks.", "journal": ""}
{"doi": "10.48550/arXiv.1909.03526", "date": "2019-09-08", "title": "Multi-Task Bidirectional Transformer Representations for Irony Detection", "authors": "Chiyu Zhang, Muhammad Abdul-Mageed", "abstract": "Supervised deep learning requires large amounts of training data. In the\ncontext of the FIRE2019 Arabic irony detection shared task (IDAT@FIRE2019), we\nshow how we mitigate this need by fine-tuning the pre-trained bidirectional\nencoders from transformers (BERT) on gold data in a multi-task setting. We\nfurther improve our models by by further pre-training BERT on `in-domain' data,\nthus alleviating an issue of dialect mismatch in the Google-released BERT\nmodel. Our best model acquires 82.4 macro F1 score, and has the unique\nadvantage of being feature-engineering free (i.e., based exclusively on deep\nlearning).", "journal": ""}
{"doi": "10.48550/arXiv.1909.08358", "date": "2019-09-18", "title": "Using BERT for Word Sense Disambiguation", "authors": "Jiaju Du, Fanchao Qi, Maosong Sun", "abstract": "Word Sense Disambiguation (WSD), which aims to identify the correct sense of\na given polyseme, is a long-standing problem in NLP. In this paper, we propose\nto use BERT to extract better polyseme representations for WSD and explore\nseveral ways of combining BERT and the classifier. We also utilize sense\ndefinitions to train a unified classifier for all words, which enables the\nmodel to disambiguate unseen polysemes. Experiments show that our model\nachieves the state-of-the-art results on the standard English All-word WSD\nevaluation.", "journal": ""}
{"doi": "10.48550/arXiv.1909.08402", "date": "2019-09-18", "title": "Enriching BERT with Knowledge Graph Embeddings for Document Classification", "authors": "Malte Ostendorff, Peter Bourgonje, Maria Berger, Julian Moreno-Schneider, Georg Rehm, Bela Gipp", "abstract": "In this paper, we focus on the classification of books using short\ndescriptive texts (cover blurbs) and additional metadata. Building upon BERT, a\ndeep neural language model, we demonstrate how to combine text representations\nwith metadata and knowledge graph embeddings, which encode author information.\nCompared to the standard BERT approach we achieve considerably better results\nfor the classification task. For a more coarse-grained classification using\neight labels we achieve an F1- score of 87.20, while a detailed classification\nusing 343 labels yields an F1-score of 64.70. We make the source code and\ntrained models of our experiments publicly available", "journal": ""}
{"doi": "10.48550/arXiv.1910.12647", "date": "2019-10-25", "title": "HUBERT Untangles BERT to Improve Transfer across NLP Tasks", "authors": "Mehrad Moradshahi, Hamid Palangi, Monica S. Lam, Paul Smolensky, Jianfeng Gao", "abstract": "We introduce HUBERT which combines the structured-representational power of\nTensor-Product Representations (TPRs) and BERT, a pre-trained bidirectional\nTransformer language model. We show that there is shared structure between\ndifferent NLP datasets that HUBERT, but not BERT, is able to learn and\nleverage. We validate the effectiveness of our model on the GLUE benchmark and\nHANS dataset. Our experiment results show that untangling data-specific\nsemantics from general language structure is key for better transfer among NLP\ntasks.", "journal": ""}
{"doi": "10.48550/arXiv.2105.11408", "date": "2021-05-24", "title": "Diacritics Restoration using BERT with Analysis on Czech language", "authors": "Jakub N\u00e1plava, Milan Straka, Jana Strakov\u00e1", "abstract": "We propose a new architecture for diacritics restoration based on\ncontextualized embeddings, namely BERT, and we evaluate it on 12 languages with\ndiacritics. Furthermore, we conduct a detailed error analysis on Czech, a\nmorphologically rich language with a high level of diacritization. Notably, we\nmanually annotate all mispredictions, showing that roughly 44% of them are\nactually not errors, but either plausible variants (19%), or the system\ncorrections of erroneous data (25%). Finally, we categorize the real errors in\ndetail. We release the code at\nhttps://github.com/ufal/bert-diacritics-restoration.", "journal": "The Prague Bulletin of Mathematical Linguistics No. 116, 2021, pp.\n  27-42"}
{"doi": "10.48550/arXiv.2105.13479", "date": "2021-05-27", "title": "Leveraging Linguistic Coordination in Reranking N-Best Candidates For End-to-End Response Selection Using BERT", "authors": "Mingzhi Yu, Diane Litman", "abstract": "Retrieval-based dialogue systems select the best response from many\ncandidates. Although many state-of-the-art models have shown promising\nperformance in dialogue response selection tasks, there is still quite a gap\nbetween R@1 and R@10 performance. To address this, we propose to leverage\nlinguistic coordination (a phenomenon that individuals tend to develop similar\nlinguistic behaviors in conversation) to rerank the N-best candidates produced\nby BERT, a state-of-the-art pre-trained language model. Our results show an\nimprovement in R@1 compared to BERT baselines, demonstrating the utility of\nrepairing machine-generated outputs by leveraging a linguistic theory.", "journal": ""}
{"doi": "10.48550/arXiv.2201.07449", "date": "2022-01-19", "title": "TourBERT: A pretrained language model for the tourism industry", "authors": "Veronika Arefieva, Roman Egger", "abstract": "The Bidirectional Encoder Representations from Transformers (BERT) is\ncurrently one of the most important and state-of-the-art models for natural\nlanguage. However, it has also been shown that for domain-specific tasks it is\nhelpful to pretrain BERT on a domain-specific corpus. In this paper, we present\nTourBERT, a pretrained language model for tourism. We describe how TourBERT was\ndeveloped and evaluated. The evaluations show that TourBERT is outperforming\nBERT in all tourism-specific tasks.", "journal": ""}
{"doi": "10.48550/arXiv.1911.00473", "date": "2019-11-01", "title": "BERT Goes to Law School: Quantifying the Competitive Advantage of Access to Large Legal Corpora in Contract Understanding", "authors": "Emad Elwany, Dave Moore, Gaurav Oberoi", "abstract": "Fine-tuning language models, such as BERT, on domain specific corpora has\nproven to be valuable in domains like scientific papers and biomedical text. In\nthis paper, we show that fine-tuning BERT on legal documents similarly provides\nvaluable improvements on NLP tasks in the legal domain. Demonstrating this\noutcome is significant for analyzing commercial agreements, because obtaining\nlarge legal corpora is challenging due to their confidential nature. As such,\nwe show that having access to large legal corpora is a competitive advantage\nfor commercial applications, and academic research on analyzing contracts.", "journal": ""}
{"doi": "10.48550/arXiv.2001.11985", "date": "2020-01-31", "title": "Pretrained Transformers for Simple Question Answering over Knowledge Graphs", "authors": "D. Lukovnikov, A. Fischer, J. Lehmann", "abstract": "Answering simple questions over knowledge graphs is a well-studied problem in\nquestion answering. Previous approaches for this task built on recurrent and\nconvolutional neural network based architectures that use pretrained word\nembeddings. It was recently shown that finetuning pretrained transformer\nnetworks (e.g. BERT) can outperform previous approaches on various natural\nlanguage processing tasks. In this work, we investigate how well BERT performs\non SimpleQuestions and provide an evaluation of both BERT and BiLSTM-based\nmodels in datasparse scenarios.", "journal": ""}
{"doi": "10.48550/arXiv.2002.08562", "date": "2020-02-20", "title": "Federated pretraining and fine tuning of BERT using clinical notes from multiple silos", "authors": "Dianbo Liu, Tim Miller", "abstract": "Large scale contextual representation models, such as BERT, have\nsignificantly advanced natural language processing (NLP) in recently years.\nHowever, in certain area like healthcare, accessing diverse large scale text\ndata from multiple institutions is extremely challenging due to privacy and\nregulatory reasons. In this article, we show that it is possible to both\npretrain and fine tune BERT models in a federated manner using clinical texts\nfrom different silos without moving the data.", "journal": ""}
{"doi": "10.48550/arXiv.2004.08731", "date": "2020-04-18", "title": "Enhancing Pharmacovigilance with Drug Reviews and Social Media", "authors": "Brent Biseda, Katie Mo", "abstract": "This paper explores whether the use of drug reviews and social media could be\nleveraged as potential alternative sources for pharmacovigilance of adverse\ndrug reactions (ADRs). We examined the performance of BERT alongside two\nvariants that are trained on biomedical papers, BioBERT7, and clinical notes,\nClinical BERT8. A variety of 8 different BERT models were fine-tuned and\ncompared across three different tasks in order to evaluate their relative\nperformance to one another in the ADR tasks. The tasks include sentiment\nclassification of drug reviews, presence of ADR in twitter postings, and named\nentity recognition of ADRs in twitter postings. BERT demonstrates its\nflexibility with high performance across all three different pharmacovigilance\nrelated tasks.", "journal": ""}
{"doi": "10.48550/arXiv.2004.14577", "date": "2020-04-30", "title": "Exploring Contextualized Neural Language Models for Temporal Dependency Parsing", "authors": "Hayley Ross, Jonathon Cai, Bonan Min", "abstract": "Extracting temporal relations between events and time expressions has many\napplications such as constructing event timelines and time-related question\nanswering. It is a challenging problem which requires syntactic and semantic\ninformation at sentence or discourse levels, which may be captured by deep\ncontextualized language models (LMs) such as BERT (Devlin et al., 2019). In\nthis paper, we develop several variants of BERT-based temporal dependency\nparser, and show that BERT significantly improves temporal dependency parsing\n(Zhang and Xue, 2018a). We also present a detailed analysis on why deep\ncontextualized neural LMs help and where they may fall short. Source code and\nresources are made available at https://github.com/bnmin/tdp_ranking.", "journal": ""}
{"doi": "10.48550/arXiv.2007.11088", "date": "2020-07-21", "title": "Understanding BERT Rankers Under Distillation", "authors": "Luyu Gao, Zhuyun Dai, Jamie Callan", "abstract": "Deep language models such as BERT pre-trained on large corpus have given a\nhuge performance boost to the state-of-the-art information retrieval ranking\nsystems. Knowledge embedded in such models allows them to pick up complex\nmatching signals between passages and queries. However, the high computation\ncost during inference limits their deployment in real-world search scenarios.\nIn this paper, we study if and how the knowledge for search within BERT can be\ntransferred to a smaller ranker through distillation. Our experiments\ndemonstrate that it is crucial to use a proper distillation procedure, which\nproduces up to nine times speedup while preserving the state-of-the-art\nperformance.", "journal": ""}
{"doi": "10.48550/arXiv.2008.00805", "date": "2020-08-03", "title": "LT@Helsinki at SemEval-2020 Task 12: Multilingual or language-specific BERT?", "authors": "Marc P\u00e0mies, Emily \u00d6hman, Kaisla Kajava, J\u00f6rg Tiedemann", "abstract": "This paper presents the different models submitted by the LT@Helsinki team\nfor the SemEval 2020 Shared Task 12. Our team participated in sub-tasks A and\nC; titled offensive language identification and offense target identification,\nrespectively. In both cases we used the so-called Bidirectional Encoder\nRepresentation from Transformer (BERT), a model pre-trained by Google and\nfine-tuned by us on the OLID and SOLID datasets. The results show that\noffensive tweet classification is one of several language-based tasks where\nBERT can achieve state-of-the-art results.", "journal": ""}
{"doi": "10.48550/arXiv.2010.02686", "date": "2020-10-06", "title": "BERT Knows Punta Cana is not just beautiful, it's gorgeous: Ranking Scalar Adjectives with Contextualised Representations", "authors": "Aina Gar\u00ed Soler, Marianna Apidianaki", "abstract": "Adjectives like pretty, beautiful and gorgeous describe positive properties\nof the nouns they modify but with different intensity. These differences are\nimportant for natural language understanding and reasoning. We propose a novel\nBERT-based approach to intensity detection for scalar adjectives. We model\nintensity by vectors directly derived from contextualised representations and\nshow they can successfully rank scalar adjectives. We evaluate our models both\nintrinsically, on gold standard datasets, and on an Indirect Question Answering\ntask. Our results demonstrate that BERT encodes rich knowledge about the\nsemantics of scalar adjectives, and is able to provide better quality intensity\nrankings than static embeddings and previous models with access to dedicated\nresources.", "journal": ""}
{"doi": "10.48550/arXiv.2011.05007", "date": "2020-11-10", "title": "To What Degree Can Language Borders Be Blurred In BERT-based Multilingual Spoken Language Understanding?", "authors": "Quynh Do, Judith Gaspers, Tobias Roding, Melanie Bradford", "abstract": "This paper addresses the question as to what degree a BERT-based multilingual\nSpoken Language Understanding (SLU) model can transfer knowledge across\nlanguages. Through experiments we will show that, although it works\nsubstantially well even on distant language groups, there is still a gap to the\nideal multilingual performance. In addition, we propose a novel BERT-based\nadversarial model architecture to learn language-shared and language-specific\nrepresentations for multilingual SLU. Our experimental results prove that the\nproposed model is capable of narrowing the gap to the ideal multilingual\nperformance.", "journal": ""}
{"doi": "10.48550/arXiv.2012.14763", "date": "2020-12-29", "title": "CMV-BERT: Contrastive multi-vocab pretraining of BERT", "authors": "Wei Zhu, Daniel Cheung", "abstract": "In this work, we represent CMV-BERT, which improves the pretraining of a\nlanguage model via two ingredients: (a) contrastive learning, which is well\nstudied in the area of computer vision; (b) multiple vocabularies, one of which\nis fine-grained and the other is coarse-grained. The two methods both provide\ndifferent views of an original sentence, and both are shown to be beneficial.\nDownstream tasks demonstrate our proposed CMV-BERT are effective in improving\nthe pretrained language models.", "journal": ""}
{"doi": "10.48550/arXiv.2102.09727", "date": "2021-02-19", "title": "Learning Dynamic BERT via Trainable Gate Variables and a Bi-modal Regularizer", "authors": "Seohyeong Jeong, Nojun Kwak", "abstract": "The BERT model has shown significant success on various natural language\nprocessing tasks. However, due to the heavy model size and high computational\ncost, the model suffers from high latency, which is fatal to its deployments on\nresource-limited devices. To tackle this problem, we propose a dynamic\ninference method on BERT via trainable gate variables applied on input tokens\nand a regularizer that has a bi-modal property. Our method shows reduced\ncomputational cost on the GLUE dataset with a minimal performance drop.\nMoreover, the model adjusts with a trade-off between performance and\ncomputational cost with the user-specified hyperparameter.", "journal": ""}
{"doi": "10.48550/arXiv.2102.10684", "date": "2021-02-21", "title": "Pre-Training BERT on Arabic Tweets: Practical Considerations", "authors": "Ahmed Abdelali, Sabit Hassan, Hamdy Mubarak, Kareem Darwish, Younes Samih", "abstract": "Pretraining Bidirectional Encoder Representations from Transformers (BERT)\nfor downstream NLP tasks is a non-trival task. We pretrained 5 BERT models that\ndiffer in the size of their training sets, mixture of formal and informal\nArabic, and linguistic preprocessing. All are intended to support Arabic\ndialects and social media. The experiments highlight the centrality of data\ndiversity and the efficacy of linguistically aware segmentation. They also\nhighlight that more data or more training step do not necessitate better\nmodels. Our new models achieve new state-of-the-art results on several\ndownstream tasks. The resulting models are released to the community under the\nname QARiB.", "journal": ""}
{"doi": "10.48550/arXiv.2102.12896", "date": "2021-02-20", "title": "Predicting times of waiting on red signals using BERT", "authors": "Witold Szejgis, Anna Warno, Pawe\u0142 Gora", "abstract": "We present a method for approximating outcomes of road traffic simulations\nusing BERT-based models, which may find applications in, e.g., optimizing\ntraffic signal settings, especially with the presence of autonomous and\nconnected vehicles. The experiments were conducted on a dataset generated using\nthe Traffic Simulation Framework software runs on a realistic road network. The\nBERT-based models were compared with 4 other types of machine learning models\n(LightGBM, fully connected neural networks and 2 types of graph neural\nnetworks) and gave the best results in terms of all the considered metrics.", "journal": ""}
{"doi": "10.48550/arXiv.2103.03732", "date": "2021-03-05", "title": "Fine-tuning Pretrained Multilingual BERT Model for Indonesian Aspect-based Sentiment Analysis", "authors": "Annisa Nurul Azhar, Masayu Leylia Khodra", "abstract": "Although previous research on Aspect-based Sentiment Analysis (ABSA) for\nIndonesian reviews in hotel domain has been conducted using CNN and XGBoost,\nits model did not generalize well in test data and high number of OOV words\ncontributed to misclassification cases. Nowadays, most state-of-the-art results\nfor wide array of NLP tasks are achieved by utilizing pretrained language\nrepresentation. In this paper, we intend to incorporate one of the foremost\nlanguage representation model, BERT, to perform ABSA in Indonesian reviews\ndataset. By combining multilingual BERT (m-BERT) with task transformation\nmethod, we manage to achieve significant improvement by 8% on the F1-score\ncompared to the result from our previous study.", "journal": ""}
{"doi": "10.48550/arXiv.2104.02831", "date": "2021-04-07", "title": "Better Neural Machine Translation by Extracting Linguistic Information from BERT", "authors": "Hassan S. Shavarani, Anoop Sarkar", "abstract": "Adding linguistic information (syntax or semantics) to neural machine\ntranslation (NMT) has mostly focused on using point estimates from pre-trained\nmodels. Directly using the capacity of massive pre-trained contextual word\nembedding models such as BERT (Devlin et al., 2019) has been marginally useful\nin NMT because effective fine-tuning is difficult to obtain for NMT without\nmaking training brittle and unreliable. We augment NMT by extracting dense\nfine-tuned vector-based linguistic information from BERT instead of using point\nestimates. Experimental results show that our method of incorporating\nlinguistic information helps NMT to generalize better in a variety of training\ncontexts and is no more difficult to train than conventional Transformer-based\nNMT.", "journal": ""}
{"doi": "10.48550/arXiv.2104.07143", "date": "2021-04-14", "title": "An Interpretability Illusion for BERT", "authors": "Tolga Bolukbasi, Adam Pearce, Ann Yuan, Andy Coenen, Emily Reif, Fernanda Vi\u00e9gas, Martin Wattenberg", "abstract": "We describe an \"interpretability illusion\" that arises when analyzing the\nBERT model. Activations of individual neurons in the network may spuriously\nappear to encode a single, simple concept, when in fact they are encoding\nsomething far more complex. The same effect holds for linear combinations of\nactivations. We trace the source of this illusion to geometric properties of\nBERT's embedding space as well as the fact that common text corpora represent\nonly narrow slices of possible English sentences. We provide a taxonomy of\nmodel-learned concepts and discuss methodological implications for\ninterpretability research, especially the importance of testing hypotheses on\nmultiple data sets.", "journal": ""}
{"doi": "10.48550/arXiv.2104.07705", "date": "2021-04-15", "title": "How to Train BERT with an Academic Budget", "authors": "Peter Izsak, Moshe Berchansky, Omer Levy", "abstract": "While large language models a la BERT are used ubiquitously in NLP,\npretraining them is considered a luxury that only a few well-funded industry\nlabs can afford. How can one train such models with a more modest budget? We\npresent a recipe for pretraining a masked language model in 24 hours using a\nsingle low-end deep learning server. We demonstrate that through a combination\nof software optimizations, design choices, and hyperparameter tuning, it is\npossible to produce models that are competitive with BERT-base on GLUE tasks at\na fraction of the original pretraining cost.", "journal": ""}
{"doi": "10.48550/arXiv.2106.03484", "date": "2021-06-07", "title": "BERTGEN: Multi-task Generation through BERT", "authors": "Faidon Mitzalis, Ozan Caglayan, Pranava Madhyastha, Lucia Specia", "abstract": "We present BERTGEN, a novel generative, decoder-only model which extends BERT\nby fusing multimodal and multilingual pretrained models VL-BERT and M-BERT,\nrespectively. BERTGEN is auto-regressively trained for language generation\ntasks, namely image captioning, machine translation and multimodal machine\ntranslation, under a multitask setting. With a comprehensive set of\nevaluations, we show that BERTGEN outperforms many strong baselines across the\ntasks explored. We also show BERTGEN's ability for zero-shot language\ngeneration, where it exhibits competitive performance to supervised\ncounterparts. Finally, we conduct ablation studies which demonstrate that\nBERTGEN substantially benefits from multi-tasking and effectively transfers\nrelevant inductive biases from the pre-trained models.", "journal": ""}
{"doi": "10.48550/arXiv.2107.00807", "date": "2021-07-02", "title": "He Thinks He Knows Better than the Doctors: BERT for Event Factuality Fails on Pragmatics", "authors": "Nanjiang Jiang, Marie-Catherine de Marneffe", "abstract": "We investigate how well BERT performs on predicting factuality in several\nexisting English datasets, encompassing various linguistic constructions.\nAlthough BERT obtains a strong performance on most datasets, it does so by\nexploiting common surface patterns that correlate with certain factuality\nlabels, and it fails on instances where pragmatic reasoning is necessary.\nContrary to what the high performance suggests, we are still far from having a\nrobust system for factuality prediction.", "journal": ""}
{"doi": "10.48550/arXiv.2108.09814", "date": "2021-08-22", "title": "UzBERT: pretraining a BERT model for Uzbek", "authors": "B. Mansurov, A. Mansurov", "abstract": "Pretrained language models based on the Transformer architecture have\nachieved state-of-the-art results in various natural language processing tasks\nsuch as part-of-speech tagging, named entity recognition, and question\nanswering. However, no such monolingual model for the Uzbek language is\npublicly available. In this paper, we introduce UzBERT, a pretrained Uzbek\nlanguage model based on the BERT architecture. Our model greatly outperforms\nmultilingual BERT on masked language model accuracy. We make the model publicly\navailable under the MIT open-source license.", "journal": ""}
{"doi": "10.48550/arXiv.2109.04607", "date": "2021-09-10", "title": "IndoBERTweet: A Pretrained Language Model for Indonesian Twitter with Effective Domain-Specific Vocabulary Initialization", "authors": "Fajri Koto, Jey Han Lau, Timothy Baldwin", "abstract": "We present IndoBERTweet, the first large-scale pretrained model for\nIndonesian Twitter that is trained by extending a monolingually-trained\nIndonesian BERT model with additive domain-specific vocabulary. We focus in\nparticular on efficient model adaptation under vocabulary mismatch, and\nbenchmark different ways of initializing the BERT embedding layer for new word\ntypes. We find that initializing with the average BERT subword embedding makes\npretraining five times faster, and is more effective than proposed methods for\nvocabulary adaptation in terms of extrinsic evaluation over seven Twitter-based\ndatasets.", "journal": ""}
{"doi": "10.48550/arXiv.2205.11987", "date": "2022-05-24", "title": "Word-order typology in Multilingual BERT: A case study in subordinate-clause detection", "authors": "Dmitry Nikolaev, Sebastian Pad\u00f3", "abstract": "The capabilities and limitations of BERT and similar models are still unclear\nwhen it comes to learning syntactic abstractions, in particular across\nlanguages. In this paper, we use the task of subordinate-clause detection\nwithin and across languages to probe these properties. We show that this task\nis deceptively simple, with easy gains offset by a long tail of harder cases,\nand that BERT's zero-shot performance is dominated by word-order effects,\nmirroring the SVO/VSO/SOV typology.", "journal": ""}
{"doi": "10.48550/arXiv.2207.05553", "date": "2022-07-12", "title": "Using Paraphrases to Study Properties of Contextual Embeddings", "authors": "Laura Burdick, Jonathan K. Kummerfeld, Rada Mihalcea", "abstract": "We use paraphrases as a unique source of data to analyze contextualized\nembeddings, with a particular focus on BERT. Because paraphrases naturally\nencode consistent word and phrase semantics, they provide a unique lens for\ninvestigating properties of embeddings. Using the Paraphrase Database's\nalignments, we study words within paraphrases as well as phrase\nrepresentations. We find that contextual embeddings effectively handle\npolysemous words, but give synonyms surprisingly different representations in\nmany cases. We confirm previous findings that BERT is sensitive to word order,\nbut find slightly different patterns than prior work in terms of the level of\ncontextualization across BERT's layers.", "journal": ""}
{"doi": "10.48550/arXiv.2208.04547", "date": "2022-08-09", "title": "Emotion Detection From Tweets Using a BERT and SVM Ensemble Model", "authors": "Ionu\u0163-Alexandru Albu, Stelian Sp\u00eenu", "abstract": "Automatic identification of emotions expressed in Twitter data has a wide\nrange of applications. We create a well-balanced dataset by adding a neutral\nclass to a benchmark dataset consisting of four emotions: fear, sadness, joy,\nand anger. On this extended dataset, we investigate the use of Support Vector\nMachine (SVM) and Bidirectional Encoder Representations from Transformers\n(BERT) for emotion recognition. We propose a novel ensemble model by combining\nthe two BERT and SVM models. Experiments show that the proposed model achieves\na state-of-the-art accuracy of 0.91 on emotion recognition in tweets.", "journal": "U.P.B. Sci. Bull., Series C, Vol. 84, Iss. 1, 2022 ISSN 2286-3540"}
{"doi": "10.48550/arXiv.2304.08369", "date": "2023-04-17", "title": "New Product Development (NPD) through Social Media-based Analysis by Comparing Word2Vec and BERT Word Embeddings", "authors": "Princessa Cintaqia, Matheus Inoue", "abstract": "This study introduces novel methods for sentiment and opinion classification\nof tweets to support the New Product Development (NPD) process. Two popular\nword embedding techniques, Word2Vec and BERT, were evaluated as inputs for\nclassic Machine Learning and Deep Learning algorithms to identify the\nbest-performing approach in sentiment analysis and opinion detection with\nlimited data. The results revealed that BERT word embeddings combined with\nBalanced Random Forest yielded the most accurate single model for both\nsentiment analysis and opinion detection on a use case. Additionally, the paper\nprovides feedback for future product development performing word graph analysis\nof the tweets with same sentiment to highlight potential areas of improvement.", "journal": ""}
{"doi": "10.48550/arXiv.2306.04628", "date": "2023-06-06", "title": "Systematic Analysis of Music Representations from BERT", "authors": "Sangjun Han, Hyeongrae Ihm, Woohyung Lim", "abstract": "There have been numerous attempts to represent raw data as numerical vectors\nthat effectively capture semantic and contextual information. However, in the\nfield of symbolic music, previous works have attempted to validate their music\nembeddings by observing the performance improvement of various fine-tuning\ntasks. In this work, we directly analyze embeddings from BERT and BERT with\ncontrastive learning trained on bar-level MIDI, inspecting their musical\ninformation that can be obtained from MIDI events. We observe that the\nembeddings exhibit distinct characteristics of information depending on the\ncontrastive objectives and the choice of layers. Our code is available at\nhttps://github.com/sjhan91/MusicBERT.", "journal": ""}
{"doi": "10.48550/arXiv.2402.05034", "date": "2024-02-07", "title": "How BERT Speaks Shakespearean English? Evaluating Historical Bias in Contextual Language Models", "authors": "Miriam Cuscito, Alfio Ferrara, Martin Ruskov", "abstract": "In this paper, we explore the idea of analysing the historical bias of\ncontextual language models based on BERT by measuring their adequacy with\nrespect to Early Modern (EME) and Modern (ME) English. In our preliminary\nexperiments, we perform fill-in-the-blank tests with 60 masked sentences (20\nEME-specific, 20 ME-specific and 20 generic) and three different models (i.e.,\nBERT Base, MacBERTh, English HLM). We then rate the model predictions according\nto a 5-point bipolar scale between the two language varieties and derive a\nweighted score to measure the adequacy of each model to EME and ME varieties of\nEnglish.", "journal": ""}
{"doi": "10.48550/arXiv.2406.13827", "date": "2024-06-19", "title": "Fine-Tuning BERTs for Definition Extraction from Mathematical Text", "authors": "Lucy Horowitz, Ryan Hathaway", "abstract": "In this paper, we fine-tuned three pre-trained BERT models on the task of\n\"definition extraction\" from mathematical English written in LaTeX. This is\npresented as a binary classification problem, where either a sentence contains\na definition of a mathematical term or it does not. We used two original data\nsets, \"Chicago\" and \"TAC,\" to fine-tune and test these models. We also tested\non WFMALL, a dataset presented by Vanetik and Litvak in 2021 and compared the\nperformance of our models to theirs. We found that a high-performance\nSentence-BERT transformer model performed best based on overall accuracy,\nrecall, and precision metrics, achieving comparable results to the earlier\nmodels with less computational effort.", "journal": ""}
{"doi": "10.48550/arXiv.2501.10107", "date": "2025-01-17", "title": "BBPOS: BERT-based Part-of-Speech Tagging for Uzbek", "authors": "Latofat Bobojonova, Arofat Akhundjanova, Phil Ostheimer, Sophie Fellenz", "abstract": "This paper advances NLP research for the low-resource Uzbek language by\nevaluating two previously untested monolingual Uzbek BERT models on the\npart-of-speech (POS) tagging task and introducing the first publicly available\nUPOS-tagged benchmark dataset for Uzbek. Our fine-tuned models achieve 91%\naverage accuracy, outperforming the baseline multi-lingual BERT as well as the\nrule-based tagger. Notably, these models capture intermediate POS changes\nthrough affixes and demonstrate context sensitivity, unlike existing rule-based\ntaggers.", "journal": ""}
{"doi": "10.48550/arXiv.2502.18653", "date": "2025-02-25", "title": "Enhancing Text Classification with a Novel Multi-Agent Collaboration Framework Leveraging BERT", "authors": "Hediyeh Baban, Sai A Pidapar, Aashutosh Nema, Sichen Lu", "abstract": "We introduce a novel multi-agent collaboration framework designed to enhance\nthe accuracy and robustness of text classification models. Leveraging BERT as\nthe primary classifier, our framework dynamically escalates low-confidence\npredictions to a specialized multi-agent system comprising Lexical, Contextual,\nLogic, Consensus, and Explainability agents. This collaborative approach allows\nfor comprehensive analysis and consensus-driven decision-making, significantly\nimproving classification performance across diverse text classification tasks.\nEmpirical evaluations on benchmark datasets demonstrate that our framework\nachieves a 5.5% increase in accuracy compared to standard BERT-based\nclassifiers, underscoring its effectiveness and academic novelty in advancing\nmulti-agent systems within natural language processing.", "journal": ""}
{"doi": "10.48550/arXiv.2506.08581", "date": "2025-06-10", "title": "Evaluating the Performance and Efficiency of Sentence-BERT for Code Comment Classification", "authors": "Fabian C. Pe\u00f1a, Steffen Herbold", "abstract": "This work evaluates Sentence-BERT for a multi-label code comment\nclassification task seeking to maximize the classification performance while\ncontrolling efficiency constraints during inference. Using a dataset of 13,216\nlabeled comment sentences, Sentence-BERT models are fine-tuned and combined\nwith different classification heads to recognize comment types. While larger\nmodels outperform smaller ones in terms of F1, the latter offer outstanding\nefficiency, both in runtime and GFLOPS. As result, a balance between a\nreasonable F1 improvement (+0.0346) and a minimal efficiency degradation (+1.4x\nin runtime and +2.1x in GFLOPS) is reached.", "journal": ""}
{"doi": "10.48550/arXiv.1908.05787", "date": "2019-08-15", "title": "Integrating Multimodal Information in Large Pretrained Transformers", "authors": "Wasifur Rahman, Md. Kamrul Hasan, Sangwu Lee, Amir Zadeh, Chengfeng Mao, Louis-Philippe Morency, Ehsan Hoque", "abstract": "Recent Transformer-based contextual word representations, including BERT and\nXLNet, have shown state-of-the-art performance in multiple disciplines within\nNLP. Fine-tuning the trained contextual models on task-specific datasets has\nbeen the key to achieving superior performance downstream. While fine-tuning\nthese pre-trained models is straightforward for lexical applications\n(applications with only language modality), it is not trivial for multimodal\nlanguage (a growing area in NLP focused on modeling face-to-face\ncommunication). Pre-trained models don't have the necessary components to\naccept two extra modalities of vision and acoustic. In this paper, we proposed\nan attachment to BERT and XLNet called Multimodal Adaptation Gate (MAG). MAG\nallows BERT and XLNet to accept multimodal nonverbal data during fine-tuning.\nIt does so by generating a shift to internal representation of BERT and XLNet;\na shift that is conditioned on the visual and acoustic modalities. In our\nexperiments, we study the commonly used CMU-MOSI and CMU-MOSEI datasets for\nmultimodal sentiment analysis. Fine-tuning MAG-BERT and MAG-XLNet significantly\nboosts the sentiment analysis performance over previous baselines as well as\nlanguage-only fine-tuning of BERT and XLNet. On the CMU-MOSI dataset, MAG-XLNet\nachieves human-level multimodal sentiment analysis performance for the first\ntime in the NLP community.", "journal": ""}
{"doi": "10.48550/arXiv.1909.04925", "date": "2019-09-11", "title": "How Does BERT Answer Questions? A Layer-Wise Analysis of Transformer Representations", "authors": "Betty van Aken, Benjamin Winter, Alexander L\u00f6ser, Felix A. Gers", "abstract": "Bidirectional Encoder Representations from Transformers (BERT) reach\nstate-of-the-art results in a variety of Natural Language Processing tasks.\nHowever, understanding of their internal functioning is still insufficient and\nunsatisfactory. In order to better understand BERT and other Transformer-based\nmodels, we present a layer-wise analysis of BERT's hidden states. Unlike\nprevious research, which mainly focuses on explaining Transformer models by\ntheir attention weights, we argue that hidden states contain equally valuable\ninformation. Specifically, our analysis focuses on models fine-tuned on the\ntask of Question Answering (QA) as an example of a complex downstream task. We\ninspect how QA models transform token vectors in order to find the correct\nanswer. To this end, we apply a set of general and QA-specific probing tasks\nthat reveal the information stored in each representation layer. Our\nqualitative analysis of hidden state visualizations provides additional\ninsights into BERT's reasoning process. Our results show that the\ntransformations within BERT go through phases that are related to traditional\npipeline tasks. The system can therefore implicitly incorporate task-specific\ninformation into its token representations. Furthermore, our analysis reveals\nthat fine-tuning has little impact on the models' semantic abilities and that\nprediction errors can be recognized in the vector representations of even early\nlayers.", "journal": ""}
{"doi": "10.48550/arXiv.1909.10351", "date": "2019-09-23", "title": "TinyBERT: Distilling BERT for Natural Language Understanding", "authors": "Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin Li, Fang Wang, Qun Liu", "abstract": "Language model pre-training, such as BERT, has significantly improved the\nperformances of many natural language processing tasks. However, pre-trained\nlanguage models are usually computationally expensive, so it is difficult to\nefficiently execute them on resource-restricted devices. To accelerate\ninference and reduce model size while maintaining accuracy, we first propose a\nnovel Transformer distillation method that is specially designed for knowledge\ndistillation (KD) of the Transformer-based models. By leveraging this new KD\nmethod, the plenty of knowledge encoded in a large teacher BERT can be\neffectively transferred to a small student Tiny-BERT. Then, we introduce a new\ntwo-stage learning framework for TinyBERT, which performs Transformer\ndistillation at both the pretraining and task-specific learning stages. This\nframework ensures that TinyBERT can capture he general-domain as well as the\ntask-specific knowledge in BERT.\n  TinyBERT with 4 layers is empirically effective and achieves more than 96.8%\nthe performance of its teacher BERTBASE on GLUE benchmark, while being 7.5x\nsmaller and 9.4x faster on inference. TinyBERT with 4 layers is also\nsignificantly better than 4-layer state-of-the-art baselines on BERT\ndistillation, with only about 28% parameters and about 31% inference time of\nthem. Moreover, TinyBERT with 6 layers performs on-par with its teacher\nBERTBASE.", "journal": ""}
{"doi": "10.48550/arXiv.2111.14819", "date": "2021-11-29", "title": "Point-BERT: Pre-training 3D Point Cloud Transformers with Masked Point Modeling", "authors": "Xumin Yu, Lulu Tang, Yongming Rao, Tiejun Huang, Jie Zhou, Jiwen Lu", "abstract": "We present Point-BERT, a new paradigm for learning Transformers to generalize\nthe concept of BERT to 3D point cloud. Inspired by BERT, we devise a Masked\nPoint Modeling (MPM) task to pre-train point cloud Transformers. Specifically,\nwe first divide a point cloud into several local point patches, and a point\ncloud Tokenizer with a discrete Variational AutoEncoder (dVAE) is designed to\ngenerate discrete point tokens containing meaningful local information. Then,\nwe randomly mask out some patches of input point clouds and feed them into the\nbackbone Transformers. The pre-training objective is to recover the original\npoint tokens at the masked locations under the supervision of point tokens\nobtained by the Tokenizer. Extensive experiments demonstrate that the proposed\nBERT-style pre-training strategy significantly improves the performance of\nstandard point cloud Transformers. Equipped with our pre-training strategy, we\nshow that a pure Transformer architecture attains 93.8% accuracy on ModelNet40\nand 83.1% accuracy on the hardest setting of ScanObjectNN, surpassing carefully\ndesigned point cloud models with much fewer hand-made designs. We also\ndemonstrate that the representations learned by Point-BERT transfer well to new\ntasks and domains, where our models largely advance the state-of-the-art of\nfew-shot point cloud classification task. The code and pre-trained models are\navailable at https://github.com/lulutang0608/Point-BERT", "journal": ""}
{"doi": "10.48550/arXiv.2201.06774", "date": "2022-01-18", "title": "Hierarchical Neural Network Approaches for Long Document Classification", "authors": "Snehal Khandve, Vedangi Wagh, Apurva Wani, Isha Joshi, Raviraj Joshi", "abstract": "Text classification algorithms investigate the intricate relationships\nbetween words or phrases and attempt to deduce the document's interpretation.\nIn the last few years, these algorithms have progressed tremendously.\nTransformer architecture and sentence encoders have proven to give superior\nresults on natural language processing tasks. But a major limitation of these\narchitectures is their applicability for text no longer than a few hundred\nwords. In this paper, we explore hierarchical transfer learning approaches for\nlong document classification. We employ pre-trained Universal Sentence Encoder\n(USE) and Bidirectional Encoder Representations from Transformers (BERT) in a\nhierarchical setup to capture better representations efficiently. Our proposed\nmodels are conceptually simple where we divide the input data into chunks and\nthen pass this through base models of BERT and USE. Then output representation\nfor each chunk is then propagated through a shallow neural network comprising\nof LSTMs or CNNs for classifying the text data. These extensions are evaluated\non 6 benchmark datasets. We show that USE + CNN/LSTM performs better than its\nstand-alone baseline. Whereas the BERT + CNN/LSTM performs on par with its\nstand-alone counterpart. However, the hierarchical BERT models are still\ndesirable as it avoids the quadratic complexity of the attention mechanism in\nBERT. Along with the hierarchical approaches, this work also provides a\ncomparison of different deep learning algorithms like USE, BERT, HAN,\nLongformer, and BigBird for long document classification. The Longformer\napproach consistently performs well on most of the datasets.", "journal": ""}
{"doi": "10.48550/arXiv.2001.11316", "date": "2020-01-30", "title": "Adversarial Training for Aspect-Based Sentiment Analysis with BERT", "authors": "Akbar Karimi, Leonardo Rossi, Andrea Prati", "abstract": "Aspect-Based Sentiment Analysis (ABSA) deals with the extraction of\nsentiments and their targets. Collecting labeled data for this task in order to\nhelp neural networks generalize better can be laborious and time-consuming. As\nan alternative, similar data to the real-world examples can be produced\nartificially through an adversarial process which is carried out in the\nembedding space. Although these examples are not real sentences, they have been\nshown to act as a regularization method which can make neural networks more\nrobust. In this work, we apply adversarial training, which was put forward by\nGoodfellow et al. (2014), to the post-trained BERT (BERT-PT) language model\nproposed by Xu et al. (2019) on the two major tasks of Aspect Extraction and\nAspect Sentiment Classification in sentiment analysis. After improving the\nresults of post-trained BERT by an ablation study, we propose a novel\narchitecture called BERT Adversarial Training (BAT) to utilize adversarial\ntraining in ABSA. The proposed model outperforms post-trained BERT in both\ntasks. To the best of our knowledge, this is the first study on the application\nof adversarial training in ABSA.", "journal": ""}
{"doi": "10.48550/arXiv.2005.07421", "date": "2020-05-15", "title": "Spelling Error Correction with Soft-Masked BERT", "authors": "Shaohua Zhang, Haoran Huang, Jicong Liu, Hang Li", "abstract": "Spelling error correction is an important yet challenging task because a\nsatisfactory solution of it essentially needs human-level language\nunderstanding ability. Without loss of generality we consider Chinese spelling\nerror correction (CSC) in this paper. A state-of-the-art method for the task\nselects a character from a list of candidates for correction (including\nnon-correction) at each position of the sentence on the basis of BERT, the\nlanguage representation model. The accuracy of the method can be sub-optimal,\nhowever, because BERT does not have sufficient capability to detect whether\nthere is an error at each position, apparently due to the way of pre-training\nit using mask language modeling. In this work, we propose a novel neural\narchitecture to address the aforementioned issue, which consists of a network\nfor error detection and a network for error correction based on BERT, with the\nformer being connected to the latter with what we call soft-masking technique.\nOur method of using `Soft-Masked BERT' is general, and it may be employed in\nother language detection-correction problems. Experimental results on two\ndatasets demonstrate that the performance of our proposed method is\nsignificantly better than the baselines including the one solely based on BERT.", "journal": ""}
{"doi": "10.48550/arXiv.2009.05021", "date": "2020-09-10", "title": "Investigating Gender Bias in BERT", "authors": "Rishabh Bhardwaj, Navonil Majumder, Soujanya Poria", "abstract": "Contextual language models (CLMs) have pushed the NLP benchmarks to a new\nheight. It has become a new norm to utilize CLM provided word embeddings in\ndownstream tasks such as text classification. However, unless addressed, CLMs\nare prone to learn intrinsic gender-bias in the dataset. As a result,\npredictions of downstream NLP models can vary noticeably by varying gender\nwords, such as replacing \"he\" to \"she\", or even gender-neutral words. In this\npaper, we focus our analysis on a popular CLM, i.e., BERT. We analyse the\ngender-bias it induces in five downstream tasks related to emotion and\nsentiment intensity prediction. For each task, we train a simple regressor\nutilizing BERT's word embeddings. We then evaluate the gender-bias in\nregressors using an equity evaluation corpus. Ideally and from the specific\ndesign, the models should discard gender informative features from the input.\nHowever, the results show a significant dependence of the system's predictions\non gender-particular words and phrases. We claim that such biases can be\nreduced by removing genderspecific features from word embedding. Hence, for\neach layer in BERT, we identify directions that primarily encode gender\ninformation. The space formed by such directions is referred to as the gender\nsubspace in the semantic space of word embeddings. We propose an algorithm that\nfinds fine-grained gender directions, i.e., one primary direction for each BERT\nlayer. This obviates the need of realizing gender subspace in multiple\ndimensions and prevents other crucial information from being omitted.\nExperiments show that removing embedding components in such directions achieves\ngreat success in reducing BERT-induced bias in the downstream tasks.", "journal": ""}
{"doi": "10.48550/arXiv.2010.06138", "date": "2020-10-13", "title": "Incorporating BERT into Parallel Sequence Decoding with Adapters", "authors": "Junliang Guo, Zhirui Zhang, Linli Xu, Hao-Ran Wei, Boxing Chen, Enhong Chen", "abstract": "While large scale pre-trained language models such as BERT have achieved\ngreat success on various natural language understanding tasks, how to\nefficiently and effectively incorporate them into sequence-to-sequence models\nand the corresponding text generation tasks remains a non-trivial problem. In\nthis paper, we propose to address this problem by taking two different BERT\nmodels as the encoder and decoder respectively, and fine-tuning them by\nintroducing simple and lightweight adapter modules, which are inserted between\nBERT layers and tuned on the task-specific dataset. In this way, we obtain a\nflexible and efficient model which is able to jointly leverage the information\ncontained in the source-side and target-side BERT models, while bypassing the\ncatastrophic forgetting problem. Each component in the framework can be\nconsidered as a plug-in unit, making the framework flexible and task agnostic.\nOur framework is based on a parallel sequence decoding algorithm named\nMask-Predict considering the bi-directional and conditional independent nature\nof BERT, and can be adapted to traditional autoregressive decoding easily. We\nconduct extensive experiments on neural machine translation tasks where the\nproposed method consistently outperforms autoregressive baselines while\nreducing the inference latency by half, and achieves $36.49$/$33.57$ BLEU\nscores on IWSLT14 German-English/WMT14 German-English translation. When adapted\nto autoregressive decoding, the proposed method achieves $30.60$/$43.56$ BLEU\nscores on WMT14 English-German/English-French translation, on par with the\nstate-of-the-art baseline models.", "journal": ""}
{"doi": "10.48550/arXiv.2012.07000", "date": "2020-12-13", "title": "KVL-BERT: Knowledge Enhanced Visual-and-Linguistic BERT for Visual Commonsense Reasoning", "authors": "Dandan Song, Siyi Ma, Zhanchen Sun, Sicheng Yang, Lejian Liao", "abstract": "Reasoning is a critical ability towards complete visual understanding. To\ndevelop machine with cognition-level visual understanding and reasoning\nabilities, the visual commonsense reasoning (VCR) task has been introduced. In\nVCR, given a challenging question about an image, a machine must answer\ncorrectly and then provide a rationale justifying its answer. The methods\nadopting the powerful BERT model as the backbone for learning joint\nrepresentation of image content and natural language have shown promising\nimprovements on VCR. However, none of the existing methods have utilized\ncommonsense knowledge in visual commonsense reasoning, which we believe will be\ngreatly helpful in this task. With the support of commonsense knowledge,\ncomplex questions even if the required information is not depicted in the image\ncan be answered with cognitive reasoning. Therefore, we incorporate commonsense\nknowledge into the cross-modal BERT, and propose a novel Knowledge Enhanced\nVisual-and-Linguistic BERT (KVL-BERT for short) model. Besides taking visual\nand linguistic contents as input, external commonsense knowledge extracted from\nConceptNet is integrated into the multi-layer Transformer. In order to reserve\nthe structural information and semantic representation of the original\nsentence, we propose using relative position embedding and mask-self-attention\nto weaken the effect between the injected commonsense knowledge and other\nunrelated components in the input sequence. Compared to other task-specific\nmodels and general task-agnostic pre-training models, our KVL-BERT outperforms\nthem by a large margin.", "journal": ""}
{"doi": "10.48550/arXiv.2104.08027", "date": "2021-04-16", "title": "Fast, Effective, and Self-Supervised: Transforming Masked Language Models into Universal Lexical and Sentence Encoders", "authors": "Fangyu Liu, Ivan Vuli\u0107, Anna Korhonen, Nigel Collier", "abstract": "Pretrained Masked Language Models (MLMs) have revolutionised NLP in recent\nyears. However, previous work has indicated that off-the-shelf MLMs are not\neffective as universal lexical or sentence encoders without further\ntask-specific fine-tuning on NLI, sentence similarity, or paraphrasing tasks\nusing annotated task data. In this work, we demonstrate that it is possible to\nturn MLMs into effective universal lexical and sentence encoders even without\nany additional data and without any supervision. We propose an extremely\nsimple, fast and effective contrastive learning technique, termed Mirror-BERT,\nwhich converts MLMs (e.g., BERT and RoBERTa) into such encoders in 20-30\nseconds without any additional external knowledge. Mirror-BERT relies on fully\nidentical or slightly modified string pairs as positive (i.e., synonymous)\nfine-tuning examples, and aims to maximise their similarity during identity\nfine-tuning. We report huge gains over off-the-shelf MLMs with Mirror-BERT in\nboth lexical-level and sentence-level tasks, across different domains and\ndifferent languages. Notably, in the standard sentence semantic similarity\n(STS) tasks, our self-supervised Mirror-BERT model even matches the performance\nof the task-tuned Sentence-BERT models from prior work. Finally, we delve\ndeeper into the inner workings of MLMs, and suggest some evidence on why this\nsimple approach can yield effective universal lexical and sentence encoders.", "journal": ""}
{"doi": "10.48550/arXiv.2106.02902", "date": "2021-06-05", "title": "BERTnesia: Investigating the capture and forgetting of knowledge in BERT", "authors": "Jonas Wallat, Jaspreet Singh, Avishek Anand", "abstract": "Probing complex language models has recently revealed several insights into\nlinguistic and semantic patterns found in the learned representations. In this\narticle, we probe BERT specifically to understand and measure the relational\nknowledge it captures in its parametric memory. While probing for linguistic\nunderstanding is commonly applied to all layers of BERT as well as fine-tuned\nmodels, this has not been done for factual knowledge. We utilize existing\nknowledge base completion tasks (LAMA) to probe every layer of pre-trained as\nwell as fine-tuned BERT models(ranking, question answering, NER). Our findings\nshow that knowledge is not just contained in BERT's final layers. Intermediate\nlayers contribute a significant amount (17-60%) to the total knowledge found.\nProbing intermediate layers also reveals how different types of knowledge\nemerge at varying rates. When BERT is fine-tuned, relational knowledge is\nforgotten. The extent of forgetting is impacted by the fine-tuning objective\nand the training data. We found that ranking models forget the least and retain\nmore knowledge in their final layer compared to masked language modeling and\nquestion-answering. However, masked language modeling performed the best at\nacquiring new knowledge from the training data. When it comes to learning\nfacts, we found that capacity and fact density are key factors. We hope this\ninitial work will spur further research into understanding the parametric\nmemory of language models and the effect of training objectives on factual\nknowledge. The code to repeat the experiments is publicly available on GitHub.", "journal": ""}
{"doi": "10.48550/arXiv.2106.04312", "date": "2021-06-08", "title": "Speech BERT Embedding For Improving Prosody in Neural TTS", "authors": "Liping Chen, Yan Deng, Xi Wang, Frank K. Soong, Lei He", "abstract": "This paper presents a speech BERT model to extract embedded prosody\ninformation in speech segments for improving the prosody of synthesized speech\nin neural text-to-speech (TTS). As a pre-trained model, it can learn prosody\nattributes from a large amount of speech data, which can utilize more data than\nthe original training data used by the target TTS. The embedding is extracted\nfrom the previous segment of a fixed length in the proposed BERT. The extracted\nembedding is then used together with the mel-spectrogram to predict the\nfollowing segment in the TTS decoder. Experimental results obtained by the\nTransformer TTS show that the proposed BERT can extract fine-grained,\nsegment-level prosody, which is complementary to utterance-level prosody to\nimprove the final prosody of the TTS speech. The objective distortions measured\non a single speaker TTS are reduced between the generated speech and original\nrecordings. Subjective listening tests also show that the proposed approach is\nfavorably preferred over the TTS without the BERT prosody embedding module, for\nboth in-domain and out-of-domain applications. For Microsoft professional,\nsingle/multiple speakers and the LJ Speaker in the public database, subjective\npreference is similarly confirmed with the new BERT prosody embedding. TTS demo\naudio samples are in https://judy44chen.github.io/TTSSpeechBERT/.", "journal": "ICASSP 2021"}
{"doi": "10.48550/arXiv.2109.07222", "date": "2021-09-15", "title": "EfficientBERT: Progressively Searching Multilayer Perceptron via Warm-up Knowledge Distillation", "authors": "Chenhe Dong, Guangrun Wang, Hang Xu, Jiefeng Peng, Xiaozhe Ren, Xiaodan Liang", "abstract": "Pre-trained language models have shown remarkable results on various NLP\ntasks. Nevertheless, due to their bulky size and slow inference speed, it is\nhard to deploy them on edge devices. In this paper, we have a critical insight\nthat improving the feed-forward network (FFN) in BERT has a higher gain than\nimproving the multi-head attention (MHA) since the computational cost of FFN is\n2$\\sim$3 times larger than MHA. Hence, to compact BERT, we are devoted to\ndesigning efficient FFN as opposed to previous works that pay attention to MHA.\nSince FFN comprises a multilayer perceptron (MLP) that is essential in BERT\noptimization, we further design a thorough search space towards an advanced MLP\nand perform a coarse-to-fine mechanism to search for an efficient BERT\narchitecture. Moreover, to accelerate searching and enhance model\ntransferability, we employ a novel warm-up knowledge distillation strategy at\neach search stage. Extensive experiments show our searched EfficientBERT is\n6.9$\\times$ smaller and 4.4$\\times$ faster than BERT$\\rm_{BASE}$, and has\ncompetitive performances on GLUE and SQuAD Benchmarks. Concretely,\nEfficientBERT attains a 77.7 average score on GLUE \\emph{test}, 0.7 higher than\nMobileBERT$\\rm_{TINY}$, and achieves an 85.3/74.5 F1 score on SQuAD v1.1/v2.0\n\\emph{dev}, 3.2/2.7 higher than TinyBERT$_4$ even without data augmentation.\nThe code is released at https://github.com/cheneydon/efficient-bert.", "journal": ""}
{"doi": "10.48550/arXiv.2112.07515", "date": "2021-12-14", "title": "CoCo-BERT: Improving Video-Language Pre-training with Contrastive Cross-modal Matching and Denoising", "authors": "Jianjie Luo, Yehao Li, Yingwei Pan, Ting Yao, Hongyang Chao, Tao Mei", "abstract": "BERT-type structure has led to the revolution of vision-language pre-training\nand the achievement of state-of-the-art results on numerous vision-language\ndownstream tasks. Existing solutions dominantly capitalize on the multi-modal\ninputs with mask tokens to trigger mask-based proxy pre-training tasks (e.g.,\nmasked language modeling and masked object/frame prediction). In this work, we\nargue that such masked inputs would inevitably introduce noise for cross-modal\nmatching proxy task, and thus leave the inherent vision-language association\nunder-explored. As an alternative, we derive a particular form of cross-modal\nproxy objective for video-language pre-training, i.e., Contrastive Cross-modal\nmatching and denoising (CoCo). By viewing the masked frame/word sequences as\nthe noisy augmentation of primary unmasked ones, CoCo strengthens\nvideo-language association by simultaneously pursuing inter-modal matching and\nintra-modal denoising between masked and unmasked inputs in a contrastive\nmanner. Our CoCo proxy objective can be further integrated into any BERT-type\nencoder-decoder structure for video-language pre-training, named as Contrastive\nCross-modal BERT (CoCo-BERT). We pre-train CoCo-BERT on TV dataset and a newly\ncollected large-scale GIF video dataset (ACTION). Through extensive experiments\nover a wide range of downstream tasks (e.g., cross-modal retrieval, video\nquestion answering, and video captioning), we demonstrate the superiority of\nCoCo-BERT as a pre-trained structure.", "journal": ""}
{"doi": "10.48550/arXiv.2112.12650", "date": "2021-12-23", "title": "Distilling the Knowledge of Romanian BERTs Using Multiple Teachers", "authors": "Andrei-Marius Avram, Darius Catrina, Dumitru-Clementin Cercel, Mihai Dasc\u0103lu, Traian Rebedea, Vasile P\u0103i\u015f, Dan Tufi\u015f", "abstract": "Running large-scale pre-trained language models in computationally\nconstrained environments remains a challenging problem yet to be addressed,\nwhile transfer learning from these models has become prevalent in Natural\nLanguage Processing tasks. Several solutions, including knowledge distillation,\nnetwork quantization, or network pruning have been previously proposed;\nhowever, these approaches focus mostly on the English language, thus widening\nthe gap when considering low-resource languages. In this work, we introduce\nthree light and fast versions of distilled BERT models for the Romanian\nlanguage: Distil-BERT-base-ro, Distil-RoBERT-base, and\nDistilMulti-BERT-base-ro. The first two models resulted from the individual\ndistillation of knowledge from two base versions of Romanian BERTs available in\nliterature, while the last one was obtained by distilling their ensemble. To\nour knowledge, this is the first attempt to create publicly available Romanian\ndistilled BERT models, which were thoroughly evaluated on five tasks:\npart-of-speech tagging, named entity recognition, sentiment analysis, semantic\ntextual similarity, and dialect identification. Our experimental results argue\nthat the three distilled models offer performance comparable to their teachers,\nwhile being twice as fast on a GPU and ~35% smaller. In addition, we further\ntest the similarity between the predictions of our students versus their\nteachers by measuring their label and probability loyalty, together with\nregression loyalty - a new metric introduced in this work.", "journal": ""}
{"doi": "10.48550/arXiv.2203.02838", "date": "2022-03-06", "title": "Leveraging Pre-trained BERT for Audio Captioning", "authors": "Xubo Liu, Xinhao Mei, Qiushi Huang, Jianyuan Sun, Jinzheng Zhao, Haohe Liu, Mark D. Plumbley, Volkan K\u0131l\u0131\u00e7, Wenwu Wang", "abstract": "Audio captioning aims at using natural language to describe the content of an\naudio clip. Existing audio captioning systems are generally based on an\nencoder-decoder architecture, in which acoustic information is extracted by an\naudio encoder and then a language decoder is used to generate the captions.\nTraining an audio captioning system often encounters the problem of data\nscarcity. Transferring knowledge from pre-trained audio models such as\nPre-trained Audio Neural Networks (PANNs) have recently emerged as a useful\nmethod to mitigate this issue. However, there is less attention on exploiting\npre-trained language models for the decoder, compared with the encoder. BERT is\na pre-trained language model that has been extensively used in Natural Language\nProcessing (NLP) tasks. Nevertheless, the potential of BERT as the language\ndecoder for audio captioning has not been investigated. In this study, we\ndemonstrate the efficacy of the pre-trained BERT model for audio captioning.\nSpecifically, we apply PANNs as the encoder and initialize the decoder from the\npublic pre-trained BERT models. We conduct an empirical study on the use of\nthese BERT models for the decoder in the audio captioning model. Our models\nachieve competitive results with the existing audio captioning methods on the\nAudioCaps dataset.", "journal": ""}
{"doi": "10.48550/arXiv.2203.06224", "date": "2022-03-11", "title": "verBERT: Automating Brazilian Case Law Document Multi-label Categorization Using BERT", "authors": "Felipe R. Serras, Marcelo Finger", "abstract": "In this work, we carried out a study about the use of attention-based\nalgorithms to automate the categorization of Brazilian case law documents. We\nused data from the Kollemata Project to produce two distinct datasets with\nadequate class systems. Then, we implemented a multi-class and multi-label\nversion of BERT and fine-tuned different BERT models with the produced\ndatasets. We evaluated several metrics, adopting the micro-averaged F1-Score as\nour main metric for which we obtained a performance value of F1-micro=0.72\ncorresponding to gains of 30 percent points over the tested statistical\nbaseline. In this work, we carried out a study about the use of attention-based\nalgorithms to automate the categorization of Brazilian case law documents. We\nused data from the \\textit{Kollemata} Project to produce two distinct datasets\nwith adequate class systems. Then, we implemented a multi-class and multi-label\nversion of BERT and fine-tuned different BERT models with the produced\ndatasets. We evaluated several metrics, adopting the micro-averaged F1-Score as\nour main metric for which we obtained a performance value of $\\langle\n\\mathcal{F}_1 \\rangle_{micro}=0.72$ corresponding to gains of 30 percent points\nover the tested statistical baseline.", "journal": "SERRAS, F. R.; FINGER, M. verBERT: Automating Brazilian Case Law\n  Document Multi-label Categorization Using BERT. In 13th Brazilian Simposiun\n  on Human Language and Information Technology (STIL), 2021. pp. 237-246"}
{"doi": "10.48550/arXiv.2203.07828", "date": "2022-03-15", "title": "Do BERTs Learn to Use Browser User Interface? Exploring Multi-Step Tasks with Unified Vision-and-Language BERTs", "authors": "Taichi Iki, Akiko Aizawa", "abstract": "Pre-trained Transformers are good foundations for unified multi-task models\nowing to their task-agnostic representation. Pre-trained Transformers are often\ncombined with text-to-text framework to execute multiple tasks by a single\nmodel. Performing a task through a graphical user interface (GUI) is another\ncandidate to accommodate various tasks, including multi-step tasks with vision\nand language inputs. However, few papers combine pre-trained Transformers with\nperforming through GUI. To fill this gap, we explore a framework in which a\nmodel performs a task by manipulating the GUI implemented with web pages in\nmultiple steps. We develop task pages with and without page transitions and\npropose a BERT extension for the framework. We jointly trained our BERT\nextension with those task pages, and made the following observations. (1) The\nmodel learned to use both task pages with and without page transition. (2) In\nfour out of five tasks without page transitions, the model performs greater\nthan 75% of the performance of the original BERT, which does not use browsers.\n(3) The model did not generalize effectively on unseen tasks. These results\nsuggest that we can fine-tune BERTs to multi-step tasks through GUIs, and there\nis room for improvement in their generalizability. Code will be available\nonline.", "journal": ""}
{"doi": "10.48550/arXiv.2210.09439", "date": "2022-10-17", "title": "CAN-BERT do it? Controller Area Network Intrusion Detection System based on BERT Language Model", "authors": "Natasha Alkhatib, Maria Mushtaq, Hadi Ghauch, Jean-Luc Danger", "abstract": "Due to the rising number of sophisticated customer functionalities,\nelectronic control units (ECUs) are increasingly integrated into modern\nautomotive systems. However, the high connectivity between the in-vehicle and\nthe external networks paves the way for hackers who could exploit in-vehicle\nnetwork protocols' vulnerabilities. Among these protocols, the Controller Area\nNetwork (CAN), known as the most widely used in-vehicle networking technology,\nlacks encryption and authentication mechanisms, making the communications\ndelivered by distributed ECUs insecure. Inspired by the outstanding performance\nof bidirectional encoder representations from transformers (BERT) for improving\nmany natural language processing tasks, we propose in this paper ``CAN-BERT\", a\ndeep learning based network intrusion detection system, to detect cyber attacks\non CAN bus protocol. We show that the BERT model can learn the sequence of\narbitration identifiers (IDs) in the CAN bus for anomaly detection using the\n``masked language model\" unsupervised training objective. The experimental\nresults on the ``Car Hacking: Attack \\& Defense Challenge 2020\" dataset show\nthat ``CAN-BERT\" outperforms state-of-the-art approaches. In addition to being\nable to identify in-vehicle intrusions in real-time within 0.8 ms to 3 ms w.r.t\nCAN ID sequence length, it can also detect a wide variety of cyberattacks with\nan F1-score of between 0.81 and 0.99.", "journal": ""}
{"doi": "10.48550/arXiv.2304.04717", "date": "2023-04-10", "title": "Incorporating Structured Sentences with Time-enhanced BERT for Fully-inductive Temporal Relation Prediction", "authors": "Zhongwu Chen, Chengjin Xu, Fenglong Su, Zhen Huang, Yong Dou", "abstract": "Temporal relation prediction in incomplete temporal knowledge graphs (TKGs)\nis a popular temporal knowledge graph completion (TKGC) problem in both\ntransductive and inductive settings. Traditional embedding-based TKGC models\n(TKGE) rely on structured connections and can only handle a fixed set of\nentities, i.e., the transductive setting. In the inductive setting where test\nTKGs contain emerging entities, the latest methods are based on symbolic rules\nor pre-trained language models (PLMs). However, they suffer from being\ninflexible and not time-specific, respectively. In this work, we extend the\nfully-inductive setting, where entities in the training and test sets are\ntotally disjoint, into TKGs and take a further step towards a more flexible and\ntime-sensitive temporal relation prediction approach SST-BERT, incorporating\nStructured Sentences with Time-enhanced BERT. Our model can obtain the entity\nhistory and implicitly learn rules in the semantic space by encoding structured\nsentences, solving the problem of inflexibility. We propose to use a time\nmasking MLM task to pre-train BERT in a corpus rich in temporal tokens\nspecially generated for TKGs, enhancing the time sensitivity of SST-BERT. To\ncompute the probability of occurrence of a target quadruple, we aggregate all\nits structured sentences from both temporal and semantic perspectives into a\nscore. Experiments on the transductive datasets and newly generated\nfully-inductive benchmarks show that SST-BERT successfully improves over\nstate-of-the-art baselines.", "journal": ""}
{"doi": "10.48550/arXiv.2304.11434", "date": "2023-04-22", "title": "L3Cube-IndicSBERT: A simple approach for learning cross-lingual sentence representations using multilingual BERT", "authors": "Samruddhi Deode, Janhavi Gadre, Aditi Kajale, Ananya Joshi, Raviraj Joshi", "abstract": "The multilingual Sentence-BERT (SBERT) models map different languages to\ncommon representation space and are useful for cross-language similarity and\nmining tasks. We propose a simple yet effective approach to convert vanilla\nmultilingual BERT models into multilingual sentence BERT models using synthetic\ncorpus. We simply aggregate translated NLI or STS datasets of the low-resource\ntarget languages together and perform SBERT-like fine-tuning of the vanilla\nmultilingual BERT model. We show that multilingual BERT models are inherent\ncross-lingual learners and this simple baseline fine-tuning approach without\nexplicit cross-lingual training yields exceptional cross-lingual properties. We\nshow the efficacy of our approach on 10 major Indic languages and also show the\napplicability of our approach to non-Indic languages German and French. Using\nthis approach, we further present L3Cube-IndicSBERT, the first multilingual\nsentence representation model specifically for Indian languages Hindi, Marathi,\nKannada, Telugu, Malayalam, Tamil, Gujarati, Odia, Bengali, and Punjabi. The\nIndicSBERT exhibits strong cross-lingual capabilities and performs\nsignificantly better than alternatives like LaBSE, LASER, and\nparaphrase-multilingual-mpnet-base-v2 on Indic cross-lingual and monolingual\nsentence similarity tasks. We also release monolingual SBERT models for each of\nthe languages and show that IndicSBERT performs competitively with its\nmonolingual counterparts. These models have been evaluated using embedding\nsimilarity scores and classification accuracy.", "journal": ""}
{"doi": "10.48550/arXiv.2306.09877", "date": "2023-06-16", "title": "Revealing the impact of social circumstances on the selection of cancer therapy through natural language processing of social work notes", "authors": "Shenghuan Sun, Travis Zack, Christopher Y. K. Williams, Atul J. Butte, Madhumita Sushil", "abstract": "We aimed to investigate the impact of social circumstances on cancer therapy\nselection using natural language processing to derive insights from social\nworker documentation. We developed and employed a Bidirectional Encoder\nRepresentations from Transformers (BERT) based approach, using a hierarchical\nmulti-step BERT model (BERT-MS) to predict the prescription of targeted cancer\ntherapy to patients based solely on documentation by clinical social workers.\nOur corpus included free-text clinical social work notes, combined with\nmedication prescription information, for all patients treated for breast\ncancer. We conducted a feature importance analysis to pinpoint the specific\nsocial circumstances that impact cancer therapy selection. Using only social\nwork notes, we consistently predicted the administration of targeted therapies,\nsuggesting systematic differences in treatment selection exist due to\nnon-clinical factors. The UCSF-BERT model, pretrained on clinical text at UCSF,\noutperformed other publicly available language models with an AUROC of 0.675\nand a Macro F1 score of 0.599. The UCSF BERT-MS model, capable of leveraging\nmultiple pieces of notes, surpassed the UCSF-BERT model in both AUROC and\nMacro-F1. Our feature importance analysis identified several clinically\nintuitive social determinants of health (SDOH) that potentially contribute to\ndisparities in treatment. Our findings indicate that significant disparities\nexist among breast cancer patients receiving different types of therapies based\non social determinants of health. Social work reports play a crucial role in\nunderstanding these disparities in clinical decision-making.", "journal": ""}
{"doi": "10.48550/arXiv.2310.20558", "date": "2023-10-31", "title": "Breaking the Token Barrier: Chunking and Convolution for Efficient Long Text Classification with BERT", "authors": "Aman Jaiswal, Evangelos Milios", "abstract": "Transformer-based models, specifically BERT, have propelled research in\nvarious NLP tasks. However, these models are limited to a maximum token limit\nof 512 tokens. Consequently, this makes it non-trivial to apply it in a\npractical setting with long input. Various complex methods have claimed to\novercome this limit, but recent research questions the efficacy of these models\nacross different classification tasks. These complex architectures evaluated on\ncarefully curated long datasets perform at par or worse than simple baselines.\nIn this work, we propose a relatively simple extension to vanilla BERT\narchitecture called ChunkBERT that allows finetuning of any pretrained models\nto perform inference on arbitrarily long text. The proposed method is based on\nchunking token representations and CNN layers, making it compatible with any\npre-trained BERT. We evaluate chunkBERT exclusively on a benchmark for\ncomparing long-text classification models across a variety of tasks (including\nbinary classification, multi-class classification, and multi-label\nclassification). A BERT model finetuned using the ChunkBERT method performs\nconsistently across long samples in the benchmark while utilizing only a\nfraction (6.25\\%) of the original memory footprint. These findings suggest that\nefficient finetuning and inference can be achieved through simple modifications\nto pre-trained BERT models.", "journal": ""}
{"doi": "10.48550/arXiv.2401.15861", "date": "2024-01-29", "title": "BPDec: Unveiling the Potential of Masked Language Modeling Decoder in BERT pretraining", "authors": "Wen Liang, Youzhi Liang", "abstract": "BERT (Bidirectional Encoder Representations from Transformers) has\nrevolutionized the field of natural language processing through its exceptional\nperformance on numerous tasks. Yet, the majority of researchers have mainly\nconcentrated on enhancements related to the model structure, such as relative\nposition embedding and more efficient attention mechanisms. Others have delved\ninto pretraining tricks associated with Masked Language Modeling, including\nwhole word masking. DeBERTa introduced an enhanced decoder adapted for BERT's\nencoder model for pretraining, proving to be highly effective. We argue that\nthe design and research around enhanced masked language modeling decoders have\nbeen underappreciated. In this paper, we propose several designs of enhanced\ndecoders and introduce BPDec (BERT Pretraining Decoder), a novel method for\nmodeling training. Typically, a pretrained BERT model is fine-tuned for\nspecific Natural Language Understanding (NLU) tasks. In our approach, we\nutilize the original BERT model as the encoder, making only changes to the\ndecoder without altering the encoder. This approach does not necessitate\nextensive modifications to the encoder architecture and can be seamlessly\nintegrated into existing fine-tuning pipelines and services, offering an\nefficient and effective enhancement strategy. Compared to other methods, while\nwe also incur a moderate training cost for the decoder during the pretraining\nprocess, our approach does not introduce additional training costs during the\nfine-tuning phase. We test multiple enhanced decoder structures after\npretraining and evaluate their performance on the GLUE tasks and SQuAD tasks.\nOur results demonstrate that BPDec, having only undergone subtle refinements to\nthe model structure during pretraining, significantly enhances model\nperformance without escalating the finetuning cost, inference time and serving\nbudget.", "journal": ""}
{"doi": "10.48550/arXiv.2409.13331", "date": "2024-09-20", "title": "Applying Pre-trained Multilingual BERT in Embeddings for Improved Malicious Prompt Injection Attacks Detection", "authors": "Md Abdur Rahman, Hossain Shahriar, Fan Wu, Alfredo Cuzzocrea", "abstract": "Large language models (LLMs) are renowned for their exceptional capabilities,\nand applying to a wide range of applications. However, this widespread use\nbrings significant vulnerabilities. Also, it is well observed that there are\nhuge gap which lies in the need for effective detection and mitigation\nstrategies against malicious prompt injection attacks in large language models,\nas current approaches may not adequately address the complexity and evolving\nnature of these vulnerabilities in real-world applications. Therefore, this\nwork focuses the impact of malicious prompt injection attacks which is one of\nmost dangerous vulnerability on real LLMs applications. It examines to apply\nvarious BERT (Bidirectional Encoder Representations from Transformers) like\nmultilingual BERT, DistilBert for classifying malicious prompts from legitimate\nprompts. Also, we observed how tokenizing the prompt texts and generating\nembeddings using multilingual BERT contributes to improve the performance of\nvarious machine learning methods: Gaussian Naive Bayes, Random Forest, Support\nVector Machine, and Logistic Regression. The performance of each model is\nrigorously analyzed with various parameters to improve the binary\nclassification to discover malicious prompts. Multilingual BERT approach to\nembed the prompts significantly improved and outperformed the existing works\nand achieves an outstanding accuracy of 96.55% by Logistic regression.\nAdditionally, we investigated the incorrect predictions of the model to gain\ninsights into its limitations. The findings can guide researchers in tuning\nvarious BERT for finding the most suitable model for diverse LLMs\nvulnerabilities.", "journal": ""}
{"doi": "10.48550/arXiv.2411.18021", "date": "2024-11-27", "title": "Can bidirectional encoder become the ultimate winner for downstream applications of foundation models?", "authors": "Lewen Yang, Xuanyu Zhou, Juao Fan, Xinyi Xie, Shengxin Zhu", "abstract": "Over the past few decades, Artificial Intelligence(AI) has progressed from\nthe initial machine learning stage to the deep learning stage, and now to the\nstage of foundational models. Foundational models have the characteristics of\npre-training, transfer learning, and self-supervised learning, and pre-trained\nmodels can be fine-tuned and applied to various downstream tasks. Under the\nframework of foundational models, models such as Bidirectional Encoder\nRepresentations from Transformers(BERT) and Generative Pre-trained\nTransformer(GPT) have greatly advanced the development of natural language\nprocessing(NLP), especially the emergence of many models based on BERT. BERT\nbroke through the limitation of only using one-way methods for language\nmodeling in pre-training by using a masked language model. It can capture\nbidirectional context information to predict the masked words in the sequence,\nthis can improve the feature extraction ability of the model. This makes the\nmodel very useful for downstream tasks, especially for specialized\napplications. The model using the bidirectional encoder can better understand\nthe domain knowledge and be better applied to these downstream tasks. So we\nhope to help understand how this technology has evolved and improved model\nperformance in various natural language processing tasks under the background\nof foundational models and reveal its importance in capturing context\ninformation and improving the model's performance on downstream tasks. This\narticle analyzes one-way and bidirectional models based on GPT and BERT and\ncompares their differences based on the purpose of the model. It also briefly\nanalyzes BERT and the improvements of some models based on BERT. The model's\nperformance on the Stanford Question Answering Dataset(SQuAD) and General\nLanguage Understanding Evaluation(GLUE) was compared.", "journal": ""}
{"doi": "10.48550/arXiv.2412.03625", "date": "2024-12-04", "title": "Multimodal Sentiment Analysis Based on BERT and ResNet", "authors": "JiaLe Ren", "abstract": "With the rapid development of the Internet and social media, multi-modal data\n(text and image) is increasingly important in sentiment analysis tasks.\nHowever, the existing methods are difficult to effectively fuse text and image\nfeatures, which limits the accuracy of analysis. To solve this problem, a\nmultimodal sentiment analysis framework combining BERT and ResNet was proposed.\nBERT has shown strong text representation ability in natural language\nprocessing, and ResNet has excellent image feature extraction performance in\nthe field of computer vision. Firstly, BERT is used to extract the text feature\nvector, and ResNet is used to extract the image feature representation. Then, a\nvariety of feature fusion strategies are explored, and finally the fusion model\nbased on attention mechanism is selected to make full use of the complementary\ninformation between text and image. Experimental results on the public dataset\nMAVA-single show that compared with the single-modal models that only use BERT\nor ResNet, the proposed multi-modal model improves the accuracy and F1 score,\nreaching the best accuracy of 74.5%. This study not only provides new ideas and\nmethods for multimodal sentiment analysis, but also demonstrates the\napplication potential of BERT and ResNet in cross-domain fusion. In the future,\nmore advanced feature fusion techniques and optimization strategies will be\nexplored to further improve the accuracy and generalization ability of\nmultimodal sentiment analysis.", "journal": ""}
{"doi": "10.48550/arXiv.2501.00031", "date": "2024-12-21", "title": "Distilling Large Language Models for Efficient Clinical Information Extraction", "authors": "Karthik S. Vedula, Annika Gupta, Akshay Swaminathan, Ivan Lopez, Suhana Bedi, Nigam H. Shah", "abstract": "Large language models (LLMs) excel at clinical information extraction but\ntheir computational demands limit practical deployment. Knowledge\ndistillation--the process of transferring knowledge from larger to smaller\nmodels--offers a potential solution. We evaluate the performance of distilled\nBERT models, which are approximately 1,000 times smaller than modern LLMs, for\nclinical named entity recognition (NER) tasks. We leveraged state-of-the-art\nLLMs (Gemini and OpenAI models) and medical ontologies (RxNorm and SNOMED) as\nteacher labelers for medication, disease, and symptom extraction. We applied\nour approach to over 3,300 clinical notes spanning five publicly available\ndatasets, comparing distilled BERT models against both their teacher labelers\nand BERT models fine-tuned on human labels. External validation was conducted\nusing clinical notes from the MedAlign dataset. For disease extraction, F1\nscores were 0.82 (teacher model), 0.89 (BioBERT trained on human labels), and\n0.84 (BioBERT-distilled). For medication, F1 scores were 0.84 (teacher model),\n0.91 (BioBERT-human), and 0.87 (BioBERT-distilled). For symptoms: F1 score of\n0.73 (teacher model) and 0.68 (BioBERT-distilled). Distilled BERT models had\nfaster inference (12x, 4x, 8x faster than GPT-4o, o1-mini, and Gemini Flash\nrespectively) and lower costs (85x, 101x, 2x cheaper than GPT-4o, o1-mini, and\nGemini Flash respectively). On the external validation dataset, the distilled\nBERT model achieved F1 scores of 0.883 (medication), 0.726 (disease), and 0.699\n(symptom). Distilled BERT models were up to 101x cheaper and 12x faster than\nstate-of-the-art LLMs while achieving similar performance on NER tasks.\nDistillation offers a computationally efficient and scalable alternative to\nlarge LLMs for clinical information extraction.", "journal": ""}
{"doi": "10.48550/arXiv.0002042", "date": "2000-02-07", "title": "Dilatonic Randall-Sundrum Theory and renormalisation group", "authors": "Cesar Gomez, Bert Janssen, Pedro Silva", "abstract": "We extend Randall-Sundrum dynamics to non-conformal metrics corresponding to\nnon-constant dilaton. We study the appareance of space-time naked singularities\nand the renormalization group evolution of four-dimensional Newton constant.", "journal": "JHEP 0004 (2000) 024"}
{"doi": "10.48550/arXiv.0412037", "date": "2004-12-03", "title": "Giant Gravitons as Fuzzy Manifolds", "authors": "Bert Janssen, Yolanda Lozano, Diego Rodriguez-Gomez", "abstract": "Giant gravitons are described microscopically in terms of dielectric\ngravitational waves expanding into fuzzy manifolds. We review these\nconstructions in AdS_m \\times S^n spacetimes, discussing the different fuzzy\nmanifolds that appear in each case.", "journal": ""}
{"doi": "10.48550/arXiv.0502125", "date": "2005-02-15", "title": "Two-dimensional models", "authors": "Bert Schroer", "abstract": "Proposal for contribution to the quantum field theory section in\n\"Encyclopedia of Mathematical Physics\".", "journal": ""}
{"doi": "10.48550/arXiv.0008076", "date": "2000-08-10", "title": "Half twists of Hodge structures of CM-type", "authors": "Bert van Geemen", "abstract": "To a Hodge structure V of weight k with CM by a field K we associate Hodge\nstructures V_{-n/2} of weight k+n for n positive and, under certain\ncircumstances, also for n negative. We show that these `half twists' come up\nnaturally in the Kuga-Satake varieties of weight two Hodge structures with CM\nby an imaginary quadratic field.", "journal": ""}
{"doi": "10.48550/arXiv.0908.1769", "date": "2009-08-12", "title": "Approximating the Permanent with Belief Propagation", "authors": "Bert Huang, Tony Jebara", "abstract": "This work describes a method of approximating matrix permanents efficiently\nusing belief propagation. We formulate a probability distribution whose\npartition function is exactly the permanent, then use Bethe free energy to\napproximate this partition function. After deriving some speedups to standard\nbelief propagation, the resulting algorithm requires $(n^2)$ time per\niteration. Finally, we demonstrate the advantages of using this approximation.", "journal": ""}
{"doi": "10.48550/arXiv.1103.4441", "date": "2011-03-23", "title": "Dynnikov coordinates on virtual braid groups", "authors": "Valerij G. Bardakov, Andrei Vesnin, Bert Wiest", "abstract": "We define Dynnikov coordinates on virtual braid groups. We prove that they\nare faithful invariants of virtual 2-braids, and present evidence that they are\nalso very powerful invariants for general virtual braids.", "journal": ""}
{"doi": "10.48550/arXiv.1312.5012", "date": "2013-12-18", "title": "The Highly Connected Matroids in Minor-closed Classes", "authors": "Jim Geelen, Bert Gerards, Geoff Whittle", "abstract": "For any minor-closed class of matroids over a fixed finite field, we state an\nexact structural characterization for the sufficiently connected matroids in\nthe class. We also state a number of conjectures that might be approachable\nusing the structural characterization.", "journal": ""}
{"doi": "10.48550/arXiv.1603.06247", "date": "2016-03-20", "title": "Genus three curves and 56 nodal sextic surfaces", "authors": "Bert van Geemen, Yan Zhao", "abstract": "Catanese and Tonoli showed that the maximal cardinality for an even set of\nnodes on a sextic surface is 56 and they constructed such nodal surfaces. In\nthis paper we give an alternative, rather simple, construction for these\nsurfaces starting from a non-hyperelliptic genus three curve. We illustrate our\nmethod by giving explicitly the equation of such a sextic surface starting from\nthe Klein curve.", "journal": ""}
{"doi": "10.48550/arXiv.2006.01563", "date": "2020-06-02", "title": "Exploring Cross-sentence Contexts for Named Entity Recognition with BERT", "authors": "Jouni Luoma, Sampo Pyysalo", "abstract": "Named entity recognition (NER) is frequently addressed as a sequence\nclassification task where each input consists of one sentence of text. It is\nnevertheless clear that useful information for the task can often be found\noutside of the scope of a single-sentence context. Recently proposed\nself-attention models such as BERT can both efficiently capture long-distance\nrelationships in input as well as represent inputs consisting of several\nsentences, creating new opportunitites for approaches that incorporate\ncross-sentence information in natural language processing tasks. In this paper,\nwe present a systematic study exploring the use of cross-sentence information\nfor NER using BERT models in five languages. We find that adding context in the\nform of additional sentences to BERT input systematically increases NER\nperformance on all of the tested languages and models. Including multiple\nsentences in each input also allows us to study the predictions of the same\nsentences in different contexts. We propose a straightforward method,\nContextual Majority Voting (CMV), to combine different predictions for\nsentences and demonstrate this to further increase NER performance with BERT.\nOur approach does not require any changes to the underlying BERT architecture,\nrather relying on restructuring examples for training and prediction.\nEvaluation on established datasets, including the CoNLL'02 and CoNLL'03 NER\nbenchmarks, demonstrates that our proposed approach can improve on the\nstate-of-the-art NER results on English, Dutch, and Finnish, achieves the best\nreported BERT-based results on German, and is on par with performance reported\nwith other BERT-based approaches in Spanish. We release all methods implemented\nin this work under open licenses.", "journal": "Proceedings of the 28th International Conference on Computational\n  Linguistics, dec,2020, Barcelona, Spain (Online),International Committee on\n  Computational Linguistics, pages 904-914"}
{"doi": "10.48550/arXiv.1904.02234", "date": "2019-04-03", "title": "Hyperbolic structures for Artin-Tits groups of spherical type", "authors": "Matthieu Calvez, Bert Wiest", "abstract": "The goal of this mostly expository paper is to present several candidates for\nhyperbolic structures on irreducible Artin-Tits groups of spherical type and to\nelucidate some relations between them. Most constructions are algebraic\nanalogues of previously known hyperbolic structures on Artin braid groups\ncoming from natural actions of these groups on curve graphs and (modified) arc\ngraphs of punctured disks.", "journal": ""}
{"doi": "10.48550/arXiv.1907.12679", "date": "2019-07-29", "title": "Machine Translation Evaluation with BERT Regressor", "authors": "Hiroki Shimanaka, Tomoyuki Kajiwara, Mamoru Komachi", "abstract": "We introduce the metric using BERT (Bidirectional Encoder Representations\nfrom Transformers) (Devlin et al., 2019) for automatic machine translation\nevaluation. The experimental results of the WMT-2017 Metrics Shared Task\ndataset show that our metric achieves state-of-the-art performance in\nsegment-level metrics task for all to-English language pairs.", "journal": ""}
{"doi": "10.48550/arXiv.2001.03521", "date": "2020-01-10", "title": "Towards Minimal Supervision BERT-based Grammar Error Correction", "authors": "Yiyuan Li, Antonios Anastasopoulos, Alan W Black", "abstract": "Current grammatical error correction (GEC) models typically consider the task\nas sequence generation, which requires large amounts of annotated data and\nlimit the applications in data-limited settings. We try to incorporate\ncontextual information from pre-trained language model to leverage annotation\nand benefit multilingual scenarios. Results show strong potential of\nBidirectional Encoder Representations from Transformers (BERT) in grammatical\nerror correction task.", "journal": ""}
{"doi": "10.48550/arXiv.2104.02041", "date": "2021-04-05", "title": "Exploring Transformers in Emotion Recognition: a comparison of BERT, DistillBERT, RoBERTa, XLNet and ELECTRA", "authors": "Diogo Cortiz", "abstract": "This paper investigates how Natural Language Understanding (NLU) could be\napplied in Emotion Recognition, a specific task in affective computing. We\nfinetuned different transformers language models (BERT, DistilBERT, RoBERTa,\nXLNet, and ELECTRA) using a fine-grained emotion dataset and evaluating them in\nterms of performance (f1-score) and time to complete.", "journal": ""}
{"doi": "10.48550/arXiv.2106.07340", "date": "2021-06-02", "title": "MathBERT: A Pre-trained Language Model for General NLP Tasks in Mathematics Education", "authors": "Jia Tracy Shen, Michiharu Yamashita, Ethan Prihar, Neil Heffernan, Xintao Wu, Ben Graff, Dongwon Lee", "abstract": "Since the introduction of the original BERT (i.e., BASE BERT), researchers\nhave developed various customized BERT models with improved performance for\nspecific domains and tasks by exploiting the benefits of transfer learning. Due\nto the nature of mathematical texts, which often use domain specific vocabulary\nalong with equations and math symbols, we posit that the development of a new\nBERT model for mathematics would be useful for many mathematical downstream\ntasks. In this resource paper, we introduce our multi-institutional effort\n(i.e., two learning platforms and three academic institutions in the US) toward\nthis need: MathBERT, a model created by pre-training the BASE BERT model on a\nlarge mathematical corpus ranging from pre-kindergarten (pre-k), to\nhigh-school, to college graduate level mathematical content. In addition, we\nselect three general NLP tasks that are often used in mathematics education:\nprediction of knowledge component, auto-grading open-ended Q&A, and knowledge\ntracing, to demonstrate the superiority of MathBERT over BASE BERT. Our\nexperiments show that MathBERT outperforms prior best methods by 1.2-22% and\nBASE BERT by 2-8% on these tasks. In addition, we build a mathematics specific\nvocabulary 'mathVocab' to train with MathBERT. We discover that MathBERT\npre-trained with 'mathVocab' outperforms MathBERT trained with the BASE BERT\nvocabulary (i.e., 'origVocab'). MathBERT is currently being adopted at the\nparticipated leaning platforms: Stride, Inc, a commercial educational resource\nprovider, and ASSISTments.org, a free online educational platform. We release\nMathBERT for public usage at: https://github.com/tbs17/MathBERT.", "journal": ""}
{"doi": "10.48550/arXiv.2212.14648", "date": "2022-12-30", "title": "Distant Reading of the German Coalition Deal: Recognizing Policy Positions with BERT-based Text Classification", "authors": "Michael Zylla, Thomas Haider", "abstract": "Automated text analysis has become a widely used tool in political science.\nIn this research, we use a BERT model trained on German party manifestos to\nidentify the individual parties' contribution to the coalition agreement of\n2021.", "journal": ""}
{"doi": "10.48550/arXiv.2301.01543", "date": "2023-01-04", "title": "A note on the variance in principal component regression", "authors": "Bert van der Veen", "abstract": "Principal component regression results in lack of fit when important\ndimensions are omitted, which cannot be assessed from the eigenvalues. I show\nthat the PC-regression estimator can also suffer from increased variance\nrelative to ordinary least squares in such cases.", "journal": ""}
{"doi": "10.48550/arXiv.2301.12206", "date": "2023-01-28", "title": "Semantic Tagging with LSTM-CRF", "authors": "Farshad Noravesh", "abstract": "In the present paper, two models are presented namely LSTM-CRF and\nBERT-LSTM-CRF for semantic tagging of universal semantic tag dataset. The\nexperiments show that the first model is much easier to converge while the\nsecond model that leverages BERT embedding, takes a long time to converge and\nneeds a big dataset for semtagging to be effective.", "journal": ""}
{"doi": "10.48550/arXiv.2403.00784", "date": "2024-02-18", "title": "Utilizing BERT for Information Retrieval: Survey, Applications, Resources, and Challenges", "authors": "Jiajia Wang, Jimmy X. Huang, Xinhui Tu, Junmei Wang, Angela J. Huang, Md Tahmid Rahman Laskar, Amran Bhuiyan", "abstract": "Recent years have witnessed a substantial increase in the use of deep\nlearning to solve various natural language processing (NLP) problems. Early\ndeep learning models were constrained by their sequential or unidirectional\nnature, such that they struggled to capture the contextual relationships across\ntext inputs. The introduction of bidirectional encoder representations from\ntransformers (BERT) leads to a robust encoder for the transformer model that\ncan understand the broader context and deliver state-of-the-art performance\nacross various NLP tasks. This has inspired researchers and practitioners to\napply BERT to practical problems, such as information retrieval (IR). A survey\nthat focuses on a comprehensive analysis of prevalent approaches that apply\npretrained transformer encoders like BERT to IR can thus be useful for academia\nand the industry. In light of this, we revisit a variety of BERT-based methods\nin this survey, cover a wide range of techniques of IR, and group them into six\nhigh-level categories: (i) handling long documents, (ii) integrating semantic\ninformation, (iii) balancing effectiveness and efficiency, (iv) predicting the\nweights of terms, (v) query expansion, and (vi) document expansion. We also\nprovide links to resources, including datasets and toolkits, for BERT-based IR\nsystems. A key highlight of our survey is the comparison between BERT's\nencoder-based models and the latest generative Large Language Models (LLMs),\nsuch as ChatGPT, which rely on decoders. Despite the popularity of LLMs, we\nfind that for specific tasks, finely tuned BERT encoders still outperform, and\nat a lower deployment cost. Finally, we summarize the comprehensive outcomes of\nthe survey and suggest directions for future research in the area.", "journal": ""}
{"doi": "10.48550/arXiv.1112.2587", "date": "2011-12-12", "title": "The Kreuzer bi-homomorphism", "authors": "A. N. Schellekens", "abstract": "This is a brief review of simple current techniques and their application to\nstring theory constructions, centered around Max Kreuzer's contribution to this\nfield", "journal": ""}
{"doi": "10.48550/arXiv.1905.13068", "date": "2019-05-30", "title": "Unbabel's Submission to the WMT2019 APE Shared Task: BERT-based Encoder-Decoder for Automatic Post-Editing", "authors": "Ant\u00f3nio V. Lopes, M. Amin Farajian, Gon\u00e7alo M. Correia, Jonay Trenous, Andr\u00e9 F. T. Martins", "abstract": "This paper describes Unbabel's submission to the WMT2019 APE Shared Task for\nthe English-German language pair. Following the recent rise of large, powerful,\npre-trained models, we adapt the BERT pretrained model to perform Automatic\nPost-Editing in an encoder-decoder framework. Analogously to dual-encoder\narchitectures we develop a BERT-based encoder-decoder (BED) model in which a\nsingle pretrained BERT encoder receives both the source src and machine\ntranslation tgt strings. Furthermore, we explore a conservativeness factor to\nconstrain the APE system to perform fewer edits. As the official results show,\nwhen trained on a weighted combination of in-domain and artificial training\ndata, our BED system with the conservativeness penalty improves significantly\nthe translations of a strong Neural Machine Translation system by $-0.78$ and\n$+1.23$ in terms of TER and BLEU, respectively. Finally, our submission\nachieves a new state-of-the-art, ex-aequo, in English-German APE of NMT.", "journal": ""}
{"doi": "10.48550/arXiv.1905.13497", "date": "2019-05-31", "title": "Attention Is (not) All You Need for Commonsense Reasoning", "authors": "Tassilo Klein, Moin Nabi", "abstract": "The recently introduced BERT model exhibits strong performance on several\nlanguage understanding benchmarks. In this paper, we describe a simple\nre-implementation of BERT for commonsense reasoning. We show that the\nattentions produced by BERT can be directly utilized for tasks such as the\nPronoun Disambiguation Problem and Winograd Schema Challenge. Our proposed\nattention-guided commonsense reasoning method is conceptually simple yet\nempirically powerful. Experimental analysis on multiple datasets demonstrates\nthat our proposed system performs remarkably well on all cases while\noutperforming the previously reported state of the art by a margin. While\nresults suggest that BERT seems to implicitly learn to establish complex\nrelationships between entities, solving commonsense reasoning tasks might\nrequire more than unsupervised models learned from huge text corpora.", "journal": ""}
{"doi": "10.48550/arXiv.1908.01767", "date": "2019-08-04", "title": "Exploring Neural Net Augmentation to BERT for Question Answering on SQUAD 2.0", "authors": "Suhas Gupta", "abstract": "Enhancing machine capabilities to answer questions has been a topic of\nconsiderable focus in recent years of NLP research. Language models like\nEmbeddings from Language Models (ELMo)[1] and Bidirectional Encoder\nRepresentations from Transformers (BERT) [2] have been very successful in\ndeveloping general purpose language models that can be optimized for a large\nnumber of downstream language tasks. In this work, we focused on augmenting the\npre-trained BERT language model with different output neural net architectures\nand compared their performance on question answering task posed by the Stanford\nQuestion Answering Dataset 2.0 (SQUAD 2.0) [3]. Additionally, we also\nfine-tuned the pre-trained BERT model parameters to demonstrate its\neffectiveness in adapting to specialized language tasks. Our best output\nnetwork, is the contextualized CNN that performs on both the unanswerable and\nanswerable question answering tasks with F1 scores of 75.32 and 64.85\nrespectively.", "journal": ""}
{"doi": "10.48550/arXiv.1908.06264", "date": "2019-08-17", "title": "EmotionX-IDEA: Emotion BERT -- an Affectional Model for Conversation", "authors": "Yen-Hao Huang, Ssu-Rui Lee, Mau-Yun Ma, Yi-Hsin Chen, Ya-Wen Yu, Yi-Shin Chen", "abstract": "In this paper, we investigate the emotion recognition ability of the\npre-training language model, namely BERT. By the nature of the framework of\nBERT, a two-sentence structure, we adapt BERT to continues dialogue emotion\nprediction tasks, which rely heavily on the sentence-level context-aware\nunderstanding. The experiments show that by mapping the continues dialogue into\na causal utterance pair, which is constructed by the utterance and the reply\nutterance, models can better capture the emotions of the reply utterance. The\npresent method has achieved 0.815 and 0.885 micro F1 score in the testing\ndataset of Friends and EmotionPush, respectively.", "journal": ""}
{"doi": "10.48550/arXiv.1909.00153", "date": "2019-08-31", "title": "Adversarial Learning with Contextual Embeddings for Zero-resource Cross-lingual Classification and NER", "authors": "Phillip Keung, Yichao Lu, Vikas Bhardwaj", "abstract": "Contextual word embeddings (e.g. GPT, BERT, ELMo, etc.) have demonstrated\nstate-of-the-art performance on various NLP tasks. Recent work with the\nmultilingual version of BERT has shown that the model performs very well in\nzero-shot and zero-resource cross-lingual settings, where only labeled English\ndata is used to finetune the model. We improve upon multilingual BERT's\nzero-resource cross-lingual performance via adversarial learning. We report the\nmagnitude of the improvement on the multilingual MLDoc text classification and\nCoNLL 2002/2003 named entity recognition tasks. Furthermore, we show that\nlanguage-adversarial training encourages BERT to align the embeddings of\nEnglish documents and their translations, which may be the cause of the\nobserved performance gains.", "journal": ""}
{"doi": "10.48550/arXiv.1909.04181", "date": "2019-09-09", "title": "BERT-Based Arabic Social Media Author Profiling", "authors": "Chiyu Zhang, Muhammad Abdul-Mageed", "abstract": "We report our models for detecting age, language variety, and gender from\nsocial media data in the context of the Arabic author profiling and deception\ndetection shared task (APDA). We build simple models based on pre-trained\nbidirectional encoders from transformers (BERT). We first fine-tune the\npre-trained BERT model on each of the three datasets with shared task released\ndata. Then we augment shared task data with in-house data for gender and\ndialect, showing the utility of augmenting training data. Our best models on\nthe shared task test data are acquired with a majority voting of various BERT\nmodels trained under different data conditions. We acquire 54.72% accuracy for\nage, 93.75% for dialect, 81.67% for gender, and 40.97% joint accuracy across\nthe three tasks.", "journal": ""}
{"doi": "10.48550/arXiv.1909.11687", "date": "2019-09-25", "title": "Extremely Small BERT Models from Mixed-Vocabulary Training", "authors": "Sanqiang Zhao, Raghav Gupta, Yang Song, Denny Zhou", "abstract": "Pretrained language models like BERT have achieved good results on NLP tasks,\nbut are impractical on resource-limited devices due to memory footprint. A\nlarge fraction of this footprint comes from the input embeddings with large\ninput vocabulary and embedding dimensions. Existing knowledge distillation\nmethods used for model compression cannot be directly applied to train student\nmodels with reduced vocabulary sizes. To this end, we propose a distillation\nmethod to align the teacher and student embeddings via mixed-vocabulary\ntraining. Our method compresses BERT-LARGE to a task-agnostic model with\nsmaller vocabulary and hidden dimensions, which is an order of magnitude\nsmaller than other distilled BERT models and offers a better size-accuracy\ntrade-off on language understanding benchmarks as well as a practical dialogue\ntask.", "journal": ""}
{"doi": "10.48550/arXiv.1909.11942", "date": "2019-09-26", "title": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations", "authors": "Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, Radu Soricut", "abstract": "Increasing model size when pretraining natural language representations often\nresults in improved performance on downstream tasks. However, at some point\nfurther model increases become harder due to GPU/TPU memory limitations and\nlonger training times. To address these problems, we present two\nparameter-reduction techniques to lower memory consumption and increase the\ntraining speed of BERT. Comprehensive empirical evidence shows that our\nproposed methods lead to models that scale much better compared to the original\nBERT. We also use a self-supervised loss that focuses on modeling\ninter-sentence coherence, and show it consistently helps downstream tasks with\nmulti-sentence inputs. As a result, our best model establishes new\nstate-of-the-art results on the GLUE, RACE, and \\squad benchmarks while having\nfewer parameters compared to BERT-large. The code and the pretrained models are\navailable at https://github.com/google-research/ALBERT.", "journal": ""}
{"doi": "10.48550/arXiv.2006.00607", "date": "2020-05-31", "title": "LRG at SemEval-2020 Task 7: Assessing the Ability of BERT and Derivative Models to Perform Short-Edits based Humor Grading", "authors": "Siddhant Mahurkar, Rajaswa Patil", "abstract": "In this paper, we assess the ability of BERT and its derivative models\n(RoBERTa, DistilBERT, and ALBERT) for short-edits based humor grading. We test\nthese models for humor grading and classification tasks on the Humicroedit and\nthe FunLines dataset. We perform extensive experiments with these models to\ntest their language modeling and generalization abilities via zero-shot\ninference and cross-dataset inference based approaches. Further, we also\ninspect the role of self-attention layers in humor-grading by performing a\nqualitative analysis over the self-attention weights from the final layer of\nthe trained BERT model. Our experiments show that all the pre-trained BERT\nderivative models show significant generalization capabilities for\nhumor-grading related tasks.", "journal": ""}
{"doi": "10.48550/arXiv.2006.03685", "date": "2020-05-26", "title": "BERT-XML: Large Scale Automated ICD Coding Using BERT Pretraining", "authors": "Zachariah Zhang, Jingshu Liu, Narges Razavian", "abstract": "Clinical interactions are initially recorded and documented in free text\nmedical notes. ICD coding is the task of classifying and coding all diagnoses,\nsymptoms and procedures associated with a patient's visit. The process is often\nmanual and extremely time-consuming and expensive for hospitals. In this paper,\nwe propose a machine learning model, BERT-XML, for large scale automated ICD\ncoding from EHR notes, utilizing recently developed unsupervised pretraining\nthat have achieved state of the art performance on a variety of NLP tasks. We\ntrain a BERT model from scratch on EHR notes, learning with vocabulary better\nsuited for EHR tasks and thus outperform off-the-shelf models. We adapt the\nBERT architecture for ICD coding with multi-label attention. While other works\nfocus on small public medical datasets, we have produced the first large scale\nICD-10 classification model using millions of EHR notes to predict thousands of\nunique ICD codes.", "journal": ""}
{"doi": "10.48550/arXiv.2006.05676", "date": "2020-06-02", "title": "Position Masking for Language Models", "authors": "Andy Wagner, Tiyasa Mitra, Mrinal Iyer, Godfrey Da Costa, Marc Tremblay", "abstract": "Masked language modeling (MLM) pre-training models such as BERT corrupt the\ninput by replacing some tokens with [MASK] and then train a model to\nreconstruct the original tokens. This is an effective technique which has led\nto good results on all NLP benchmarks. We propose to expand upon this idea by\nmasking the positions of some tokens along with the masked input token ids. We\nfollow the same standard approach as BERT masking a percentage of the tokens\npositions and then predicting their original values using an additional fully\nconnected classifier stage. This approach has shown good performance gains\n(.3\\% improvement) for the SQUAD additional improvement in convergence times.\nFor the Graphcore IPU the convergence of BERT Base with position masking\nrequires only 50\\% of the tokens from the original BERT paper.", "journal": ""}
{"doi": "10.48550/arXiv.2006.11991", "date": "2020-06-22", "title": "Students Need More Attention: BERT-based AttentionModel for Small Data with Application to AutomaticPatient Message Triage", "authors": "Shijing Si, Rui Wang, Jedrek Wosik, Hao Zhang, David Dov, Guoyin Wang, Ricardo Henao, Lawrence Carin", "abstract": "Small and imbalanced datasets commonly seen in healthcare represent a\nchallenge when training classifiers based on deep learning models. So\nmotivated, we propose a novel framework based on BioBERT (Bidirectional Encoder\nRepresentations from Transformers forBiomedical TextMining). Specifically, (i)\nwe introduce Label Embeddings for Self-Attention in each layer of BERT, which\nwe call LESA-BERT, and (ii) by distilling LESA-BERT to smaller variants, we aim\nto reduce overfitting and model size when working on small datasets. As an\napplication, our framework is utilized to build a model for patient portal\nmessage triage that classifies the urgency of a message into three categories:\nnon-urgent, medium and urgent. Experiments demonstrate that our approach can\noutperform several strong baseline classifiers by a significant margin of 4.3%\nin terms of macro F1 score. The code for this project is publicly available at\n\\url{https://github.com/shijing001/text_classifiers}.", "journal": ""}
{"doi": "10.48550/arXiv.2111.00526", "date": "2021-10-31", "title": "FinEAS: Financial Embedding Analysis of Sentiment", "authors": "Asier Guti\u00e9rrez-Fandi\u00f1o, Miquel Noguer i Alonso, Petter Kolm, Jordi Armengol-Estap\u00e9", "abstract": "We introduce a new language representation model in finance called Financial\nEmbedding Analysis of Sentiment (FinEAS). In financial markets, news and\ninvestor sentiment are significant drivers of security prices. Thus, leveraging\nthe capabilities of modern NLP approaches for financial sentiment analysis is a\ncrucial component in identifying patterns and trends that are useful for market\nparticipants and regulators. In recent years, methods that use transfer\nlearning from large Transformer-based language models like BERT, have achieved\nstate-of-the-art results in text classification tasks, including sentiment\nanalysis using labelled datasets. Researchers have quickly adopted these\napproaches to financial texts, but best practices in this domain are not\nwell-established. In this work, we propose a new model for financial sentiment\nanalysis based on supervised fine-tuned sentence embeddings from a standard\nBERT model. We demonstrate our approach achieves significant improvements in\ncomparison to vanilla BERT, LSTM, and FinBERT, a financial domain specific\nBERT.", "journal": ""}
{"doi": "10.48550/arXiv.2111.02844", "date": "2021-11-04", "title": "A text autoencoder from transformer for fast encoding language representation", "authors": "Tan Huang", "abstract": "In recent years BERT shows apparent advantages and great potential in natural\nlanguage processing tasks. However, both training and applying BERT requires\nintensive time and resources for computing contextual language representations,\nwhich hinders its universality and applicability. To overcome this bottleneck,\nwe propose a deep bidirectional language model by using window masking\nmechanism at attention layer. This work computes contextual language\nrepresentations without random masking as does in BERT and maintains the deep\nbidirectional architecture like BERT. To compute the same sentence\nrepresentation, our method shows O(n) complexity less compared to other\ntransformer-based models with O($n^2$). To further demonstrate its superiority,\ncomputing context language representations on CPU environments is conducted, by\nusing the embeddings from the proposed method, logistic regression shows much\nhigher accuracy in terms of SMS classification. Moverover, the proposed method\nalso achieves significant higher performance in semantic similarity tasks.", "journal": ""}
{"doi": "10.48550/arXiv.2111.04198", "date": "2021-11-07", "title": "TaCL: Improving BERT Pre-training with Token-aware Contrastive Learning", "authors": "Yixuan Su, Fangyu Liu, Zaiqiao Meng, Tian Lan, Lei Shu, Ehsan Shareghi, Nigel Collier", "abstract": "Masked language models (MLMs) such as BERT and RoBERTa have revolutionized\nthe field of Natural Language Understanding in the past few years. However,\nexisting pre-trained MLMs often output an anisotropic distribution of token\nrepresentations that occupies a narrow subset of the entire representation\nspace. Such token representations are not ideal, especially for tasks that\ndemand discriminative semantic meanings of distinct tokens. In this work, we\npropose TaCL (Token-aware Contrastive Learning), a novel continual pre-training\napproach that encourages BERT to learn an isotropic and discriminative\ndistribution of token representations. TaCL is fully unsupervised and requires\nno additional data. We extensively test our approach on a wide range of English\nand Chinese benchmarks. The results show that TaCL brings consistent and\nnotable improvements over the original BERT model. Furthermore, we conduct\ndetailed analysis to reveal the merits and inner-workings of our approach.", "journal": ""}
{"doi": "10.48550/arXiv.2111.04933", "date": "2021-11-09", "title": "DSBERT:Unsupervised Dialogue Structure learning with BERT", "authors": "Bingkun Chen, Shaobing Dai, Shenghua Zheng, Lei Liao, Yang Li", "abstract": "Unsupervised dialogue structure learning is an important and meaningful task\nin natural language processing. The extracted dialogue structure and process\ncan help analyze human dialogue, and play a vital role in the design and\nevaluation of dialogue systems. The traditional dialogue system requires\nexperts to manually design the dialogue structure, which is very costly. But\nthrough unsupervised dialogue structure learning, dialogue structure can be\nautomatically obtained, reducing the cost of developers constructing dialogue\nprocess. The learned dialogue structure can be used to promote the dialogue\ngeneration of the downstream task system, and improve the logic and consistency\nof the dialogue robot's reply.In this paper, we propose a Bert-based\nunsupervised dialogue structure learning algorithm DSBERT (Dialogue Structure\nBERT). Different from the previous SOTA models VRNN and SVRNN, we combine BERT\nand AutoEncoder, which can effectively combine context information. In order to\nbetter prevent the model from falling into the local optimal solution and make\nthe dialogue state distribution more uniform and reasonable, we also propose\nthree balanced loss functions that can be used for dialogue structure learning.\nExperimental results show that DSBERT can generate a dialogue structure closer\nto the real structure, can distinguish sentences with different semantics and\nmap them to different hidden states.", "journal": ""}
{"doi": "10.48550/arXiv.2111.10951", "date": "2021-11-22", "title": "Can depth-adaptive BERT perform better on binary classification tasks", "authors": "Jing Fan, Xin Zhang, Sheng Zhang, Yan Pan, Lixiang Guo", "abstract": "In light of the success of transferring language models into NLP tasks, we\nask whether the full BERT model is always the best and does it exist a simple\nbut effective method to find the winning ticket in state-of-the-art deep neural\nnetworks without complex calculations. We construct a series of BERT-based\nmodels with different size and compare their predictions on 8 binary\nclassification tasks. The results show there truly exist smaller sub-networks\nperforming better than the full model. Then we present a further study and\npropose a simple method to shrink BERT appropriately before fine-tuning. Some\nextended experiments indicate that our method could save time and storage\noverhead extraordinarily with little even no accuracy loss.", "journal": ""}
{"doi": "10.48550/arXiv.2111.15379", "date": "2021-11-30", "title": "Text classification problems via BERT embedding method and graph convolutional neural network", "authors": "Loc Hoang Tran, Tuan Tran, An Mai", "abstract": "This paper presents the novel way combining the BERT embedding method and the\ngraph convolutional neural network. This combination is employed to solve the\ntext classification problem. Initially, we apply the BERT embedding method to\nthe texts (in the BBC news dataset and the IMDB movie reviews dataset) in order\nto transform all the texts to numerical vector. Then, the graph convolutional\nneural network will be applied to these numerical vectors to classify these\ntexts into their ap-propriate classes/labels. Experiments show that the\nperformance of the graph convolutional neural network model is better than the\nperfor-mances of the combination of the BERT embedding method with clas-sical\nmachine learning models.", "journal": ""}
{"doi": "10.48550/arXiv.2111.15617", "date": "2021-11-30", "title": "Text Mining Drug/Chemical-Protein Interactions using an Ensemble of BERT and T5 Based Models", "authors": "Virginia Adams, Hoo-Chang Shin, Carol Anderson, Bo Liu, Anas Abidin", "abstract": "In Track-1 of the BioCreative VII Challenge participants are asked to\nidentify interactions between drugs/chemicals and proteins. In-context named\nentity annotations for each drug/chemical and protein are provided and one of\nfourteen different interactions must be automatically predicted. For this\nrelation extraction task, we attempt both a BERT-based sentence classification\napproach, and a more novel text-to-text approach using a T5 model. We find that\nlarger BERT-based models perform better in general, with our BioMegatron-based\nmodel achieving the highest scores across all metrics, achieving 0.74 F1 score.\nThough our novel T5 text-to-text method did not perform as well as most of our\nBERT-based models, it outperformed those trained on similar data, showing\npromising results, achieving 0.65 F1 score. We believe a text-to-text approach\nto relation extraction has some competitive advantages and there is a lot of\nroom for research advancement.", "journal": ""}
{"doi": "10.48550/arXiv.1901.04085", "date": "2019-01-13", "title": "Passage Re-ranking with BERT", "authors": "Rodrigo Nogueira, Kyunghyun Cho", "abstract": "Recently, neural models pretrained on a language modeling task, such as ELMo\n(Peters et al., 2017), OpenAI GPT (Radford et al., 2018), and BERT (Devlin et\nal., 2018), have achieved impressive results on various natural language\nprocessing tasks such as question-answering and natural language inference. In\nthis paper, we describe a simple re-implementation of BERT for query-based\npassage re-ranking. Our system is the state of the art on the TREC-CAR dataset\nand the top entry in the leaderboard of the MS MARCO passage retrieval task,\noutperforming the previous state of the art by 27% (relative) in MRR@10. The\ncode to reproduce our results is available at\nhttps://github.com/nyu-dl/dl4marco-bert", "journal": ""}
{"doi": "10.48550/arXiv.1904.03323", "date": "2019-04-06", "title": "Publicly Available Clinical BERT Embeddings", "authors": "Emily Alsentzer, John R. Murphy, Willie Boag, Wei-Hung Weng, Di Jin, Tristan Naumann, Matthew B. A. McDermott", "abstract": "Contextual word embedding models such as ELMo (Peters et al., 2018) and BERT\n(Devlin et al., 2018) have dramatically improved performance for many natural\nlanguage processing (NLP) tasks in recent months. However, these models have\nbeen minimally explored on specialty corpora, such as clinical text; moreover,\nin the clinical domain, no publicly-available pre-trained BERT models yet\nexist. In this work, we address this need by exploring and releasing BERT\nmodels for clinical text: one for generic clinical text and another for\ndischarge summaries specifically. We demonstrate that using a domain-specific\nmodel yields performance improvements on three common clinical NLP tasks as\ncompared to nonspecific embeddings. These domain-specific models are not as\nperformant on two clinical de-identification tasks, and argue that this is a\nnatural consequence of the differences between de-identified source text and\nsynthetically non de-identified task text.", "journal": ""}
{"doi": "10.48550/arXiv.1910.06188", "date": "2019-10-14", "title": "Q8BERT: Quantized 8Bit BERT", "authors": "Ofir Zafrir, Guy Boudoukh, Peter Izsak, Moshe Wasserblat", "abstract": "Recently, pre-trained Transformer based language models such as BERT and GPT,\nhave shown great improvement in many Natural Language Processing (NLP) tasks.\nHowever, these models contain a large amount of parameters. The emergence of\neven larger and more accurate models such as GPT2 and Megatron, suggest a trend\nof large pre-trained Transformer models. However, using these large models in\nproduction environments is a complex task requiring a large amount of compute,\nmemory and power resources. In this work we show how to perform\nquantization-aware training during the fine-tuning phase of BERT in order to\ncompress BERT by $4\\times$ with minimal accuracy loss. Furthermore, the\nproduced quantized model can accelerate inference speed if it is optimized for\n8bit Integer supporting hardware.", "journal": ""}
{"doi": "10.48550/arXiv.2003.04987", "date": "2020-02-17", "title": "A Financial Service Chatbot based on Deep Bidirectional Transformers", "authors": "Shi Yu, Yuxin Chen, Hussain Zaidi", "abstract": "We develop a chatbot using Deep Bidirectional Transformer models (BERT) to\nhandle client questions in financial investment customer service. The bot can\nrecognize 381 intents, and decides when to say \"I don't know\" and escalates\nirrelevant/uncertain questions to human operators. Our main novel contribution\nis the discussion about uncertainty measure for BERT, where three different\napproaches are systematically compared on real problems. We investigated two\nuncertainty metrics, information entropy and variance of dropout sampling in\nBERT, followed by mixed-integer programming to optimize decision thresholds.\nAnother novel contribution is the usage of BERT as a language model in\nautomatic spelling correction. Inputs with accidental spelling errors can\nsignificantly decrease intent classification performance. The proposed approach\ncombines probabilities from masked language model and word edit distances to\nfind the best corrections for misspelled words. The chatbot and the entire\nconversational AI system are developed using open-source tools, and deployed\nwithin our company's intranet. The proposed approach can be useful for\nindustries seeking similar in-house solutions in their specific business\ndomains. We share all our code and a sample chatbot built on a public dataset\non Github.", "journal": ""}
{"doi": "10.48550/arXiv.2003.11563", "date": "2020-03-16", "title": "Cost-Sensitive BERT for Generalisable Sentence Classification with Imbalanced Data", "authors": "Harish Tayyar Madabushi, Elena Kochkina, Michael Castelle", "abstract": "The automatic identification of propaganda has gained significance in recent\nyears due to technological and social changes in the way news is generated and\nconsumed. That this task can be addressed effectively using BERT, a powerful\nnew architecture which can be fine-tuned for text classification tasks, is not\nsurprising. However, propaganda detection, like other tasks that deal with news\ndocuments and other forms of decontextualized social communication (e.g.\nsentiment analysis), inherently deals with data whose categories are\nsimultaneously imbalanced and dissimilar. We show that BERT, while capable of\nhandling imbalanced classes with no additional data augmentation, does not\ngeneralise well when the training and test data are sufficiently dissimilar (as\nis often the case with news sources, whose topics evolve over time). We show\nhow to address this problem by providing a statistical measure of similarity\nbetween datasets and a method of incorporating cost-weighting into BERT when\nthe training and test sets are dissimilar. We test these methods on the\nPropaganda Techniques Corpus (PTC) and achieve the second-highest score on\nsentence-level propaganda classification.", "journal": ""}
{"doi": "10.48550/arXiv.2105.01735", "date": "2021-05-04", "title": "HerBERT: Efficiently Pretrained Transformer-based Language Model for Polish", "authors": "Robert Mroczkowski, Piotr Rybak, Alina Wr\u00f3blewska, Ireneusz Gawlik", "abstract": "BERT-based models are currently used for solving nearly all Natural Language\nProcessing (NLP) tasks and most often achieve state-of-the-art results.\nTherefore, the NLP community conducts extensive research on understanding these\nmodels, but above all on designing effective and efficient training procedures.\nSeveral ablation studies investigating how to train BERT-like models have been\ncarried out, but the vast majority of them concerned only the English language.\nA training procedure designed for English does not have to be universal and\napplicable to other especially typologically different languages. Therefore,\nthis paper presents the first ablation study focused on Polish, which, unlike\nthe isolating English language, is a fusional language. We design and\nthoroughly evaluate a pretraining procedure of transferring knowledge from\nmultilingual to monolingual BERT-based models. In addition to multilingual\nmodel initialization, other factors that possibly influence pretraining are\nalso explored, i.e. training objective, corpus size, BPE-Dropout, and\npretraining length. Based on the proposed procedure, a Polish BERT-based\nlanguage model -- HerBERT -- is trained. This model achieves state-of-the-art\nresults on multiple downstream tasks.", "journal": ""}
{"doi": "10.48550/arXiv.2105.05112", "date": "2021-05-11", "title": "Integrating extracted information from bert and multiple embedding methods with the deep neural network for humour detection", "authors": "Rida Miraj, Masaki Aono", "abstract": "Humour detection from sentences has been an interesting and challenging task\nin the last few years. In attempts to highlight humour detection, most research\nwas conducted using traditional approaches of embedding, e.g., Word2Vec or\nGlove. Recently BERT sentence embedding has also been used for this task. In\nthis paper, we propose a framework for humour detection in short texts taken\nfrom news headlines. Our proposed framework (IBEN) attempts to extract\ninformation from written text via the use of different layers of BERT. After\nseveral trials, weights were assigned to different layers of the BERT model.\nThe extracted information was then sent to a Bi-GRU neural network as an\nembedding matrix. We utilized the properties of some external embedding models.\nA multi-kernel convolution in our neural network was also employed to extract\nhigher-level sentence representations. This framework performed very well on\nthe task of humour detection.", "journal": ""}
{"doi": "10.48550/arXiv.2202.00373", "date": "2022-02-01", "title": "Improving BERT-based Query-by-Document Retrieval with Multi-Task Optimization", "authors": "Amin Abolghasemi, Suzan Verberne, Leif Azzopardi", "abstract": "Query-by-document (QBD) retrieval is an Information Retrieval task in which a\nseed document acts as the query and the goal is to retrieve related documents\n-- it is particular common in professional search tasks. In this work we\nimprove the retrieval effectiveness of the BERT re-ranker, proposing an\nextension to its fine-tuning step to better exploit the context of queries. To\nthis end, we use an additional document-level representation learning objective\nbesides the ranking objective when fine-tuning the BERT re-ranker. Our\nexperiments on two QBD retrieval benchmarks show that the proposed multi-task\noptimization significantly improves the ranking effectiveness without changing\nthe BERT re-ranker or using additional training samples. In future work, the\ngeneralizability of our approach to other retrieval tasks should be further\ninvestigated.", "journal": ""}
{"doi": "10.48550/arXiv.1902.02671", "date": "2019-02-07", "title": "BERT and PALs: Projected Attention Layers for Efficient Adaptation in Multi-Task Learning", "authors": "Asa Cooper Stickland, Iain Murray", "abstract": "Multi-task learning shares information between related tasks, sometimes\nreducing the number of parameters required. State-of-the-art results across\nmultiple natural language understanding tasks in the GLUE benchmark have\npreviously used transfer from a single large task: unsupervised pre-training\nwith BERT, where a separate BERT model was fine-tuned for each task. We explore\nmulti-task approaches that share a single BERT model with a small number of\nadditional task-specific parameters. Using new adaptation modules, PALs or\n`projected attention layers', we match the performance of separately fine-tuned\nmodels on the GLUE benchmark with roughly 7 times fewer parameters, and obtain\nstate-of-the-art results on the Recognizing Textual Entailment dataset.", "journal": ""}
{"doi": "10.48550/arXiv.1902.09243", "date": "2019-02-25", "title": "Pretraining-Based Natural Language Generation for Text Summarization", "authors": "Haoyu Zhang, Jianjun Xu, Ji Wang", "abstract": "In this paper, we propose a novel pretraining-based encoder-decoder\nframework, which can generate the output sequence based on the input sequence\nin a two-stage manner. For the encoder of our model, we encode the input\nsequence into context representations using BERT. For the decoder, there are\ntwo stages in our model, in the first stage, we use a Transformer-based decoder\nto generate a draft output sequence. In the second stage, we mask each word of\nthe draft sequence and feed it to BERT, then by combining the input sequence\nand the draft representation generated by BERT, we use a Transformer-based\ndecoder to predict the refined word for each masked position. To the best of\nour knowledge, our approach is the first method which applies the BERT into\ntext generation tasks. As the first step in this direction, we evaluate our\nproposed method on the text summarization task. Experimental results show that\nour model achieves new state-of-the-art on both CNN/Daily Mail and New York\nTimes datasets.", "journal": "CoNLL'2019"}
{"doi": "10.48550/arXiv.1902.10909", "date": "2019-02-28", "title": "BERT for Joint Intent Classification and Slot Filling", "authors": "Qian Chen, Zhu Zhuo, Wen Wang", "abstract": "Intent classification and slot filling are two essential tasks for natural\nlanguage understanding. They often suffer from small-scale human-labeled\ntraining data, resulting in poor generalization capability, especially for rare\nwords. Recently a new language representation model, BERT (Bidirectional\nEncoder Representations from Transformers), facilitates pre-training deep\nbidirectional representations on large-scale unlabeled corpora, and has created\nstate-of-the-art models for a wide variety of natural language processing tasks\nafter simple fine-tuning. However, there has not been much effort on exploring\nBERT for natural language understanding. In this work, we propose a joint\nintent classification and slot filling model based on BERT. Experimental\nresults demonstrate that our proposed model achieves significant improvement on\nintent classification accuracy, slot filling F1, and sentence-level semantic\nframe accuracy on several public benchmark datasets, compared to the\nattention-based recurrent neural network models and slot-gated models.", "journal": ""}
{"doi": "10.48550/arXiv.1906.08237", "date": "2019-06-19", "title": "XLNet: Generalized Autoregressive Pretraining for Language Understanding", "authors": "Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, Quoc V. Le", "abstract": "With the capability of modeling bidirectional contexts, denoising\nautoencoding based pretraining like BERT achieves better performance than\npretraining approaches based on autoregressive language modeling. However,\nrelying on corrupting the input with masks, BERT neglects dependency between\nthe masked positions and suffers from a pretrain-finetune discrepancy. In light\nof these pros and cons, we propose XLNet, a generalized autoregressive\npretraining method that (1) enables learning bidirectional contexts by\nmaximizing the expected likelihood over all permutations of the factorization\norder and (2) overcomes the limitations of BERT thanks to its autoregressive\nformulation. Furthermore, XLNet integrates ideas from Transformer-XL, the\nstate-of-the-art autoregressive model, into pretraining. Empirically, under\ncomparable experiment settings, XLNet outperforms BERT on 20 tasks, often by a\nlarge margin, including question answering, natural language inference,\nsentiment analysis, and document ranking.", "journal": ""}
{"doi": "10.48550/arXiv.1911.02645", "date": "2019-11-06", "title": "Unsupervised Domain Adaptation of Contextual Embeddings for Low-Resource Duplicate Question Detection", "authors": "Alexandre Rochette, Yadollah Yaghoobzadeh, Timothy J. Hazen", "abstract": "Answering questions is a primary goal of many conversational systems or\nsearch products. While most current systems have focused on answering questions\nagainst structured databases or curated knowledge graphs, on-line community\nforums or frequently asked questions (FAQ) lists offer an alternative source of\ninformation for question answering systems. Automatic duplicate question\ndetection (DQD) is the key technology need for question answering systems to\nutilize existing online forums like StackExchange. Existing annotations of\nduplicate questions in such forums are community-driven, making them sparse or\neven completely missing for many domains. Therefore, it is important to\ntransfer knowledge from related domains and tasks. Recently, contextual\nembedding models such as BERT have been outperforming many baselines by\ntransferring self-supervised information to downstream tasks. In this paper, we\napply BERT to DQD and advance it by unsupervised adaptation to StackExchange\ndomains using self-supervised learning. We show the effectiveness of this\nadaptation for low-resource settings, where little or no training data is\navailable from the target domain. Our analysis reveals that unsupervised BERT\ndomain adaptation on even small amounts of data boosts the performance of BERT.", "journal": ""}
{"doi": "10.48550/arXiv.1911.03912", "date": "2019-11-10", "title": "Effectiveness of self-supervised pre-training for speech recognition", "authors": "Alexei Baevski, Michael Auli, Abdelrahman Mohamed", "abstract": "We compare self-supervised representation learning algorithms which either\nexplicitly quantize the audio data or learn representations without\nquantization. We find the former to be more accurate since it builds a good\nvocabulary of the data through vq-wav2vec [1] to enable learning of effective\nrepresentations in subsequent BERT training. Different to previous work, we\ndirectly fine-tune the pre-trained BERT models on transcribed speech using a\nConnectionist Temporal Classification (CTC) loss instead of feeding the\nrepresentations into a task-specific model. We also propose a BERT-style model\nlearning directly from the continuous audio data and compare pre-training on\nraw audio to spectral features. Fine-tuning a BERT model on 10 hour of labeled\nLibrispeech data with a vq-wav2vec vocabulary is almost as good as the best\nknown reported system trained on 100 hours of labeled data on testclean, while\nachieving a 25% WER reduction on test-other. When using only 10 minutes of\nlabeled data, WER is 25.2 on test-other and 16.3 on test-clean. This\ndemonstrates that self-supervision can enable speech recognition systems\ntrained on a near-zero amount of transcribed data.", "journal": ""}
{"doi": "10.48550/arXiv.1911.03918", "date": "2019-11-10", "title": "Improving BERT Fine-tuning with Embedding Normalization", "authors": "Wenxuan Zhou, Junyi Du, Xiang Ren", "abstract": "Large pre-trained sentence encoders like BERT start a new chapter in natural\nlanguage processing. A common practice to apply pre-trained BERT to sequence\nclassification tasks (e.g., classification of sentences or sentence pairs) is\nby feeding the embedding of [CLS] token (in the last layer) to a task-specific\nclassification layer, and then fine tune the model parameters of BERT and\nclassifier jointly. In this paper, we conduct systematic analysis over several\nsequence classification datasets to examine the embedding values of [CLS] token\nbefore the fine tuning phase, and present the biased embedding distribution\nissue---i.e., embedding values of [CLS] concentrate on a few dimensions and are\nnon-zero centered. Such biased embedding brings challenge to the optimization\nprocess during fine-tuning as gradients of [CLS] embedding may explode and\nresult in degraded model performance. We further propose several simple yet\neffective normalization methods to modify the [CLS] embedding during the\nfine-tuning. Compared with the previous practice, neural classification model\nwith the normalized embedding shows improvements on several text classification\ntasks, demonstrates the effectiveness of our method.", "journal": ""}
{"doi": "10.48550/arXiv.1911.05758", "date": "2019-11-13", "title": "What do you mean, BERT? Assessing BERT as a Distributional Semantics Model", "authors": "Timothee Mickus, Denis Paperno, Mathieu Constant, Kees van Deemter", "abstract": "Contextualized word embeddings, i.e. vector representations for words in\ncontext, are naturally seen as an extension of previous noncontextual\ndistributional semantic models. In this work, we focus on BERT, a deep neural\nnetwork that produces contextualized embeddings and has set the\nstate-of-the-art in several semantic tasks, and study the semantic coherence of\nits embedding space. While showing a tendency towards coherence, BERT does not\nfully live up to the natural expectations for a semantic vector space. In\nparticular, we find that the position of the sentence in which a word occurs,\nwhile having no meaning correlates, leaves a noticeable trace on the word\nembeddings and disturbs similarity relationships.", "journal": "Proceedings of the Society for Computation in Linguistics: Vol. 3\n  (2020), Article 34"}
{"doi": "10.48550/arXiv.1911.06241", "date": "2019-11-03", "title": "BERT-CNN: a Hierarchical Patent Classifier Based on a Pre-Trained Language Model", "authors": "Xiaolei Lu, Bin Ni", "abstract": "The automatic classification is a process of automatically assigning text\ndocuments to predefined categories. An accurate automatic patent classifier is\ncrucial to patent inventors and patent examiners in terms of intellectual\nproperty protection, patent management, and patent information retrieval. We\npresent BERT-CNN, a hierarchical patent classifier based on pre-trained\nlanguage model by training the national patent application documents collected\nfrom the State Information Center, China. The experimental results show that\nBERT-CNN achieves 84.3% accuracy, which is far better than the two compared\nbaseline methods, Convolutional Neural Networks and Recurrent Neural Networks.\nWe didn't apply our model to the third and fourth hierarchical level of the\nInternational Patent Classification - \"subclass\" and \"group\".The visualization\nof the Attention Mechanism shows that BERT-CNN obtains new state-of-the-art\nresults in representing vocabularies and semantics. This article demonstrates\nthe practicality and effectiveness of BERT-CNN in the field of automatic patent\nclassification.", "journal": ""}
{"doi": "10.48550/arXiv.1911.12246", "date": "2019-11-27", "title": "Do Attention Heads in BERT Track Syntactic Dependencies?", "authors": "Phu Mon Htut, Jason Phang, Shikha Bordia, Samuel R. Bowman", "abstract": "We investigate the extent to which individual attention heads in pretrained\ntransformer language models, such as BERT and RoBERTa, implicitly capture\nsyntactic dependency relations. We employ two methods---taking the maximum\nattention weight and computing the maximum spanning tree---to extract implicit\ndependency relations from the attention weights of each layer/head, and compare\nthem to the ground-truth Universal Dependency (UD) trees. We show that, for\nsome UD relation types, there exist heads that can recover the dependency type\nsignificantly better than baselines on parsed English text, suggesting that\nsome self-attention heads act as a proxy for syntactic structure. We also\nanalyze BERT fine-tuned on two datasets---the syntax-oriented CoLA and the\nsemantics-oriented MNLI---to investigate whether fine-tuning affects the\npatterns of their self-attention, but we do not observe substantial differences\nin the overall dependency relations extracted using our methods. Our results\nsuggest that these models have some specialist attention heads that track\nindividual dependency types, but no generalist head that performs holistic\nparsing significantly better than a trivial baseline, and that analyzing\nattention weights directly may not reveal much of the syntactic knowledge that\nBERT-style models are known to learn.", "journal": ""}
{"doi": "10.48550/arXiv.2004.05808", "date": "2020-04-13", "title": "Unified Multi-Criteria Chinese Word Segmentation with BERT", "authors": "Zhen Ke, Liang Shi, Erli Meng, Bin Wang, Xipeng Qiu, Xuanjing Huang", "abstract": "Multi-Criteria Chinese Word Segmentation (MCCWS) aims at finding word\nboundaries in a Chinese sentence composed of continuous characters while\nmultiple segmentation criteria exist. The unified framework has been widely\nused in MCCWS and shows its effectiveness. Besides, the pre-trained BERT\nlanguage model has been also introduced into the MCCWS task in a multi-task\nlearning framework. In this paper, we combine the superiority of the unified\nframework and pretrained language model, and propose a unified MCCWS model\nbased on BERT. Moreover, we augment the unified BERT-based MCCWS model with the\nbigram features and an auxiliary criterion classification task. Experiments on\neight datasets with diverse criteria demonstrate that our methods could achieve\nnew state-of-the-art results for MCCWS.", "journal": ""}
{"doi": "10.48550/arXiv.2004.06499", "date": "2020-04-14", "title": "What's so special about BERT's layers? A closer look at the NLP pipeline in monolingual and multilingual models", "authors": "Wietse de Vries, Andreas van Cranenburgh, Malvina Nissim", "abstract": "Peeking into the inner workings of BERT has shown that its layers resemble\nthe classical NLP pipeline, with progressively more complex tasks being\nconcentrated in later layers. To investigate to what extent these results also\nhold for a language other than English, we probe a Dutch BERT-based model and\nthe multilingual BERT model for Dutch NLP tasks. In addition, through a deeper\nanalysis of part-of-speech tagging, we show that also within a given task,\ninformation is spread over different parts of the network and the pipeline\nmight not be as neat as it seems. Each layer has different specialisations, so\nthat it may be more useful to combine information from different layers,\ninstead of selecting a single one based on the best overall performance.", "journal": "Findings of the Association for Computational Linguistics: EMNLP\n  2020"}
{"doi": "10.48550/arXiv.2004.06753", "date": "2020-04-14", "title": "A Simple Yet Strong Pipeline for HotpotQA", "authors": "Dirk Groeneveld, Tushar Khot, Mausam, Ashish Sabharwal", "abstract": "State-of-the-art models for multi-hop question answering typically augment\nlarge-scale language models like BERT with additional, intuitively useful\ncapabilities such as named entity recognition, graph-based reasoning, and\nquestion decomposition. However, does their strong performance on popular\nmulti-hop datasets really justify this added design complexity? Our results\nsuggest that the answer may be no, because even our simple pipeline based on\nBERT, named Quark, performs surprisingly well. Specifically, on HotpotQA, Quark\noutperforms these models on both question answering and support identification\n(and achieves performance very close to a RoBERTa model). Our pipeline has\nthree steps: 1) use BERT to identify potentially relevant sentences\nindependently of each other; 2) feed the set of selected sentences as context\ninto a standard BERT span prediction model to choose an answer; and 3) use the\nsentence selection model, now with the chosen answer, to produce supporting\nsentences. The strong performance of Quark resurfaces the importance of\ncarefully exploring simple model designs before using popular benchmarks to\njustify the value of complex techniques.", "journal": ""}
{"doi": "10.48550/arXiv.2004.06871", "date": "2020-04-15", "title": "TOD-BERT: Pre-trained Natural Language Understanding for Task-Oriented Dialogue", "authors": "Chien-Sheng Wu, Steven Hoi, Richard Socher, Caiming Xiong", "abstract": "The underlying difference of linguistic patterns between general text and\ntask-oriented dialogue makes existing pre-trained language models less useful\nin practice. In this work, we unify nine human-human and multi-turn\ntask-oriented dialogue datasets for language modeling. To better model dialogue\nbehavior during pre-training, we incorporate user and system tokens into the\nmasked language modeling. We propose a contrastive objective function to\nsimulate the response selection task. Our pre-trained task-oriented dialogue\nBERT (TOD-BERT) outperforms strong baselines like BERT on four downstream\ntask-oriented dialogue applications, including intention recognition, dialogue\nstate tracking, dialogue act prediction, and response selection. We also show\nthat TOD-BERT has a stronger few-shot ability that can mitigate the data\nscarcity problem for task-oriented dialogue.", "journal": ""}
{"doi": "10.48550/arXiv.2004.12993", "date": "2020-04-27", "title": "DeeBERT: Dynamic Early Exiting for Accelerating BERT Inference", "authors": "Ji Xin, Raphael Tang, Jaejun Lee, Yaoliang Yu, Jimmy Lin", "abstract": "Large-scale pre-trained language models such as BERT have brought significant\nimprovements to NLP applications. However, they are also notorious for being\nslow in inference, which makes them difficult to deploy in real-time\napplications. We propose a simple but effective method, DeeBERT, to accelerate\nBERT inference. Our approach allows samples to exit earlier without passing\nthrough the entire model. Experiments show that DeeBERT is able to save up to\n~40% inference time with minimal degradation in model quality. Further analyses\nshow different behaviors in the BERT transformer layers and also reveal their\nredundancy. Our work provides new ideas to efficiently apply deep\ntransformer-based models to downstream tasks. Code is available at\nhttps://github.com/castorini/DeeBERT.", "journal": ""}
{"doi": "10.48550/arXiv.2004.14786", "date": "2020-04-30", "title": "Perturbed Masking: Parameter-free Probing for Analyzing and Interpreting BERT", "authors": "Zhiyong Wu, Yun Chen, Ben Kao, Qun Liu", "abstract": "By introducing a small set of additional parameters, a probe learns to solve\nspecific linguistic tasks (e.g., dependency parsing) in a supervised manner\nusing feature representations (e.g., contextualized embeddings). The\neffectiveness of such probing tasks is taken as evidence that the pre-trained\nmodel encodes linguistic knowledge. However, this approach of evaluating a\nlanguage model is undermined by the uncertainty of the amount of knowledge that\nis learned by the probe itself. Complementary to those works, we propose a\nparameter-free probing technique for analyzing pre-trained language models\n(e.g., BERT). Our method does not require direct supervision from the probing\ntasks, nor do we introduce additional parameters to the probing process. Our\nexperiments on BERT show that syntactic trees recovered from BERT using our\nmethod are significantly better than linguistically-uninformed baselines. We\nfurther feed the empirically induced dependency structures into a downstream\nsentiment classification task and find its improvement compatible with or even\nsuperior to a human-designed dependency schema.", "journal": ""}
{"doi": "10.48550/arXiv.2004.14848", "date": "2020-04-30", "title": "Enriched Pre-trained Transformers for Joint Slot Filling and Intent Detection", "authors": "Momchil Hardalov, Ivan Koychev, Preslav Nakov", "abstract": "Detecting the user's intent and finding the corresponding slots among the\nutterance's words are important tasks in natural language understanding. Their\ninterconnected nature makes their joint modeling a standard part of training\nsuch models. Moreover, data scarceness and specialized vocabularies pose\nadditional challenges. Recently, the advances in pre-trained language models,\nnamely contextualized models such as ELMo and BERT have revolutionized the\nfield by tapping the potential of training very large models with just a few\nsteps of fine-tuning on a task-specific dataset. Here, we leverage such models,\nnamely BERT and RoBERTa, and we design a novel architecture on top of them.\nMoreover, we propose an intent pooling attention mechanism, and we reinforce\nthe slot filling task by fusing intent distributions, word features, and token\nrepresentations. The experimental results on standard datasets show that our\nmodel outperforms both the current non-BERT state of the art as well as some\nstronger BERT-based baselines.", "journal": ""}
{"doi": "10.48550/arXiv.2005.03695", "date": "2020-05-07", "title": "LIIR at SemEval-2020 Task 12: A Cross-Lingual Augmentation Approach for Multilingual Offensive Language Identification", "authors": "Erfan Ghadery, Marie-Francine Moens", "abstract": "This paper presents our system entitled `LIIR' for SemEval-2020 Task 12 on\nMultilingual Offensive Language Identification in Social Media (OffensEval 2).\nWe have participated in sub-task A for English, Danish, Greek, Arabic, and\nTurkish languages. We adapt and fine-tune the BERT and Multilingual Bert models\nmade available by Google AI for English and non-English languages respectively.\nFor the English language, we use a combination of two fine-tuned BERT models.\nFor other languages we propose a cross-lingual augmentation approach in order\nto enrich training data and we use Multilingual BERT to obtain sentence\nrepresentations. LIIR achieved rank 14/38, 18/47, 24/86, 24/54, and 25/40 in\nGreek, Turkish, English, Arabic, and Danish languages, respectively.", "journal": ""}
{"doi": "10.48550/arXiv.2005.09093", "date": "2020-05-18", "title": "Are All Languages Created Equal in Multilingual BERT?", "authors": "Shijie Wu, Mark Dredze", "abstract": "Multilingual BERT (mBERT) trained on 104 languages has shown surprisingly\ngood cross-lingual performance on several NLP tasks, even without explicit\ncross-lingual signals. However, these evaluations have focused on cross-lingual\ntransfer with high-resource languages, covering only a third of the languages\ncovered by mBERT. We explore how mBERT performs on a much wider set of\nlanguages, focusing on the quality of representation for low-resource\nlanguages, measured by within-language performance. We consider three tasks:\nNamed Entity Recognition (99 languages), Part-of-speech Tagging, and Dependency\nParsing (54 languages each). mBERT does better than or comparable to baselines\non high resource languages but does much worse for low resource languages.\nFurthermore, monolingual BERT models for these languages do even worse. Paired\nwith similar languages, the performance gap between monolingual BERT and mBERT\ncan be narrowed. We find that better models for low resource languages require\nmore efficient pretraining techniques or more data.", "journal": ""}
{"doi": "10.48550/arXiv.2005.11640", "date": "2020-05-24", "title": "Jointly Encoding Word Confusion Network and Dialogue Context with BERT for Spoken Language Understanding", "authors": "Chen Liu, Su Zhu, Zijian Zhao, Ruisheng Cao, Lu Chen, Kai Yu", "abstract": "Spoken Language Understanding (SLU) converts hypotheses from automatic speech\nrecognizer (ASR) into structured semantic representations. ASR recognition\nerrors can severely degenerate the performance of the subsequent SLU module. To\naddress this issue, word confusion networks (WCNs) have been used to encode the\ninput for SLU, which contain richer information than 1-best or n-best\nhypotheses list. To further eliminate ambiguity, the last system act of\ndialogue context is also utilized as additional input. In this paper, a novel\nBERT based SLU model (WCN-BERT SLU) is proposed to encode WCNs and the dialogue\ncontext jointly. It can integrate both structural information and ASR posterior\nprobabilities of WCNs in the BERT architecture. Experiments on DSTC2, a\nbenchmark of SLU, show that the proposed method is effective and can outperform\nprevious state-of-the-art models significantly.", "journal": ""}
{"doi": "10.48550/arXiv.2007.13184", "date": "2020-07-26", "title": "KUISAIL at SemEval-2020 Task 12: BERT-CNN for Offensive Speech Identification in Social Media", "authors": "Ali Safaya, Moutasem Abdullatif, Deniz Yuret", "abstract": "In this paper, we describe our approach to utilize pre-trained BERT models\nwith Convolutional Neural Networks for sub-task A of the Multilingual Offensive\nLanguage Identification shared task (OffensEval 2020), which is a part of the\nSemEval 2020. We show that combining CNN with BERT is better than using BERT on\nits own, and we emphasize the importance of utilizing pre-trained language\nmodels for downstream tasks. Our system, ranked 4th with macro averaged\nF1-Score of 0.897 in Arabic, 4th with score of 0.843 in Greek, and 3rd with\nscore of 0.814 in Turkish. Additionally, we present ArabicBERT, a set of\npre-trained transformer language models for Arabic that we share with the\ncommunity.", "journal": ""}
{"doi": "10.48550/arXiv.2008.01551", "date": "2020-07-26", "title": "To BERT or Not To BERT: Comparing Speech and Language-based Approaches for Alzheimer's Disease Detection", "authors": "Aparna Balagopalan, Benjamin Eyre, Frank Rudzicz, Jekaterina Novikova", "abstract": "Research related to automatically detecting Alzheimer's disease (AD) is\nimportant, given the high prevalence of AD and the high cost of traditional\nmethods. Since AD significantly affects the content and acoustics of\nspontaneous speech, natural language processing and machine learning provide\npromising techniques for reliably detecting AD. We compare and contrast the\nperformance of two such approaches for AD detection on the recent ADReSS\nchallenge dataset: 1) using domain knowledge-based hand-crafted features that\ncapture linguistic and acoustic phenomena, and 2) fine-tuning Bidirectional\nEncoder Representations from Transformer (BERT)-based sequence classification\nmodels. We also compare multiple feature-based regression models for a\nneuropsychological score task in the challenge. We observe that fine-tuned BERT\nmodels, given the relative importance of linguistics in cognitive impairment\ndetection, outperform feature-based approaches on the AD detection task.", "journal": ""}
{"doi": "10.48550/arXiv.2008.03979", "date": "2020-08-10", "title": "KR-BERT: A Small-Scale Korean-Specific Language Model", "authors": "Sangah Lee, Hansol Jang, Yunmee Baik, Suzi Park, Hyopil Shin", "abstract": "Since the appearance of BERT, recent works including XLNet and RoBERTa\nutilize sentence embedding models pre-trained by large corpora and a large\nnumber of parameters. Because such models have large hardware and a huge amount\nof data, they take a long time to pre-train. Therefore it is important to\nattempt to make smaller models that perform comparatively. In this paper, we\ntrained a Korean-specific model KR-BERT, utilizing a smaller vocabulary and\ndataset. Since Korean is one of the morphologically rich languages with poor\nresources using non-Latin alphabets, it is also important to capture\nlanguage-specific linguistic phenomena that the Multilingual BERT model missed.\nWe tested several tokenizers including our BidirectionalWordPiece Tokenizer and\nadjusted the minimal span of tokens for tokenization ranging from sub-character\nlevel to character-level to construct a better vocabulary for our model. With\nthose adjustments, our KR-BERT model performed comparably and even better than\nother existing pre-trained models using a corpus about 1/10 of the size.", "journal": ""}
{"doi": "10.48550/arXiv.2008.06682", "date": "2020-08-15", "title": "Jointly Fine-Tuning \"BERT-like\" Self Supervised Models to Improve Multimodal Speech Emotion Recognition", "authors": "Shamane Siriwardhana, Andrew Reis, Rivindu Weerasekera, Suranga Nanayakkara", "abstract": "Multimodal emotion recognition from speech is an important area in affective\ncomputing. Fusing multiple data modalities and learning representations with\nlimited amounts of labeled data is a challenging task. In this paper, we\nexplore the use of modality-specific \"BERT-like\" pretrained Self Supervised\nLearning (SSL) architectures to represent both speech and text modalities for\nthe task of multimodal speech emotion recognition. By conducting experiments on\nthree publicly available datasets (IEMOCAP, CMU-MOSEI, and CMU-MOSI), we show\nthat jointly fine-tuning \"BERT-like\" SSL architectures achieve state-of-the-art\n(SOTA) results. We also evaluate two methods of fusing speech and text\nmodalities and show that a simple fusion mechanism can outperform more complex\nones when using SSL models that have similar architectural properties to BERT.", "journal": ""}
{"doi": "10.48550/arXiv.2008.08547", "date": "2020-08-19", "title": "UoB at SemEval-2020 Task 12: Boosting BERT with Corpus Level Information", "authors": "Wah Meng Lim, Harish Tayyar Madabushi", "abstract": "Pre-trained language model word representation, such as BERT, have been\nextremely successful in several Natural Language Processing tasks significantly\nimproving on the state-of-the-art. This can largely be attributed to their\nability to better capture semantic information contained within a sentence.\nSeveral tasks, however, can benefit from information available at a corpus\nlevel, such as Term Frequency-Inverse Document Frequency (TF-IDF). In this work\nwe test the effectiveness of integrating this information with BERT on the task\nof identifying abuse on social media and show that integrating this information\nwith BERT does indeed significantly improve performance. We participate in\nSub-Task A (abuse detection) wherein we achieve a score within two points of\nthe top performing team and in Sub-Task B (target detection) wherein we are\nranked 4 of the 44 participating teams.", "journal": ""}
{"doi": "10.48550/arXiv.2008.10813", "date": "2020-08-25", "title": "Conceptualized Representation Learning for Chinese Biomedical Text Mining", "authors": "Ningyu Zhang, Qianghuai Jia, Kangping Yin, Liang Dong, Feng Gao, Nengwei Hua", "abstract": "Biomedical text mining is becoming increasingly important as the number of\nbiomedical documents and web data rapidly grows. Recently, word representation\nmodels such as BERT has gained popularity among researchers. However, it is\ndifficult to estimate their performance on datasets containing biomedical texts\nas the word distributions of general and biomedical corpora are quite\ndifferent. Moreover, the medical domain has long-tail concepts and\nterminologies that are difficult to be learned via language models. For the\nChinese biomedical text, it is more difficult due to its complex structure and\nthe variety of phrase combinations. In this paper, we investigate how the\nrecently introduced pre-trained language model BERT can be adapted for Chinese\nbiomedical corpora and propose a novel conceptualized representation learning\napproach. We also release a new Chinese Biomedical Language Understanding\nEvaluation benchmark (\\textbf{ChineseBLUE}). We examine the effectiveness of\nChinese pre-trained models: BERT, BERT-wwm, RoBERTa, and our approach.\nExperimental results on the benchmark show that our approach could bring\nsignificant gain. We release the pre-trained model on GitHub:\nhttps://github.com/alibaba-research/ChineseBLUE.", "journal": ""}
{"doi": "10.48550/arXiv.2009.05451", "date": "2020-09-11", "title": "A Comparison of LSTM and BERT for Small Corpus", "authors": "Aysu Ezen-Can", "abstract": "Recent advancements in the NLP field showed that transfer learning helps with\nachieving state-of-the-art results for new tasks by tuning pre-trained models\ninstead of starting from scratch. Transformers have made a significant\nimprovement in creating new state-of-the-art results for many NLP tasks\nincluding but not limited to text classification, text generation, and sequence\nlabeling. Most of these success stories were based on large datasets. In this\npaper we focus on a real-life scenario that scientists in academia and industry\nface frequently: given a small dataset, can we use a large pre-trained model\nlike BERT and get better results than simple models? To answer this question,\nwe use a small dataset for intent classification collected for building\nchatbots and compare the performance of a simple bidirectional LSTM model with\na pre-trained BERT model. Our experimental results show that bidirectional LSTM\nmodels can achieve significantly higher results than a BERT model for a small\ndataset and these simple models get trained in much less time than tuning the\npre-trained counterparts. We conclude that the performance of a model is\ndependent on the task and the data, and therefore before making a model choice,\nthese factors should be taken into consideration instead of directly choosing\nthe most popular model.", "journal": ""}
{"doi": "10.48550/arXiv.2009.07258", "date": "2020-09-15", "title": "BERT-QE: Contextualized Query Expansion for Document Re-ranking", "authors": "Zhi Zheng, Kai Hui, Ben He, Xianpei Han, Le Sun, Andrew Yates", "abstract": "Query expansion aims to mitigate the mismatch between the language used in a\nquery and in a document. However, query expansion methods can suffer from\nintroducing non-relevant information when expanding the query. To bridge this\ngap, inspired by recent advances in applying contextualized models like BERT to\nthe document retrieval task, this paper proposes a novel query expansion model\nthat leverages the strength of the BERT model to select relevant document\nchunks for expansion. In evaluation on the standard TREC Robust04 and GOV2 test\ncollections, the proposed BERT-QE model significantly outperforms BERT-Large\nmodels.", "journal": ""}
{"doi": "10.48550/arXiv.2009.10053", "date": "2020-09-21", "title": "Latin BERT: A Contextual Language Model for Classical Philology", "authors": "David Bamman, Patrick J. Burns", "abstract": "We present Latin BERT, a contextual language model for the Latin language,\ntrained on 642.7 million words from a variety of sources spanning the Classical\nera to the 21st century. In a series of case studies, we illustrate the\naffordances of this language-specific model both for work in natural language\nprocessing for Latin and in using computational methods for traditional\nscholarship: we show that Latin BERT achieves a new state of the art for\npart-of-speech tagging on all three Universal Dependency datasets for Latin and\ncan be used for predicting missing text (including critical emendations); we\ncreate a new dataset for assessing word sense disambiguation for Latin and\ndemonstrate that Latin BERT outperforms static word embeddings; and we show\nthat it can be used for semantically-informed search by querying contextual\nnearest neighbors. We publicly release trained models to help drive future work\nin this space.", "journal": ""}
{"doi": "10.48550/arXiv.2009.10680", "date": "2020-09-22", "title": "AutoRC: Improving BERT Based Relation Classification Models via Architecture Search", "authors": "Wei Zhu, Xipeng Qiu, Yuan Ni, Guotong Xie", "abstract": "Although BERT based relation classification (RC) models have achieved\nsignificant improvements over the traditional deep learning models, it seems\nthat no consensus can be reached on what is the optimal architecture. Firstly,\nthere are multiple alternatives for entity span identification. Second, there\nare a collection of pooling operations to aggregate the representations of\nentities and contexts into fixed length vectors. Third, it is difficult to\nmanually decide which feature vectors, including their interactions, are\nbeneficial for classifying the relation types. In this work, we design a\ncomprehensive search space for BERT based RC models and employ neural\narchitecture search (NAS) method to automatically discover the design choices\nmentioned above. Experiments on seven benchmark RC tasks show that our method\nis efficient and effective in finding better architectures than the baseline\nBERT based RC model. Ablation study demonstrates the necessity of our search\nspace design and the effectiveness of our search method.", "journal": ""}
{"doi": "10.48550/arXiv.2010.03010", "date": "2020-10-06", "title": "Exploring BERT's Sensitivity to Lexical Cues using Tests from Semantic Priming", "authors": "Kanishka Misra, Allyson Ettinger, Julia Taylor Rayz", "abstract": "Models trained to estimate word probabilities in context have become\nubiquitous in natural language processing. How do these models use lexical cues\nin context to inform their word probabilities? To answer this question, we\npresent a case study analyzing the pre-trained BERT model with tests informed\nby semantic priming. Using English lexical stimuli that show priming in humans,\nwe find that BERT too shows \"priming,\" predicting a word with greater\nprobability when the context includes a related word versus an unrelated one.\nThis effect decreases as the amount of information provided by the context\nincreases. Follow-up analysis shows BERT to be increasingly distracted by\nrelated prime words as context becomes more informative, assigning lower\nprobabilities to related words. Our findings highlight the importance of\nconsidering contextual constraint effects when studying word prediction in\nthese models, and highlight possible parallels with human processing.", "journal": ""}
{"doi": "10.48550/arXiv.2010.06065", "date": "2020-10-12", "title": "Zero-shot Entity Linking with Efficient Long Range Sequence Modeling", "authors": "Zonghai Yao, Liangliang Cao, Huapu Pan", "abstract": "This paper considers the problem of zero-shot entity linking, in which a link\nin the test time may not present in training. Following the prevailing\nBERT-based research efforts, we find a simple yet effective way is to expand\nthe long-range sequence modeling. Unlike many previous methods, our method does\nnot require expensive pre-training of BERT with long position embedding.\nInstead, we propose an efficient position embeddings initialization method\ncalled Embedding-repeat, which initializes larger position embeddings based on\nBERT-Base. On Wikia's zero-shot EL dataset, our method improves the SOTA from\n76.06% to 79.08%, and for its long data, the corresponding improvement is from\n74.57% to 82.14%. Our experiments suggest the effectiveness of long-range\nsequence modeling without retraining the BERT model.", "journal": ""}
{"doi": "10.48550/arXiv.2010.06993", "date": "2020-10-14", "title": "Weight Squeezing: Reparameterization for Knowledge Transfer and Model Compression", "authors": "Artem Chumachenko, Daniil Gavrilov, Nikita Balagansky, Pavel Kalaidin", "abstract": "In this work, we present a novel approach for simultaneous knowledge transfer\nand model compression called Weight Squeezing. With this method, we perform\nknowledge transfer from a teacher model by learning the mapping from its\nweights to smaller student model weights.\n  We applied Weight Squeezing to a pre-trained text classification model based\non BERT-Medium model and compared our method to various other knowledge\ntransfer and model compression methods on GLUE multitask benchmark. We observed\nthat our approach produces better results while being significantly faster than\nother methods for training student models.\n  We also proposed a variant of Weight Squeezing called Gated Weight Squeezing,\nfor which we combined fine-tuning of BERT-Medium model and learning mapping\nfrom BERT-Base weights. We showed that fine-tuning with Gated Weight Squeezing\noutperforms plain fine-tuning of BERT-Medium model as well as other concurrent\nSoTA approaches while much being easier to implement.", "journal": ""}
{"doi": "10.48550/arXiv.2010.11731", "date": "2020-10-22", "title": "Improving BERT Performance for Aspect-Based Sentiment Analysis", "authors": "Akbar Karimi, Leonardo Rossi, Andrea Prati", "abstract": "Aspect-Based Sentiment Analysis (ABSA) studies the consumer opinion on the\nmarket products. It involves examining the type of sentiments as well as\nsentiment targets expressed in product reviews. Analyzing the language used in\na review is a difficult task that requires a deep understanding of the\nlanguage. In recent years, deep language models, such as BERT\n\\cite{devlin2019bert}, have shown great progress in this regard. In this work,\nwe propose two simple modules called Parallel Aggregation and Hierarchical\nAggregation to be utilized on top of BERT for two main ABSA tasks namely Aspect\nExtraction (AE) and Aspect Sentiment Classification (ASC) in order to improve\nthe model's performance. We show that applying the proposed models eliminates\nthe need for further training of the BERT model. The source code is available\non the Web for further research and reproduction of the results.", "journal": ""}
{"doi": "10.48550/arXiv.2010.12532", "date": "2020-10-23", "title": "GiBERT: Introducing Linguistic Knowledge into BERT through a Lightweight Gated Injection Method", "authors": "Nicole Peinelt, Marek Rei, Maria Liakata", "abstract": "Large pre-trained language models such as BERT have been the driving force\nbehind recent improvements across many NLP tasks. However, BERT is only trained\nto predict missing words - either behind masks or in the next sentence - and\nhas no knowledge of lexical, syntactic or semantic information beyond what it\npicks up through unsupervised pre-training. We propose a novel method to\nexplicitly inject linguistic knowledge in the form of word embeddings into any\nlayer of a pre-trained BERT. Our performance improvements on multiple semantic\nsimilarity datasets when injecting dependency-based and counter-fitted\nembeddings indicate that such information is beneficial and currently missing\nfrom the original model. Our qualitative analysis shows that counter-fitted\nembedding injection particularly helps with cases involving synonym pairs.", "journal": ""}
{"doi": "10.48550/arXiv.2010.14061", "date": "2020-10-24", "title": "Jointly Optimizing State Operation Prediction and Value Generation for Dialogue State Tracking", "authors": "Yan Zeng, Jian-Yun Nie", "abstract": "We investigate the problem of multi-domain Dialogue State Tracking (DST) with\nopen vocabulary. Existing approaches exploit BERT encoder and copy-based RNN\ndecoder, where the encoder predicts the state operation, and the decoder\ngenerates new slot values. However, in such a stacked encoder-decoder\nstructure, the operation prediction objective only affects the BERT encoder and\nthe value generation objective mainly affects the RNN decoder. In this paper,\nwe propose a purely Transformer-based framework, where a single BERT works as\nboth the encoder and the decoder. In so doing, the operation prediction\nobjective and the value generation objective can jointly optimize this BERT for\nDST. At the decoding step, we re-use the hidden states of the encoder in the\nself-attention mechanism of the corresponding decoder layers to construct a\nflat encoder-decoder architecture for effective parameter updating.\nExperimental results show that our approach substantially outperforms the\nexisting state-of-the-art framework, and it also achieves very competitive\nperformance to the best ontology-based approaches.", "journal": ""}
{"doi": "10.48550/arXiv.2011.00740", "date": "2020-11-02", "title": "Influence Patterns for Explaining Information Flow in BERT", "authors": "Kaiji Lu, Zifan Wang, Piotr Mardziel, Anupam Datta", "abstract": "While attention is all you need may be proving true, we do not know why:\nattention-based transformer models such as BERT are superior but how\ninformation flows from input tokens to output predictions are unclear. We\nintroduce influence patterns, abstractions of sets of paths through a\ntransformer model. Patterns quantify and localize the flow of information to\npaths passing through a sequence of model nodes. Experimentally, we find that\nsignificant portion of information flow in BERT goes through skip connections\ninstead of attention heads. We further show that consistency of patterns across\ninstances is an indicator of BERT's performance. Finally, We demonstrate that\npatterns account for far more model performance than previous attention-based\nand layer-based methods.", "journal": ""}
{"doi": "10.48550/arXiv.2011.04451", "date": "2020-10-17", "title": "Hierarchical Multitask Learning Approach for BERT", "authors": "\u00c7a\u011fla Aksoy, Alper Ahmeto\u011flu, Tunga G\u00fcng\u00f6r", "abstract": "Recent works show that learning contextualized embeddings for words is\nbeneficial for downstream tasks. BERT is one successful example of this\napproach. It learns embeddings by solving two tasks, which are masked language\nmodel (masked LM) and the next sentence prediction (NSP). The pre-training of\nBERT can also be framed as a multitask learning problem. In this work, we adopt\nhierarchical multitask learning approaches for BERT pre-training. Pre-training\ntasks are solved at different layers instead of the last layer, and information\nfrom the NSP task is transferred to the masked LM task. Also, we propose a new\npre-training task bigram shift to encode word order information. We choose two\ndownstream tasks, one of which requires sentence-level embeddings (textual\nentailment), and the other requires contextualized embeddings of words\n(question answering). Due to computational restrictions, we use the downstream\ntask data instead of a large dataset for the pre-training to see the\nperformance of proposed models when given a restricted dataset. We test their\nperformance on several probing tasks to analyze learned embeddings. Our results\nshow that imposing a task hierarchy in pre-training improves the performance of\nembeddings.", "journal": ""}
{"doi": "10.48550/arXiv.2011.11673", "date": "2020-11-23", "title": "Does BERT Understand Sentiment? Leveraging Comparisons Between Contextual and Non-Contextual Embeddings to Improve Aspect-Based Sentiment Models", "authors": "Natesh Reddy, Pranaydeep Singh, Muktabh Mayank Srivastava", "abstract": "When performing Polarity Detection for different words in a sentence, we need\nto look at the words around to understand the sentiment. Massively pretrained\nlanguage models like BERT can encode not only just the words in a document but\nalso the context around the words along with them. This begs the questions,\n\"Does a pretrain language model also automatically encode sentiment information\nabout each word?\" and \"Can it be used to infer polarity towards different\naspects?\". In this work we try to answer this question by showing that training\na comparison of a contextual embedding from BERT and a generic word embedding\ncan be used to infer sentiment. We also show that if we finetune a subset of\nweights the model built on comparison of BERT and generic word embedding, it\ncan get state of the art results for Polarity Detection in Aspect Based\nSentiment Classification datasets.", "journal": ""}
{"doi": "10.48550/arXiv.2012.07335", "date": "2020-12-14", "title": "LRC-BERT: Latent-representation Contrastive Knowledge Distillation for Natural Language Understanding", "authors": "Hao Fu, Shaojun Zhou, Qihong Yang, Junjie Tang, Guiquan Liu, Kaikui Liu, Xiaolong Li", "abstract": "The pre-training models such as BERT have achieved great results in various\nnatural language processing problems. However, a large number of parameters\nneed significant amounts of memory and the consumption of inference time, which\nmakes it difficult to deploy them on edge devices. In this work, we propose a\nknowledge distillation method LRC-BERT based on contrastive learning to fit the\noutput of the intermediate layer from the angular distance aspect, which is not\nconsidered by the existing distillation methods. Furthermore, we introduce a\ngradient perturbation-based training architecture in the training phase to\nincrease the robustness of LRC-BERT, which is the first attempt in knowledge\ndistillation. Additionally, in order to better capture the distribution\ncharacteristics of the intermediate layer, we design a two-stage training\nmethod for the total distillation loss. Finally, by verifying 8 datasets on the\nGeneral Language Understanding Evaluation (GLUE) benchmark, the performance of\nthe proposed LRC-BERT exceeds the existing state-of-the-art methods, which\nproves the effectiveness of our method.", "journal": ""}
{"doi": "10.48550/arXiv.2012.07587", "date": "2020-12-07", "title": "Detecting Insincere Questions from Text: A Transfer Learning Approach", "authors": "Ashwin Rachha, Gaurav Vanmane", "abstract": "The internet today has become an unrivalled source of information where\npeople converse on content based websites such as Quora, Reddit, StackOverflow\nand Twitter asking doubts and sharing knowledge with the world. A major arising\nproblem with such websites is the proliferation of toxic comments or instances\nof insincerity wherein the users instead of maintaining a sincere motive\nindulge in spreading toxic and divisive content. The straightforward course of\naction in confronting this situation is detecting such content beforehand and\npreventing it from subsisting online. In recent times Transfer Learning in\nNatural Language Processing has seen an unprecedented growth. Today with the\nexistence of transformers and various state of the art innovations, a\ntremendous growth has been made in various NLP domains. The introduction of\nBERT has caused quite a stir in the NLP community. As mentioned, when\npublished, BERT dominated performance benchmarks and thereby inspired many\nother authors to experiment with it and publish similar models. This led to the\ndevelopment of a whole BERT-family, each member being specialized on a\ndifferent task. In this paper we solve the Insincere Questions Classification\nproblem by fine tuning four cutting age models viz BERT, RoBERTa, DistilBERT\nand ALBERT.", "journal": ""}
{"doi": "10.48550/arXiv.2012.11145", "date": "2020-12-21", "title": "Domain specific BERT representation for Named Entity Recognition of lab protocol", "authors": "Tejas Vaidhya, Ayush Kaushal", "abstract": "Supervised models trained to predict properties from representations have\nbeen achieving high accuracy on a variety of tasks. For instance, the BERT\nfamily seems to work exceptionally well on the downstream task from NER tagging\nto the range of other linguistic tasks. But the vocabulary used in the medical\nfield contains a lot of different tokens used only in the medical industry such\nas the name of different diseases, devices, organisms, medicines, etc. that\nmakes it difficult for traditional BERT model to create contextualized\nembedding. In this paper, we are going to illustrate the System for Named\nEntity Tagging based on Bio-Bert. Experimental results show that our model\ngives substantial improvements over the baseline and stood the fourth runner up\nin terms of F1 score, and first runner up in terms of Recall with just 2.21 F1\nscore behind the best one.", "journal": ""}
{"doi": "10.48550/arXiv.2012.11881", "date": "2020-12-22", "title": "Undivided Attention: Are Intermediate Layers Necessary for BERT?", "authors": "Sharath Nittur Sridhar, Anthony Sarah", "abstract": "In recent times, BERT-based models have been extremely successful in solving\na variety of natural language processing (NLP) tasks such as reading\ncomprehension, natural language inference, sentiment analysis, etc. All\nBERT-based architectures have a self-attention block followed by a block of\nintermediate layers as the basic building component. However, a strong\njustification for the inclusion of these intermediate layers remains missing in\nthe literature. In this work we investigate the importance of intermediate\nlayers on the overall network performance of downstream tasks. We show that\nreducing the number of intermediate layers and modifying the architecture for\nBERT-BASE results in minimal loss in fine-tuning accuracy for downstream tasks\nwhile decreasing the number of parameters and training time of the model.\nAdditionally, we use centered kernel alignment and probing linear classifiers\nto gain insight into our architectural modifications and justify that removal\nof intermediate layers has little impact on the fine-tuned accuracy.", "journal": ""}
{"doi": "10.48550/arXiv.2012.15150", "date": "2020-12-30", "title": "Improving BERT with Syntax-aware Local Attention", "authors": "Zhongli Li, Qingyu Zhou, Chao Li, Ke Xu, Yunbo Cao", "abstract": "Pre-trained Transformer-based neural language models, such as BERT, have\nachieved remarkable results on varieties of NLP tasks. Recent works have shown\nthat attention-based models can benefit from more focused attention over local\nregions. Most of them restrict the attention scope within a linear span, or\nconfine to certain tasks such as machine translation and question answering. In\nthis paper, we propose a syntax-aware local attention, where the attention\nscopes are restrained based on the distances in the syntactic structure. The\nproposed syntax-aware local attention can be integrated with pretrained\nlanguage models, such as BERT, to render the model to focus on syntactically\nrelevant words. We conduct experiments on various single-sentence benchmarks,\nincluding sentence classification and sequence labeling tasks. Experimental\nresults show consistent gains over BERT on all benchmark datasets. The\nextensive studies verify that our model achieves better performance owing to\nmore focused attention over syntactically relevant words.", "journal": ""}
{"doi": "10.48550/arXiv.2012.15353", "date": "2020-12-30", "title": "Deriving Contextualised Semantic Features from BERT (and Other Transformer Model) Embeddings", "authors": "Jacob Turton, David Vinson, Robert Elliott Smith", "abstract": "Models based on the transformer architecture, such as BERT, have marked a\ncrucial step forward in the field of Natural Language Processing. Importantly,\nthey allow the creation of word embeddings that capture important semantic\ninformation about words in context. However, as single entities, these\nembeddings are difficult to interpret and the models used to create them have\nbeen described as opaque. Binder and colleagues proposed an intuitive embedding\nspace where each dimension is based on one of 65 core semantic features.\nUnfortunately, the space only exists for a small dataset of 535 words, limiting\nits uses. Previous work (Utsumi, 2018, 2020, Turton, Vinson & Smith, 2020) has\nshown that Binder features can be derived from static embeddings and\nsuccessfully extrapolated to a large new vocabulary. Taking the next step, this\npaper demonstrates that Binder features can be derived from the BERT embedding\nspace. This provides contextualised Binder embeddings, which can aid in\nunderstanding semantic differences between words in context. It additionally\nprovides insights into how semantic features are represented across the\ndifferent layers of the BERT model.", "journal": ""}
{"doi": "10.48550/arXiv.2101.00403", "date": "2021-01-02", "title": "Superbizarre Is Not Superb: Derivational Morphology Improves BERT's Interpretation of Complex Words", "authors": "Valentin Hofmann, Janet B. Pierrehumbert, Hinrich Sch\u00fctze", "abstract": "How does the input segmentation of pretrained language models (PLMs) affect\ntheir interpretations of complex words? We present the first study\ninvestigating this question, taking BERT as the example PLM and focusing on its\nsemantic representations of English derivatives. We show that PLMs can be\ninterpreted as serial dual-route models, i.e., the meanings of complex words\nare either stored or else need to be computed from the subwords, which implies\nthat maximally meaningful input tokens should allow for the best generalization\non new words. This hypothesis is confirmed by a series of semantic probing\ntasks on which DelBERT (Derivation leveraging BERT), a model with derivational\ninput segmentation, substantially outperforms BERT with WordPiece segmentation.\nOur results suggest that the generalization capabilities of PLMs could be\nfurther improved if a morphologically-informed vocabulary of input tokens were\nused.", "journal": ""}
{"doi": "10.48550/arXiv.2101.04547", "date": "2021-01-12", "title": "Of Non-Linearity and Commutativity in BERT", "authors": "Sumu Zhao, Damian Pascual, Gino Brunner, Roger Wattenhofer", "abstract": "In this work we provide new insights into the transformer architecture, and\nin particular, its best-known variant, BERT. First, we propose a method to\nmeasure the degree of non-linearity of different elements of transformers.\nNext, we focus our investigation on the feed-forward networks (FFN) inside\ntransformers, which contain 2/3 of the model parameters and have so far not\nreceived much attention. We find that FFNs are an inefficient yet important\narchitectural element and that they cannot simply be replaced by attention\nblocks without a degradation in performance. Moreover, we study the\ninteractions between layers in BERT and show that, while the layers exhibit\nsome hierarchical structure, they extract features in a fuzzy manner. Our\nresults suggest that BERT has an inductive bias towards layer commutativity,\nwhich we find is mainly due to the skip connections. This provides a\njustification for the strong performance of recurrent and weight-shared\ntransformer models.", "journal": ""}
{"doi": "10.48550/arXiv.2101.11363", "date": "2021-01-27", "title": "KoreALBERT: Pretraining a Lite BERT Model for Korean Language Understanding", "authors": "Hyunjae Lee, Jaewoong Yoon, Bonggyu Hwang, Seongho Joe, Seungjai Min, Youngjune Gwon", "abstract": "A Lite BERT (ALBERT) has been introduced to scale up deep bidirectional\nrepresentation learning for natural languages. Due to the lack of pretrained\nALBERT models for Korean language, the best available practice is the\nmultilingual model or resorting back to the any other BERT-based model. In this\npaper, we develop and pretrain KoreALBERT, a monolingual ALBERT model\nspecifically for Korean language understanding. We introduce a new training\nobjective, namely Word Order Prediction (WOP), and use alongside the existing\nMLM and SOP criteria to the same architecture and model parameters. Despite\nhaving significantly fewer model parameters (thus, quicker to train), our\npretrained KoreALBERT outperforms its BERT counterpart on 6 different NLU\ntasks. Consistent with the empirical results in English by Lan et al.,\nKoreALBERT seems to improve downstream task performance involving\nmulti-sentence encoding for Korean language. The pretrained KoreALBERT is\npublicly available to encourage research and application development for Korean\nNLP.", "journal": ""}
{"doi": "10.48550/arXiv.2102.00291", "date": "2021-01-30", "title": "Speech Recognition by Simply Fine-tuning BERT", "authors": "Wen-Chin Huang, Chia-Hua Wu, Shang-Bao Luo, Kuan-Yu Chen, Hsin-Min Wang, Tomoki Toda", "abstract": "We propose a simple method for automatic speech recognition (ASR) by\nfine-tuning BERT, which is a language model (LM) trained on large-scale\nunlabeled text data and can generate rich contextual representations. Our\nassumption is that given a history context sequence, a powerful LM can narrow\nthe range of possible choices and the speech signal can be used as a simple\nclue. Hence, comparing to conventional ASR systems that train a powerful\nacoustic model (AM) from scratch, we believe that speech recognition is\npossible by simply fine-tuning a BERT model. As an initial study, we\ndemonstrate the effectiveness of the proposed idea on the AISHELL dataset and\nshow that stacking a very simple AM on top of BERT can yield reasonable\nperformance.", "journal": ""}
{"doi": "10.48550/arXiv.2103.00728", "date": "2021-03-01", "title": "BERT-based knowledge extraction method of unstructured domain text", "authors": "Wang Zijia, Li Ye, Zhu Zhongkai", "abstract": "With the development and business adoption of knowledge graph, there is an\nincreasing demand for extracting entities and relations of knowledge graphs\nfrom unstructured domain documents. This makes the automatic knowledge\nextraction for domain text quite meaningful. This paper proposes a knowledge\nextraction method based on BERT, which is used to extract knowledge points from\nunstructured specific domain texts (such as insurance clauses in the insurance\nindustry) automatically to save manpower of knowledge graph construction.\nDifferent from the commonly used methods which are based on rules, templates or\nentity extraction models, this paper converts the domain knowledge points into\nquestion and answer pairs and uses the text around the answer in documents as\nthe context. The method adopts a BERT-based model similar to BERT's SQuAD\nreading comprehension task. The model is fine-tuned. And it is used to directly\nextract knowledge points from more insurance clauses. According to the test\nresults, the model performance is good.", "journal": ""}
{"doi": "10.48550/arXiv.2103.10360", "date": "2021-03-18", "title": "GLM: General Language Model Pretraining with Autoregressive Blank Infilling", "authors": "Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, Jie Tang", "abstract": "There have been various types of pretraining architectures including\nautoencoding models (e.g., BERT), autoregressive models (e.g., GPT), and\nencoder-decoder models (e.g., T5). However, none of the pretraining frameworks\nperforms the best for all tasks of three main categories including natural\nlanguage understanding (NLU), unconditional generation, and conditional\ngeneration. We propose a General Language Model (GLM) based on autoregressive\nblank infilling to address this challenge. GLM improves blank filling\npretraining by adding 2D positional encodings and allowing an arbitrary order\nto predict spans, which results in performance gains over BERT and T5 on NLU\ntasks. Meanwhile, GLM can be pretrained for different types of tasks by varying\nthe number and lengths of blanks. On a wide range of tasks across NLU,\nconditional and unconditional generation, GLM outperforms BERT, T5, and GPT\ngiven the same model sizes and data, and achieves the best performance from a\nsingle pretrained model with 1.25x parameters of BERT Large , demonstrating its\ngeneralizability to different downstream tasks.", "journal": ""}
{"doi": "10.48550/arXiv.2104.06835", "date": "2021-04-14", "title": "Enhancing Word-Level Semantic Representation via Dependency Structure for Expressive Text-to-Speech Synthesis", "authors": "Yixuan Zhou, Changhe Song, Jingbei Li, Zhiyong Wu, Yanyao Bian, Dan Su, Helen Meng", "abstract": "Exploiting rich linguistic information in raw text is crucial for expressive\ntext-to-speech (TTS). As large scale pre-trained text representation develops,\nbidirectional encoder representations from Transformers (BERT) has been proven\nto embody semantic information and employed to TTS recently. However, original\nor simply fine-tuned BERT embeddings still cannot provide sufficient semantic\nknowledge that expressive TTS models should take into account. In this paper,\nwe propose a word-level semantic representation enhancing method based on\ndependency structure and pre-trained BERT embedding. The BERT embedding of each\nword is reprocessed considering its specific dependencies and related words in\nthe sentence, to generate more effective semantic representation for TTS. To\nbetter utilize the dependency structure, relational gated graph network (RGGN)\nis introduced to make semantic information flow and aggregate through the\ndependency structure. The experimental results show that the proposed method\ncan further improve the naturalness and expressiveness of synthesized speeches\non both Mandarin and English datasets.", "journal": ""}
{"doi": "10.48550/arXiv.2104.07762", "date": "2021-04-15", "title": "Does BERT Pretrained on Clinical Notes Reveal Sensitive Data?", "authors": "Eric Lehman, Sarthak Jain, Karl Pichotta, Yoav Goldberg, Byron C. Wallace", "abstract": "Large Transformers pretrained over clinical notes from Electronic Health\nRecords (EHR) have afforded substantial gains in performance on predictive\nclinical tasks. The cost of training such models (and the necessity of data\naccess to do so) coupled with their utility motivates parameter sharing, i.e.,\nthe release of pretrained models such as ClinicalBERT. While most efforts have\nused deidentified EHR, many researchers have access to large sets of sensitive,\nnon-deidentified EHR with which they might train a BERT model (or similar).\nWould it be safe to release the weights of such a model if they did? In this\nwork, we design a battery of approaches intended to recover Personal Health\nInformation (PHI) from a trained BERT. Specifically, we attempt to recover\npatient names and conditions with which they are associated. We find that\nsimple probing methods are not able to meaningfully extract sensitive\ninformation from BERT trained over the MIMIC-III corpus of EHR. However, more\nsophisticated \"attacks\" may succeed in doing so: To facilitate such research,\nwe make our experimental setup and baseline probing models available at\nhttps://github.com/elehman16/exposing_patient_data_release", "journal": ""}
