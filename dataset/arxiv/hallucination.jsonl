{"doi": "10.48550/arXiv.2505.04847", "date": "2025-05-07", "title": "Benchmarking LLM Faithfulness in RAG with Evolving Leaderboards", "authors": "Manveer Singh Tamber, Forrest Sheng Bao, Chenyu Xu, Ge Luo, Suleman Kazi, Minseok Bae, Miaoran Li, Ofer Mendelevitch, Renyi Qu, Jimmy Lin", "abstract": "Hallucinations remain a persistent challenge for LLMs. RAG aims to reduce\nhallucinations by grounding responses in contexts. However, even when provided\ncontext, LLMs still frequently introduce unsupported information or\ncontradictions. This paper presents our efforts to measure LLM hallucinations\nwith a focus on summarization tasks, assessing how often various LLMs introduce\nhallucinations when summarizing documents. We discuss Vectara's existing LLM\nhallucination leaderboard, based on the Hughes Hallucination Evaluation Model\n(HHEM). While HHEM and Vectara's Hallucination Leaderboard have garnered great\nresearch interest, we examine challenges faced by HHEM and current\nhallucination detection methods by analyzing the effectiveness of these methods\non existing hallucination datasets. To address these limitations, we propose\nFaithJudge, an LLM-as-a-judge approach guided by few-shot human hallucination\nannotations, which substantially improves automated LLM hallucination\nevaluation over current methods. We introduce an enhanced hallucination\nleaderboard centered on FaithJudge, alongside our current hallucination\nleaderboard, enabling more reliable benchmarking of LLMs for hallucinations in\nRAG.", "journal": ""}
{"doi": "10.48550/arXiv.2502.11113", "date": "2025-02-16", "title": "Valuable Hallucinations: Realizable Non-realistic Propositions", "authors": "Qiucheng Chen, Bo Wang", "abstract": "This paper introduces the first formal definition of valuable hallucinations\nin large language models (LLMs), addressing a gap in the existing literature.\nWe provide a systematic definition and analysis of hallucination value,\nproposing methods for enhancing the value of hallucinations. In contrast to\nprevious works, which often treat hallucinations as a broad flaw, we focus on\nthe potential value that certain types of hallucinations can offer in specific\ncontexts. Hallucinations in LLMs generally refer to the generation of\nunfaithful, fabricated, inconsistent, or nonsensical content. Rather than\nviewing all hallucinations negatively, this paper gives formal representations\nand manual judgments of \"valuable hallucinations\" and explores how realizable\nnon-realistic propositions--ideas that are not currently true but could be\nachievable under certain conditions--can have constructive value. We present\nexperiments using the Qwen2.5 model and HalluQA dataset, employing ReAct\nprompting (which involves reasoning, confidence assessment, and answer\nverification) to control and optimize hallucinations. Our findings show that\nReAct prompting results in a 5.12\\% reduction in overall hallucinations and an\nincrease in the proportion of valuable hallucinations from 6.45\\% to 7.92\\%.\nThese results demonstrate that systematically controlling hallucinations can\nimprove their usefulness without compromising factual reliability.", "journal": ""}
{"doi": "10.48550/arXiv.2312.14183", "date": "2023-12-19", "title": "On Early Detection of Hallucinations in Factual Question Answering", "authors": "Ben Snyder, Marius Moisescu, Muhammad Bilal Zafar", "abstract": "While large language models (LLMs) have taken great strides towards helping\nhumans with a plethora of tasks, hallucinations remain a major impediment\ntowards gaining user trust. The fluency and coherence of model generations even\nwhen hallucinating makes detection a difficult task. In this work, we explore\nif the artifacts associated with the model generations can provide hints that\nthe generation will contain hallucinations. Specifically, we probe LLMs at 1)\nthe inputs via Integrated Gradients based token attribution, 2) the outputs via\nthe Softmax probabilities, and 3) the internal state via self-attention and\nfully-connected layer activations for signs of hallucinations on open-ended\nquestion answering tasks. Our results show that the distributions of these\nartifacts tend to differ between hallucinated and non-hallucinated generations.\nBuilding on this insight, we train binary classifiers that use these artifacts\nas input features to classify model generations into hallucinations and\nnon-hallucinations. These hallucination classifiers achieve up to $0.80$ AUROC.\nWe also show that tokens preceding a hallucination can already predict the\nsubsequent hallucination even before it occurs.", "journal": ""}
{"doi": "10.48550/arXiv.2404.01588", "date": "2024-04-02", "title": "Hallucination Diversity-Aware Active Learning for Text Summarization", "authors": "Yu Xia, Xu Liu, Tong Yu, Sungchul Kim, Ryan A. Rossi, Anup Rao, Tung Mai, Shuai Li", "abstract": "Large Language Models (LLMs) have shown propensity to generate hallucinated\noutputs, i.e., texts that are factually incorrect or unsupported. Existing\nmethods for alleviating hallucinations typically require costly human\nannotations to identify and correct hallucinations in LLM outputs. Moreover,\nmost of these methods focus on a specific type of hallucination, e.g., entity\nor token errors, which limits their effectiveness in addressing various types\nof hallucinations exhibited in LLM outputs. To our best knowledge, in this\npaper we propose the first active learning framework to alleviate LLM\nhallucinations, reducing costly human annotations of hallucination needed. By\nmeasuring fine-grained hallucinations from errors in semantic frame, discourse\nand content verifiability in text summarization, we propose HAllucination\nDiversity-Aware Sampling (HADAS) to select diverse hallucinations for\nannotations in active learning for LLM finetuning. Extensive experiments on\nthree datasets and different backbone models demonstrate advantages of our\nmethod in effectively and efficiently mitigating LLM hallucinations.", "journal": ""}
{"doi": "10.48550/arXiv.2504.17550", "date": "2025-04-24", "title": "HalluLens: LLM Hallucination Benchmark", "authors": "Yejin Bang, Ziwei Ji, Alan Schelten, Anthony Hartshorn, Tara Fowler, Cheng Zhang, Nicola Cancedda, Pascale Fung", "abstract": "Large language models (LLMs) often generate responses that deviate from user\ninput or training data, a phenomenon known as \"hallucination.\" These\nhallucinations undermine user trust and hinder the adoption of generative AI\nsystems. Addressing hallucinations is essential for the advancement of LLMs.\nThis paper introduces a comprehensive hallucination benchmark, incorporating\nboth new extrinsic and existing intrinsic evaluation tasks, built upon clear\ntaxonomy of hallucination. A major challenge in benchmarking hallucinations is\nthe lack of a unified framework due to inconsistent definitions and\ncategorizations. We disentangle LLM hallucination from \"factuality,\" proposing\na clear taxonomy that distinguishes between extrinsic and intrinsic\nhallucinations, to promote consistency and facilitate research. Extrinsic\nhallucinations, where the generated content is not consistent with the training\ndata, are increasingly important as LLMs evolve. Our benchmark includes dynamic\ntest set generation to mitigate data leakage and ensure robustness against such\nleakage. We also analyze existing benchmarks, highlighting their limitations\nand saturation. The work aims to: (1) establish a clear taxonomy of\nhallucinations, (2) introduce new extrinsic hallucination tasks, with data that\ncan be dynamically regenerated to prevent saturation by leakage, (3) provide a\ncomprehensive analysis of existing benchmarks, distinguishing them from\nfactuality evaluations.", "journal": ""}
{"doi": "10.48550/arXiv.2407.04693", "date": "2024-07-05", "title": "ANAH-v2: Scaling Analytical Hallucination Annotation of Large Language Models", "authors": "Yuzhe Gu, Ziwei Ji, Wenwei Zhang, Chengqi Lyu, Dahua Lin, Kai Chen", "abstract": "Large language models (LLMs) exhibit hallucinations in long-form\nquestion-answering tasks across various domains and wide applications. Current\nhallucination detection and mitigation datasets are limited in domains and\nsizes, which struggle to scale due to prohibitive labor costs and insufficient\nreliability of existing hallucination annotators. To facilitate the scalable\noversight of LLM hallucinations, this paper introduces an iterative\nself-training framework that simultaneously and progressively scales up the\nhallucination annotation dataset and improves the accuracy of the hallucination\nannotator. Based on the Expectation Maximization (EM) algorithm, in each\niteration, the framework first applies a hallucination annotation pipeline to\nannotate a scaled dataset and then trains a more accurate hallucination\nannotator on the dataset. This new hallucination annotator is adopted in the\nhallucination annotation pipeline used for the next iteration. Extensive\nexperimental results demonstrate that the finally obtained hallucination\nannotator with only 7B parameters surpasses the performance of GPT-4 and\nobtains new state-of-the-art hallucination detection results on HaluEval and\nHalluQA by zero-shot inference. Such an annotator can not only evaluate the\nhallucination levels of various LLMs on the large-scale dataset but also help\nto mitigate the hallucination of LLMs generations, with the Natural Language\nInference (NLI) metric increasing from 25% to 37% on HaluEval.", "journal": ""}
{"doi": "10.48550/arXiv.2402.15721", "date": "2024-02-24", "title": "Hal-Eval: A Universal and Fine-grained Hallucination Evaluation Framework for Large Vision Language Models", "authors": "Chaoya Jiang, Hongrui Jia, Wei Ye, Mengfan Dong, Haiyang Xu, Ming Yan, Ji Zhang, Shikun Zhang", "abstract": "Large Vision Language Models exhibit remarkable capabilities but struggle\nwith hallucinations inconsistencies between images and their descriptions.\nPrevious hallucination evaluation studies on LVLMs have identified\nhallucinations in terms of objects, attributes, and relations but overlooked\ncomplex hallucinations that create an entire narrative around a fictional\nentity. In this paper, we introduce a refined taxonomy of hallucinations,\nfeaturing a new category: Event Hallucination. We then utilize advanced LLMs to\ngenerate and filter fine grained hallucinatory data consisting of various types\nof hallucinations, with a particular focus on event hallucinations, laying the\ngroundwork for integrating discriminative and generative evaluation methods\nwithin our universal evaluation framework. The proposed benchmark distinctively\nassesses LVLMs ability to tackle a broad spectrum of hallucinations, making it\na reliable and comprehensive tool for gauging LVLMs efficacy in handling\nhallucinations. We will release our code and data.", "journal": ""}
{"doi": "10.48550/arXiv.2407.20505", "date": "2024-07-30", "title": "Interpreting and Mitigating Hallucination in MLLMs through Multi-agent Debate", "authors": "Zheng Lin, Zhenxing Niu, Zhibin Wang, Yinghui Xu", "abstract": "MLLMs often generate outputs that are inconsistent with the visual content, a\nchallenge known as hallucination. Previous methods focus on determining whether\na generated output is hallucinated, without identifying which image region\nleads to the hallucination or interpreting why such hallucinations occur. In\nthis paper, we argue that hallucination in MLLMs is partially due to a lack of\nslow-thinking and divergent-thinking in these models. To address this, we\npropose adopting a self-reflection scheme to promote slow-thinking.\nFurthermore, we consider eliminating hallucination as a complex reasoning task\nand propose a multi-agent debate approach to encourage divergent-thinking.\nConsequently, our approach can not only mitigate hallucinations but also\ninterpret why they occur and detail the specifics of hallucination. In\naddition, we propose to distinguish creativity from hallucination in the\ncontext of MLLMs, and illustrate how to evaluate MLLMs' creativity capability.\nExtensive experiments on various benchmarks demonstrate that our approach\nexhibits generalized hallucinations-mitigating performance across several\nMLLMs.", "journal": ""}
{"doi": "10.48550/arXiv.2410.09997", "date": "2024-10-13", "title": "Collu-Bench: A Benchmark for Predicting Language Model Hallucinations in Code", "authors": "Nan Jiang, Qi Li, Lin Tan, Tianyi Zhang", "abstract": "Despite their success, large language models (LLMs) face the critical\nchallenge of hallucinations, generating plausible but incorrect content. While\nmuch research has focused on hallucinations in multiple modalities including\nimages and natural language text, less attention has been given to\nhallucinations in source code, which leads to incorrect and vulnerable code\nthat causes significant financial loss. To pave the way for research in LLMs'\nhallucinations in code, we introduce Collu-Bench, a benchmark for predicting\ncode hallucinations of LLMs across code generation (CG) and automated program\nrepair (APR) tasks. Collu-Bench includes 13,234 code hallucination instances\ncollected from five datasets and 11 diverse LLMs, ranging from open-source\nmodels to commercial ones. To better understand and predict code\nhallucinations, Collu-Bench provides detailed features such as the per-step log\nprobabilities of LLMs' output, token types, and the execution feedback of LLMs'\ngenerated code for in-depth analysis. In addition, we conduct experiments to\npredict hallucination on Collu-Bench, using both traditional machine learning\ntechniques and neural networks, which achieves 22.03 -- 33.15% accuracy. Our\nexperiments draw insightful findings of code hallucination patterns, reveal the\nchallenge of accurately localizing LLMs' hallucinations, and highlight the need\nfor more sophisticated techniques.", "journal": ""}
{"doi": "10.48550/arXiv.2503.00436", "date": "2025-03-01", "title": "HalCECE: A Framework for Explainable Hallucination Detection through Conceptual Counterfactuals in Image Captioning", "authors": "Maria Lymperaiou, Giorgos FIlandrianos, Angeliki Dimitriou, Athanasios Voulodimos, Giorgos Stamou", "abstract": "In the dynamic landscape of artificial intelligence, the exploration of\nhallucinations within vision-language (VL) models emerges as a critical\nfrontier. This work delves into the intricacies of hallucinatory phenomena\nexhibited by widely used image captioners, unraveling interesting patterns.\nSpecifically, we step upon previously introduced techniques of conceptual\ncounterfactual explanations to address VL hallucinations. The deterministic and\nefficient nature of the employed conceptual counterfactuals backbone is able to\nsuggest semantically minimal edits driven by hierarchical knowledge, so that\nthe transition from a hallucinated caption to a non-hallucinated one is\nperformed in a black-box manner. HalCECE, our proposed hallucination detection\nframework is highly interpretable, by providing semantically meaningful edits\napart from standalone numbers, while the hierarchical decomposition of\nhallucinated concepts leads to a thorough hallucination analysis. Another\nnovelty tied to the current work is the investigation of role hallucinations,\nbeing one of the first works to involve interconnections between visual\nconcepts in hallucination detection. Overall, HalCECE recommends an explainable\ndirection to the crucial field of VL hallucination detection, thus fostering\ntrustworthy evaluation of current and future VL systems.", "journal": ""}
{"doi": "10.48550/arXiv.2503.12908", "date": "2025-03-17", "title": "HICD: Hallucination-Inducing via Attention Dispersion for Contrastive Decoding to Mitigate Hallucinations in Large Language Models", "authors": "Xinyan Jiang, Hang Ye, Yongxin Zhu, Xiaoying Zheng, Zikang Chen, Jun Gong", "abstract": "Large Language Models (LLMs) often generate hallucinations, producing outputs\nthat are contextually inaccurate or factually incorrect. We introduce HICD, a\nnovel method designed to induce hallucinations for contrastive decoding to\nmitigate hallucinations. Unlike existing contrastive decoding methods, HICD\nselects attention heads crucial to the model's prediction as inducing heads,\nthen induces hallucinations by dispersing attention of these inducing heads and\ncompares the hallucinated outputs with the original outputs to obtain the final\nresult. Our approach significantly improves performance on tasks requiring\ncontextual faithfulness, such as context completion, reading comprehension, and\nquestion answering. It also improves factuality in tasks requiring accurate\nknowledge recall. We demonstrate that our inducing heads selection and\nattention dispersion method leads to more \"contrast-effective\" hallucinations\nfor contrastive decoding, outperforming other hallucination-inducing methods.\nOur findings provide a promising strategy for reducing hallucinations by\ninducing hallucinations in a controlled manner, enhancing the performance of\nLLMs in a wide range of tasks.", "journal": ""}
{"doi": "10.48550/arXiv.2104.06683", "date": "2021-04-14", "title": "The Curious Case of Hallucinations in Neural Machine Translation", "authors": "Vikas Raunak, Arul Menezes, Marcin Junczys-Dowmunt", "abstract": "In this work, we study hallucinations in Neural Machine Translation (NMT),\nwhich lie at an extreme end on the spectrum of NMT pathologies. Firstly, we\nconnect the phenomenon of hallucinations under source perturbation to the\nLong-Tail theory of Feldman (2020), and present an empirically validated\nhypothesis that explains hallucinations under source perturbation. Secondly, we\nconsider hallucinations under corpus-level noise (without any source\nperturbation) and demonstrate that two prominent types of natural\nhallucinations (detached and oscillatory outputs) could be generated and\nexplained through specific corpus-level noise patterns. Finally, we elucidate\nthe phenomenon of hallucination amplification in popular data-generation\nprocesses such as Backtranslation and sequence-level Knowledge Distillation.", "journal": ""}
{"doi": "10.48550/arXiv.2309.05922", "date": "2023-09-12", "title": "A Survey of Hallucination in Large Foundation Models", "authors": "Vipula Rawte, Amit Sheth, Amitava Das", "abstract": "Hallucination in a foundation model (FM) refers to the generation of content\nthat strays from factual reality or includes fabricated information. This\nsurvey paper provides an extensive overview of recent efforts that aim to\nidentify, elucidate, and tackle the problem of hallucination, with a particular\nfocus on ``Large'' Foundation Models (LFMs). The paper classifies various types\nof hallucination phenomena that are specific to LFMs and establishes evaluation\ncriteria for assessing the extent of hallucination. It also examines existing\nstrategies for mitigating hallucination in LFMs and discusses potential\ndirections for future research in this area. Essentially, the paper offers a\ncomprehensive examination of the challenges and solutions related to\nhallucination in LFMs.", "journal": ""}
{"doi": "10.48550/arXiv.2401.06792", "date": "2024-01-08", "title": "LightHouse: A Survey of AGI Hallucination", "authors": "Feng Wang", "abstract": "With the development of artificial intelligence, large-scale models have\nbecome increasingly intelligent. However, numerous studies indicate that\nhallucinations within these large models are a bottleneck hindering the\ndevelopment of AI research. In the pursuit of achieving strong artificial\nintelligence, a significant volume of research effort is being invested in the\nAGI (Artificial General Intelligence) hallucination research. Previous\nexplorations have been conducted in researching hallucinations within LLMs\n(Large Language Models). As for multimodal AGI, research on hallucinations is\nstill in an early stage. To further the progress of research in the domain of\nhallucinatory phenomena, we present a bird's eye view of hallucinations in AGI,\nsummarizing the current work on AGI hallucinations and proposing some\ndirections for future research.", "journal": ""}
{"doi": "10.48550/arXiv.2405.05256", "date": "2024-05-08", "title": "THRONE: An Object-based Hallucination Benchmark for the Free-form Generations of Large Vision-Language Models", "authors": "Prannay Kaul, Zhizhong Li, Hao Yang, Yonatan Dukler, Ashwin Swaminathan, C. J. Taylor, Stefano Soatto", "abstract": "Mitigating hallucinations in large vision-language models (LVLMs) remains an\nopen problem. Recent benchmarks do not address hallucinations in open-ended\nfree-form responses, which we term \"Type I hallucinations\". Instead, they focus\non hallucinations responding to very specific question formats -- typically a\nmultiple-choice response regarding a particular object or attribute -- which we\nterm \"Type II hallucinations\". Additionally, such benchmarks often require\nexternal API calls to models which are subject to change. In practice, we\nobserve that a reduction in Type II hallucinations does not lead to a reduction\nin Type I hallucinations but rather that the two forms of hallucinations are\noften anti-correlated. To address this, we propose THRONE, a novel object-based\nautomatic framework for quantitatively evaluating Type I hallucinations in LVLM\nfree-form outputs. We use public language models (LMs) to identify\nhallucinations in LVLM responses and compute informative metrics. By evaluating\na large selection of recent LVLMs using public datasets, we show that an\nimprovement in existing metrics do not lead to a reduction in Type I\nhallucinations, and that established benchmarks for measuring Type I\nhallucinations are incomplete. Finally, we provide a simple and effective data\naugmentation method to reduce Type I and Type II hallucinations as a strong\nbaseline. Code is now available at https://github.com/amazon-science/THRONE .", "journal": ""}
{"doi": "10.48550/arXiv.2501.15046", "date": "2025-01-25", "title": "Evaluating Hallucination in Large Vision-Language Models based on Context-Aware Object Similarities", "authors": "Shounak Datta, Dhanasekar Sundararaman", "abstract": "Despite their impressive performance on multi-modal tasks, large\nvision-language models (LVLMs) tend to suffer from hallucinations. An important\ntype is object hallucination, where LVLMs generate objects that are\ninconsistent with the images shown to the model. Existing works typically\nattempt to quantify object hallucinations by detecting and measuring the\nfraction of hallucinated objects in generated captions. Additionally, more\nrecent work also measures object hallucinations by directly querying the LVLM\nwith binary questions about the presence of likely hallucinated objects based\non object statistics like top-k frequent objects and top-k co-occurring\nobjects. In this paper, we present Context-Aware Object Similarities (CAOS), a\nnovel approach for evaluating object hallucination in LVLMs using object\nstatistics as well as the generated captions. CAOS uniquely integrates object\nstatistics with semantic relationships between objects in captions and\nground-truth data. Moreover, existing approaches usually only detect and\nmeasure hallucinations belonging to a predetermined set of in-domain objects\n(typically the set of all ground-truth objects for the training dataset) and\nignore generated objects that are not part of this set, leading to\nunder-evaluation. To address this, we further employ language model--based\nobject recognition to detect potentially out-of-domain hallucinated objects and\nuse an ensemble of LVLMs for verifying the presence of such objects in the\nquery image. CAOS also examines the sequential dynamics of object generation,\nshedding light on how the order of object appearance influences hallucinations,\nand employs word embedding models to analyze the semantic reasons behind\nhallucinations. CAOS aims to offer a nuanced understanding of the hallucination\ntendencies of LVLMs by providing a systematic framework to identify and\ninterpret object hallucinations.", "journal": ""}
{"doi": "10.48550/arXiv.2506.00448", "date": "2025-05-31", "title": "Fact-Controlled Diagnosis of Hallucinations in Medical Text Summarization", "authors": "Suhas BN, Han-Chin Shing, Lei Xu, Mitch Strong, Jon Burnsky, Jessica Ofor, Jordan R. Mason, Susan Chen, Sundararajan Srinivasan, Chaitanya Shivade, Jack Moriarty, Joseph Paul Cohen", "abstract": "Hallucinations in large language models (LLMs) during summarization of\npatient-clinician dialogues pose significant risks to patient care and clinical\ndecision-making. However, the phenomenon remains understudied in the clinical\ndomain, with uncertainty surrounding the applicability of general-domain\nhallucination detectors. The rarity and randomness of hallucinations further\ncomplicate their investigation. In this paper, we conduct an evaluation of\nhallucination detection methods in the medical domain, and construct two\ndatasets for the purpose: A fact-controlled Leave-N-out dataset -- generated by\nsystematically removing facts from source dialogues to induce hallucinated\ncontent in summaries; and a natural hallucination dataset -- arising\norganically during LLM-based medical summarization. We show that general-domain\ndetectors struggle to detect clinical hallucinations, and that performance on\nfact-controlled hallucinations does not reliably predict effectiveness on\nnatural hallucinations. We then develop fact-based approaches that count\nhallucinations, offering explainability not available with existing methods.\nNotably, our LLM-based detectors, which we developed using fact-controlled\nhallucinations, generalize well to detecting real-world clinical\nhallucinations. This research contributes a suite of specialized metrics\nsupported by expert-annotated datasets to advance faithful clinical\nsummarization systems.", "journal": ""}
{"doi": "10.48550/arXiv.2305.11747", "date": "2023-05-19", "title": "HaluEval: A Large-Scale Hallucination Evaluation Benchmark for Large Language Models", "authors": "Junyi Li, Xiaoxue Cheng, Wayne Xin Zhao, Jian-Yun Nie, Ji-Rong Wen", "abstract": "Large language models (LLMs), such as ChatGPT, are prone to generate\nhallucinations, i.e., content that conflicts with the source or cannot be\nverified by the factual knowledge. To understand what types of content and to\nwhich extent LLMs are apt to hallucinate, we introduce the Hallucination\nEvaluation benchmark for Large Language Models (HaluEval), a large collection\nof generated and human-annotated hallucinated samples for evaluating the\nperformance of LLMs in recognizing hallucination. To generate these samples, we\npropose a ChatGPT-based two-step framework, i.e., sampling-then-filtering.\nBesides, we also hire some human labelers to annotate the hallucinations in\nChatGPT responses. The empirical results suggest that ChatGPT is likely to\ngenerate hallucinated content in specific topics by fabricating unverifiable\ninformation (i.e., about $19.5\\%$ responses). Moreover, existing LLMs face\ngreat challenges in recognizing the hallucinations in texts. However, our\nexperiments also prove that providing external knowledge or adding reasoning\nsteps can help LLMs recognize hallucinations. Our benchmark can be accessed at\nhttps://github.com/RUCAIBox/HaluEval.", "journal": ""}
{"doi": "10.48550/arXiv.2308.15126", "date": "2023-08-29", "title": "Evaluation and Analysis of Hallucination in Large Vision-Language Models", "authors": "Junyang Wang, Yiyang Zhou, Guohai Xu, Pengcheng Shi, Chenlin Zhao, Haiyang Xu, Qinghao Ye, Ming Yan, Ji Zhang, Jihua Zhu, Jitao Sang, Haoyu Tang", "abstract": "Large Vision-Language Models (LVLMs) have recently achieved remarkable\nsuccess. However, LVLMs are still plagued by the hallucination problem, which\nlimits the practicality in many scenarios. Hallucination refers to the\ninformation of LVLMs' responses that does not exist in the visual input, which\nposes potential risks of substantial consequences. There has been limited work\nstudying hallucination evaluation in LVLMs. In this paper, we propose\nHallucination Evaluation based on Large Language Models (HaELM), an LLM-based\nhallucination evaluation framework. HaELM achieves an approximate 95%\nperformance comparable to ChatGPT and has additional advantages including low\ncost, reproducibility, privacy preservation and local deployment. Leveraging\nthe HaELM, we evaluate the hallucination in current LVLMs. Furthermore, we\nanalyze the factors contributing to hallucination in LVLMs and offer helpful\nsuggestions to mitigate the hallucination problem. Our training data and human\nannotation hallucination data will be made public soon.", "journal": ""}
{"doi": "10.48550/arXiv.2402.00253", "date": "2024-02-01", "title": "A Survey on Hallucination in Large Vision-Language Models", "authors": "Hanchao Liu, Wenyuan Xue, Yifei Chen, Dapeng Chen, Xiutian Zhao, Ke Wang, Liping Hou, Rongjun Li, Wei Peng", "abstract": "Recent development of Large Vision-Language Models (LVLMs) has attracted\ngrowing attention within the AI landscape for its practical implementation\npotential. However, ``hallucination'', or more specifically, the misalignment\nbetween factual visual content and corresponding textual generation, poses a\nsignificant challenge of utilizing LVLMs. In this comprehensive survey, we\ndissect LVLM-related hallucinations in an attempt to establish an overview and\nfacilitate future mitigation. Our scrutiny starts with a clarification of the\nconcept of hallucinations in LVLMs, presenting a variety of hallucination\nsymptoms and highlighting the unique challenges inherent in LVLM\nhallucinations. Subsequently, we outline the benchmarks and methodologies\ntailored specifically for evaluating hallucinations unique to LVLMs.\nAdditionally, we delve into an investigation of the root causes of these\nhallucinations, encompassing insights from the training data and model\ncomponents. We also critically review existing methods for mitigating\nhallucinations. The open questions and future directions pertaining to\nhallucinations within LVLMs are discussed to conclude this survey.", "journal": ""}
{"doi": "10.48550/arXiv.2406.16338", "date": "2024-06-24", "title": "VideoHallucer: Evaluating Intrinsic and Extrinsic Hallucinations in Large Video-Language Models", "authors": "Yuxuan Wang, Yueqian Wang, Dongyan Zhao, Cihang Xie, Zilong Zheng", "abstract": "Recent advancements in Multimodal Large Language Models (MLLMs) have extended\ntheir capabilities to video understanding. Yet, these models are often plagued\nby \"hallucinations\", where irrelevant or nonsensical content is generated,\ndeviating from the actual video context. This work introduces VideoHallucer,\nthe first comprehensive benchmark for hallucination detection in large\nvideo-language models (LVLMs). VideoHallucer categorizes hallucinations into\ntwo main types: intrinsic and extrinsic, offering further subcategories for\ndetailed analysis, including object-relation, temporal, semantic detail,\nextrinsic factual, and extrinsic non-factual hallucinations. We adopt an\nadversarial binary VideoQA method for comprehensive evaluation, where pairs of\nbasic and hallucinated questions are crafted strategically. By evaluating\neleven LVLMs on VideoHallucer, we reveal that i) the majority of current models\nexhibit significant issues with hallucinations; ii) while scaling datasets and\nparameters improves models' ability to detect basic visual cues and\ncounterfactuals, it provides limited benefit for detecting extrinsic factual\nhallucinations; iii) existing models are more adept at detecting facts than\nidentifying hallucinations. As a byproduct, these analyses further instruct the\ndevelopment of our self-PEP framework, achieving an average of 5.38%\nimprovement in hallucination resistance across all model architectures.", "journal": ""}
{"doi": "10.48550/arXiv.2410.08393", "date": "2024-10-10", "title": "The Effects of Hallucinations in Synthetic Training Data for Relation Extraction", "authors": "Steven Rogulsky, Nicholas Popovic, Michael F\u00e4rber", "abstract": "Relation extraction is crucial for constructing knowledge graphs, with large\nhigh-quality datasets serving as the foundation for training, fine-tuning, and\nevaluating models. Generative data augmentation (GDA) is a common approach to\nexpand such datasets. However, this approach often introduces hallucinations,\nsuch as spurious facts, whose impact on relation extraction remains\nunderexplored. In this paper, we examine the effects of hallucinations on the\nperformance of relation extraction on the document and sentence levels. Our\nempirical study reveals that hallucinations considerably compromise the ability\nof models to extract relations from text, with recall reductions between 19.1%\nand 39.2%. We identify that relevant hallucinations impair the model's\nperformance, while irrelevant hallucinations have a minimal impact.\nAdditionally, we develop methods for the detection of hallucinations to improve\ndata quality and model performance. Our approaches successfully classify texts\nas either 'hallucinated' or 'clean,' achieving high F1-scores of 83.8% and\n92.2%. These methods not only assist in removing hallucinations but also help\nin estimating their prevalence within datasets, which is crucial for selecting\nhigh-quality data. Overall, our work confirms the profound impact of relevant\nhallucinations on the effectiveness of relation extraction models.", "journal": ""}
{"doi": "10.48550/arXiv.2410.12278", "date": "2024-10-16", "title": "Controlled Automatic Task-Specific Synthetic Data Generation for Hallucination Detection", "authors": "Yong Xie, Karan Aggarwal, Aitzaz Ahmad, Stephen Lau", "abstract": "We present a novel approach to automatically generate non-trivial\ntask-specific synthetic datasets for hallucination detection. Our approach\nfeatures a two-step generation-selection pipeline, using hallucination pattern\nguidance and a language style alignment during generation. Hallucination\npattern guidance leverages the most important task-specific hallucination\npatterns while language style alignment aligns the style of the synthetic\ndataset with benchmark text. To obtain robust supervised detectors from\nsynthetic datasets, we also adopt a data mixture strategy to improve\nperformance robustness and generalization. Our results on three datasets show\nthat our generated hallucination text is more closely aligned with\nnon-hallucinated text versus baselines, to train hallucination detectors with\nbetter generalization. Our hallucination detectors trained on synthetic\ndatasets outperform in-context-learning (ICL)-based detectors by a large margin\nof 32%. Our extensive experiments confirm the benefits of our approach with\ncross-task and cross-generator generalization. Our data-mixture-based training\nfurther improves the generalization and robustness of hallucination detection.", "journal": ""}
{"doi": "10.48550/arXiv.2411.18659", "date": "2024-11-27", "title": "DHCP: Detecting Hallucinations by Cross-modal Attention Pattern in Large Vision-Language Models", "authors": "Yudong Zhang, Ruobing Xie, Jiansheng Chen, Xingwu Sun, Zhanhui kang, Yu Wang", "abstract": "Large vision-language models (LVLMs) have demonstrated exceptional\nperformance on complex multimodal tasks. However, they continue to suffer from\nsignificant hallucination issues, including object, attribute, and relational\nhallucinations. To accurately detect these hallucinations, we investigated the\nvariations in cross-modal attention patterns between hallucination and\nnon-hallucination states. Leveraging these distinctions, we developed a\nlightweight detector capable of identifying hallucinations. Our proposed\nmethod, Detecting Hallucinations by Cross-modal Attention Patterns (DHCP), is\nstraightforward and does not require additional LVLM training or extra LVLM\ninference steps. Experimental results show that DHCP achieves remarkable\nperformance in hallucination detection. By offering novel insights into the\nidentification and analysis of hallucinations in LVLMs, DHCP contributes to\nadvancing the reliability and trustworthiness of these models.", "journal": ""}
{"doi": "10.48550/arXiv.2412.04939", "date": "2024-12-06", "title": "Verb Mirage: Unveiling and Assessing Verb Concept Hallucinations in Multimodal Large Language Models", "authors": "Zehao Wang, Xinpeng Liu, Xiaoqian Wu, Yudonglin Zhang, Zhou Fang, Yifan Fang, Junfu Pu, Cewu Lu, Yong-Lu Li", "abstract": "Multimodal Large Language Models (MLLMs) have garnered significant attention\nrecently and demonstrate outstanding capabilities in various tasks such as OCR,\nVQA, captioning, $\\textit{etc}$. However, hallucination remains a persistent\nissue. While numerous methods have been proposed to mitigate hallucinations,\nachieving notable improvements, these methods primarily focus on mitigating\nhallucinations about $\\textbf{object/noun-related}$ concepts. Verb concepts,\ncrucial for understanding human actions, have been largely overlooked. In this\npaper, to the best of our knowledge, we are the $\\textbf{first}$ to investigate\nthe $\\textbf{verb hallucination}$ phenomenon of MLLMs from various\nperspectives. Our findings reveal that most state-of-the-art MLLMs suffer from\nsevere verb hallucination. To assess the effectiveness of existing mitigation\nmethods for object concept hallucination on verb hallucination, we evaluated\nthese methods and found that they do not effectively address verb\nhallucination. To address this issue, we propose a novel rich verb\nknowledge-based tuning method to mitigate verb hallucination. The experiment\nresults demonstrate that our method significantly reduces hallucinations\nrelated to verbs. $\\textit{Our code and data will be made publicly available}$.", "journal": ""}
{"doi": "10.48550/arXiv.2503.07833", "date": "2025-03-10", "title": "HalluVerse25: Fine-grained Multilingual Benchmark Dataset for LLM Hallucinations", "authors": "Samir Abdaljalil, Hasan Kurban, Erchin Serpedin", "abstract": "Large Language Models (LLMs) are increasingly used in various contexts, yet\nremain prone to generating non-factual content, commonly referred to as\n\"hallucinations\". The literature categorizes hallucinations into several types,\nincluding entity-level, relation-level, and sentence-level hallucinations.\nHowever, existing hallucination datasets often fail to capture fine-grained\nhallucinations in multilingual settings. In this work, we introduce\nHalluVerse25, a multilingual LLM hallucination dataset that categorizes\nfine-grained hallucinations in English, Arabic, and Turkish. Our dataset\nconstruction pipeline uses an LLM to inject hallucinations into factual\nbiographical sentences, followed by a rigorous human annotation process to\nensure data quality. We evaluate several LLMs on HalluVerse25, providing\nvaluable insights into how proprietary models perform in detecting\nLLM-generated hallucinations across different contexts.", "journal": ""}
{"doi": "10.48550/arXiv.2506.06539", "date": "2025-06-06", "title": "Beyond Facts: Evaluating Intent Hallucination in Large Language Models", "authors": "Yijie Hao, Haofei Yu, Jiaxuan You", "abstract": "When exposed to complex queries containing multiple conditions, today's large\nlanguage models (LLMs) tend to produce responses that only partially satisfy\nthe query while neglecting certain conditions. We therefore introduce the\nconcept of Intent Hallucination. In this phenomenon, LLMs either omit\n(neglecting to address certain parts) or misinterpret (responding to invented\nquery parts) elements of the given query, leading to intent hallucinated\ngeneration. To systematically evaluate intent hallucination, we introduce\nFAITHQA, a novel benchmark for intent hallucination that contains 20,068\nproblems, covering both query-only and retrieval-augmented generation (RAG)\nsetups with varying topics and difficulty. FAITHQA is the first hallucination\nbenchmark that goes beyond factual verification, tailored to identify the\nfundamental cause of intent hallucination. By evaluating various LLMs on\nFAITHQA, we find that (1) intent hallucination is a common issue even for\nstate-of-the-art models, and (2) the phenomenon stems from omission or\nmisinterpretation of LLMs. To facilitate future research, we introduce an\nautomatic LLM generation evaluation metric, CONSTRAINT SCORE, for detecting\nintent hallucination. Human evaluation results demonstrate that CONSTRAINT\nSCORE is closer to human performance for intent hallucination compared to\nbaselines.", "journal": "Proceedings of the 62nd Annual Meeting of the Association for\n  Computational Linguistics (ACL 2025)"}
{"doi": "10.48550/arXiv.2506.07184", "date": "2025-06-08", "title": "Mitigating Behavioral Hallucination in Multimodal Large Language Models for Sequential Images", "authors": "Liangliang You, Junchi Yao, Shu Yang, Guimin Hu, Lijie Hu, Di Wang", "abstract": "While multimodal large language models excel at various tasks, they still\nsuffer from hallucinations, which limit their reliability and scalability for\nbroader domain applications. To address this issue, recent research mainly\nfocuses on objective hallucination. However, for sequential images, besides\nobjective hallucination, there is also behavioral hallucination, which is less\nstudied. This work aims to fill in the gap. We first reveal that behavioral\nhallucinations mainly arise from two key factors: prior-driven bias and the\nsnowball effect. Based on these observations, we introduce SHE (Sequence\nHallucination Eradication), a lightweight, two-stage framework that (1) detects\nhallucinations via visual-textual alignment check using our proposed adaptive\ntemporal window and (2) mitigates them via orthogonal projection onto the joint\nembedding space. We also propose a new metric (BEACH) to quantify behavioral\nhallucination severity. Empirical results on standard benchmarks demonstrate\nthat SHE reduces behavioral hallucination by over 10% on BEACH while\nmaintaining descriptive accuracy.", "journal": ""}
{"doi": "10.48550/arXiv.2506.11417", "date": "2025-06-13", "title": "Stop learning it all to mitigate visual hallucination, Focus on the hallucination target", "authors": "Dokyoon Yoon, Youngsook Song, Woomyong Park", "abstract": "Multimodal Large Language Models (MLLMs) frequently suffer from hallucination\nissues, generating information about objects that are not present in input\nimages during vision-language tasks. These hallucinations particularly\nundermine model reliability in practical applications requiring accurate object\nidentification. To address this challenge, we propose \\mymethod,\\ a preference\nlearning approach that mitigates hallucinations by focusing on targeted areas\nwhere they occur. To implement this, we build a dataset containing hallucinated\nresponses, correct responses, and target information (i.e., objects present in\nthe images and the corresponding chunk positions in responses affected by\nhallucinations). By applying a preference learning method restricted to these\nspecific targets, the model can filter out irrelevant signals and focus on\ncorrecting hallucinations. This allows the model to produce more factual\nresponses by concentrating solely on relevant information. Experimental results\ndemonstrate that \\mymethod\\ effectively reduces hallucinations across multiple\nvision hallucination tasks, improving the reliability and performance of MLLMs\nwithout diminishing overall performance.", "journal": "Proceedings of the IEEE/CVF Conference on Computer Vision and\n  Pattern Recognition (CVPR), 2025"}
{"doi": "10.48550/arXiv.2306.13832", "date": "2023-06-24", "title": "Evidence for Reduced Sensory Precision and Increased Reliance on Priors in Hallucination-Prone Individuals in a General Population Sample", "authors": "David Benrimoh, Victoria L. Fisher, Rashina Seabury, Ely Sibarium, Catalina Mourgues, Doris Chen, Albert Powers", "abstract": "There is increasing evidence that people with hallucinations overweight\nperceptual beliefs relative to incoming sensory evidence. Much past work\ndemonstrating prior overweighting has used simple, non-linguistic stimuli.\nHowever, auditory hallucinations in psychosis are often complex and linguistic.\nThere may be an interaction between the type of auditory information being\nprocessed and its perceived quality in engendering hallucinations. We\nadministered a linguistic version of the Conditioned Hallucinations (CH) task\nto an online sample of 88 general population participants. Metrics related to\nhallucination-proneness, recent auditory hallucinations, stimulus thresholds,\nand stimulus detection were collected; data was used to fit parameters of a\nHierarchical Gaussian Filter model of perceptual inference to determine how\nlatent perceptual states influenced task behavior. Replicating past results,\nhigher CH rates were associated with measures of higher hallucination-proneness\nand recent hallucinatory experiences; CH rates were positively correlated with\nincreased prior weighting; and increased prior weighting was related to recent\nhallucinatory experiences. Unlike past results, participants with recent\nhallucinatory experiences as well as those with higher hallucination-proneness\nhad higher stimulus thresholds, lower sensitivity to stimuli presented at the\nhighest threshold, and tended to have lower response confidence, consistent\nwith lower precision of sensory evidence. We show that hallucination-prone\nindividuals in the general population have increased conditioned hallucination\nrates using a linguistic version of the CH task, and replicated the finding\nthat increased CH rates and recent hallucinations correlate with increased\nprior weighting. Results support a role for reduced sensory precision in the\ninterplay between prior weighting and hallucination-proneness. *contributed\nequally", "journal": ""}
{"doi": "10.48550/arXiv.2311.05232", "date": "2023-11-09", "title": "A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions", "authors": "Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Haotian Wang, Qianglong Chen, Weihua Peng, Xiaocheng Feng, Bing Qin, Ting Liu", "abstract": "The emergence of large language models (LLMs) has marked a significant\nbreakthrough in natural language processing (NLP), fueling a paradigm shift in\ninformation acquisition. Nevertheless, LLMs are prone to hallucination,\ngenerating plausible yet nonfactual content. This phenomenon raises significant\nconcerns over the reliability of LLMs in real-world information retrieval (IR)\nsystems and has attracted intensive research to detect and mitigate such\nhallucinations. Given the open-ended general-purpose attributes inherent to\nLLMs, LLM hallucinations present distinct challenges that diverge from prior\ntask-specific models. This divergence highlights the urgency for a nuanced\nunderstanding and comprehensive overview of recent advances in LLM\nhallucinations. In this survey, we begin with an innovative taxonomy of\nhallucination in the era of LLM and then delve into the factors contributing to\nhallucinations. Subsequently, we present a thorough overview of hallucination\ndetection methods and benchmarks. Our discussion then transfers to\nrepresentative methodologies for mitigating LLM hallucinations. Additionally,\nwe delve into the current limitations faced by retrieval-augmented LLMs in\ncombating hallucinations, offering insights for developing more robust IR\nsystems. Finally, we highlight the promising research directions on LLM\nhallucinations, including hallucination in large vision-language models and\nunderstanding of knowledge boundaries in LLM hallucinations.", "journal": ""}
{"doi": "10.48550/arXiv.2401.03205", "date": "2024-01-06", "title": "The Dawn After the Dark: An Empirical Study on Factuality Hallucination in Large Language Models", "authors": "Junyi Li, Jie Chen, Ruiyang Ren, Xiaoxue Cheng, Wayne Xin Zhao, Jian-Yun Nie, Ji-Rong Wen", "abstract": "In the era of large language models (LLMs), hallucination (i.e., the tendency\nto generate factually incorrect content) poses great challenge to trustworthy\nand reliable deployment of LLMs in real-world applications. To tackle the LLM\nhallucination, three key questions should be well studied: how to detect\nhallucinations (detection), why do LLMs hallucinate (source), and what can be\ndone to mitigate them (mitigation). To address these challenges, this work\npresents a systematic empirical study on LLM hallucination, focused on the the\nthree aspects of hallucination detection, source and mitigation. Specially, we\nconstruct a new hallucination benchmark HaluEval 2.0, and designs a simple yet\neffective detection method for LLM hallucination. Furthermore, we zoom into the\ndifferent training or utilization stages of LLMs and extensively analyze the\npotential factors that lead to the LLM hallucination. Finally, we implement and\nexamine a series of widely used techniques to mitigate the hallucinations in\nLLMs. Our work has led to several important findings to understand the\nhallucination origin and mitigate the hallucinations in LLMs. Our code and data\ncan be accessed at https://github.com/RUCAIBox/HaluEval-2.0.", "journal": ""}
{"doi": "10.48550/arXiv.2404.00971", "date": "2024-04-01", "title": "Exploring and Evaluating Hallucinations in LLM-Powered Code Generation", "authors": "Fang Liu, Yang Liu, Lin Shi, Houkun Huang, Ruifeng Wang, Zhen Yang, Li Zhang, Zhongqi Li, Yuchi Ma", "abstract": "The rise of Large Language Models (LLMs) has significantly advanced many\napplications on software engineering tasks, particularly in code generation.\nDespite the promising performance, LLMs are prone to generate hallucinations,\nwhich means LLMs might produce outputs that deviate from users' intent, exhibit\ninternal inconsistencies, or misalign with the factual knowledge, making the\ndeployment of LLMs potentially risky in a wide range of applications. Existing\nwork mainly focuses on investing the hallucination in the domain of natural\nlanguage generation (NLG), leaving a gap in understanding the types and extent\nof hallucinations in the context of code generation. To bridge the gap, we\nconducted a thematic analysis of the LLM-generated code to summarize and\ncategorize the hallucinations present in it. Our study established a\ncomprehensive taxonomy of hallucinations in LLM-generated code, encompassing 5\nprimary categories of hallucinations depending on the conflicting objectives\nand varying degrees of deviation observed in code generation. Furthermore, we\nsystematically analyzed the distribution of hallucinations, exploring\nvariations among different LLMs and their correlation with code correctness.\nBased on the results, we proposed HalluCode, a benchmark for evaluating the\nperformance of code LLMs in recognizing hallucinations. Hallucination\nrecognition and mitigation experiments with HalluCode and HumanEval show\nexisting LLMs face great challenges in recognizing hallucinations, particularly\nin identifying their types, and are hardly able to mitigate hallucinations. We\nbelieve our findings will shed light on future research about hallucination\nevaluation, detection, and mitigation, ultimately paving the way for building\nmore effective and reliable code LLMs in the future.", "journal": ""}
{"doi": "10.48550/arXiv.2404.14233", "date": "2024-04-22", "title": "Detecting and Mitigating Hallucination in Large Vision Language Models via Fine-Grained AI Feedback", "authors": "Wenyi Xiao, Ziwei Huang, Leilei Gan, Wanggui He, Haoyuan Li, Zhelun Yu, Fangxun Shu, Hao Jiang, Linchao Zhu", "abstract": "The rapidly developing Large Vision Language Models (LVLMs) have shown\nnotable capabilities on a range of multi-modal tasks, but still face the\nhallucination phenomena where the generated texts do not align with the given\ncontexts, significantly restricting the usages of LVLMs. Most previous work\ndetects and mitigates hallucination at the coarse-grained level or requires\nexpensive annotation (e.g., labeling by proprietary models or human experts).\nTo address these issues, we propose detecting and mitigating hallucinations in\nLVLMs via fine-grained AI feedback. The basic idea is that we generate a\nsmall-size sentence-level hallucination annotation dataset by proprietary\nmodels, whereby we train a hallucination detection model which can perform\nsentence-level hallucination detection, covering primary hallucination types\n(i.e., object, attribute, and relationship). Then, we propose a\ndetect-then-rewrite pipeline to automatically construct preference dataset for\ntraining hallucination mitigating model. Furthermore, we propose\ndifferentiating the severity of hallucinations, and introducing a Hallucination\nSeverity-Aware Direct Preference Optimization (HSA-DPO) for mitigating\nhallucination in LVLMs by incorporating the severity of hallucinations into\npreference learning. Extensive experiments demonstrate the effectiveness of our\nmethod.", "journal": ""}
{"doi": "10.48550/arXiv.2406.07070", "date": "2024-06-11", "title": "HalluDial: A Large-Scale Benchmark for Automatic Dialogue-Level Hallucination Evaluation", "authors": "Wen Luo, Tianshu Shen, Wei Li, Guangyue Peng, Richeng Xuan, Houfeng Wang, Xi Yang", "abstract": "Large Language Models (LLMs) have significantly advanced the field of Natural\nLanguage Processing (NLP), achieving remarkable performance across diverse\ntasks and enabling widespread real-world applications. However, LLMs are prone\nto hallucination, generating content that either conflicts with established\nknowledge or is unfaithful to the original sources. Existing hallucination\nbenchmarks primarily focus on sentence- or passage-level hallucination\ndetection, neglecting dialogue-level evaluation, hallucination localization,\nand rationale provision. They also predominantly target factuality\nhallucinations while underestimating faithfulness hallucinations, often relying\non labor-intensive or non-specialized evaluators. To address these limitations,\nwe propose HalluDial, the first comprehensive large-scale benchmark for\nautomatic dialogue-level hallucination evaluation. HalluDial encompasses both\nspontaneous and induced hallucination scenarios, covering factuality and\nfaithfulness hallucinations. The benchmark includes 4,094 dialogues with a\ntotal of 146,856 samples. Leveraging HalluDial, we conduct a comprehensive\nmeta-evaluation of LLMs' hallucination evaluation capabilities in\ninformation-seeking dialogues and introduce a specialized judge language model,\nHalluJudge. The high data quality of HalluDial enables HalluJudge to achieve\nsuperior or competitive performance in hallucination evaluation, facilitating\nthe automatic assessment of dialogue-level hallucinations in LLMs and providing\nvaluable insights into this phenomenon. The dataset and the code are available\nat https://github.com/FlagOpen/HalluDial.", "journal": ""}
{"doi": "10.48550/arXiv.2410.23114", "date": "2024-10-30", "title": "Unified Triplet-Level Hallucination Evaluation for Large Vision-Language Models", "authors": "Junjie Wu, Tsz Ting Chung, Kai Chen, Dit-Yan Yeung", "abstract": "Despite the outstanding performance in vision-language reasoning, Large\nVision-Language Models (LVLMs) might generate hallucinated contents that do not\nexist in the given image. Most existing LVLM hallucination benchmarks are\nconstrained to evaluate the object-related hallucinations. However, the\npotential hallucination on the relations between two objects, i.e., relation\nhallucination, still lacks investigation. To remedy that, in this paper we\ndesign a unified framework to measure object and relation hallucination in\nLVLMs simultaneously. The core idea of our framework is to conduct\nhallucination evaluation on (object, relation, object) triplets extracted from\nLVLMs' responses, and thus, could be easily generalized to different\nvision-language tasks. Based on our framework, we further introduce Tri-HE, a\nnovel Triplet-level Hallucination Evaluation benchmark which can be used to\nstudy both object and relation hallucination at the same time. We conduct\ncomprehensive evaluations on Tri-HE and observe that the relation hallucination\nissue is even more serious than object hallucination among existing LVLMs,\nhighlighting a previously neglected problem towards reliable LVLMs. Moreover,\nbased on our findings, we design a simple yet effective training-free approach\nto mitigate hallucinations for LVLMs, with which, we exceed all open-sourced\ncounterparts on Tri-HE, achieving comparable performance with the powerful\nGPT-4V. Our dataset and code for the reproduction of our experiments are\navailable publicly at https://github.com/wujunjie1998/Tri-HE.", "journal": ""}
{"doi": "10.48550/arXiv.2206.12529", "date": "2022-06-25", "title": "Probing Causes of Hallucinations in Neural Machine Translations", "authors": "Jianhao Yan, Fandong Meng, Jie Zhou", "abstract": "Hallucination, one kind of pathological translations that bothers Neural\nMachine Translation, has recently drawn much attention. In simple terms,\nhallucinated translations are fluent sentences but barely related to source\ninputs. Arguably, it remains an open problem how hallucination occurs. In this\npaper, we propose to use probing methods to investigate the causes of\nhallucinations from the perspective of model architecture, aiming to avoid such\nproblems in future architecture designs. By conducting experiments over various\nNMT datasets, we find that hallucination is often accompanied by the deficient\nencoder, especially embeddings, and vulnerable cross-attentions, while,\ninterestingly, cross-attention mitigates some errors caused by the encoder.", "journal": ""}
{"doi": "10.48550/arXiv.2406.07239", "date": "2024-06-11", "title": "On the Hallucination in Simultaneous Machine Translation", "authors": "Meizhi Zhong, Kehai Chen, Zhengshan Xue, Lemao Liu, Mingming Yang, Min Zhang", "abstract": "It is widely known that hallucination is a critical issue in Simultaneous\nMachine Translation (SiMT) due to the absence of source-side information. While\nmany efforts have been made to enhance performance for SiMT, few of them\nattempt to understand and analyze hallucination in SiMT. Therefore, we conduct\na comprehensive analysis of hallucination in SiMT from two perspectives:\nunderstanding the distribution of hallucination words and the target-side\ncontext usage of them. Intensive experiments demonstrate some valuable findings\nand particularly show that it is possible to alleviate hallucination by\ndecreasing the over usage of target-side information for SiMT.", "journal": ""}
{"doi": "10.48550/arXiv.2503.05481", "date": "2025-03-07", "title": "Maximum Hallucination Standards for Domain-Specific Large Language Models", "authors": "Tingmingke Lu", "abstract": "Large language models (LLMs) often generate inaccurate yet credible-sounding\ncontent, known as hallucinations. This inherent feature of LLMs poses\nsignificant risks, especially in critical domains. I analyze LLMs as a new\nclass of engineering products, treating hallucinations as a product attribute.\nI demonstrate that, in the presence of imperfect awareness of LLM\nhallucinations and misinformation externalities, net welfare improves when the\nmaximum acceptable level of LLM hallucinations is designed to vary with two\ndomain-specific factors: the willingness to pay for reduced LLM hallucinations\nand the marginal damage associated with misinformation.", "journal": ""}
{"doi": "10.48550/arXiv.2309.05217", "date": "2023-09-11", "title": "Quantifying and Attributing the Hallucination of Large Language Models via Association Analysis", "authors": "Li Du, Yequan Wang, Xingrun Xing, Yiqun Ya, Xiang Li, Xin Jiang, Xuezhi Fang", "abstract": "Although demonstrating superb performance on various NLP tasks, large\nlanguage models (LLMs) still suffer from the hallucination problem, which\nthreatens the reliability of LLMs. To measure the level of hallucination of\nLLMs, previous works first categorize the hallucination according to the\nphenomenon similarity, then quantify the proportion that model outputs contain\nhallucinatory contents. However, such hallucination rates could easily be\ndistorted by confounders. Moreover, such hallucination rates could not reflect\nthe reasons for the hallucination, as similar hallucinatory phenomena may\noriginate from different sources. To address these issues, we propose to\ncombine the hallucination level quantification and hallucination reason\ninvestigation through an association analysis, which builds the relationship\nbetween the hallucination rate of LLMs with a set of risk factors. In this way,\nwe are able to observe the hallucination level under each value of each risk\nfactor, examining the contribution and statistical significance of each risk\nfactor, meanwhile excluding the confounding effect of other factors.\nAdditionally, by recognizing the risk factors according to a taxonomy of model\ncapability, we reveal a set of potential deficiencies in commonsense\nmemorization, relational reasoning, and instruction following, which may\nfurther provide guidance for the pretraining and supervised fine-tuning process\nof LLMs to mitigate the hallucination.", "journal": ""}
{"doi": "10.48550/arXiv.2408.08333", "date": "2024-08-14", "title": "CodeMirage: Hallucinations in Code Generated by Large Language Models", "authors": "Vibhor Agarwal, Yulong Pei, Salwa Alamir, Xiaomo Liu", "abstract": "Large Language Models (LLMs) have shown promising potentials in program\ngeneration and no-code automation. However, LLMs are prone to generate\nhallucinations, i.e., they generate text which sounds plausible but is\nincorrect. Although there has been a recent surge in research on LLM\nhallucinations for text generation, similar hallucination phenomenon can happen\nin code generation. Sometimes the generated code can have syntactical or\nlogical errors as well as more advanced issues like security vulnerabilities,\nmemory leaks, etc. Given the wide adaptation of LLMs to enhance efficiency in\ncode generation and development in general, it becomes imperative to\ninvestigate hallucinations in code generation. To the best of our knowledge,\nthis is the first attempt at studying hallucinations in the code generated by\nLLMs. We start by introducing the code hallucination definition and a\ncomprehensive taxonomy of code hallucination types. We propose the first\nbenchmark CodeMirage dataset for code hallucinations. The benchmark contains\n1,137 GPT-3.5 generated hallucinated code snippets for Python programming\nproblems from two base datasets - HumanEval and MBPP. We then propose the\nmethodology for code hallucination detection and experiment with open source\nLLMs such as CodeLLaMA as well as OpenAI's GPT-3.5 and GPT-4 models using\none-shot prompt. We find that GPT-4 performs the best on HumanEval dataset and\ngives comparable results to the fine-tuned CodeBERT baseline on MBPP dataset.\nTowards the end, we discuss various mitigation strategies for code\nhallucinations and conclude our work.", "journal": ""}
{"doi": "10.48550/arXiv.2409.19492", "date": "2024-09-29", "title": "MedHalu: Hallucinations in Responses to Healthcare Queries by Large Language Models", "authors": "Vibhor Agarwal, Yiqiao Jin, Mohit Chandra, Munmun De Choudhury, Srijan Kumar, Nishanth Sastry", "abstract": "The remarkable capabilities of large language models (LLMs) in language\nunderstanding and generation have not rendered them immune to hallucinations.\nLLMs can still generate plausible-sounding but factually incorrect or\nfabricated information. As LLM-empowered chatbots become popular, laypeople may\nfrequently ask health-related queries and risk falling victim to these LLM\nhallucinations, resulting in various societal and healthcare implications. In\nthis work, we conduct a pioneering study of hallucinations in LLM-generated\nresponses to real-world healthcare queries from patients. We propose MedHalu, a\ncarefully crafted first-of-its-kind medical hallucination dataset with a\ndiverse range of health-related topics and the corresponding hallucinated\nresponses from LLMs with labeled hallucination types and hallucinated text\nspans. We also introduce MedHaluDetect framework to evaluate capabilities of\nvarious LLMs in detecting hallucinations. We also employ three groups of\nevaluators -- medical experts, LLMs, and laypeople -- to study who are more\nvulnerable to these medical hallucinations. We find that LLMs are much worse\nthan the experts. They also perform no better than laypeople and even worse in\nfew cases in detecting hallucinations. To fill this gap, we propose\nexpert-in-the-loop approach to improve hallucination detection through LLMs by\ninfusing expert reasoning. We observe significant performance gains for all the\nLLMs with an average macro-F1 improvement of 6.3 percentage points for GPT-4.", "journal": ""}
{"doi": "10.48550/arXiv.2410.06304", "date": "2024-10-08", "title": "FG-PRM: Fine-grained Hallucination Detection and Mitigation in Language Model Mathematical Reasoning", "authors": "Ruosen Li, Ziming Luo, Xinya Du", "abstract": "Hallucinations in large language models (LLMs) pose significant challenges in\ntasks requiring complex multi-step reasoning, such as mathematical\nproblem-solving. Existing approaches primarily detect the presence of\nhallucinations but lack a nuanced understanding of their types and\nmanifestations. In this paper, we first introduce a comprehensive taxonomy that\ncategorizes the common hallucinations in mathematical reasoning task into six\ntypes: fabrication, factual inconsistency, context inconsistency, instruction\ninconsistency, logical inconsistency, and logical error. We then propose FG-PRM\n(Fine-Grained Process Reward Model), an augmented model designed to detect and\nmitigate hallucinations in a fine-grained, step-level manner. To address the\nlimitations of manually labeling training data, we propose an automated method\nfor generating fine-grained hallucination data using LLMs. By injecting\nhallucinations into reasoning steps of correct solutions, we create a diverse\nand balanced synthetic dataset for training FG-PRM, which consists of six\nspecialized Process Reward Models (PRMs), each tailored to detect a specific\nhallucination type. Our FG-PRM demonstrates superior performance across two key\ntasks: 1) Fine-grained hallucination detection: classifying hallucination types\nfor each reasoning step; and 2) Verification: ranking multiple LLM-generated\noutputs to select the most accurate solution, mitigating reasoning\nhallucinations. Our experiments show that FG-PRM outperforms ChatGPT-3.5 and\nClaude-3 on fine-grained hallucination detection and substantially boosts the\nperformance of LLMs on GSM8K and MATH benchmarks.", "journal": ""}
{"doi": "10.48550/arXiv.2502.12769", "date": "2025-02-18", "title": "How Much Do LLMs Hallucinate across Languages? On Multilingual Estimation of LLM Hallucination in the Wild", "authors": "Saad Obaid ul Islam, Anne Lauscher, Goran Glava\u0161", "abstract": "In the age of misinformation, hallucination -- the tendency of Large Language\nModels (LLMs) to generate non-factual or unfaithful responses -- represents the\nmain risk for their global utility. Despite LLMs becoming increasingly\nmultilingual, the vast majority of research on detecting and quantifying LLM\nhallucination are (a) English-centric and (b) focus on machine translation (MT)\nand summarization, tasks that are less common ``in the wild'' than open\ninformation seeking. In contrast, we aim to quantify the extent of LLM\nhallucination across languages in knowledge-intensive long-form question\nanswering. To this end, we train a multilingual hallucination detection model\nand conduct a large-scale study across 30 languages and 6 open-source LLM\nfamilies. We start from an English hallucination detection dataset and rely on\nMT to generate (noisy) training data in other languages. We also manually\nannotate gold data for five high-resource languages; we then demonstrate, for\nthese languages, that the estimates of hallucination rates are similar between\nsilver (LLM-generated) and gold test sets, validating the use of silver data\nfor estimating hallucination rates for other languages. For the final rates\nestimation, we build a knowledge-intensive QA dataset for 30 languages with\nLLM-generated prompts and Wikipedia articles as references. We find that, while\nLLMs generate longer responses with more hallucinated tokens for\nhigher-resource languages, there is no correlation between length-normalized\nhallucination rates of languages and their digital representation. Further, we\nfind that smaller LLMs exhibit larger hallucination rates than larger models.", "journal": ""}
{"doi": "10.48550/arXiv.2505.24238", "date": "2025-05-30", "title": "MIRAGE: Assessing Hallucination in Multimodal Reasoning Chains of MLLM", "authors": "Bowen Dong, Minheng Ni, Zitong Huang, Guanglei Yang, Wangmeng Zuo, Lei Zhang", "abstract": "Multimodal hallucination in multimodal large language models (MLLMs)\nrestricts the correctness of MLLMs. However, multimodal hallucinations are\nmulti-sourced and arise from diverse causes. Existing benchmarks fail to\nadequately distinguish between perception-induced hallucinations and\nreasoning-induced hallucinations. This failure constitutes a significant issue\nand hinders the diagnosis of multimodal reasoning failures within MLLMs. To\naddress this, we propose the {\\dataset} benchmark, which isolates reasoning\nhallucinations by constructing questions where input images are correctly\nperceived by MLLMs yet reasoning errors persist. {\\dataset} introduces\nmulti-granular evaluation metrics: accuracy, factuality, and LLMs hallucination\nscore for hallucination quantification. Our analysis reveals that (1) the model\nscale, data scale, and training stages significantly affect the degree of\nlogical, fabrication, and factual hallucinations; (2) current MLLMs show no\neffective improvement on spatial hallucinations caused by misinterpreted\nspatial relationships, indicating their limited visual reasoning capabilities;\nand (3) question types correlate with distinct hallucination patterns,\nhighlighting targeted challenges and potential mitigation strategies. To\naddress these challenges, we propose {\\method}, a method that combines\ncurriculum reinforcement fine-tuning to encourage models to generate\nlogic-consistent reasoning chains by stepwise reducing learning difficulty, and\ncollaborative hint inference to reduce reasoning complexity. {\\method}\nestablishes a baseline on {\\dataset}, and reduces the logical hallucinations in\noriginal base models.", "journal": ""}
{"doi": "10.48550/arXiv.2210.07688", "date": "2022-10-14", "title": "Plausible May Not Be Faithful: Probing Object Hallucination in Vision-Language Pre-training", "authors": "Wenliang Dai, Zihan Liu, Ziwei Ji, Dan Su, Pascale Fung", "abstract": "Large-scale vision-language pre-trained (VLP) models are prone to hallucinate\nnon-existent visual objects when generating text based on visual information.\nIn this paper, we systematically study the object hallucination problem from\nthree aspects. First, we examine recent state-of-the-art VLP models, showing\nthat they still hallucinate frequently, and models achieving better scores on\nstandard metrics (e.g., CIDEr) could be more unfaithful. Second, we investigate\nhow different types of image encoding in VLP influence hallucination, including\nregion-based, grid-based, and patch-based. Surprisingly, we find that\npatch-based features perform the best and smaller patch resolution yields a\nnon-trivial reduction in object hallucination. Third, we decouple various VLP\nobjectives and demonstrate that token-level image-text alignment and controlled\ngeneration are crucial to reducing hallucination. Based on that, we propose a\nsimple yet effective VLP loss named ObjMLM to further mitigate object\nhallucination. Results show that it reduces object hallucination by up to 17.4%\nwhen tested on two benchmarks (COCO Caption for in-domain and NoCaps for\nout-of-domain evaluation).", "journal": ""}
{"doi": "10.48550/arXiv.2401.11817", "date": "2024-01-22", "title": "Hallucination is Inevitable: An Innate Limitation of Large Language Models", "authors": "Ziwei Xu, Sanjay Jain, Mohan Kankanhalli", "abstract": "Hallucination has been widely recognized to be a significant drawback for\nlarge language models (LLMs). There have been many works that attempt to reduce\nthe extent of hallucination. These efforts have mostly been empirical so far,\nwhich cannot answer the fundamental question whether it can be completely\neliminated. In this paper, we formalize the problem and show that it is\nimpossible to eliminate hallucination in LLMs. Specifically, we define a formal\nworld where hallucination is defined as inconsistencies between a computable\nLLM and a computable ground truth function. By employing results from learning\ntheory, we show that LLMs cannot learn all the computable functions and will\ntherefore inevitably hallucinate if used as general problem solvers. Since the\nformal world is a part of the real world which is much more complicated,\nhallucinations are also inevitable for real world LLMs. Furthermore, for real\nworld LLMs constrained by provable time complexity, we describe the\nhallucination-prone tasks and empirically validate our claims. Finally, using\nthe formal world framework, we discuss the possible mechanisms and efficacies\nof existing hallucination mitigators as well as the practical implications on\nthe safe deployment of LLMs.", "journal": ""}
{"doi": "10.48550/arXiv.2403.18167", "date": "2024-03-27", "title": "Mechanistic Understanding and Mitigation of Language Model Non-Factual Hallucinations", "authors": "Lei Yu, Meng Cao, Jackie Chi Kit Cheung, Yue Dong", "abstract": "State-of-the-art language models (LMs) sometimes generate non-factual\nhallucinations that misalign with world knowledge. To explore the mechanistic\ncauses of these hallucinations, we create diagnostic datasets with\nsubject-relation queries and adapt interpretability methods to trace\nhallucinations through internal model representations. We discover two general\nand distinct mechanistic causes of hallucinations shared across LMs (Llama-2,\nPythia, GPT-J): 1) knowledge enrichment hallucinations: insufficient subject\nattribute knowledge in lower layer MLPs, and 2) answer extraction\nhallucinations: failure to select the correct object attribute in upper layer\nattention heads. We also found these two internal mechanistic causes of\nhallucinations are reflected in external manifestations. Based on insights from\nour mechanistic analysis, we propose a novel hallucination mitigation method\nthrough targeted restoration of the LM's internal fact recall pipeline,\ndemonstrating superior performance compared to baselines.", "journal": ""}
{"doi": "10.48550/arXiv.2407.09417", "date": "2024-07-12", "title": "Mitigating Entity-Level Hallucination in Large Language Models", "authors": "Weihang Su, Yichen Tang, Qingyao Ai, Changyue Wang, Zhijing Wu, Yiqun Liu", "abstract": "The emergence of Large Language Models (LLMs) has revolutionized how users\naccess information, shifting from traditional search engines to direct\nquestion-and-answer interactions with LLMs. However, the widespread adoption of\nLLMs has revealed a significant challenge known as hallucination, wherein LLMs\ngenerate coherent yet factually inaccurate responses. This hallucination\nphenomenon has led to users' distrust in information retrieval systems based on\nLLMs. To tackle this challenge, this paper proposes Dynamic Retrieval\nAugmentation based on hallucination Detection (DRAD) as a novel method to\ndetect and mitigate hallucinations in LLMs. DRAD improves upon traditional\nretrieval augmentation by dynamically adapting the retrieval process based on\nreal-time hallucination detection. It features two main components: Real-time\nHallucination Detection (RHD) for identifying potential hallucinations without\nexternal models, and Self-correction based on External Knowledge (SEK) for\ncorrecting these errors using external knowledge. Experiment results show that\nDRAD demonstrates superior performance in both detecting and mitigating\nhallucinations in LLMs. All of our code and data are open-sourced at\nhttps://github.com/oneal2000/EntityHallucination.", "journal": ""}
{"doi": "10.48550/arXiv.2407.12943", "date": "2024-07-17", "title": "Halu-J: Critique-Based Hallucination Judge", "authors": "Binjie Wang, Steffi Chern, Ethan Chern, Pengfei Liu", "abstract": "Large language models (LLMs) frequently generate non-factual content, known\nas hallucinations. Existing retrieval-augmented-based hallucination detection\napproaches typically address this by framing it as a classification task,\nevaluating hallucinations based on their consistency with retrieved evidence.\nHowever, this approach usually lacks detailed explanations for these\nevaluations and does not assess the reliability of these explanations.\nFurthermore, deficiencies in retrieval systems can lead to irrelevant or\npartially relevant evidence retrieval, impairing the detection process.\nMoreover, while real-world hallucination detection requires analyzing multiple\npieces of evidence, current systems usually treat all evidence uniformly\nwithout considering its relevance to the content. To address these challenges,\nwe introduce Halu-J, a critique-based hallucination judge with 7 billion\nparameters. Halu-J enhances hallucination detection by selecting pertinent\nevidence and providing detailed critiques. Our experiments indicate that Halu-J\noutperforms GPT-4o in multiple-evidence hallucination detection and matches its\ncapability in critique generation and evidence selection. We also introduce\nME-FEVER, a new dataset designed for multiple-evidence hallucination detection.\nOur code and dataset can be found in https://github.com/GAIR-NLP/factool .", "journal": ""}
{"doi": "10.48550/arXiv.2408.09429", "date": "2024-08-18", "title": "Reefknot: A Comprehensive Benchmark for Relation Hallucination Evaluation, Analysis and Mitigation in Multimodal Large Language Models", "authors": "Kening Zheng, Junkai Chen, Yibo Yan, Xin Zou, Xuming Hu", "abstract": "Hallucination issues continue to affect multimodal large language models\n(MLLMs), with existing research mainly addressing object-level or\nattribute-level hallucinations, neglecting the more complex relation\nhallucinations that require advanced reasoning. Current benchmarks for relation\nhallucinations lack detailed evaluation and effective mitigation, and their\ndatasets often suffer from biases due to systematic annotation processes. To\naddress these challenges, we introduce Reefknot, a comprehensive benchmark\ntargeting relation hallucinations, comprising over 20,000 real-world samples.\nWe provide a systematic definition of relation hallucinations, integrating\nperceptive and cognitive perspectives, and construct a relation-based corpus\nusing the Visual Genome scene graph dataset. Our comparative evaluation reveals\nsignificant limitations in current MLLMs' ability to handle relation\nhallucinations. Additionally, we propose a novel confidence-based mitigation\nstrategy, which reduces the hallucination rate by an average of 9.75% across\nthree datasets, including Reefknot. Our work offers valuable insights for\nachieving trustworthy multimodal intelligence.", "journal": ""}
{"doi": "10.48550/arXiv.2409.16494", "date": "2024-09-24", "title": "A Unified Hallucination Mitigation Framework for Large Vision-Language Models", "authors": "Yue Chang, Liqiang Jing, Xiaopeng Zhang, Yue Zhang", "abstract": "Hallucination is a common problem for Large Vision-Language Models (LVLMs)\nwith long generations which is difficult to eradicate. The generation with\nhallucinations is partially inconsistent with the image content. To mitigate\nhallucination, current studies either focus on the process of model inference\nor the results of model generation, but the solutions they design sometimes do\nnot deal appropriately with various types of queries and the hallucinations of\nthe generations about these queries. To accurately deal with various\nhallucinations, we present a unified framework, Dentist, for hallucination\nmitigation. The core step is to first classify the queries, then perform\ndifferent processes of hallucination mitigation based on the classification\nresult, just like a dentist first observes the teeth and then makes a plan. In\na simple deployment, Dentist can classify queries as perception or reasoning\nand easily mitigate potential hallucinations in answers which has been\ndemonstrated in our experiments. On MMbench, we achieve a 13.44%/10.2%/15.8%\nimprovement in accuracy on Image Quality, a Coarse Perception visual question\nanswering (VQA) task, over the baseline InstructBLIP/LLaVA/VisualGLM.", "journal": ""}
{"doi": "10.48550/arXiv.2410.03176", "date": "2024-10-04", "title": "Investigating and Mitigating Object Hallucinations in Pretrained Vision-Language (CLIP) Models", "authors": "Yufang Liu, Tao Ji, Changzhi Sun, Yuanbin Wu, Aimin Zhou", "abstract": "Large Vision-Language Models (LVLMs) have achieved impressive performance,\nyet research has pointed out a serious issue with object hallucinations within\nthese models. However, there is no clear conclusion as to which part of the\nmodel these hallucinations originate from. In this paper, we present an\nin-depth investigation into the object hallucination problem specifically\nwithin the CLIP model, which serves as the backbone for many state-of-the-art\nvision-language systems. We unveil that even in isolation, the CLIP model is\nprone to object hallucinations, suggesting that the hallucination problem is\nnot solely due to the interaction between vision and language modalities. To\naddress this, we propose a counterfactual data augmentation method by creating\nnegative samples with a variety of hallucination issues. We demonstrate that\nour method can effectively mitigate object hallucinations for CLIP model, and\nwe show the the enhanced model can be employed as a visual encoder, effectively\nalleviating the object hallucination issue in LVLMs.", "journal": ""}
{"doi": "10.48550/arXiv.2410.11414", "date": "2024-10-15", "title": "ReDeEP: Detecting Hallucination in Retrieval-Augmented Generation via Mechanistic Interpretability", "authors": "Zhongxiang Sun, Xiaoxue Zang, Kai Zheng, Yang Song, Jun Xu, Xiao Zhang, Weijie Yu, Yang Song, Han Li", "abstract": "Retrieval-Augmented Generation (RAG) models are designed to incorporate\nexternal knowledge, reducing hallucinations caused by insufficient parametric\n(internal) knowledge. However, even with accurate and relevant retrieved\ncontent, RAG models can still produce hallucinations by generating outputs that\nconflict with the retrieved information. Detecting such hallucinations requires\ndisentangling how Large Language Models (LLMs) utilize external and parametric\nknowledge. Current detection methods often focus on one of these mechanisms or\nwithout decoupling their intertwined effects, making accurate detection\ndifficult. In this paper, we investigate the internal mechanisms behind\nhallucinations in RAG scenarios. We discover hallucinations occur when the\nKnowledge FFNs in LLMs overemphasize parametric knowledge in the residual\nstream, while Copying Heads fail to effectively retain or integrate external\nknowledge from retrieved content. Based on these findings, we propose ReDeEP, a\nnovel method that detects hallucinations by decoupling LLM's utilization of\nexternal context and parametric knowledge. Our experiments show that ReDeEP\nsignificantly improves RAG hallucination detection accuracy. Additionally, we\nintroduce AARF, which mitigates hallucinations by modulating the contributions\nof Knowledge FFNs and Copying Heads.", "journal": ""}
{"doi": "10.48550/arXiv.2410.22071", "date": "2024-10-29", "title": "Distinguishing Ignorance from Error in LLM Hallucinations", "authors": "Adi Simhi, Jonathan Herzig, Idan Szpektor, Yonatan Belinkov", "abstract": "Large language models (LLMs) are susceptible to hallucinations -- factually\nincorrect outputs -- leading to a large body of work on detecting and\nmitigating such cases. We argue that it is important to distinguish between two\ntypes of hallucinations: ones where the model does not hold the correct answer\nin its parameters, which we term HK-, and ones where the model answers\nincorrectly despite having the required knowledge, termed HK+. We first find\nthat HK+ hallucinations are prevalent and occur across models and datasets.\nThen, we demonstrate that distinguishing between these two cases is beneficial\nfor mitigating hallucinations. Importantly, we show that different models\nhallucinate on different examples, which motivates constructing model-specific\nhallucination datasets for training detectors. Overall, our findings draw\nattention to classifying types of hallucinations and provide means to handle\nthem more effectively. The code is available at\nhttps://github.com/technion-cs-nlp/hallucination-mitigation .", "journal": ""}
{"doi": "10.48550/arXiv.2411.10436", "date": "2024-11-15", "title": "Mitigating Hallucination in Multimodal Large Language Model via Hallucination-targeted Direct Preference Optimization", "authors": "Yuhan Fu, Ruobing Xie, Xingwu Sun, Zhanhui Kang, Xirong Li", "abstract": "Multimodal Large Language Models (MLLMs) are known to hallucinate, which\nlimits their practical applications. Recent works have attempted to apply\nDirect Preference Optimization (DPO) to enhance the performance of MLLMs, but\nhave shown inconsistent improvements in mitigating hallucinations. To address\nthis issue more effectively, we introduce Hallucination-targeted Direct\nPreference Optimization (HDPO) to reduce hallucinations in MLLMs. Unlike\nprevious approaches, our method tackles hallucinations from their diverse forms\nand causes. Specifically, we develop three types of preference pair data\ntargeting the following causes of MLLM hallucinations: (1) insufficient visual\ncapabilities, (2) long context generation, and (3) multimodal conflicts.\nExperimental results demonstrate that our method achieves superior performance\nacross multiple hallucination evaluation datasets, surpassing most\nstate-of-the-art (SOTA) methods and highlighting the potential of our approach.\nAblation studies and in-depth analyses further confirm the effectiveness of our\nmethod and suggest the potential for further improvements through scaling up.", "journal": ""}
{"doi": "10.48550/arXiv.2412.06007", "date": "2024-12-08", "title": "Hallucination-aware Optimization for Large Language Model-empowered Communications", "authors": "Yinqiu Liu, Guangyuan Liu, Ruichen Zhang, Dusit Niyato, Zehui Xiong, Dong In Kim, Kaibin Huang, Hongyang Du", "abstract": "Large Language Models (LLMs) have significantly advanced communications\nfields, such as Telecom Q\\&A, mathematical modeling, and coding. However, LLMs\nencounter an inherent issue known as hallucination, i.e., generating\nfact-conflicting or irrelevant content. This problem critically undermines the\napplicability of LLMs in communication systems yet has not been systematically\nexplored. Hence, this paper provides a comprehensive review of LLM applications\nin communications, with a particular emphasis on hallucination mitigation.\nSpecifically, we analyze hallucination causes and summarize hallucination\nmitigation strategies from both model- and system-based perspectives.\nAfterward, we review representative LLM-empowered communication schemes,\ndetailing potential hallucination scenarios and comparing the mitigation\nstrategies they adopted. Finally, we present a case study of a Telecom-oriented\nLLM that utilizes a novel hybrid approach to enhance the hallucination-aware\nservice experience. On the model side, we publish a Telecom hallucination\ndataset and apply direct preference optimization to fine-tune LLMs, resulting\nin a 20.6\\% correct rate improvement. Moreover, we construct a mobile-edge\nmixture-of-experts architecture for optimal LLM expert activation. Our research\naims to propel the field of LLM-empowered communications forward by detecting\nand minimizing hallucination impacts.", "journal": ""}
{"doi": "10.48550/arXiv.2501.11378", "date": "2025-01-20", "title": "Investigation of Whisper ASR Hallucinations Induced by Non-Speech Audio", "authors": "Mateusz Bara\u0144ski, Jan Jasi\u0144ski, Julitta Bartolewska, Stanis\u0142aw Kacprzak, Marcin Witkowski, Konrad Kowalczyk", "abstract": "Hallucinations of deep neural models are amongst key challenges in automatic\nspeech recognition (ASR). In this paper, we investigate hallucinations of the\nWhisper ASR model induced by non-speech audio segments present during\ninference. By inducting hallucinations with various types of sounds, we show\nthat there exists a set of hallucinations that appear frequently. We then study\nhallucinations caused by the augmentation of speech with such sounds. Finally,\nwe describe the creation of a bag of hallucinations (BoH) that allows to remove\nthe effect of hallucinations through the post-processing of text\ntranscriptions. The results of our experiments show that such post-processing\nis capable of reducing word error rate (WER) and acts as a good safeguard\nagainst problematic hallucinations.", "journal": "ICASSP 2025 - 2025 IEEE International Conference on Acoustics,\n  Speech and Signal Processing (ICASSP), Hyderabad, India, 2025"}
{"doi": "10.48550/arXiv.2501.17295", "date": "2025-01-28", "title": "Mitigating Hallucinated Translations in Large Language Models with Hallucination-focused Preference Optimization", "authors": "Zilu Tang, Rajen Chatterjee, Sarthak Garg", "abstract": "Machine Translation (MT) is undergoing a paradigm shift, with systems based\non fine-tuned large language models (LLM) becoming increasingly competitive\nwith traditional encoder-decoder models trained specifically for translation\ntasks. However, LLM-based systems are at a higher risk of generating\nhallucinations, which can severely undermine user's trust and safety. Most\nprior research on hallucination mitigation focuses on traditional MT models,\nwith solutions that involve post-hoc mitigation - detecting hallucinated\ntranslations and re-translating them. While effective, this approach introduces\nadditional complexity in deploying extra tools in production and also increases\nlatency. To address these limitations, we propose a method that intrinsically\nlearns to mitigate hallucinations during the model training phase.\nSpecifically, we introduce a data creation framework to generate hallucination\nfocused preference datasets. Fine-tuning LLMs on these preference datasets\nreduces the hallucination rate by an average of 96% across five language pairs,\nwhile preserving overall translation quality. In a zero-shot setting our\napproach reduces hallucinations by 89% on an average across three unseen target\nlanguages.", "journal": "NAACL 2025"}
{"doi": "10.48550/arXiv.2502.11948", "date": "2025-02-17", "title": "Can Your Uncertainty Scores Detect Hallucinated Entity?", "authors": "Min-Hsuan Yeh, Max Kamachee, Seongheon Park, Yixuan Li", "abstract": "To mitigate the impact of hallucination nature of LLMs, many studies propose\ndetecting hallucinated generation through uncertainty estimation. However,\nthese approaches predominantly operate at the sentence or paragraph level,\nfailing to pinpoint specific spans or entities responsible for hallucinated\ncontent. This lack of granularity is especially problematic for long-form\noutputs that mix accurate and fabricated information. To address this\nlimitation, we explore entity-level hallucination detection. We propose a new\ndata set, HalluEntity, which annotates hallucination at the entity level. Based\non the dataset, we comprehensively evaluate uncertainty-based hallucination\ndetection approaches across 17 modern LLMs. Our experimental results show that\nuncertainty estimation approaches focusing on individual token probabilities\ntend to over-predict hallucinations, while context-aware methods show better\nbut still suboptimal performance. Through an in-depth qualitative study, we\nidentify relationships between hallucination tendencies and linguistic\nproperties and highlight important directions for future research. HalluEntity:\nhttps://huggingface.co/datasets/samuelyeh/HalluEntity", "journal": ""}
{"doi": "10.48550/arXiv.2503.23573", "date": "2025-03-30", "title": "DASH: Detection and Assessment of Systematic Hallucinations of VLMs", "authors": "Maximilian Augustin, Yannic Neuhaus, Matthias Hein", "abstract": "Vision-language models (VLMs) are prone to object hallucinations, where they\nerroneously indicate the presenceof certain objects in an image. Existing\nbenchmarks quantify hallucinations using relatively small, labeled datasets.\nHowever, this approach is i) insufficient to assess hallucinations that arise\nin open-world settings, where VLMs are widely used, and ii) inadequate for\ndetecting systematic errors in VLMs. We propose DASH (Detection and Assessment\nof Systematic Hallucinations), an automatic, large-scale pipeline designed to\nidentify systematic hallucinations of VLMs on real-world images in an\nopen-world setting. A key component is DASH-OPT for image-based retrieval,\nwhere we optimize over the ''natural image manifold'' to generate images that\nmislead the VLM. The output of DASH consists of clusters of real and\nsemantically similar images for which the VLM hallucinates an object. We apply\nDASH to PaliGemma and two LLaVA-NeXT models across 380 object classes and, in\ntotal, find more than 19k clusters with 950k images. We study the transfer of\nthe identified systematic hallucinations to other VLMs and show that\nfine-tuning PaliGemma with the model-specific images obtained with DASH\nmitigates object hallucinations. Code and data are available at\nhttps://YanNeu.github.io/DASH.", "journal": ""}
{"doi": "10.48550/arXiv.2504.07863", "date": "2025-04-10", "title": "Robust Hallucination Detection in LLMs via Adaptive Token Selection", "authors": "Mengjia Niu, Hamed Haddadi, Guansong Pang", "abstract": "Hallucinations in large language models (LLMs) pose significant safety\nconcerns that impede their broader deployment. Recent research in hallucination\ndetection has demonstrated that LLMs' internal representations contain\ntruthfulness hints, which can be harnessed for detector training. However, the\nperformance of these detectors is heavily dependent on the internal\nrepresentations of predetermined tokens, fluctuating considerably when working\non free-form generations with varying lengths and sparse distributions of\nhallucinated entities. To address this, we propose HaMI, a novel approach that\nenables robust detection of hallucinations through adaptive selection and\nlearning of critical tokens that are most indicative of hallucinations. We\nachieve this robustness by an innovative formulation of the Hallucination\ndetection task as Multiple Instance (HaMI) learning over token-level\nrepresentations within a sequence, thereby facilitating a joint optimisation of\ntoken selection and hallucination detection on generation sequences of diverse\nforms. Comprehensive experimental results on four hallucination benchmarks show\nthat HaMI significantly outperforms existing state-of-the-art approaches.", "journal": ""}
{"doi": "10.48550/arXiv.2504.12314", "date": "2025-04-10", "title": "How to Detect and Defeat Molecular Mirage: A Metric-Driven Benchmark for Hallucination in LLM-based Molecular Comprehension", "authors": "Hao Li, Liuzhenghao Lv, He Cao, Zijing Liu, Zhiyuan Yan, Yu Wang, Yonghong Tian, Yu Li, Li Yuan", "abstract": "Large language models are increasingly used in scientific domains, especially\nfor molecular understanding and analysis. However, existing models are affected\nby hallucination issues, resulting in errors in drug design and utilization. In\nthis paper, we first analyze the sources of hallucination in LLMs for molecular\ncomprehension tasks, specifically the knowledge shortcut phenomenon observed in\nthe PubChem dataset. To evaluate hallucination in molecular comprehension tasks\nwith computational efficiency, we introduce \\textbf{Mol-Hallu}, a novel\nfree-form evaluation metric that quantifies the degree of hallucination based\non the scientific entailment relationship between generated text and actual\nmolecular properties. Utilizing the Mol-Hallu metric, we reassess and analyze\nthe extent of hallucination in various LLMs performing molecular comprehension\ntasks. Furthermore, the Hallucination Reduction Post-processing stage~(HRPP) is\nproposed to alleviate molecular hallucinations, Experiments show the\neffectiveness of HRPP on decoder-only and encoder-decoder molecular LLMs. Our\nfindings provide critical insights into mitigating hallucination and improving\nthe reliability of LLMs in scientific applications.", "journal": ""}
{"doi": "10.48550/arXiv.2505.00557", "date": "2025-05-01", "title": "Triggering Hallucinations in LLMs: A Quantitative Study of Prompt-Induced Hallucination in Large Language Models", "authors": "Makoto Sato", "abstract": "Hallucinations in large language models (LLMs) present a growing challenge\nacross real-world applications, from healthcare to law, where factual\nreliability is essential. Despite advances in alignment and instruction tuning,\nLLMs can still generate outputs that are fluent yet fundamentally untrue.\nUnderstanding the cognitive dynamics that underlie these hallucinations remains\nan open problem. In this study, we propose a prompt-based framework to\nsystematically trigger and quantify hallucination: a Hallucination-Inducing\nPrompt (HIP), which synthetically fuses semantically distant concepts (e.g.,\nperiodic table of elements and tarot divination) in a misleading way, and a\nHallucination Quantifying Prompt (HQP), which scores the plausibility,\nconfidence, and coherence of the output. Controlled experiments across multiple\nLLMs revealed that HIPs consistently produced less coherent and more\nhallucinated responses than their null-fusion controls. These effects varied\nacross models, with reasoning-oriented LLMs showing distinct profiles from\ngeneral-purpose ones. Our framework provides a reproducible testbed for\nstudying hallucination vulnerability, and opens the door to developing safer,\nmore introspective LLMs that can detect and self-regulate the onset of\nconceptual instability.", "journal": ""}
{"doi": "10.48550/arXiv.2505.07528", "date": "2025-05-12", "title": "SEReDeEP: Hallucination Detection in Retrieval-Augmented Models via Semantic Entropy and Context-Parameter Fusion", "authors": "Lei Wang", "abstract": "Retrieval-Augmented Generation (RAG) models frequently encounter\nhallucination phenomena when integrating external information with internal\nparametric knowledge. Empirical studies demonstrate that the disequilibrium\nbetween external contextual information and internal parametric knowledge\nconstitutes a primary factor in hallucination generation. Existing\nhallucination detection methodologies predominantly emphasize either the\nexternal or internal mechanism in isolation, thereby overlooking their\nsynergistic effects. The recently proposed ReDeEP framework decouples these\ndual mechanisms, identifying two critical contributors to hallucinations:\nexcessive reliance on parametric knowledge encoded in feed-forward networks\n(FFN) and insufficient utilization of external information by attention\nmechanisms (particularly copy heads). ReDeEP quantitatively assesses these\nfactors to detect hallucinations and dynamically modulates the contributions of\nFFNs and copy heads to attenuate their occurrence. Nevertheless, ReDeEP and\nnumerous other hallucination detection approaches have been employed at\nlogit-level uncertainty estimation or language-level self-consistency\nevaluation, inadequately address the semantic dimensions of model responses,\nresulting in inconsistent hallucination assessments in RAG implementations.\nBuilding upon ReDeEP's foundation, this paper introduces SEReDeEP, which\nenhances computational processes through semantic entropy captured via trained\nlinear probes, thereby achieving hallucination assessments that more accurately\nreflect ground truth evaluations.", "journal": ""}
{"doi": "10.48550/arXiv.2505.12826", "date": "2025-05-19", "title": "Mitigating Hallucination in VideoLLMs via Temporal-Aware Activation Engineering", "authors": "Jianfeng Cai, Wengang Zhou, Zongmeng Zhang, Jiale Hong, Nianji Zhan, Houqiang Li", "abstract": "Multimodal large language models (MLLMs) have achieved remarkable progress in\nvideo understanding.However, hallucination, where the model generates plausible\nyet incorrect outputs, persists as a significant and under-addressed challenge\nin the video domain. Among existing solutions, activation engineering has\nproven successful in mitigating hallucinations in LLMs and ImageLLMs, yet its\napplicability to VideoLLMs remains largely unexplored. In this work, we are the\nfirst to systematically investigate the effectiveness and underlying mechanisms\nof activation engineering for mitigating hallucinations in VideoLLMs. We\ninitially conduct an investigation of the key factors affecting the performance\nof activation engineering and find that a model's sensitivity to hallucination\ndepends on $\\textbf{temporal variation}$ rather than task type. Moreover,\nselecting appropriate internal modules and dataset for activation engineering\nis critical for reducing hallucination. Guided by these findings, we propose a\ntemporal-aware activation engineering framework for VideoLLMs, which adaptively\nidentifies and manipulates hallucination-sensitive modules based on the\ntemporal variation characteristic, substantially mitigating hallucinations\nwithout additional LLM fine-tuning. Experiments across multiple models and\nbenchmarks demonstrate that our method markedly reduces hallucination in\nVideoLLMs, thereby validating the robustness of our findings.", "journal": ""}
{"doi": "10.48550/arXiv.2403.16167", "date": "2024-03-24", "title": "ESREAL: Exploiting Semantic Reconstruction to Mitigate Hallucinations in Vision-Language Models", "authors": "Minchan Kim, Minyeong Kim, Junik Bae, Suhwan Choi, Sungkyung Kim, Buru Chang", "abstract": "Hallucinations in vision-language models pose a significant challenge to\ntheir reliability, particularly in the generation of long captions. Current\nmethods fall short of accurately identifying and mitigating these\nhallucinations. To address this issue, we introduce ESREAL, a novel\nunsupervised learning framework designed to suppress the generation of\nhallucinations through accurate localization and penalization of hallucinated\ntokens. Initially, ESREAL creates a reconstructed image based on the generated\ncaption and aligns its corresponding regions with those of the original image.\nThis semantic reconstruction aids in identifying both the presence and type of\ntoken-level hallucinations within the generated caption. Subsequently, ESREAL\ncomputes token-level hallucination scores by assessing the semantic similarity\nof aligned regions based on the type of hallucination. Finally, ESREAL employs\na proximal policy optimization algorithm, where it selectively penalizes\nhallucinated tokens according to their token-level hallucination scores. Our\nframework notably reduces hallucinations in LLaVA, InstructBLIP, and mPLUG-Owl2\nby 32.81%, 27.08%, and 7.46% on the CHAIR metric. This improvement is achieved\nsolely through signals derived from the image itself, without the need for any\nimage-text pairs.", "journal": ""}
{"doi": "10.48550/arXiv.2404.03745", "date": "2024-04-04", "title": "Fakes of Varying Shades: How Warning Affects Human Perception and Engagement Regarding LLM Hallucinations", "authors": "Mahjabin Nahar, Haeseung Seo, Eun-Ju Lee, Aiping Xiong, Dongwon Lee", "abstract": "The widespread adoption and transformative effects of large language models\n(LLMs) have sparked concerns regarding their capacity to produce inaccurate and\nfictitious content, referred to as `hallucinations'. Given the potential risks\nassociated with hallucinations, humans should be able to identify them. This\nresearch aims to understand the human perception of LLM hallucinations by\nsystematically varying the degree of hallucination (genuine, minor\nhallucination, major hallucination) and examining its interaction with warning\n(i.e., a warning of potential inaccuracies: absent vs. present). Participants\n(N=419) from Prolific rated the perceived accuracy and engaged with content\n(e.g., like, dislike, share) in a Q/A format. Participants ranked content as\ntruthful in the order of genuine, minor hallucination, and major hallucination,\nand user engagement behaviors mirrored this pattern. More importantly, we\nobserved that warning improved the detection of hallucination without\nsignificantly affecting the perceived truthfulness of genuine content. We\nconclude by offering insights for future tools to aid human detection of\nhallucinations. All survey materials, demographic questions, and post-session\nquestions are available at:\nhttps://github.com/MahjabinNahar/fakes-of-varying-shades-survey-materials", "journal": ""}
{"doi": "10.48550/arXiv.2404.10332", "date": "2024-04-16", "title": "Prescribing the Right Remedy: Mitigating Hallucinations in Large Vision-Language Models via Targeted Instruction Tuning", "authors": "Rui Hu, Yahan Tu, Shuyu Wei, Dongyuan Lu, Jitao Sang", "abstract": "Despite achieving outstanding performance on various cross-modal tasks,\ncurrent large vision-language models (LVLMs) still suffer from hallucination\nissues, manifesting as inconsistencies between their generated responses and\nthe corresponding images. Prior research has implicated that the low quality of\ninstruction data, particularly the skewed balance between positive and negative\nsamples, is a significant contributor to model hallucinations. Recently,\nresearchers have proposed high-quality instruction datasets, such as\nLRV-Instruction, to mitigate model hallucination. Nonetheless, our\ninvestigation reveals that hallucinatory concepts from different LVLMs exhibit\nspecificity, i.e. the distribution of hallucinatory concepts varies\nsignificantly across models. Existing datasets did not consider the\nhallucination specificity of different models in the design processes, thereby\ndiminishing their efficacy in mitigating model hallucination. In this paper, we\npropose a targeted instruction data generation framework named DFTG that\ntailored to the hallucination specificity of different models. Concretely, DFTG\nconsists of two stages: hallucination diagnosis, which extracts the necessary\ninformation from the model's responses and images for hallucination diagnosis;\nand targeted data generation, which generates targeted instruction data based\non diagnostic results. The experimental results on hallucination benchmarks\ndemonstrate that the targeted instruction data generated by our method are more\neffective in mitigating hallucinations compared to previous datasets.", "journal": ""}
{"doi": "10.48550/arXiv.2405.13684", "date": "2024-05-22", "title": "CrossCheckGPT: Universal Hallucination Ranking for Multimodal Foundation Models", "authors": "Guangzhi Sun, Potsawee Manakul, Adian Liusie, Kunat Pipatanakul, Chao Zhang, Phil Woodland, Mark Gales", "abstract": "Multimodal foundation models are prone to hallucination, generating outputs\nthat either contradict the input or are not grounded by factual information.\nGiven the diversity in architectures, training data and instruction tuning\ntechniques, there can be large variations in systems' susceptibility to\nhallucinations. To assess system hallucination robustness, hallucination\nranking approaches have been developed for specific tasks such as image\ncaptioning, question answering, summarization, or biography generation.\nHowever, these approaches typically compare model outputs to gold-standard\nreferences or labels, limiting hallucination benchmarking for new domains. This\nwork proposes \"CrossCheckGPT\", a reference-free universal hallucination ranking\nfor multimodal foundation models. The core idea of CrossCheckGPT is that the\nsame hallucinated content is unlikely to be generated by different independent\nsystems, hence cross-system consistency can provide meaningful and accurate\nhallucination assessment scores. CrossCheckGPT can be applied to any model or\ntask, provided that the information consistency between outputs can be measured\nthrough an appropriate distance metric. Focusing on multimodal large language\nmodels that generate text, we explore two information consistency measures:\nCrossCheck-explicit and CrossCheck-implicit. We showcase the applicability of\nour method for hallucination ranking across various modalities, namely the\ntext, image, and audio-visual domains. Further, we propose the first\naudio-visual hallucination benchmark, \"AVHalluBench\", and illustrate the\neffectiveness of CrossCheckGPT, achieving correlations of 98% and 89% with\nhuman judgements on MHaluBench and AVHalluBench, respectively.", "journal": ""}
{"doi": "10.48550/arXiv.2406.10185", "date": "2024-06-14", "title": "Detecting and Evaluating Medical Hallucinations in Large Vision Language Models", "authors": "Jiawei Chen, Dingkang Yang, Tong Wu, Yue Jiang, Xiaolu Hou, Mingcheng Li, Shunli Wang, Dongling Xiao, Ke Li, Lihua Zhang", "abstract": "Large Vision Language Models (LVLMs) are increasingly integral to healthcare\napplications, including medical visual question answering and imaging report\ngeneration. While these models inherit the robust capabilities of foundational\nLarge Language Models (LLMs), they also inherit susceptibility to\nhallucinations-a significant concern in high-stakes medical contexts where the\nmargin for error is minimal. However, currently, there are no dedicated methods\nor benchmarks for hallucination detection and evaluation in the medical field.\nTo bridge this gap, we introduce Med-HallMark, the first benchmark specifically\ndesigned for hallucination detection and evaluation within the medical\nmultimodal domain. This benchmark provides multi-tasking hallucination support,\nmultifaceted hallucination data, and hierarchical hallucination categorization.\nFurthermore, we propose the MediHall Score, a new medical evaluative metric\ndesigned to assess LVLMs' hallucinations through a hierarchical scoring system\nthat considers the severity and type of hallucination, thereby enabling a\ngranular assessment of potential clinical impacts. We also present\nMediHallDetector, a novel Medical LVLM engineered for precise hallucination\ndetection, which employs multitask training for hallucination detection.\nThrough extensive experimental evaluations, we establish baselines for popular\nLVLMs using our benchmark. The findings indicate that MediHall Score provides a\nmore nuanced understanding of hallucination impacts compared to traditional\nmetrics and demonstrate the enhanced performance of MediHallDetector. We hope\nthis work can significantly improve the reliability of LVLMs in medical\napplications. All resources of this work will be released soon.", "journal": ""}
{"doi": "10.48550/arXiv.2407.12780", "date": "2024-07-17", "title": "Hallucination Index: An Image Quality Metric for Generative Reconstruction Models", "authors": "Matthew Tivnan, Siyeop Yoon, Zhennong Chen, Xiang Li, Dufan Wu, Quanzheng Li", "abstract": "Generative image reconstruction algorithms such as measurement conditioned\ndiffusion models are increasingly popular in the field of medical imaging.\nThese powerful models can transform low signal-to-noise ratio (SNR) inputs into\noutputs with the appearance of high SNR. However, the outputs can have a new\ntype of error called hallucinations. In medical imaging, these hallucinations\nmay not be obvious to a Radiologist but could cause diagnostic errors.\nGenerally, hallucination refers to error in estimation of object structure\ncaused by a machine learning model, but there is no widely accepted method to\nevaluate hallucination magnitude. In this work, we propose a new image quality\nmetric called the hallucination index. Our approach is to compute the Hellinger\ndistance from the distribution of reconstructed images to a zero hallucination\nreference distribution. To evaluate our approach, we conducted a numerical\nexperiment with electron microscopy images, simulated noisy measurements, and\napplied diffusion based reconstructions. We sampled the measurements and the\ngenerative reconstructions repeatedly to compute the sample mean and\ncovariance. For the zero hallucination reference, we used the forward diffusion\nprocess applied to ground truth. Our results show that higher measurement SNR\nleads to lower hallucination index for the same apparent image quality. We also\nevaluated the impact of early stopping in the reverse diffusion process and\nfound that more modest denoising strengths can reduce hallucination. We believe\nthis metric could be useful for evaluation of generative image reconstructions\nor as a warning label to inform radiologists about the degree of hallucinations\nin medical images.", "journal": ""}
{"doi": "10.48550/arXiv.2408.00550", "date": "2024-08-01", "title": "Mitigating Multilingual Hallucination in Large Vision-Language Models", "authors": "Xiaoye Qu, Mingyang Song, Wei Wei, Jianfeng Dong, Yu Cheng", "abstract": "While Large Vision-Language Models (LVLMs) have exhibited remarkable\ncapabilities across a wide range of tasks, they suffer from hallucination\nproblems, where models generate plausible yet incorrect answers given the input\nimage-query pair. This hallucination phenomenon is even more severe when\nquerying the image in non-English languages, while existing methods for\nmitigating hallucinations in LVLMs only consider the English scenarios. In this\npaper, we make the first attempt to mitigate this important multilingual\nhallucination in LVLMs. With thorough experiment analysis, we found that\nmultilingual hallucination in LVLMs is a systemic problem that could arise from\ndeficiencies in multilingual capabilities or inadequate multimodal abilities.\nTo this end, we propose a two-stage Multilingual Hallucination Removal (MHR)\nframework for LVLMs, aiming to improve resistance to hallucination for both\nhigh-resource and low-resource languages. Instead of relying on the intricate\nmanual annotations of multilingual resources, we fully leverage the inherent\ncapabilities of the LVLM and propose a novel cross-lingual alignment method,\nwhich generates multiple responses for each image-query input and then\nidentifies the hallucination-aware pairs for each language. These data pairs\nare finally used for direct preference optimization to prompt the LVLMs to\nfavor non-hallucinating responses. Experimental results show that our MHR\nachieves a substantial reduction in hallucination generation for LVLMs.\nNotably, on our extended multilingual POPE benchmark, our framework delivers an\naverage increase of 19.0% in accuracy across 13 different languages. Our code\nand model weights are available at https://github.com/ssmisya/MHR", "journal": ""}
{"doi": "10.48550/arXiv.2505.23646", "date": "2025-05-29", "title": "Are Reasoning Models More Prone to Hallucination?", "authors": "Zijun Yao, Yantao Liu, Yanxu Chen, Jianhui Chen, Junfeng Fang, Lei Hou, Juanzi Li, Tat-Seng Chua", "abstract": "Recently evolved large reasoning models (LRMs) show powerful performance in\nsolving complex tasks with long chain-of-thought (CoT) reasoning capability. As\nthese LRMs are mostly developed by post-training on formal reasoning tasks,\nwhether they generalize the reasoning capability to help reduce hallucination\nin fact-seeking tasks remains unclear and debated. For instance, DeepSeek-R1\nreports increased performance on SimpleQA, a fact-seeking benchmark, while\nOpenAI-o3 observes even severer hallucination. This discrepancy naturally\nraises the following research question: Are reasoning models more prone to\nhallucination? This paper addresses the question from three perspectives. (1)\nWe first conduct a holistic evaluation for the hallucination in LRMs. Our\nanalysis reveals that LRMs undergo a full post-training pipeline with cold\nstart supervised fine-tuning (SFT) and verifiable reward RL generally alleviate\ntheir hallucination. In contrast, both distillation alone and RL training\nwithout cold start fine-tuning introduce more nuanced hallucinations. (2) To\nexplore why different post-training pipelines alters the impact on\nhallucination in LRMs, we conduct behavior analysis. We characterize two\ncritical cognitive behaviors that directly affect the factuality of a LRM: Flaw\nRepetition, where the surface-level reasoning attempts repeatedly follow the\nsame underlying flawed logic, and Think-Answer Mismatch, where the final answer\nfails to faithfully match the previous CoT process. (3) Further, we investigate\nthe mechanism behind the hallucination of LRMs from the perspective of model\nuncertainty. We find that increased hallucination of LRMs is usually associated\nwith the misalignment between model uncertainty and factual accuracy. Our work\nprovides an initial understanding of the hallucination in LRMs.", "journal": ""}
{"doi": "10.48550/arXiv.1809.02156", "date": "2018-09-06", "title": "Object Hallucination in Image Captioning", "authors": "Anna Rohrbach, Lisa Anne Hendricks, Kaylee Burns, Trevor Darrell, Kate Saenko", "abstract": "Despite continuously improving performance, contemporary image captioning\nmodels are prone to \"hallucinating\" objects that are not actually in a scene.\nOne problem is that standard metrics only measure similarity to ground truth\ncaptions and may not fully capture image relevance. In this work, we propose a\nnew image relevance metric to evaluate current models with veridical visual\nlabels and assess their rate of object hallucination. We analyze how captioning\nmodel architectures and learning objectives contribute to object hallucination,\nexplore when hallucination is likely due to image misclassification or language\npriors, and assess how well current sentence metrics capture object\nhallucination. We investigate these questions on the standard image captioning\nbenchmark, MSCOCO, using a diverse set of models. Our analysis yields several\ninteresting findings, including that models which score best on standard\nsentence metrics do not always have lower hallucination and that models which\nhallucinate more tend to make errors driven by language priors.", "journal": ""}
{"doi": "10.48550/arXiv.2301.07779", "date": "2023-01-18", "title": "Understanding and Detecting Hallucinations in Neural Machine Translation via Model Introspection", "authors": "Weijia Xu, Sweta Agrawal, Eleftheria Briakou, Marianna J. Martindale, Marine Carpuat", "abstract": "Neural sequence generation models are known to \"hallucinate\", by producing\noutputs that are unrelated to the source text. These hallucinations are\npotentially harmful, yet it remains unclear in what conditions they arise and\nhow to mitigate their impact. In this work, we first identify internal model\nsymptoms of hallucinations by analyzing the relative token contributions to the\ngeneration in contrastive hallucinated vs. non-hallucinated outputs generated\nvia source perturbations. We then show that these symptoms are reliable\nindicators of natural hallucinations, by using them to design a lightweight\nhallucination detector which outperforms both model-free baselines and strong\nclassifiers based on quality estimation or large pre-trained models on manually\nannotated English-Chinese and German-English translation test beds.", "journal": ""}
{"doi": "10.48550/arXiv.2310.03368", "date": "2023-10-05", "title": "Evaluating Hallucinations in Chinese Large Language Models", "authors": "Qinyuan Cheng, Tianxiang Sun, Wenwei Zhang, Siyin Wang, Xiangyang Liu, Mozhi Zhang, Junliang He, Mianqiu Huang, Zhangyue Yin, Kai Chen, Xipeng Qiu", "abstract": "In this paper, we establish a benchmark named HalluQA (Chinese Hallucination\nQuestion-Answering) to measure the hallucination phenomenon in Chinese large\nlanguage models. HalluQA contains 450 meticulously designed adversarial\nquestions, spanning multiple domains, and takes into account Chinese historical\nculture, customs, and social phenomena. During the construction of HalluQA, we\nconsider two types of hallucinations: imitative falsehoods and factual errors,\nand we construct adversarial samples based on GLM-130B and ChatGPT. For\nevaluation, we design an automated evaluation method using GPT-4 to judge\nwhether a model output is hallucinated. We conduct extensive experiments on 24\nlarge language models, including ERNIE-Bot, Baichuan2, ChatGLM, Qwen, SparkDesk\nand etc. Out of the 24 models, 18 achieved non-hallucination rates lower than\n50%. This indicates that HalluQA is highly challenging. We analyze the primary\ntypes of hallucinations in different types of models and their causes.\nAdditionally, we discuss which types of hallucinations should be prioritized\nfor different types of models.", "journal": ""}
{"doi": "10.48550/arXiv.2310.10627", "date": "2023-10-16", "title": "Factored Verification: Detecting and Reducing Hallucination in Summaries of Academic Papers", "authors": "Charlie George, Andreas Stuhlm\u00fcller", "abstract": "Hallucination plagues even frontier LLMs--but how bad is it really for\nsummarizing academic papers? We evaluate Factored Verification, a simple\nautomated method for detecting hallucinations in abstractive summaries. This\nmethod sets a new SotA on hallucination detection in the summarization task of\nthe HaluEval benchmark, achieving 76.2% accuracy. We then use this method to\nestimate how often language models hallucinate when summarizing across multiple\nacademic papers and find 0.62 hallucinations in the average ChatGPT (16k)\nsummary, 0.84 for GPT-4, and 1.55 for Claude 2. We ask models to self-correct\nusing Factored Critiques and find that this lowers the number of hallucinations\nto 0.49 for ChatGPT, 0.46 for GPT-4, and 0.95 for Claude 2. The hallucinations\nwe find are often subtle, so we advise caution when using models to synthesize\nacademic papers.", "journal": ""}
{"doi": "10.48550/arXiv.2312.05200", "date": "2023-12-08", "title": "DelucionQA: Detecting Hallucinations in Domain-specific Question Answering", "authors": "Mobashir Sadat, Zhengyu Zhou, Lukas Lange, Jun Araki, Arsalan Gundroo, Bingqing Wang, Rakesh R Menon, Md Rizwan Parvez, Zhe Feng", "abstract": "Hallucination is a well-known phenomenon in text generated by large language\nmodels (LLMs). The existence of hallucinatory responses is found in almost all\napplication scenarios e.g., summarization, question-answering (QA) etc. For\napplications requiring high reliability (e.g., customer-facing assistants), the\npotential existence of hallucination in LLM-generated text is a critical\nproblem. The amount of hallucination can be reduced by leveraging information\nretrieval to provide relevant background information to the LLM. However, LLMs\ncan still generate hallucinatory content for various reasons (e.g.,\nprioritizing its parametric knowledge over the context, failure to capture the\nrelevant information from the context, etc.). Detecting hallucinations through\nautomated methods is thus paramount. To facilitate research in this direction,\nwe introduce a sophisticated dataset, DelucionQA, that captures hallucinations\nmade by retrieval-augmented LLMs for a domain-specific QA task. Furthermore, we\npropose a set of hallucination detection methods to serve as baselines for\nfuture works from the research community. Analysis and case study are also\nprovided to share valuable insights on hallucination phenomena in the target\nscenario.", "journal": ""}
{"doi": "10.48550/arXiv.2401.06855", "date": "2024-01-12", "title": "Fine-grained Hallucination Detection and Editing for Language Models", "authors": "Abhika Mishra, Akari Asai, Vidhisha Balachandran, Yizhong Wang, Graham Neubig, Yulia Tsvetkov, Hannaneh Hajishirzi", "abstract": "Large language models (LMs) are prone to generate factual errors, which are\noften called hallucinations. In this paper, we introduce a comprehensive\ntaxonomy of hallucinations and argue that hallucinations manifest in diverse\nforms, each requiring varying degrees of careful assessments to verify\nfactuality. We propose a novel task of automatic fine-grained hallucination\ndetection and construct a new evaluation benchmark, FavaBench, that includes\nabout one thousand fine-grained human judgments on three LM outputs across\nvarious domains. Our analysis reveals that ChatGPT and Llama2-Chat (70B, 7B)\nexhibit diverse types of hallucinations in the majority of their outputs in\ninformation-seeking scenarios. We train FAVA, a retrieval-augmented LM by\ncarefully creating synthetic data to detect and correct fine-grained\nhallucinations. On our benchmark, our automatic and human evaluations show that\nFAVA significantly outperforms ChatGPT and GPT-4 on fine-grained hallucination\ndetection, and edits suggested by FAVA improve the factuality of LM-generated\ntext.", "journal": ""}
{"doi": "10.48550/arXiv.2401.09774", "date": "2024-01-18", "title": "On the Audio Hallucinations in Large Audio-Video Language Models", "authors": "Taichi Nishimura, Shota Nakada, Masayoshi Kondo", "abstract": "Large audio-video language models can generate descriptions for both video\nand audio. However, they sometimes ignore audio content, producing audio\ndescriptions solely reliant on visual information. This paper refers to this as\naudio hallucinations and analyzes them in large audio-video language models. We\ngather 1,000 sentences by inquiring about audio information and annotate them\nwhether they contain hallucinations. If a sentence is hallucinated, we also\ncategorize the type of hallucination. The results reveal that 332 sentences are\nhallucinated with distinct trends observed in nouns and verbs for each\nhallucination type. Based on this, we tackle a task of audio hallucination\nclassification using pre-trained audio-text models in the zero-shot and\nfine-tuning settings. Our experimental results reveal that the zero-shot models\nachieve higher performance (52.2% in F1) than the random (40.3%) and the\nfine-tuning models achieve 87.9%, outperforming the zero-shot models.", "journal": ""}
{"doi": "10.48550/arXiv.2402.16211", "date": "2024-02-25", "title": "HypoTermQA: Hypothetical Terms Dataset for Benchmarking Hallucination Tendency of LLMs", "authors": "Cem Uluoglakci, Tugba Taskaya Temizel", "abstract": "Hallucinations pose a significant challenge to the reliability and alignment\nof Large Language Models (LLMs), limiting their widespread acceptance beyond\nchatbot applications. Despite ongoing efforts, hallucinations remain a\nprevalent challenge in LLMs. The detection of hallucinations itself is also a\nformidable task, frequently requiring manual labeling or constrained\nevaluations. This paper introduces an automated scalable framework that\ncombines benchmarking LLMs' hallucination tendencies with efficient\nhallucination detection. We leverage LLMs to generate challenging tasks related\nto hypothetical phenomena, subsequently employing them as agents for efficient\nhallucination detection. The framework is domain-agnostic, allowing the use of\nany language model for benchmark creation or evaluation in any domain. We\nintroduce the publicly available HypoTermQA Benchmarking Dataset, on which\nstate-of-the-art models' performance ranged between 3% and 11%, and evaluator\nagents demonstrated a 6% error rate in hallucination prediction. The proposed\nframework provides opportunities to test and improve LLMs. Additionally, it has\nthe potential to generate benchmarking datasets tailored to specific domains,\nsuch as law, health, and finance.", "journal": ""}
{"doi": "10.48550/arXiv.2407.00488", "date": "2024-06-29", "title": "PFME: A Modular Approach for Fine-grained Hallucination Detection and Editing of Large Language Models", "authors": "Kunquan Deng, Zeyu Huang, Chen Li, Chenghua Lin, Min Gao, Wenge Rong", "abstract": "Large Language Models (LLMs) excel in fluency but risk producing inaccurate\ncontent, called \"hallucinations.\" This paper outlines a standardized process\nfor categorizing fine-grained hallucination types and proposes an innovative\nframework--the Progressive Fine-grained Model Editor (PFME)--specifically\ndesigned to detect and correct fine-grained hallucinations in LLMs. PFME\nconsists of two collaborative modules: the Real-time Fact Retrieval Module and\nthe Fine-grained Hallucination Detection and Editing Module. The former\nidentifies key entities in the document and retrieves the latest factual\nevidence from credible sources. The latter further segments the document into\nsentence-level text and, based on relevant evidence and previously edited\ncontext, identifies, locates, and edits each sentence's hallucination type.\nExperimental results on FavaBench and FActScore demonstrate that PFME\noutperforms existing methods in fine-grained hallucination detection tasks.\nParticularly, when using the Llama3-8B-Instruct model, PFME's performance in\nfine-grained hallucination detection with external knowledge assistance\nimproves by 8.7 percentage points (pp) compared to ChatGPT. In editing tasks,\nPFME further enhances the FActScore of FActScore-Alpaca13B and\nFActScore-ChatGPT datasets, increasing by 16.2pp and 4.6pp, respectively.", "journal": ""}
{"doi": "10.48550/arXiv.2407.10153", "date": "2024-07-14", "title": "Look Within, Why LLMs Hallucinate: A Causal Perspective", "authors": "He Li, Haoang Chi, Mingyu Liu, Wenjing Yang", "abstract": "The emergence of large language models (LLMs) is a milestone in generative\nartificial intelligence, achieving significant success in text comprehension\nand generation tasks. Despite the tremendous success of LLMs in many downstream\ntasks, they suffer from severe hallucination problems, posing significant\nchallenges to the practical applications of LLMs. Most of the works about LLMs'\nhallucinations focus on data quality. Self-attention is a core module in\ntransformer-based LLMs, while its potential relationship with LLMs'\nhallucination has been hardly investigated. To fill this gap, we study this\nproblem from a causal perspective. We propose a method to intervene in LLMs'\nself-attention layers and maintain their structures and sizes intact.\nSpecifically, we disable different self-attention layers in several popular\nopen-source LLMs and then compare their degrees of hallucination with the\noriginal ones. We evaluate the intervened LLMs on hallucination assessment\nbenchmarks and conclude that disabling some specific self-attention layers in\nthe front or tail of the LLMs can alleviate hallucination issues. The study\npaves a new way for understanding and mitigating LLMs' hallucinations.", "journal": ""}
{"doi": "10.48550/arXiv.2409.20550", "date": "2024-09-30", "title": "LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation", "authors": "Ziyao Zhang, Yanlin Wang, Chong Wang, Jiachi Chen, Zibin Zheng", "abstract": "Code generation aims to automatically generate code from input requirements,\nsignificantly enhancing development efficiency. Recent large language models\n(LLMs) based approaches have shown promising results and revolutionized code\ngeneration task. Despite the promising performance, LLMs often generate\ncontents with hallucinations, especially for the code generation scenario\nrequiring the handling of complex contextual dependencies in practical\ndevelopment process. Although previous study has analyzed hallucinations in\nLLM-powered code generation, the study is limited to standalone function\ngeneration. In this paper, we conduct an empirical study to study the\nphenomena, mechanism, and mitigation of LLM hallucinations within more\npractical and complex development contexts in repository-level generation\nscenario. First, we manually examine the code generation results from six\nmainstream LLMs to establish a hallucination taxonomy of LLM-generated code.\nNext, we elaborate on the phenomenon of hallucinations, analyze their\ndistribution across different models. We then analyze causes of hallucinations\nand identify four potential factors contributing to hallucinations. Finally, we\npropose an RAG-based mitigation method, which demonstrates consistent\neffectiveness in all studied LLMs. The replication package including code,\ndata, and experimental results is available at\nhttps://github.com/DeepSoftwareAnalytics/LLMCodingHallucination", "journal": ""}
{"doi": "10.48550/arXiv.2410.06795", "date": "2024-10-09", "title": "From Pixels to Tokens: Revisiting Object Hallucinations in Large Vision-Language Models", "authors": "Yuying Shang, Xinyi Zeng, Yutao Zhu, Xiao Yang, Zhengwei Fang, Jingyuan Zhang, Jiawei Chen, Zinan Liu, Yu Tian", "abstract": "Hallucinations in large vision-language models (LVLMs) are a significant\nchallenge, i.e., generating objects that are not presented in the visual input,\nwhich impairs their reliability. Recent studies often attribute hallucinations\nto a lack of understanding of visual input, yet ignore a more fundamental\nissue: the model's inability to effectively extract or decouple visual\nfeatures. In this paper, we revisit the hallucinations in LVLMs from an\narchitectural perspective, investigating whether the primary cause lies in the\nvisual encoder (feature extraction) or the modal alignment module (feature\ndecoupling). Motivated by our findings on the preliminary investigation, we\npropose a novel tuning strategy, PATCH, to mitigate hallucinations in LVLMs.\nThis plug-and-play method can be integrated into various LVLMs, utilizing\nadaptive virtual tokens to extract object features from bounding boxes, thereby\naddressing hallucinations caused by insufficient decoupling of visual features.\nPATCH achieves state-of-the-art performance on multiple multi-modal\nhallucination datasets. We hope this approach provides researchers with deeper\ninsights into the underlying causes of hallucinations in LVLMs, fostering\nfurther advancements and innovation in this field.", "journal": ""}
{"doi": "10.48550/arXiv.2410.10408", "date": "2024-10-14", "title": "Medico: Towards Hallucination Detection and Correction with Multi-source Evidence Fusion", "authors": "Xinping Zhao, Jindi Yu, Zhenyu Liu, Jifang Wang, Dongfang Li, Yibin Chen, Baotian Hu, Min Zhang", "abstract": "As we all know, hallucinations prevail in Large Language Models (LLMs), where\nthe generated content is coherent but factually incorrect, which inflicts a\nheavy blow on the widespread application of LLMs. Previous studies have shown\nthat LLMs could confidently state non-existent facts rather than answering ``I\ndon't know''. Therefore, it is necessary to resort to external knowledge to\ndetect and correct the hallucinated content. Since manual detection and\ncorrection of factual errors is labor-intensive, developing an automatic\nend-to-end hallucination-checking approach is indeed a needful thing. To this\nend, we present Medico, a Multi-source evidence fusion enhanced hallucination\ndetection and correction framework. It fuses diverse evidence from multiple\nsources, detects whether the generated content contains factual errors,\nprovides the rationale behind the judgment, and iteratively revises the\nhallucinated content. Experimental results on evidence retrieval (0.964 HR@5,\n0.908 MRR@5), hallucination detection (0.927-0.951 F1), and hallucination\ncorrection (0.973-0.979 approval rate) manifest the great potential of Medico.\nA video demo of Medico can be found at https://youtu.be/RtsO6CSesBI.", "journal": ""}
{"doi": "10.48550/arXiv.2410.13210", "date": "2024-10-17", "title": "FaithBench: A Diverse Hallucination Benchmark for Summarization by Modern LLMs", "authors": "Forrest Sheng Bao, Miaoran Li, Renyi Qu, Ge Luo, Erana Wan, Yujia Tang, Weisi Fan, Manveer Singh Tamber, Suleman Kazi, Vivek Sourabh, Mike Qi, Ruixuan Tu, Chenyu Xu, Matthew Gonzales, Ofer Mendelevitch, Amin Ahmad", "abstract": "Summarization is one of the most common tasks performed by large language\nmodels (LLMs), especially in applications like Retrieval-Augmented Generation\n(RAG). However, existing evaluations of hallucinations in LLM-generated\nsummaries, and evaluations of hallucination detection models both suffer from a\nlack of diversity and recency in the LLM and LLM families considered. This\npaper introduces FaithBench, a summarization hallucination benchmark comprising\nchallenging hallucinations made by 10 modern LLMs from 8 different families,\nwith ground truth annotations by human experts. ``Challenging'' here means\nsummaries on which popular, state-of-the-art hallucination detection models,\nincluding GPT-4o-as-a-judge, disagreed on. Our results show GPT-4o and\nGPT-3.5-Turbo produce the least hallucinations. However, even the best\nhallucination detection models have near 50\\% accuracies on FaithBench,\nindicating lots of room for future improvement. The repo is\nhttps://github.com/vectara/FaithBench", "journal": ""}
{"doi": "10.48550/arXiv.2410.18270", "date": "2024-10-23", "title": "Multilingual Hallucination Gaps in Large Language Models", "authors": "Cl\u00e9a Chataigner, Afaf Ta\u00efk, Golnoosh Farnadi", "abstract": "Large language models (LLMs) are increasingly used as alternatives to\ntraditional search engines given their capacity to generate text that resembles\nhuman language. However, this shift is concerning, as LLMs often generate\nhallucinations, misleading or false information that appears highly credible.\nIn this study, we explore the phenomenon of hallucinations across multiple\nlanguages in freeform text generation, focusing on what we call multilingual\nhallucination gaps. These gaps reflect differences in the frequency of\nhallucinated answers depending on the prompt and language used. To quantify\nsuch hallucinations, we used the FactScore metric and extended its framework to\na multilingual setting. We conducted experiments using LLMs from the LLaMA,\nQwen, and Aya families, generating biographies in 19 languages and comparing\nthe results to Wikipedia pages. Our results reveal variations in hallucination\nrates, especially between high and low resource languages, raising important\nquestions about LLM multilingual performance and the challenges in evaluating\nhallucinations in multilingual freeform text generation.", "journal": ""}
{"doi": "10.48550/arXiv.2410.19385", "date": "2024-10-25", "title": "Investigating the Role of Prompting and External Tools in Hallucination Rates of Large Language Models", "authors": "Liam Barkley, Brink van der Merwe", "abstract": "Large Language Models (LLMs) are powerful computational models trained on\nextensive corpora of human-readable text, enabling them to perform\ngeneral-purpose language understanding and generation. LLMs have garnered\nsignificant attention in both industry and academia due to their exceptional\nperformance across various natural language processing (NLP) tasks. Despite\nthese successes, LLMs often produce inaccuracies, commonly referred to as\nhallucinations. Prompt engineering, the process of designing and formulating\ninstructions for LLMs to perform specific tasks, has emerged as a key approach\nto mitigating hallucinations. This paper provides a comprehensive empirical\nevaluation of different prompting strategies and frameworks aimed at reducing\nhallucinations in LLMs. Various prompting techniques are applied to a broad set\nof benchmark datasets to assess the accuracy and hallucination rate of each\nmethod. Additionally, the paper investigates the influence of tool-calling\nagents (LLMs augmented with external tools to enhance their capabilities beyond\nlanguage generation) on hallucination rates in the same benchmarks. The\nfindings demonstrate that the optimal prompting technique depends on the type\nof problem, and that simpler techniques often outperform more complex methods\nin reducing hallucinations. Furthermore, it is shown that LLM agents can\nexhibit significantly higher hallucination rates due to the added complexity of\nexternal tool usage.", "journal": ""}
{"doi": "10.48550/arXiv.2411.09689", "date": "2024-11-14", "title": "Probing LLM Hallucination from Within: Perturbation-Driven Approach via Internal Knowledge", "authors": "Seongmin Lee, Hsiang Hsu, Chun-Fu Chen, Duen Horng, Chau", "abstract": "LLM hallucination, where unfaithful text is generated, presents a critical\nchallenge for LLMs' practical applications. Current detection methods often\nresort to external knowledge, LLM fine-tuning, or supervised training with\nlarge hallucination-labeled datasets. Moreover, these approaches do not\ndistinguish between different types of hallucinations, which is crucial for\nenhancing detection performance. To address such limitations, we introduce\nhallucination probing, a new task that classifies LLM-generated text into three\ncategories: aligned, misaligned, and fabricated. Driven by our novel discovery\nthat perturbing key entities in prompts affects LLM's generation of these three\ntypes of text differently, we propose SHINE, a novel hallucination probing\nmethod that does not require external knowledge, supervised training, or LLM\nfine-tuning. SHINE is effective in hallucination probing across three modern\nLLMs, and achieves state-of-the-art performance in hallucination detection,\noutperforming seven competing methods across four datasets and four LLMs,\nunderscoring the importance of probing for accurate detection.", "journal": ""}
{"doi": "10.48550/arXiv.2412.02946", "date": "2024-12-04", "title": "Who Brings the Frisbee: Probing Hidden Hallucination Factors in Large Vision-Language Model via Causality Analysis", "authors": "Po-Hsuan Huang, Jeng-Lin Li, Chin-Po Chen, Ming-Ching Chang, Wei-Chao Chen", "abstract": "Recent advancements in large vision-language models (LVLM) have significantly\nenhanced their ability to comprehend visual inputs alongside natural language.\nHowever, a major challenge in their real-world application is hallucination,\nwhere LVLMs generate non-existent visual elements, eroding user trust. The\nunderlying mechanism driving this multimodal hallucination is poorly\nunderstood. Minimal research has illuminated whether contexts such as sky,\ntree, or grass field involve the LVLM in hallucinating a frisbee. We\nhypothesize that hidden factors, such as objects, contexts, and semantic\nforeground-background structures, induce hallucination. This study proposes a\nnovel causal approach: a hallucination probing system to identify these hidden\nfactors. By analyzing the causality between images, text prompts, and network\nsaliency, we systematically explore interventions to block these factors. Our\nexperimental findings show that a straightforward technique based on our\nanalysis can significantly reduce hallucinations. Additionally, our analyses\nindicate the potential to edit network internals to minimize hallucinated\noutputs.", "journal": "https://openaccess.thecvf.com/content/WACV2025/papers/Huang_Who_Brings_the_Frisbee_Probing_Hidden_Hallucination_Factors_in_Large_WACV_2025_paper.pdf"}
{"doi": "10.48550/arXiv.2412.03735", "date": "2024-12-04", "title": "VidHalluc: Evaluating Temporal Hallucinations in Multimodal Large Language Models for Video Understanding", "authors": "Chaoyu Li, Eun Woo Im, Pooyan Fazli", "abstract": "Multimodal large language models (MLLMs) have recently shown significant\nadvancements in video understanding, excelling in content reasoning and\ninstruction-following tasks. However, hallucination, where models generate\ninaccurate or misleading content, remains underexplored in the video domain.\nBuilding on the observation that MLLM visual encoders often fail to distinguish\nvisually different yet semantically similar video pairs, we introduce\nVidHalluc, the largest benchmark designed to examine hallucinations in MLLMs\nfor video understanding. It consists of 5,002 videos, paired to highlight cases\nprone to hallucinations. VidHalluc assesses hallucinations across three\ncritical dimensions: (1) action, (2) temporal sequence, and (3) scene\ntransition. Comprehensive testing shows that most MLLMs are vulnerable to\nhallucinations across these dimensions. Furthermore, we propose DINO-HEAL, a\ntraining-free method that reduces hallucinations by incorporating spatial\nsaliency from DINOv2 to reweight visual features during inference. Our results\nshow that DINO-HEAL consistently improves performance on VidHalluc, achieving\nan average improvement of 3.02% in mitigating hallucinations across all tasks.\nBoth the VidHalluc benchmark and DINO-HEAL code are available at\nhttps://people-robots.github.io/vidhalluc.", "journal": ""}
{"doi": "10.48550/arXiv.2501.19012", "date": "2025-01-31", "title": "Importing Phantoms: Measuring LLM Package Hallucination Vulnerabilities", "authors": "Arjun Krishna, Erick Galinkin, Leon Derczynski, Jeffrey Martin", "abstract": "Large Language Models (LLMs) have become an essential tool in the\nprogrammer's toolkit, but their tendency to hallucinate code can be used by\nmalicious actors to introduce vulnerabilities to broad swathes of the software\nsupply chain. In this work, we analyze package hallucination behaviour in LLMs\nacross popular programming languages examining both existing package references\nand fictional dependencies. By analyzing this package hallucination behaviour\nwe find potential attacks and suggest defensive strategies to defend against\nthese attacks. We discover that package hallucination rate is predicated not\nonly on model choice, but also programming language, model size, and\nspecificity of the coding task request. The Pareto optimality boundary between\ncode generation performance and package hallucination is sparsely populated,\nsuggesting that coding models are not being optimized for secure code.\nAdditionally, we find an inverse correlation between package hallucination rate\nand the HumanEval coding benchmark, offering a heuristic for evaluating the\npropensity of a model to hallucinate packages. Our metrics, findings and\nanalyses provide a base for future models, securing AI-assisted software\ndevelopment workflows against package supply chain attacks.", "journal": ""}
{"doi": "10.48550/arXiv.2502.12964", "date": "2025-02-18", "title": "Trust Me, I'm Wrong: High-Certainty Hallucinations in LLMs", "authors": "Adi Simhi, Itay Itzhak, Fazl Barez, Gabriel Stanovsky, Yonatan Belinkov", "abstract": "Large Language Models (LLMs) often generate outputs that lack grounding in\nreal-world facts, a phenomenon known as hallucinations. Prior research has\nassociated hallucinations with model uncertainty, leveraging this relationship\nfor hallucination detection and mitigation. In this paper, we challenge the\nunderlying assumption that all hallucinations are associated with uncertainty.\nUsing knowledge detection and uncertainty measurement methods, we demonstrate\nthat models can hallucinate with high certainty even when they have the correct\nknowledge. We further show that high-certainty hallucinations are consistent\nacross models and datasets, distinctive enough to be singled out, and challenge\nexisting mitigation methods. Our findings reveal an overlooked aspect of\nhallucinations, emphasizing the need to understand their origins and improve\nmitigation strategies to enhance LLM safety. The code is available at\nhttps://github.com/technion-cs-nlp/Trust_me_Im_wrong .", "journal": ""}
{"doi": "10.48550/arXiv.2502.15888", "date": "2025-02-18", "title": "Understanding and Evaluating Hallucinations in 3D Visual Language Models", "authors": "Ruiying Peng, Kaiyuan Li, Weichen Zhang, Chen Gao, Xinlei Chen, Yong Li", "abstract": "Recently, 3D-LLMs, which combine point-cloud encoders with large models, have\nbeen proposed to tackle complex tasks in embodied intelligence and scene\nunderstanding. In addition to showing promising results on 3D tasks, we found\nthat they are significantly affected by hallucinations. For instance, they may\ngenerate objects that do not exist in the scene or produce incorrect\nrelationships between objects. To investigate this issue, this work presents\nthe first systematic study of hallucinations in 3D-LLMs. We begin by quickly\nevaluating hallucinations in several representative 3D-LLMs and reveal that\nthey are all significantly affected by hallucinations. We then define\nhallucinations in 3D scenes and, through a detailed analysis of datasets,\nuncover the underlying causes of these hallucinations. We find three main\ncauses: (1) Uneven frequency distribution of objects in the dataset. (2) Strong\ncorrelations between objects. (3) Limited diversity in object attributes.\nAdditionally, we propose new evaluation metrics for hallucinations, including\nRandom Point Cloud Pair and Opposite Question Evaluations, to assess whether\nthe model generates responses based on visual information and aligns it with\nthe text's meaning.", "journal": ""}
{"doi": "10.48550/arXiv.2503.01670", "date": "2025-03-03", "title": "Evaluating LLMs' Assessment of Mixed-Context Hallucination Through the Lens of Summarization", "authors": "Siya Qi, Rui Cao, Yulan He, Zheng Yuan", "abstract": "With the rapid development of large language models (LLMs), LLM-as-a-judge\nhas emerged as a widely adopted approach for text quality evaluation, including\nhallucination evaluation. While previous studies have focused exclusively on\nsingle-context evaluation (e.g., discourse faithfulness or world factuality),\nreal-world hallucinations typically involve mixed contexts, which remains\ninadequately evaluated. In this study, we use summarization as a representative\ntask to comprehensively evaluate LLMs' capability in detecting mixed-context\nhallucinations, specifically distinguishing between factual and non-factual\nhallucinations. Through extensive experiments across direct generation and\nretrieval-based models of varying scales, our main observations are: (1) LLMs'\nintrinsic knowledge introduces inherent biases in hallucination evaluation; (2)\nThese biases particularly impact the detection of factual hallucinations,\nyielding a significant performance bottleneck; (3) The fundamental challenge\nlies in effective knowledge utilization, balancing between LLMs' intrinsic\nknowledge and external context for accurate mixed-context hallucination\nevaluation.", "journal": ""}
{"doi": "10.48550/arXiv.2503.02157", "date": "2025-03-04", "title": "MedHEval: Benchmarking Hallucinations and Mitigation Strategies in Medical Large Vision-Language Models", "authors": "Aofei Chang, Le Huang, Parminder Bhatia, Taha Kass-Hout, Fenglong Ma, Cao Xiao", "abstract": "Large Vision Language Models (LVLMs) are becoming increasingly important in\nthe medical domain, yet Medical LVLMs (Med-LVLMs) frequently generate\nhallucinations due to limited expertise and the complexity of medical\napplications. Existing benchmarks fail to effectively evaluate hallucinations\nbased on their underlying causes and lack assessments of mitigation strategies.\nTo address this gap, we introduce MedHEval, a novel benchmark that\nsystematically evaluates hallucinations and mitigation strategies in Med-LVLMs\nby categorizing them into three underlying causes: visual misinterpretation,\nknowledge deficiency, and context misalignment. We construct a diverse set of\nclose- and open-ended medical VQA datasets with comprehensive evaluation\nmetrics to assess these hallucination types. We conduct extensive experiments\nacross 11 popular (Med)-LVLMs and evaluate 7 state-of-the-art hallucination\nmitigation techniques. Results reveal that Med-LVLMs struggle with\nhallucinations arising from different causes while existing mitigation methods\nshow limited effectiveness, especially for knowledge- and context-based errors.\nThese findings underscore the need for improved alignment training and\nspecialized mitigation strategies to enhance Med-LVLMs' reliability. MedHEval\nestablishes a standardized framework for evaluating and mitigating medical\nhallucinations, guiding the development of more trustworthy Med-LVLMs.", "journal": ""}
{"doi": "10.48550/arXiv.2503.19482", "date": "2025-03-25", "title": "KSHSeek: Data-Driven Approaches to Mitigating and Detecting Knowledge-Shortcut Hallucinations in Generative Models", "authors": "Zhiwei Wang, Zhongxin Liu, Ying Li, Hongyu Sun, Meng Xu, Yuqing Zhang", "abstract": "The emergence of large language models (LLMs) has significantly advanced the\ndevelopment of natural language processing (NLP), especially in text generation\ntasks like question answering. However, model hallucinations remain a major\nchallenge in natural language generation (NLG) tasks due to their complex\ncauses. We systematically expand on the causes of factual hallucinations from\nthe perspective of knowledge shortcuts, analyzing hallucinations arising from\ncorrect and defect-free data and demonstrating that knowledge-shortcut\nhallucinations are prevalent in generative models. To mitigate this issue, we\npropose a high similarity pruning algorithm at the data preprocessing level to\nreduce spurious correlations in the data. Additionally, we design a specific\ndetection method for knowledge-shortcut hallucinations to evaluate the\neffectiveness of our mitigation strategy. Experimental results show that our\napproach effectively reduces knowledge-shortcut hallucinations, particularly in\nfine-tuning tasks, without negatively impacting model performance in question\nanswering. This work introduces a new paradigm for mitigating specific\nhallucination issues in generative models, enhancing their robustness and\nreliability in real-world applications.", "journal": ""}
{"doi": "10.48550/arXiv.2504.12137", "date": "2025-04-16", "title": "Efficient Contrastive Decoding with Probabilistic Hallucination Detection - Mitigating Hallucinations in Large Vision Language Models -", "authors": "Laura Fieback, Nishilkumar Balar, Jakob Spiegelberg, Hanno Gottschalk", "abstract": "Despite recent advances in Large Vision Language Models (LVLMs), these models\nstill suffer from generating hallucinatory responses that do not align with the\nvisual input provided. To mitigate such hallucinations, we introduce Efficient\nContrastive Decoding (ECD), a simple method that leverages probabilistic\nhallucination detection to shift the output distribution towards contextually\naccurate answers at inference time. By contrasting token probabilities and\nhallucination scores, ECD subtracts hallucinated concepts from the original\ndistribution, effectively suppressing hallucinations. Notably, our proposed\nmethod can be applied to any open-source LVLM and does not require additional\nLVLM training. We evaluate our method on several benchmark datasets and across\ndifferent LVLMs. Our experiments show that ECD effectively mitigates\nhallucinations, outperforming state-of-the-art methods with respect to\nperformance on LVLM benchmarks and computation time.", "journal": ""}
{"doi": "10.48550/arXiv.2505.16894", "date": "2025-05-22", "title": "Shadows in the Attention: Contextual Perturbation and Representation Drift in the Dynamics of Hallucination in LLMs", "authors": "Zeyu Wei, Shuo Wang, Xiaohui Rong, Xuemin Liu, He Li", "abstract": "Hallucinations -- plausible yet erroneous outputs -- remain a critical\nbarrier to reliable deployment of large language models (LLMs). We present the\nfirst systematic study linking hallucination incidence to internal-state drift\ninduced by incremental context injection. Using TruthfulQA, we construct two\n16-round \"titration\" tracks per question: one appends relevant but partially\nflawed snippets, the other injects deliberately misleading content. Across six\nopen-source LLMs, we track overt hallucination rates with a tri-perspective\ndetector and covert dynamics via cosine, entropy, JS and Spearman drifts of\nhidden states and attention maps. Results reveal (1) monotonic growth of\nhallucination frequency and representation drift that plateaus after 5--7\nrounds; (2) relevant context drives deeper semantic assimilation, producing\nhigh-confidence \"self-consistent\" hallucinations, whereas irrelevant context\ninduces topic-drift errors anchored by attention re-routing; and (3)\nconvergence of JS-Drift ($\\sim0.69$) and Spearman-Drift ($\\sim0$) marks an\n\"attention-locking\" threshold beyond which hallucinations solidify and become\nresistant to correction. Correlation analyses expose a seesaw between\nassimilation capacity and attention diffusion, clarifying size-dependent error\nmodes. These findings supply empirical foundations for intrinsic hallucination\nprediction and context-aware mitigation mechanisms.", "journal": ""}
{"doi": "10.48550/arXiv.2506.04832", "date": "2025-06-05", "title": "Joint Evaluation of Answer and Reasoning Consistency for Hallucination Detection in Large Reasoning Models", "authors": "Changyue Wang, Weihang Su, Qingyao Ai, Yiqun Liu", "abstract": "Large Reasoning Models (LRMs) extend large language models with explicit,\nmulti-step reasoning traces to enhance transparency and performance on complex\ntasks. However, these reasoning traces can be redundant or logically\ninconsistent, making them a new source of hallucination that is difficult to\ndetect. Existing hallucination detection methods focus primarily on\nanswer-level uncertainty and often fail to detect hallucinations or logical\ninconsistencies arising from the model's reasoning trace. This oversight is\nparticularly problematic for LRMs, where the explicit thinking trace is not\nonly an important support to the model's decision-making process but also a key\nsource of potential hallucination. To this end, we propose RACE (Reasoning and\nAnswer Consistency Evaluation), a novel framework specifically tailored for\nhallucination detection in LRMs. RACE operates by extracting essential\nreasoning steps and computing four diagnostic signals: inter-sample consistency\nof reasoning traces, entropy-based answer uncertainty, semantic alignment\nbetween reasoning and answers, and internal coherence of reasoning. This joint\nanalysis enables fine-grained hallucination detection even when the final\nanswer appears correct. Experiments across datasets and different LLMs\ndemonstrate that RACE outperforms existing hallucination detection baselines,\noffering a robust and generalizable solution for evaluating LRMs. Our code is\navailable at: https://github.com/bebr2/RACE.", "journal": ""}
{"doi": "10.48550/arXiv.2506.10286", "date": "2025-06-12", "title": "HalLoc: Token-level Localization of Hallucinations for Vision Language Models", "authors": "Eunkyu Park, Minyeong Kim, Gunhee Kim", "abstract": "Hallucinations pose a significant challenge to the reliability of large\nvision-language models, making their detection essential for ensuring accuracy\nin critical applications. Current detection methods often rely on\ncomputationally intensive models, leading to high latency and resource demands.\nTheir definitive outcomes also fail to account for real-world scenarios where\nthe line between hallucinated and truthful information is unclear. To address\nthese issues, we propose HalLoc, a dataset designed for efficient,\nprobabilistic hallucination detection. It features 150K token-level annotated\nsamples, including hallucination types, across Visual Question Answering (VQA),\ninstruction-following, and image captioning tasks. This dataset facilitates the\ndevelopment of models that detect hallucinations with graded confidence,\nenabling more informed user interactions. Additionally, we introduce a baseline\nmodel trained on HalLoc, offering low-overhead, concurrent hallucination\ndetection during generation. The model can be seamlessly integrated into\nexisting VLMs, improving reliability while preserving efficiency. The prospect\nof a robust plug-and-play hallucination detection module opens new avenues for\nenhancing the trustworthiness of vision-language models in real-world\napplications. The HalLoc dataset and code are publicly available at:\nhttps://github.com/dbsltm/cvpr25_halloc.", "journal": ""}
{"doi": "10.48550/arXiv.2308.00399", "date": "2023-08-01", "title": "Tackling Hallucinations in Neural Chart Summarization", "authors": "Saad Obaid ul Islam, Iza \u0160krjanec, Ond\u0159ej Du\u0161ek, Vera Demberg", "abstract": "Hallucinations in text generation occur when the system produces text that is\nnot grounded in the input. In this work, we tackle the problem of\nhallucinations in neural chart summarization. Our analysis shows that the\ntarget side of chart summarization training datasets often contains additional\ninformation, leading to hallucinations. We propose a natural language inference\n(NLI) based method to preprocess the training data and show through human\nevaluation that our method significantly reduces hallucinations. We also found\nthat shortening long-distance dependencies in the input sequence and adding\nchart-related information like title and legends improves the overall\nperformance.", "journal": ""}
{"doi": "10.48550/arXiv.2407.16908", "date": "2024-07-23", "title": "Generation Constraint Scaling Can Mitigate Hallucination", "authors": "Georgios Kollias, Payel Das, Subhajit Chaudhury", "abstract": "Addressing the issue of hallucinations in large language models (LLMs) is a\ncritical challenge. As the cognitive mechanisms of hallucination have been\nrelated to memory, here we explore hallucination for LLM that is enabled with\nexplicit memory mechanisms. We empirically demonstrate that by simply scaling\nthe readout vector that constrains generation in a memory-augmented LLM\ndecoder, hallucination mitigation can be achieved in a training-free manner.\nOur method is geometry-inspired and outperforms a state-of-the-art LLM editing\nmethod on the task of generation of Wikipedia-like biography entries both in\nterms of generation quality and runtime complexity.", "journal": ""}
{"doi": "10.48550/arXiv.2411.12759", "date": "2024-11-16", "title": "A Novel Approach to Eliminating Hallucinations in Large Language Model-Assisted Causal Discovery", "authors": "Grace Sng, Yanming Zhang, Klaus Mueller", "abstract": "The increasing use of large language models (LLMs) in causal discovery as a\nsubstitute for human domain experts highlights the need for optimal model\nselection. This paper presents the first hallucination survey of popular LLMs\nfor causal discovery. We show that hallucinations exist when using LLMs in\ncausal discovery so the choice of LLM is important. We propose using Retrieval\nAugmented Generation (RAG) to reduce hallucinations when quality data is\navailable. Additionally, we introduce a novel method employing multiple LLMs\nwith an arbiter in a debate to audit edges in causal graphs, achieving a\ncomparable reduction in hallucinations to RAG.", "journal": ""}
{"doi": "10.48550/arXiv.2505.17345", "date": "2025-05-22", "title": "Language models should be subject to repeatable, open, domain-contextualized hallucination benchmarking", "authors": "Justin D. Norman, Michael U. Rivera, D. Alex Hughes", "abstract": "Plausible, but inaccurate, tokens in model-generated text are widely believed\nto be pervasive and problematic for the responsible adoption of language\nmodels. Despite this concern, there is little scientific work that attempts to\nmeasure the prevalence of language model hallucination in a comprehensive way.\nIn this paper, we argue that language models should be evaluated using\nrepeatable, open, and domain-contextualized hallucination benchmarking. We\npresent a taxonomy of hallucinations alongside a case study that demonstrates\nthat when experts are absent from the early stages of data creation, the\nresulting hallucination metrics lack validity and practical utility.", "journal": ""}
{"doi": "10.48550/arXiv.2312.06968", "date": "2023-12-12", "title": "Hallucination Augmented Contrastive Learning for Multimodal Large Language Model", "authors": "Chaoya Jiang, Haiyang Xu, Mengfan Dong, Jiaxing Chen, Wei Ye, Ming Yan, Qinghao Ye, Ji Zhang, Fei Huang, Shikun Zhang", "abstract": "Multi-modal large language models (MLLMs) have been shown to efficiently\nintegrate natural language with visual information to handle multi-modal tasks.\nHowever, MLLMs still face a fundamental limitation of hallucinations, where\nthey tend to generate erroneous or fabricated information. In this paper, we\naddress hallucinations in MLLMs from a novel perspective of representation\nlearning. We first analyzed the representation distribution of textual and\nvisual tokens in MLLM, revealing two important findings: 1) there is a\nsignificant gap between textual and visual representations, indicating\nunsatisfactory cross-modal representation alignment; 2) representations of\ntexts that contain and do not contain hallucinations are entangled, making it\nchallenging to distinguish them. These two observations inspire us with a\nsimple yet effective method to mitigate hallucinations. Specifically, we\nintroduce contrastive learning into MLLMs and use text with hallucination as\nhard negative examples, naturally bringing representations of non-hallucinative\ntext and visual samples closer while pushing way representations of\nnon-hallucinating and hallucinative text. We evaluate our method quantitatively\nand qualitatively, showing its effectiveness in reducing hallucination\noccurrences and improving performance across multiple benchmarks. On the\nMMhal-Bench benchmark, our method obtains a 34.66% /29.5% improvement over the\nbaseline MiniGPT-4/LLaVA. Our code is available on\nhttps://github.com/X-PLUG/mPLUG-HalOwl/tree/main/hacl.", "journal": ""}
{"doi": "10.48550/arXiv.2405.18654", "date": "2024-05-28", "title": "Mitigating Object Hallucination in MLLMs via Data-augmented Phrase-level Alignment", "authors": "Pritam Sarkar, Sayna Ebrahimi, Ali Etemad, Ahmad Beirami, Sercan \u00d6. Ar\u0131k, Tomas Pfister", "abstract": "Despite their significant advancements, Multimodal Large Language Models\n(MLLMs) often generate factually inaccurate information, referred to as\nhallucination. In this work, we address object hallucinations in MLLMs, where\ninformation is generated about an object not present in the input image. We\nintroduce Data-augmented Phrase-level Alignment (DPA), a novel loss which can\nbe applied to instruction-tuned off-the-shelf MLLMs to mitigate hallucinations,\nwhile preserving their general vision-language capabilities. To fine-tune MLLMs\nwith DPA, we first generate a set of `hallucinated' and `correct' response\npairs through generative data augmentation by selectively altering the\nground-truth information of the correct responses at a phrase level. The DPA\nloss is then used to train MLLMs to reduce the likelihood of hallucinated\nphrases compared to the correct ones. Our thorough evaluation on various\nbenchmarks confirms the effectiveness of DPA in mitigating hallucination while\nretaining the out-of-the-box performance of the MLLMs on general tasks. For\ninstance, MLLMs finetuned with DPA, which we refer to as Hallucination\nAttenuated Language and Vision Assistant (HALVA), improve F1 by up to 13.4% on\nhallucination visual question-answering and reduce the hallucination rate by up\nto 4.2% on image description tasks.", "journal": ""}
{"doi": "10.48550/arXiv.2406.09358", "date": "2024-06-13", "title": "Understanding Hallucinations in Diffusion Models through Mode Interpolation", "authors": "Sumukh K Aithal, Pratyush Maini, Zachary C. Lipton, J. Zico Kolter", "abstract": "Colloquially speaking, image generation models based upon diffusion processes\nare frequently said to exhibit \"hallucinations,\" samples that could never occur\nin the training data. But where do such hallucinations come from? In this\npaper, we study a particular failure mode in diffusion models, which we term\nmode interpolation. Specifically, we find that diffusion models smoothly\n\"interpolate\" between nearby data modes in the training set, to generate\nsamples that are completely outside the support of the original training\ndistribution; this phenomenon leads diffusion models to generate artifacts that\nnever existed in real data (i.e., hallucinations). We systematically study the\nreasons for, and the manifestation of this phenomenon. Through experiments on\n1D and 2D Gaussians, we show how a discontinuous loss landscape in the\ndiffusion model's decoder leads to a region where any smooth approximation will\ncause such hallucinations. Through experiments on artificial datasets with\nvarious shapes, we show how hallucination leads to the generation of\ncombinations of shapes that never existed. Finally, we show that diffusion\nmodels in fact know when they go out of support and hallucinate. This is\ncaptured by the high variance in the trajectory of the generated sample towards\nthe final few backward sampling process. Using a simple metric to capture this\nvariance, we can remove over 95% of hallucinations at generation time while\nretaining 96% of in-support samples. We conclude our exploration by showing the\nimplications of such hallucination (and its removal) on the collapse (and\nstabilization) of recursive training on synthetic data with experiments on\nMNIST and 2D Gaussians dataset. We release our code at\nhttps://github.com/locuslab/diffusion-model-hallucination.", "journal": ""}
{"doi": "10.48550/arXiv.2410.13961", "date": "2024-10-17", "title": "From Single to Multi: How LLMs Hallucinate in Multi-Document Summarization", "authors": "Catarina G. Belem, Pouya Pezeshkpour, Hayate Iso, Seiji Maekawa, Nikita Bhutani, Estevam Hruschka", "abstract": "Although many studies have investigated and reduced hallucinations in large\nlanguage models (LLMs) for single-document tasks, research on hallucination in\nmulti-document summarization (MDS) tasks remains largely unexplored.\nSpecifically, it is unclear how the challenges arising from handling multiple\ndocuments (e.g., repetition and diversity of information) affect models\noutputs. In this work, we investigate how hallucinations manifest in LLMs when\nsummarizing topic-specific information from multiple documents. Since no\nbenchmarks exist for investigating hallucinations in MDS, we use existing news\nand conversation datasets, annotated with topic-specific insights, to create\ntwo novel multi-document benchmarks. When evaluating 5 LLMs on our benchmarks,\nwe observe that on average, up to 75% of the content in LLM-generated summary\nis hallucinated, with hallucinations more likely to occur towards the end of\nthe summaries. Moreover, when summarizing non-existent topic-related\ninformation, gpt-3.5-turbo and GPT-4o still generate summaries about 79.35% and\n44% of the time, raising concerns about their tendency to fabricate content. To\nunderstand the characteristics of these hallucinations, we manually evaluate\n700+ insights and find that most errors stem from either failing to follow\ninstructions or producing overly generic insights. Motivated by these\nobservations, we investigate the efficacy of simple post-hoc baselines in\nmitigating hallucinations but find them only moderately effective. Our results\nunderscore the need for more effective approaches to systematically mitigate\nhallucinations in MDS. We release our dataset and code at\ngithub.com/megagonlabs/Hallucination_MDS.", "journal": ""}
{"doi": "10.48550/arXiv.2503.19622", "date": "2025-03-25", "title": "Exploring Hallucination of Large Multimodal Models in Video Understanding: Benchmark, Analysis and Mitigation", "authors": "Hongcheng Gao, Jiashu Qu, Jingyi Tang, Baolong Bi, Yue Liu, Hongyu Chen, Li Liang, Li Su, Qingming Huang", "abstract": "The hallucination of large multimodal models (LMMs), providing responses that\nappear correct but are actually incorrect, limits their reliability and\napplicability. This paper aims to study the hallucination problem of LMMs in\nvideo modality, which is dynamic and more challenging compared to static\nmodalities like images and text. From this motivation, we first present a\ncomprehensive benchmark termed HAVEN for evaluating hallucinations of LMMs in\nvideo understanding tasks. It is built upon three dimensions, i.e.,\nhallucination causes, hallucination aspects, and question formats, resulting in\n6K questions. Then, we quantitatively study 7 influential factors on\nhallucinations, e.g., duration time of videos, model sizes, and model\nreasoning, via experiments of 16 LMMs on the presented benchmark. In addition,\ninspired by recent thinking models like OpenAI o1, we propose a video-thinking\nmodel to mitigate the hallucinations of LMMs via supervised reasoning\nfine-tuning (SRFT) and direct preference optimization (TDPO)-- where SRFT\nenhances reasoning capabilities while TDPO reduces hallucinations in the\nthinking process. Extensive experiments and analyses demonstrate the\neffectiveness. Remarkably, it improves the baseline by 7.65% in accuracy on\nhallucination evaluation and reduces the bias score by 4.5%. The code and data\nare public at https://github.com/Hongcheng-Gao/HAVEN.", "journal": ""}
{"doi": "10.48550/arXiv.2504.04099", "date": "2025-04-05", "title": "TARAC: Mitigating Hallucination in LVLMs via Temporal Attention Real-time Accumulative Connection", "authors": "Chunzhao Xie, Tongxuan Liu, Lei Jiang, Yuting Zeng, jinrong Guo, Yunheng Shen, Weizhe Huang, Jing Li, Xiaohua Xu", "abstract": "Large Vision-Language Models have demonstrated remarkable performance across\nvarious tasks; however, the challenge of hallucinations constrains their\npractical applications. The hallucination problem arises from multiple factors,\nincluding the inherent hallucinations in language models, the limitations of\nvisual encoders in perception, and biases introduced by multimodal data.\nExtensive research has explored ways to mitigate hallucinations. For instance,\nOPERA prevents the model from overly focusing on \"anchor tokens\", thereby\nreducing hallucinations, whereas VCD mitigates hallucinations by employing a\ncontrastive decoding approach. In this paper, we investigate the correlation\nbetween the decay of attention to image tokens and the occurrence of\nhallucinations. Based on this finding, we propose Temporal Attention Real-time\nAccumulative Connection (TARAC), a novel training-free method that dynamically\naccumulates and updates LVLMs' attention on image tokens during generation. By\nenhancing the model's attention to image tokens, TARAC mitigates hallucinations\ncaused by the decay of attention on image tokens. We validate the effectiveness\nof TARAC across multiple models and datasets, demonstrating that our approach\nsubstantially mitigates hallucinations. In particular, TARAC reduces $C_S$ by\n25.2 and $C_I$ by 8.7 compared to VCD on the CHAIR benchmark.", "journal": ""}
{"doi": "10.48550/arXiv.2504.17004", "date": "2025-04-23", "title": "(Im)possibility of Automated Hallucination Detection in Large Language Models", "authors": "Amin Karbasi, Omar Montasser, John Sous, Grigoris Velegkas", "abstract": "Is automated hallucination detection possible? In this work, we introduce a\ntheoretical framework to analyze the feasibility of automatically detecting\nhallucinations produced by large language models (LLMs). Inspired by the\nclassical Gold-Angluin framework for language identification and its recent\nadaptation to language generation by Kleinberg and Mullainathan, we investigate\nwhether an algorithm, trained on examples drawn from an unknown target language\n$K$ (selected from a countable collection) and given access to an LLM, can\nreliably determine whether the LLM's outputs are correct or constitute\nhallucinations.\n  First, we establish an equivalence between hallucination detection and the\nclassical task of language identification. We prove that any hallucination\ndetection method can be converted into a language identification method, and\nconversely, algorithms solving language identification can be adapted for\nhallucination detection. Given the inherent difficulty of language\nidentification, this implies that hallucination detection is fundamentally\nimpossible for most language collections if the detector is trained using only\ncorrect examples from the target language.\n  Second, we show that the use of expert-labeled feedback, i.e., training the\ndetector with both positive examples (correct statements) and negative examples\n(explicitly labeled incorrect statements), dramatically changes this\nconclusion. Under this enriched training regime, automated hallucination\ndetection becomes possible for all countable language collections.\n  These results highlight the essential role of expert-labeled examples in\ntraining hallucination detectors and provide theoretical support for\nfeedback-based methods, such as reinforcement learning with human feedback\n(RLHF), which have proven critical for reliable LLM deployment.", "journal": ""}
{"doi": "10.48550/arXiv.2505.11405", "date": "2025-05-16", "title": "EmotionHallucer: Evaluating Emotion Hallucinations in Multimodal Large Language Models", "authors": "Bohao Xing, Xin Liu, Guoying Zhao, Chengyu Liu, Xiaolan Fu, Heikki K\u00e4lvi\u00e4inen", "abstract": "Emotion understanding is a critical yet challenging task. Recent advances in\nMultimodal Large Language Models (MLLMs) have significantly enhanced their\ncapabilities in this area. However, MLLMs often suffer from hallucinations,\ngenerating irrelevant or nonsensical content. To the best of our knowledge,\ndespite the importance of this issue, there has been no dedicated effort to\nevaluate emotion-related hallucinations in MLLMs. In this work, we introduce\nEmotionHallucer, the first benchmark for detecting and analyzing emotion\nhallucinations in MLLMs. Unlike humans, whose emotion understanding stems from\nthe interplay of biology and social learning, MLLMs rely solely on data-driven\nlearning and lack innate emotional instincts. Fortunately, emotion psychology\nprovides a solid foundation of knowledge about human emotions. Building on\nthis, we assess emotion hallucinations from two dimensions: emotion psychology\nknowledge and real-world multimodal perception. To support robust evaluation,\nwe utilize an adversarial binary question-answer (QA) framework, which employs\ncarefully crafted basic and hallucinated pairs to assess the emotion\nhallucination tendencies of MLLMs. By evaluating 38 LLMs and MLLMs on\nEmotionHallucer, we reveal that: i) most current models exhibit substantial\nissues with emotion hallucinations; ii) closed-source models outperform\nopen-source ones in detecting emotion hallucinations, and reasoning capability\nprovides additional advantages; iii) existing models perform better in emotion\npsychology knowledge than in multimodal emotion perception. As a byproduct,\nthese findings inspire us to propose the PEP-MEK framework, which yields an\naverage improvement of 9.90% in emotion hallucination detection across selected\nmodels. Resources will be available at\nhttps://github.com/xxtars/EmotionHallucer.", "journal": ""}
{"doi": "10.48550/arXiv.1904.03472", "date": "2019-04-06", "title": "Few-Shot Learning via Saliency-guided Hallucination of Samples", "authors": "Hongguang Zhang, Jing Zhang, Piotr Koniusz", "abstract": "Learning new concepts from a few of samples is a standard challenge in\ncomputer vision. The main directions to improve the learning ability of\nfew-shot training models include (i) a robust similarity learning and (ii)\ngenerating or hallucinating additional data from the limited existing samples.\nIn this paper, we follow the latter direction and present a novel data\nhallucination model. Currently, most datapoint generators contain a specialized\nnetwork (i.e., GAN) tasked with hallucinating new datapoints, thus requiring\nlarge numbers of annotated data for their training in the first place. In this\npaper, we propose a novel less-costly hallucination method for few-shot\nlearning which utilizes saliency maps. To this end, we employ a saliency\nnetwork to obtain the foregrounds and backgrounds of available image samples\nand feed the resulting maps into a two-stream network to hallucinate datapoints\ndirectly in the feature space from viable foreground-background combinations.\nTo the best of our knowledge, we are the first to leverage saliency maps for\nsuch a task and we demonstrate their usefulness in hallucinating additional\ndatapoints for few-shot learning. Our proposed network achieves the state of\nthe art on publicly available datasets.", "journal": ""}
{"doi": "10.48550/arXiv.1906.10592", "date": "2019-06-25", "title": "Tactile Hallucinations on Artificial Skin Induced by Homeostasis in a Deep Boltzmann Machine", "authors": "Michael Deistler, Yagmur Yener, Florian Bergner, Pablo Lanillos, Gordon Cheng", "abstract": "Perceptual hallucinations are present in neurological and psychiatric\ndisorders and amputees. While the hallucinations can be drug-induced, it has\nbeen described that they can even be provoked in healthy subjects.\nUnderstanding their manifestation could thus unveil how the brain processes\nsensory information and might evidence the generative nature of perception. In\nthis work, we investigate the generation of tactile hallucinations on\nbiologically inspired, artificial skin. To model tactile hallucinations, we\napply homeostasis, a change in the excitability of neurons during sensory\ndeprivation, in a Deep Boltzmann Machine (DBM). We find that homeostasis\nprompts hallucinations of previously learned patterns on the artificial skin in\nthe absence of sensory input. Moreover, we show that homeostasis is capable of\ninducing the formation of meaningful latent representations in a DBM and that\nit significantly increases the quality of the reconstruction of these latent\nstates. Through this, our work provides a possible explanation for the nature\nof tactile hallucinations and highlights homeostatic processes as a potential\nunderlying mechanism.", "journal": ""}
{"doi": "10.48550/arXiv.2103.15025", "date": "2021-03-28", "title": "On Hallucination and Predictive Uncertainty in Conditional Language Generation", "authors": "Yijun Xiao, William Yang Wang", "abstract": "Despite improvements in performances on different natural language generation\ntasks, deep neural models are prone to hallucinating facts that are incorrect\nor nonexistent. Different hypotheses are proposed and examined separately for\ndifferent tasks, but no systematic explanations are available across these\ntasks. In this study, we draw connections between hallucinations and predictive\nuncertainty in conditional language generation. We investigate their\nrelationship in both image captioning and data-to-text generation and propose a\nsimple extension to beam search to reduce hallucination. Our analysis shows\nthat higher predictive uncertainty corresponds to a higher chance of\nhallucination. Epistemic uncertainty is more indicative of hallucination than\naleatoric or total uncertainties. It helps to achieve better results of trading\nperformance in standard metric for less hallucination with the proposed beam\nsearch variant.", "journal": ""}
{"doi": "10.48550/arXiv.2206.00100", "date": "2022-05-31", "title": "VALHALLA: Visual Hallucination for Machine Translation", "authors": "Yi Li, Rameswar Panda, Yoon Kim, Chun-Fu Chen, Rogerio Feris, David Cox, Nuno Vasconcelos", "abstract": "Designing better machine translation systems by considering auxiliary inputs\nsuch as images has attracted much attention in recent years. While existing\nmethods show promising performance over the conventional text-only translation\nsystems, they typically require paired text and image as input during\ninference, which limits their applicability to real-world scenarios. In this\npaper, we introduce a visual hallucination framework, called VALHALLA, which\nrequires only source sentences at inference time and instead uses hallucinated\nvisual representations for multimodal machine translation. In particular, given\na source sentence an autoregressive hallucination transformer is used to\npredict a discrete visual representation from the input text, and the combined\ntext and hallucinated representations are utilized to obtain the target\ntranslation. We train the hallucination transformer jointly with the\ntranslation transformer using standard backpropagation with cross-entropy\nlosses while being guided by an additional loss that encourages consistency\nbetween predictions using either ground-truth or hallucinated visual\nrepresentations. Extensive experiments on three standard translation datasets\nwith a diverse set of language pairs demonstrate the effectiveness of our\napproach over both text-only baselines and state-of-the-art methods. Project\npage: http://www.svcl.ucsd.edu/projects/valhalla.", "journal": ""}
{"doi": "10.48550/arXiv.2309.06794", "date": "2023-09-13", "title": "Cognitive Mirage: A Review of Hallucinations in Large Language Models", "authors": "Hongbin Ye, Tong Liu, Aijia Zhang, Wei Hua, Weiqiang Jia", "abstract": "As large language models continue to develop in the field of AI, text\ngeneration systems are susceptible to a worrisome phenomenon known as\nhallucination. In this study, we summarize recent compelling insights into\nhallucinations in LLMs. We present a novel taxonomy of hallucinations from\nvarious text generation tasks, thus provide theoretical insights, detection\nmethods and improvement approaches. Based on this, future research directions\nare proposed. Our contribution are threefold: (1) We provide a detailed and\ncomplete taxonomy for hallucinations appearing in text generation tasks; (2) We\nprovide theoretical analyses of hallucinations in LLMs and provide existing\ndetection and improvement methods; (3) We propose several research directions\nthat can be developed in the future. As hallucinations garner significant\nattention from the community, we will maintain updates on relevant research\nprogress.", "journal": ""}
{"doi": "10.48550/arXiv.2309.16781", "date": "2023-09-28", "title": "Hallucination Reduction in Long Input Text Summarization", "authors": "Tohida Rehman, Ronit Mandal, Abhishek Agarwal, Debarshi Kumar Sanyal", "abstract": "Hallucination in text summarization refers to the phenomenon where the model\ngenerates information that is not supported by the input source document.\nHallucination poses significant obstacles to the accuracy and reliability of\nthe generated summaries. In this paper, we aim to reduce hallucinated outputs\nor hallucinations in summaries of long-form text documents. We have used the\nPubMed dataset, which contains long scientific research documents and their\nabstracts. We have incorporated the techniques of data filtering and joint\nentity and summary generation (JAENS) in the fine-tuning of the Longformer\nEncoder-Decoder (LED) model to minimize hallucinations and thereby improve the\nquality of the generated summary. We have used the following metrics to measure\nfactual consistency at the entity level: precision-source, and F1-target. Our\nexperiments show that the fine-tuned LED model performs well in generating the\npaper abstract. Data filtering techniques based on some preprocessing steps\nreduce entity-level hallucinations in the generated summaries in terms of some\nof the factual consistency metrics.", "journal": ""}
{"doi": "10.48550/arXiv.2310.03951", "date": "2023-10-06", "title": "Chain of Natural Language Inference for Reducing Large Language Model Ungrounded Hallucinations", "authors": "Deren Lei, Yaxi Li, Mengya Hu, Mingyu Wang, Vincent Yun, Emily Ching, Eslam Kamal", "abstract": "Large language models (LLMs) can generate fluent natural language texts when\ngiven relevant documents as background context. This ability has attracted\nconsiderable interest in developing industry applications of LLMs. However,\nLLMs are prone to generate hallucinations that are not supported by the\nprovided sources. In this paper, we propose a hierarchical framework to detect\nand mitigate such ungrounded hallucination. Our framework uses Chain of Natural\nLanguage Inference (CoNLI) for hallucination detection and hallucination\nreduction via post-editing. Our approach achieves state-of-the-art performance\non hallucination detection and enhances text quality through rewrite, using\nLLMs without any fine-tuning or domain-specific prompt engineering. We show\nthat this simple plug-and-play framework can serve as an effective choice for\nhallucination detection and reduction, achieving competitive performance across\nvarious contexts.", "journal": ""}
{"doi": "10.48550/arXiv.2310.18344", "date": "2023-10-22", "title": "Chainpoll: A high efficacy method for LLM hallucination detection", "authors": "Robert Friel, Atindriyo Sanyal", "abstract": "Large language models (LLMs) have experienced notable advancements in\ngenerating coherent and contextually relevant responses. However,\nhallucinations - incorrect or unfounded claims - are still prevalent, prompting\nthe creation of automated metrics to detect these in LLM outputs. Our\ncontributions include: introducing ChainPoll, an innovative hallucination\ndetection method that excels compared to its counterparts, and unveiling\nRealHall, a refined collection of benchmark datasets to assess hallucination\ndetection metrics from recent studies. While creating RealHall, we assessed\ntasks and datasets from previous hallucination detection studies and observed\nthat many are not suitable for the potent LLMs currently in use. Overcoming\nthis, we opted for four datasets challenging for modern LLMs and pertinent to\nreal-world scenarios. Using RealHall, we conducted a comprehensive comparison\nof ChainPoll with numerous hallucination metrics from recent studies. Our\nfindings indicate that ChainPoll outperforms in all RealHall benchmarks,\nachieving an overall AUROC of 0.781. This surpasses the next best theoretical\nmethod by 11% and exceeds industry standards by over 23%. Additionally,\nChainPoll is cost-effective and offers greater transparency than other metrics.\nWe introduce two novel metrics to assess LLM hallucinations: Adherence and\nCorrectness. Adherence is relevant to Retrieval Augmented Generation workflows,\nevaluating an LLM's analytical capabilities within given documents and\ncontexts. In contrast, Correctness identifies logical and reasoning errors.", "journal": ""}
{"doi": "10.48550/arXiv.2312.14346", "date": "2023-12-22", "title": "Don't Believe Everything You Read: Enhancing Summarization Interpretability through Automatic Identification of Hallucinations in Large Language Models", "authors": "Priyesh Vakharia, Devavrat Joshi, Meenal Chavan, Dhananjay Sonawane, Bhrigu Garg, Parsa Mazaheri", "abstract": "Large Language Models (LLMs) are adept at text manipulation -- tasks such as\nmachine translation and text summarization. However, these models can also be\nprone to hallucination, which can be detrimental to the faithfulness of any\nanswers that the model provides. Recent works in combating hallucinations in\nLLMs deal with identifying hallucinated sentences and categorizing the\ndifferent ways in which models hallucinate. This paper takes a deep dive into\nLLM behavior with respect to hallucinations, defines a token-level approach to\nidentifying different kinds of hallucinations, and further utilizes this\ntoken-level tagging to improve the interpretability and faithfulness of LLMs in\ndialogue summarization tasks. Through this, the paper presents a new, enhanced\ndataset and a new training paradigm.", "journal": ""}
{"doi": "10.48550/arXiv.2312.14504", "date": "2023-12-22", "title": "Theory of Hallucinations based on Equivariance", "authors": "Hisaichi Shibata", "abstract": "This study aims to acquire knowledge for creating very large language models\nthat are immune to hallucinations. Hallucinations in contemporary large\nlanguage models are often attributed to a misunderstanding of real-world social\nrelationships. Therefore, I hypothesize that very large language models capable\nof thoroughly grasping all these relationships will be free from\nhallucinations. Additionally, I propose that certain types of equivariant\nlanguage models are adept at learning and understanding these relationships.\nBuilding on this, I have developed a specialized cross-entropy error function\nto create a hallucination scale for language models, which measures their\nextent of equivariance acquisition. Utilizing this scale, I tested language\nmodels for their ability to acquire character-level equivariance. In\nparticular, I introduce and employ a novel technique based on T5 (Text To Text\nTransfer Transformer) that efficiently understands permuted input texts without\nthe need for explicit dictionaries to convert token IDs (integers) to texts\n(strings). This T5 model demonstrated a moderate ability to acquire\ncharacter-level equivariance. Additionally, I discovered scale laws that can\naid in developing hallucination-free language models at the character level.\nThis methodology can be extended to assess equivariance acquisition at the word\nlevel, paving the way for very large language models that can comprehensively\nunderstand relationships and, consequently, avoid hallucinations.", "journal": ""}
{"doi": "10.48550/arXiv.2312.15710", "date": "2023-12-25", "title": "Alleviating Hallucinations of Large Language Models through Induced Hallucinations", "authors": "Yue Zhang, Leyang Cui, Wei Bi, Shuming Shi", "abstract": "Despite their impressive capabilities, large language models (LLMs) have been\nobserved to generate responses that include inaccurate or fabricated\ninformation, a phenomenon commonly known as ``hallucination''. In this work, we\npropose a simple \\textit{Induce-then-Contrast} Decoding (ICD) strategy to\nalleviate hallucinations. We first construct a factually weak LLM by inducing\nhallucinations from the original LLMs. Then, we penalize these induced\nhallucinations during decoding to enhance the factuality of the generated\ncontent. Concretely, we determine the final next-token predictions by\namplifying the predictions from the original model and downplaying the induced\nuntruthful predictions via contrastive decoding. Experimental results on both\ndiscrimination-based and generation-based hallucination evaluation benchmarks,\nsuch as TruthfulQA and \\textsc{FActScore}, demonstrate that our proposed ICD\nmethods can effectively enhance the factuality of LLMs across various model\nsizes and families. For example, when equipped with ICD, Llama2-7B-Chat and\nMistral-7B-Instruct achieve performance comparable to ChatGPT and GPT4 on\nTruthfulQA, respectively.", "journal": ""}
{"doi": "10.48550/arXiv.2402.06647", "date": "2024-02-02", "title": "A Survey on Large Language Model Hallucination via a Creativity Perspective", "authors": "Xuhui Jiang, Yuxing Tian, Fengrui Hua, Chengjin Xu, Yuanzhuo Wang, Jian Guo", "abstract": "Hallucinations in large language models (LLMs) are always seen as\nlimitations. However, could they also be a source of creativity? This survey\nexplores this possibility, suggesting that hallucinations may contribute to LLM\napplication by fostering creativity. This survey begins with a review of the\ntaxonomy of hallucinations and their negative impact on LLM reliability in\ncritical applications. Then, through historical examples and recent relevant\ntheories, the survey explores the potential creative benefits of hallucinations\nin LLMs. To elucidate the value and evaluation criteria of this connection, we\ndelve into the definitions and assessment methods of creativity. Following the\nframework of divergent and convergent thinking phases, the survey\nsystematically reviews the literature on transforming and harnessing\nhallucinations for creativity in LLMs. Finally, the survey discusses future\nresearch directions, emphasizing the need to further explore and refine the\napplication of hallucinations in creative processes within LLMs.", "journal": ""}
{"doi": "10.48550/arXiv.2402.09733", "date": "2024-02-15", "title": "Do LLMs Know about Hallucination? An Empirical Investigation of LLM's Hidden States", "authors": "Hanyu Duan, Yi Yang, Kar Yan Tam", "abstract": "Large Language Models (LLMs) can make up answers that are not real, and this\nis known as hallucination. This research aims to see if, how, and to what\nextent LLMs are aware of hallucination. More specifically, we check whether and\nhow an LLM reacts differently in its hidden states when it answers a question\nright versus when it hallucinates. To do this, we introduce an experimental\nframework which allows examining LLM's hidden states in different hallucination\nsituations. Building upon this framework, we conduct a series of experiments\nwith language models in the LLaMA family (Touvron et al., 2023). Our empirical\nfindings suggest that LLMs react differently when processing a genuine response\nversus a fabricated one. We then apply various model interpretation techniques\nto help understand and explain the findings better. Moreover, informed by the\nempirical observations, we show great potential of using the guidance derived\nfrom LLM's hidden representation space to mitigate hallucination. We believe\nthis work provides insights into how LLMs produce hallucinated answers and how\nto make them occur less often.", "journal": ""}
{"doi": "10.48550/arXiv.2402.10412", "date": "2024-02-16", "title": "Measuring and Reducing LLM Hallucination without Gold-Standard Answers", "authors": "Jiaheng Wei, Yuanshun Yao, Jean-Francois Ton, Hongyi Guo, Andrew Estornell, Yang Liu", "abstract": "LLM hallucination, i.e. generating factually incorrect yet seemingly\nconvincing answers, is currently a major threat to the trustworthiness and\nreliability of LLMs. The first step towards solving this complicated problem is\nto measure it. However, existing hallucination metrics require having a\nbenchmark dataset with gold-standard answers, i.e. \"best\" or \"correct\" answers\nwritten by humans. Such requirements make hallucination measurement costly and\nprone to human errors. In this work, we propose Factualness Evaluations via\nWeighting LLMs (FEWL), an innovative hallucination metric that is specifically\ndesigned for the scenario when gold-standard answers are absent. FEWL leverages\nthe answers from off-the-shelf LLMs that serve as a proxy of gold-standard\nanswers. The key challenge is how to quantify the expertise of reference LLMs\nresourcefully. We show FEWL has certain theoretical guarantees and demonstrate\nempirically it gives more accurate hallucination measures than naively using\nreference LLMs. We also show how to leverage FEWL to reduce hallucination\nthrough both in-context learning and supervised fine-tuning. Extensive\nexperiment results on Truthful-QA, CHALE, and HaluEval datasets demonstrate the\neffectiveness of FEWL.", "journal": ""}
{"doi": "10.48550/arXiv.2402.10496", "date": "2024-02-16", "title": "Comparing Hallucination Detection Metrics for Multilingual Generation", "authors": "Haoqiang Kang, Terra Blevins, Luke Zettlemoyer", "abstract": "While many hallucination detection techniques have been evaluated on English\ntext, their effectiveness in multilingual contexts remains unknown. This paper\nassesses how well various factual hallucination detection metrics (lexical\nmetrics like ROUGE and Named Entity Overlap, and Natural Language Inference\n(NLI)-based metrics) identify hallucinations in generated biographical\nsummaries across languages. We compare how well automatic metrics correlate to\neach other and whether they agree with human judgments of factuality. Our\nanalysis reveals that while the lexical metrics are ineffective, NLI-based\nmetrics perform well, correlating with human annotations in many settings and\noften outperforming supervised models. However, NLI metrics are still limited,\nas they do not detect single-fact hallucinations well and fail for\nlower-resource languages. Therefore, our findings highlight the gaps in\nexisiting hallucination detection methods for non-English languages and\nmotivate future research to develop more robust multilingual detection methods\nfor LLM hallucinations.", "journal": ""}
{"doi": "10.48550/arXiv.2402.19103", "date": "2024-02-29", "title": "Whispers that Shake Foundations: Analyzing and Mitigating False Premise Hallucinations in Large Language Models", "authors": "Hongbang Yuan, Pengfei Cao, Zhuoran Jin, Yubo Chen, Daojian Zeng, Kang Liu, Jun Zhao", "abstract": "Large Language Models (LLMs) have shown impressive capabilities but still\nsuffer from the issue of hallucinations. A significant type of this issue is\nthe false premise hallucination, which we define as the phenomenon when LLMs\ngenerate hallucinated text when confronted with false premise questions. In\nthis paper, we perform a comprehensive analysis of the false premise\nhallucination and elucidate its internal working mechanism: a small subset of\nattention heads (which we designate as false premise heads) disturb the\nknowledge extraction process, leading to the occurrence of false premise\nhallucination. Based on our analysis, we propose \\textbf{FAITH} (\\textbf{F}alse\npremise \\textbf{A}ttention head constra\\textbf{I}ining for mi\\textbf{T}igating\n\\textbf{H}allucinations), a novel and effective method to mitigate false\npremise hallucinations. It constrains the false premise attention heads during\nthe model inference process. Impressively, extensive experiments demonstrate\nthat constraining only approximately $1\\%$ of the attention heads in the model\nyields a notable increase of nearly $20\\%$ of model performance.", "journal": ""}
{"doi": "10.48550/arXiv.2403.01373", "date": "2024-03-03", "title": "Quantity Matters: Towards Assessing and Mitigating Number Hallucination in Large Vision-Language Models", "authors": "Huixuan Zhang, Junzhe Zhang, Xiaojun Wan", "abstract": "Large-scale vision-language models have demonstrated impressive skill in\nhandling tasks that involve both areas. Nevertheless, these models frequently\nexperience significant issues with generating inaccurate information, which is\nhallucination. In this study, we concentrate on a specific type of\nhallucination-number hallucination, referring to models incorrectly identifying\nthe number of certain objects in pictures. We perform quantitative evaluations\nregarding number hallucination, showing it to be critical in major open-source\nlarge vision-language models. Furthermore, we utilizes two related tasks to\nconduct an in-depth analysis of number hallucination, revealing the severe\ninner and outer inconsistency among all tasks. Based on this examination, we\ndevise a training approach aimed at improving consistency to reduce number\nhallucinations, which leads to an 8% enhancement in performance over direct\nfinetuning methods. Our code and dataset will be released to the community.", "journal": ""}
{"doi": "10.48550/arXiv.2403.01548", "date": "2024-03-03", "title": "In-Context Sharpness as Alerts: An Inner Representation Perspective for Hallucination Mitigation", "authors": "Shiqi Chen, Miao Xiong, Junteng Liu, Zhengxuan Wu, Teng Xiao, Siyang Gao, Junxian He", "abstract": "Large language models (LLMs) frequently hallucinate and produce factual\nerrors, yet our understanding of why they make these errors remains limited. In\nthis study, we delve into the underlying mechanisms of LLM hallucinations from\nthe perspective of inner representations, and discover a salient pattern\nassociated with hallucinations: correct generations tend to have sharper\ncontext activations in the hidden states of the in-context tokens, compared to\nthe incorrect ones. Leveraging this insight, we propose an entropy-based metric\nto quantify the ``sharpness'' among the in-context hidden states and\nincorporate it into the decoding process to formulate a constrained decoding\napproach. Experiments on various knowledge-seeking and hallucination benchmarks\ndemonstrate our approach's consistent effectiveness, for example, achieving up\nto an 8.6 point improvement on TruthfulQA. We believe this study can improve\nour understanding of hallucinations and serve as a practical solution for\nhallucination mitigation.", "journal": ""}
{"doi": "10.48550/arXiv.2403.06448", "date": "2024-03-11", "title": "Unsupervised Real-Time Hallucination Detection based on the Internal States of Large Language Models", "authors": "Weihang Su, Changyue Wang, Qingyao Ai, Yiran HU, Zhijing Wu, Yujia Zhou, Yiqun Liu", "abstract": "Hallucinations in large language models (LLMs) refer to the phenomenon of\nLLMs producing responses that are coherent yet factually inaccurate. This issue\nundermines the effectiveness of LLMs in practical applications, necessitating\nresearch into detecting and mitigating hallucinations of LLMs. Previous studies\nhave mainly concentrated on post-processing techniques for hallucination\ndetection, which tend to be computationally intensive and limited in\neffectiveness due to their separation from the LLM's inference process. To\novercome these limitations, we introduce MIND, an unsupervised training\nframework that leverages the internal states of LLMs for real-time\nhallucination detection without requiring manual annotations. Additionally, we\npresent HELM, a new benchmark for evaluating hallucination detection across\nmultiple LLMs, featuring diverse LLM outputs and the internal states of LLMs\nduring their inference process. Our experiments demonstrate that MIND\noutperforms existing state-of-the-art methods in hallucination detection.", "journal": ""}
{"doi": "10.48550/arXiv.2403.17306", "date": "2024-03-26", "title": "Visual Hallucination: Definition, Quantification, and Prescriptive Remediations", "authors": "Anku Rani, Vipula Rawte, Harshad Sharma, Neeraj Anand, Krishnav Rajbangshi, Amit Sheth, Amitava Das", "abstract": "The troubling rise of hallucination presents perhaps the most significant\nimpediment to the advancement of responsible AI. In recent times, considerable\nresearch has focused on detecting and mitigating hallucination in Large\nLanguage Models (LLMs). However, it's worth noting that hallucination is also\nquite prevalent in Vision-Language models (VLMs). In this paper, we offer a\nfine-grained discourse on profiling VLM hallucination based on two tasks: i)\nimage captioning, and ii) Visual Question Answering (VQA). We delineate eight\nfine-grained orientations of visual hallucination: i) Contextual Guessing, ii)\nIdentity Incongruity, iii) Geographical Erratum, iv) Visual Illusion, v) Gender\nAnomaly, vi) VLM as Classifier, vii) Wrong Reading, and viii) Numeric\nDiscrepancy. We curate Visual HallucInation eLiciTation (VHILT), a publicly\navailable dataset comprising 2,000 samples generated using eight VLMs across\ntwo tasks of captioning and VQA along with human annotations for the categories\nas mentioned earlier.", "journal": ""}
{"doi": "10.48550/arXiv.2403.18715", "date": "2024-03-27", "title": "Mitigating Hallucinations in Large Vision-Language Models with Instruction Contrastive Decoding", "authors": "Xintong Wang, Jingheng Pan, Liang Ding, Chris Biemann", "abstract": "Large Vision-Language Models (LVLMs) are increasingly adept at generating\ncontextually detailed and coherent responses from visual inputs. However, their\napplication in multimodal decision-making and open-ended generation is hindered\nby a notable rate of hallucinations, where generated text inaccurately\nrepresents the visual contents. To address this issue, this paper introduces\nthe Instruction Contrastive Decoding (ICD) method, a novel approach designed to\nreduce hallucinations during LVLM inference. Our method is inspired by our\nobservation that what we call disturbance instructions significantly exacerbate\nhallucinations in multimodal fusion modules. ICD contrasts distributions from\nstandard and instruction disturbance, thereby increasing alignment uncertainty\nand effectively subtracting hallucinated concepts from the original\ndistribution. Through comprehensive experiments on discriminative benchmarks\n(POPE and MME) and a generative benchmark (LLaVa-Bench), we demonstrate that\nICD significantly mitigates both object-level and attribute-level\nhallucinations. Moreover, our method not only addresses hallucinations but also\nsignificantly enhances the general perception and recognition capabilities of\nLVLMs.", "journal": ""}
{"doi": "10.48550/arXiv.2404.02904", "date": "2024-04-03", "title": "ALOHa: A New Measure for Hallucination in Captioning Models", "authors": "Suzanne Petryk, David M. Chan, Anish Kachinthaya, Haodi Zou, John Canny, Joseph E. Gonzalez, Trevor Darrell", "abstract": "Despite recent advances in multimodal pre-training for visual description,\nstate-of-the-art models still produce captions containing errors, such as\nhallucinating objects not present in a scene. The existing prominent metric for\nobject hallucination, CHAIR, is limited to a fixed set of MS COCO objects and\nsynonyms. In this work, we propose a modernized open-vocabulary metric, ALOHa,\nwhich leverages large language models (LLMs) to measure object hallucinations.\nSpecifically, we use an LLM to extract groundable objects from a candidate\ncaption, measure their semantic similarity to reference objects from captions\nand object detections, and use Hungarian matching to produce a final\nhallucination score. We show that ALOHa correctly identifies 13.6% more\nhallucinated objects than CHAIR on HAT, a new gold-standard subset of MS COCO\nCaptions annotated for hallucinations, and 30.8% more on nocaps, where objects\nextend beyond MS COCO categories. Our code is available at\nhttps://davidmchan.github.io/aloha/.", "journal": ""}
{"doi": "10.48550/arXiv.2404.02935", "date": "2024-04-03", "title": "KnowHalu: Hallucination Detection via Multi-Form Knowledge Based Factual Checking", "authors": "Jiawei Zhang, Chejian Xu, Yu Gai, Freddy Lecue, Dawn Song, Bo Li", "abstract": "This paper introduces KnowHalu, a novel approach for detecting hallucinations\nin text generated by large language models (LLMs), utilizing step-wise\nreasoning, multi-formulation query, multi-form knowledge for factual checking,\nand fusion-based detection mechanism. As LLMs are increasingly applied across\nvarious domains, ensuring that their outputs are not hallucinated is critical.\nRecognizing the limitations of existing approaches that either rely on the\nself-consistency check of LLMs or perform post-hoc fact-checking without\nconsidering the complexity of queries or the form of knowledge, KnowHalu\nproposes a two-phase process for hallucination detection. In the first phase,\nit identifies non-fabrication hallucinations--responses that, while factually\ncorrect, are irrelevant or non-specific to the query. The second phase,\nmulti-form based factual checking, contains five key steps: reasoning and query\ndecomposition, knowledge retrieval, knowledge optimization, judgment\ngeneration, and judgment aggregation. Our extensive evaluations demonstrate\nthat KnowHalu significantly outperforms SOTA baselines in detecting\nhallucinations across diverse tasks, e.g., improving by 15.65% in QA tasks and\n5.50% in summarization tasks, highlighting its effectiveness and versatility in\ndetecting hallucinations in LLM-generated content.", "journal": ""}
{"doi": "10.48550/arXiv.2406.14492", "date": "2024-06-20", "title": "Does Object Grounding Really Reduce Hallucination of Large Vision-Language Models?", "authors": "Gregor Geigle, Radu Timofte, Goran Glava\u0161", "abstract": "Large vision-language models (LVLMs) have recently dramatically pushed the\nstate of the art in image captioning and many image understanding tasks (e.g.,\nvisual question answering). LVLMs, however, often \\textit{hallucinate} and\nproduce captions that mention concepts that cannot be found in the image. These\nhallucinations erode the trustworthiness of LVLMs and are arguably among the\nmain obstacles to their ubiquitous adoption. Recent work suggests that addition\nof grounding objectives -- those that explicitly align image regions or objects\nto text spans -- reduces the amount of LVLM hallucination. Although intuitive,\nthis claim is not empirically justified as the reduction effects have been\nestablished, we argue, with flawed evaluation protocols that (i) rely on data\n(i.e., MSCOCO) that has been extensively used in LVLM training and (ii) measure\nhallucination via question answering rather than open-ended caption generation.\nIn this work, in contrast, we offer the first systematic analysis of the effect\nof fine-grained object grounding on LVLM hallucination under an evaluation\nprotocol that more realistically captures LVLM hallucination in open\ngeneration. Our extensive experiments over three backbone LLMs reveal that\ngrounding objectives have little to no effect on object hallucination in open\ncaption generation.", "journal": ""}
{"doi": "10.48550/arXiv.2406.17642", "date": "2024-06-25", "title": "Banishing LLM Hallucinations Requires Rethinking Generalization", "authors": "Johnny Li, Saksham Consul, Eda Zhou, James Wong, Naila Farooqui, Yuxin Ye, Nithyashree Manohar, Zhuxiaona Wei, Tian Wu, Ben Echols, Sharon Zhou, Gregory Diamos", "abstract": "Despite their powerful chat, coding, and reasoning abilities, Large Language\nModels (LLMs) frequently hallucinate. Conventional wisdom suggests that\nhallucinations are a consequence of a balance between creativity and\nfactuality, which can be mitigated, but not eliminated, by grounding the LLM in\nexternal knowledge sources. Through extensive systematic experiments, we show\nthat these traditional approaches fail to explain why LLMs hallucinate in\npractice. Specifically, we show that LLMs augmented with a massive Mixture of\nMemory Experts (MoME) can easily memorize large datasets of random numbers. We\ncorroborate these experimental findings with a theoretical construction showing\nthat simple neural networks trained to predict the next token hallucinate when\nthe training loss is above a threshold as it usually does in practice when\ntraining on internet scale data. We interpret our findings by comparing against\ntraditional retrieval methods for mitigating hallucinations. We use our\nfindings to design a first generation model for removing hallucinations --\nLamini-1 -- that stores facts in a massive mixture of millions of memory\nexperts that are retrieved dynamically.", "journal": ""}
{"doi": "10.48550/arXiv.2406.19537", "date": "2024-06-27", "title": "Handling Ontology Gaps in Semantic Parsing", "authors": "Andrea Bacciu, Marco Damonte, Marco Basaldella, Emilio Monti", "abstract": "The majority of Neural Semantic Parsing (NSP) models are developed with the\nassumption that there are no concepts outside the ones such models can\nrepresent with their target symbols (closed-world assumption). This assumption\nleads to generate hallucinated outputs rather than admitting their lack of\nknowledge. Hallucinations can lead to wrong or potentially offensive responses\nto users. Hence, a mechanism to prevent this behavior is crucial to build\ntrusted NSP-based Question Answering agents. To that end, we propose the\nHallucination Simulation Framework (HSF), a general setting for stimulating and\nanalyzing NSP model hallucinations. The framework can be applied to any NSP\ntask with a closed-ontology. Using the proposed framework and KQA Pro as the\nbenchmark dataset, we assess state-of-the-art techniques for hallucination\ndetection. We then present a novel hallucination detection strategy that\nexploits the computational graph of the NSP model to detect the NSP\nhallucinations in the presence of ontology gaps, out-of-domain utterances, and\nto recognize NSP errors, improving the F1-Score respectively by ~21, ~24% and\n~1%. This is the first work in closed-ontology NSP that addresses the problem\nof recognizing ontology gaps. We release our code and checkpoints at\nhttps://github.com/amazon-science/handling-ontology-gaps-in-semantic-parsing.", "journal": ""}
{"doi": "10.48550/arXiv.2407.02730", "date": "2024-07-03", "title": "MedVH: Towards Systematic Evaluation of Hallucination for Large Vision Language Models in the Medical Context", "authors": "Zishan Gu, Changchang Yin, Fenglin Liu, Ping Zhang", "abstract": "Large Vision Language Models (LVLMs) have recently achieved superior\nperformance in various tasks on natural image and text data, which inspires a\nlarge amount of studies for LVLMs fine-tuning and training. Despite their\nadvancements, there has been scant research on the robustness of these models\nagainst hallucination when fine-tuned on smaller datasets. In this study, we\nintroduce a new benchmark dataset, the Medical Visual Hallucination Test\n(MedVH), to evaluate the hallucination of domain-specific LVLMs. MedVH\ncomprises five tasks to evaluate hallucinations in LVLMs within the medical\ncontext, which includes tasks for comprehensive understanding of textual and\nvisual input, as well as long textual response generation. Our extensive\nexperiments with both general and medical LVLMs reveal that, although medical\nLVLMs demonstrate promising performance on standard medical tasks, they are\nparticularly susceptible to hallucinations, often more so than the general\nmodels, raising significant concerns about the reliability of these\ndomain-specific models. For medical LVLMs to be truly valuable in real-world\napplications, they must not only accurately integrate medical knowledge but\nalso maintain robust reasoning abilities to prevent hallucination. Our work\npaves the way for future evaluations of these studies.", "journal": ""}
{"doi": "10.48550/arXiv.2407.04121", "date": "2024-07-04", "title": "Hallucination Detection: Robustly Discerning Reliable Answers in Large Language Models", "authors": "Yuyan Chen, Qiang Fu, Yichen Yuan, Zhihao Wen, Ge Fan, Dayiheng Liu, Dongmei Zhang, Zhixu Li, Yanghua Xiao", "abstract": "Large Language Models (LLMs) have gained widespread adoption in various\nnatural language processing tasks, including question answering and dialogue\nsystems. However, a major drawback of LLMs is the issue of hallucination, where\nthey generate unfaithful or inconsistent content that deviates from the input\nsource, leading to severe consequences. In this paper, we propose a robust\ndiscriminator named RelD to effectively detect hallucination in LLMs' generated\nanswers. RelD is trained on the constructed RelQA, a bilingual\nquestion-answering dialogue dataset along with answers generated by LLMs and a\ncomprehensive set of metrics. Our experimental results demonstrate that the\nproposed RelD successfully detects hallucination in the answers generated by\ndiverse LLMs. Moreover, it performs well in distinguishing hallucination in\nLLMs' generated answers from both in-distribution and out-of-distribution\ndatasets. Additionally, we also conduct a thorough analysis of the types of\nhallucinations that occur and present valuable insights. This research\nsignificantly contributes to the detection of reliable answers generated by\nLLMs and holds noteworthy implications for mitigating hallucination in the\nfuture work.", "journal": ""}
{"doi": "10.48550/arXiv.2407.04485", "date": "2024-07-05", "title": "Leveraging Graph Structures to Detect Hallucinations in Large Language Models", "authors": "Noa Nonkes, Sergei Agaronian, Evangelos Kanoulas, Roxana Petcu", "abstract": "Large language models are extensively applied across a wide range of tasks,\nsuch as customer support, content creation, educational tutoring, and providing\nfinancial guidance. However, a well-known drawback is their predisposition to\ngenerate hallucinations. This damages the trustworthiness of the information\nthese models provide, impacting decision-making and user confidence. We propose\na method to detect hallucinations by looking at the structure of the latent\nspace and finding associations within hallucinated and non-hallucinated\ngenerations. We create a graph structure that connects generations that lie\nclosely in the embedding space. Moreover, we employ a Graph Attention Network\nwhich utilizes message passing to aggregate information from neighboring nodes\nand assigns varying degrees of importance to each neighbor based on their\nrelevance. Our findings show that 1) there exists a structure in the latent\nspace that differentiates between hallucinated and non-hallucinated\ngenerations, 2) Graph Attention Networks can learn this structure and\ngeneralize it to unseen generations, and 3) the robustness of our method is\nenhanced when incorporating contrastive learning. When evaluated against\nevidence-based benchmarks, our model performs similarly without access to\nsearch-based methods.", "journal": "Proceedings of the TextGraphs-17 Workshop, ACL 2024"}
{"doi": "10.48550/arXiv.2407.04831", "date": "2024-07-05", "title": "Code Hallucination", "authors": "Mirza Masfiqur Rahman, Ashish Kundu", "abstract": "Generative models such as large language models are extensively used as code\ncopilots and for whole program generation. However, the programs they generate\noften have questionable correctness, authenticity and reliability in terms of\nintegration as they might not follow the user requirements, provide incorrect\nand/or nonsensical outputs, or even contain semantic/syntactic errors - overall\nknown as LLM hallucination. In this work, we present several types of code\nhallucination. We have generated such hallucinated code manually using large\nlanguage models. We also present a technique - HallTrigger, in order to\ndemonstrate efficient ways of generating arbitrary code hallucination. Our\nmethod leverages 3 different dynamic attributes of LLMs to craft prompts that\ncan successfully trigger hallucinations from models without the need to access\nmodel architecture or parameters. Results from popular blackbox models suggest\nthat HallTrigger is indeed effective and the pervasive LLM hallucination have\nsheer impact on software development.", "journal": ""}
{"doi": "10.48550/arXiv.2407.13702", "date": "2024-07-18", "title": "ANHALTEN: Cross-Lingual Transfer for German Token-Level Reference-Free Hallucination Detection", "authors": "Janek Herrlein, Chia-Chien Hung, Goran Glava\u0161", "abstract": "Research on token-level reference-free hallucination detection has\npredominantly focused on English, primarily due to the scarcity of robust\ndatasets in other languages. This has hindered systematic investigations into\nthe effectiveness of cross-lingual transfer for this important NLP application.\nTo address this gap, we introduce ANHALTEN, a new evaluation dataset that\nextends the English hallucination detection dataset to German. To the best of\nour knowledge, this is the first work that explores cross-lingual transfer for\ntoken-level reference-free hallucination detection. ANHALTEN contains gold\nannotations in German that are parallel (i.e., directly comparable to the\noriginal English instances). We benchmark several prominent cross-lingual\ntransfer approaches, demonstrating that larger context length leads to better\nhallucination detection in German, even without succeeding context.\nImportantly, we show that the sample-efficient few-shot transfer is the most\neffective approach in most setups. This highlights the practical benefits of\nminimal annotation effort in the target language for reference-free\nhallucination detection. Aiming to catalyze future research on cross-lingual\ntoken-level reference-free hallucination detection, we make ANHALTEN publicly\navailable: https://github.com/janekh24/anhalten", "journal": ""}
{"doi": "10.48550/arXiv.2407.15975", "date": "2024-07-22", "title": "Multilingual Fine-Grained News Headline Hallucination Detection", "authors": "Jiaming Shen, Tianqi Liu, Jialu Liu, Zhen Qin, Jay Pavagadhi, Simon Baumgartner, Michael Bendersky", "abstract": "The popularity of automated news headline generation has surged with\nadvancements in pre-trained language models. However, these models often suffer\nfrom the ``hallucination'' problem, where the generated headline is not fully\nsupported by its source article. Efforts to address this issue have\npredominantly focused on English, using over-simplistic classification schemes\nthat overlook nuanced hallucination types. In this study, we introduce the\nfirst multilingual, fine-grained news headline hallucination detection dataset\nthat contains over 11 thousand pairs in 5 languages, each annotated with\ndetailed hallucination types by experts. We conduct extensive experiments on\nthis dataset under two settings. First, we implement several supervised\nfine-tuning approaches as preparatory solutions and demonstrate this dataset's\nchallenges and utilities. Second, we test various large language models'\nin-context learning abilities and propose two novel techniques,\nlanguage-dependent demonstration selection and coarse-to-fine prompting, to\nboost the few-shot hallucination detection performance in terms of the\nexample-F1 metric. We release this dataset to foster further research in\nmultilingual, fine-grained headline hallucination detection.", "journal": ""}
{"doi": "10.48550/arXiv.2409.00238", "date": "2024-08-30", "title": "Pre-Training Multimodal Hallucination Detectors with Corrupted Grounding Data", "authors": "Spencer Whitehead, Jacob Phillips, Sean Hendryx", "abstract": "Multimodal language models can exhibit hallucinations in their outputs, which\nlimits their reliability. The ability to automatically detect these errors is\nimportant for mitigating them, but has been less explored and existing efforts\ndo not localize hallucinations, instead framing this as a classification task.\nIn this work, we first pose multimodal hallucination detection as a sequence\nlabeling task where models must localize hallucinated text spans and present a\nstrong baseline model. Given the high cost of human annotations for this task,\nwe propose an approach to improve the sample efficiency of these models by\ncreating corrupted grounding data, which we use for pre-training. Leveraging\nphrase grounding data, we generate hallucinations to replace grounded spans and\ncreate hallucinated text. Experiments show that pre-training on this data\nimproves sample efficiency when fine-tuning, and that the learning signal from\nthe grounding data plays an important role in these improvements.", "journal": ""}
{"doi": "10.48550/arXiv.2409.16727", "date": "2024-09-25", "title": "RoleBreak: Character Hallucination as a Jailbreak Attack in Role-Playing Systems", "authors": "Yihong Tang, Bo Wang, Xu Wang, Dongming Zhao, Jing Liu, Jijun Zhang, Ruifang He, Yuexian Hou", "abstract": "Role-playing systems powered by large language models (LLMs) have become\nincreasingly influential in emotional communication applications. However,\nthese systems are susceptible to character hallucinations, where the model\ndeviates from predefined character roles and generates responses that are\ninconsistent with the intended persona. This paper presents the first\nsystematic analysis of character hallucination from an attack perspective,\nintroducing the RoleBreak framework. Our framework identifies two core\nmechanisms-query sparsity and role-query conflict-as key factors driving\ncharacter hallucination. Leveraging these insights, we construct a novel\ndataset, RoleBreakEval, to evaluate existing hallucination mitigation\ntechniques. Our experiments reveal that even enhanced models trained to\nminimize hallucination remain vulnerable to attacks. To address these\nvulnerabilities, we propose a novel defence strategy, the Narrator Mode, which\ngenerates supplemental context through narration to mitigate role-query\nconflicts and improve query generalization. Experimental results demonstrate\nthat Narrator Mode significantly outperforms traditional refusal-based\nstrategies by reducing hallucinations, enhancing fidelity to character roles\nand queries, and improving overall narrative coherence.", "journal": ""}
{"doi": "10.48550/arXiv.2409.17504", "date": "2024-09-26", "title": "HaloScope: Harnessing Unlabeled LLM Generations for Hallucination Detection", "authors": "Xuefeng Du, Chaowei Xiao, Yixuan Li", "abstract": "The surge in applications of large language models (LLMs) has prompted\nconcerns about the generation of misleading or fabricated information, known as\nhallucinations. Therefore, detecting hallucinations has become critical to\nmaintaining trust in LLM-generated content. A primary challenge in learning a\ntruthfulness classifier is the lack of a large amount of labeled truthful and\nhallucinated data. To address the challenge, we introduce HaloScope, a novel\nlearning framework that leverages the unlabeled LLM generations in the wild for\nhallucination detection. Such unlabeled data arises freely upon deploying LLMs\nin the open world, and consists of both truthful and hallucinated information.\nTo harness the unlabeled data, we present an automated membership estimation\nscore for distinguishing between truthful and untruthful generations within\nunlabeled mixture data, thereby enabling the training of a binary truthfulness\nclassifier on top. Importantly, our framework does not require extra data\ncollection and human annotations, offering strong flexibility and practicality\nfor real-world applications. Extensive experiments show that HaloScope can\nachieve superior hallucination detection performance, outperforming the\ncompetitive rivals by a significant margin. Code is available at\nhttps://github.com/deeplearningwisc/haloscope.", "journal": ""}
{"doi": "10.48550/arXiv.2409.20429", "date": "2024-09-30", "title": "HELPD: Mitigating Hallucination of LVLMs by Hierarchical Feedback Learning with Vision-enhanced Penalty Decoding", "authors": "Fan Yuan, Chi Qin, Xiaogang Xu, Piji Li", "abstract": "Large Vision-Language Models (LVLMs) have shown remarkable performance on\nmany visual-language tasks. However, these models still suffer from multimodal\nhallucination, which means the generation of objects or content that violates\nthe images. Many existing work detects hallucination by directly judging\nwhether an object exists in an image, overlooking the association between the\nobject and semantics. To address this issue, we propose Hierarchical Feedback\nLearning with Vision-enhanced Penalty Decoding (HELPD). This framework\nincorporates hallucination feedback at both object and sentence semantic\nlevels. Remarkably, even with a marginal degree of training, this approach can\nalleviate over 15% of hallucination. Simultaneously, HELPD penalizes the output\nlogits according to the image attention window to avoid being overly affected\nby generated text. HELPD can be seamlessly integrated with any LVLMs. Our\nexperiments demonstrate that the proposed framework yields favorable results\nacross multiple hallucination benchmarks. It effectively mitigates\nhallucination for different LVLMs and concurrently improves their text\ngeneration quality.", "journal": ""}
{"doi": "10.48550/arXiv.2410.15359", "date": "2024-10-20", "title": "A Survey of Hallucination in Large Visual Language Models", "authors": "Wei Lan, Wenyi Chen, Qingfeng Chen, Shirui Pan, Huiyu Zhou, Yi Pan", "abstract": "The Large Visual Language Models (LVLMs) enhances user interaction and\nenriches user experience by integrating visual modality on the basis of the\nLarge Language Models (LLMs). It has demonstrated their powerful information\nprocessing and generation capabilities. However, the existence of\nhallucinations has limited the potential and practical effectiveness of LVLM in\nvarious fields. Although lots of work has been devoted to the issue of\nhallucination mitigation and correction, there are few reviews to summary this\nissue. In this survey, we first introduce the background of LVLMs and\nhallucinations. Then, the structure of LVLMs and main causes of hallucination\ngeneration are introduced. Further, we summary recent works on hallucination\ncorrection and mitigation. In addition, the available hallucination evaluation\nbenchmarks for LVLMs are presented from judgmental and generative perspectives.\nFinally, we suggest some future research directions to enhance the\ndependability and utility of LVLMs.", "journal": ""}
{"doi": "10.48550/arXiv.2410.15778", "date": "2024-10-21", "title": "Reducing Hallucinations in Vision-Language Models via Latent Space Steering", "authors": "Sheng Liu, Haotian Ye, Lei Xing, James Zou", "abstract": "Hallucination poses a challenge to the deployment of large vision-language\nmodels (LVLMs) in applications. Unlike in large language models (LLMs),\nhallucination in LVLMs often arises from misalignments between visual inputs\nand textual outputs. This paper investigates the underlying mechanisms of\nhallucination, focusing on the unique structure of LVLMs that distinguishes\nthem from large language models (LLMs). We identify that hallucinations often\narise from the sensitivity of text decoders to vision inputs, a natural\nphenomenon when image encoders and text decoders are pre-trained separately.\nInspired by this, we introduce Visual and Textual Intervention (VTI), a novel\ntechnique designed to reduce hallucinations by steering latent space\nrepresentations during inference to enhance the stability of vision features.\nAs a task-agnostic test-time intervention, VTI can be easily applied to any\nproblem without additional cost. Extensive experiments demonstrate that it can\neffectively reduce hallucinations and outperform baseline methods across\nmultiple metrics, highlighting the critical role of vision feature stability in\nLVLMs.", "journal": ""}
{"doi": "10.48550/arXiv.2410.19493", "date": "2024-10-25", "title": "Conditional Hallucinations for Image Compression", "authors": "Till Aczel, Roger Wattenhofer", "abstract": "In lossy image compression, models face the challenge of either hallucinating\ndetails or generating out-of-distribution samples due to the information\nbottleneck. This implies that at times, introducing hallucinations is necessary\nto generate in-distribution samples. The optimal level of hallucination varies\ndepending on image content, as humans are sensitive to small changes that alter\nthe semantic meaning. We propose a novel compression method that dynamically\nbalances the degree of hallucination based on content. We collect data and\ntrain a model to predict user preferences on hallucinations. By using this\nprediction to adjust the perceptual weight in the reconstruction loss, we\ndevelop a Conditionally Hallucinating compression model (ConHa) that\noutperforms state-of-the-art image compression methods. Code and images are\navailable at https://polybox.ethz.ch/index.php/s/owS1k5JYs4KD4TA.", "journal": ""}
{"doi": "10.48550/arXiv.2411.10069", "date": "2024-11-15", "title": "Layer Importance and Hallucination Analysis in Large Language Models via Enhanced Activation Variance-Sparsity", "authors": "Zichen Song, Sitan Huang, Yuxin Wu, Zhongfeng Kang", "abstract": "Evaluating the importance of different layers in large language models (LLMs)\nis crucial for optimizing model performance and interpretability. This paper\nfirst explores layer importance using the Activation Variance-Sparsity Score\n(AVSS), which combines normalized activation variance and sparsity to quantify\neach layer's contribution to overall model performance. By ranking layers based\non AVSS and pruning the least impactful 25\\%, our experiments on tasks such as\nquestion answering, language modeling, and sentiment classification show that\nover 90\\% of the original performance is retained, highlighting potential\nredundancies in LLM architectures. Building on AVSS, we propose an enhanced\nversion tailored to assess hallucination propensity across layers (EAVSS). This\nimproved approach introduces Hallucination-Specific Activation Variance (HSAV)\nand Hallucination-Specific Sparsity (HSS) metrics, allowing precise\nidentification of hallucination-prone layers. By incorporating contrastive\nlearning on these layers, we effectively mitigate hallucination generation,\ncontributing to more robust and efficient LLMs(The maximum performance\nimprovement is 12\\%). Our results on the NQ, SciQ, TriviaQA, TruthfulQA, and\nWikiQA datasets demonstrate the efficacy of this method, offering a\ncomprehensive framework for both layer importance evaluation and hallucination\nmitigation in LLMs.", "journal": ""}
{"doi": "10.48550/arXiv.2411.12713", "date": "2024-11-19", "title": "CATCH: Complementary Adaptive Token-level Contrastive Decoding to Mitigate Hallucinations in LVLMs", "authors": "Zhehan Kan, Ce Zhang, Zihan Liao, Yapeng Tian, Wenming Yang, Junyuan Xiao, Xu Li, Dongmei Jiang, Yaowei Wang, Qingmin Liao", "abstract": "Large Vision-Language Model (LVLM) systems have demonstrated impressive\nvision-language reasoning capabilities but suffer from pervasive and severe\nhallucination issues, posing significant risks in critical domains such as\nhealthcare and autonomous systems. Despite previous efforts to mitigate\nhallucinations, a persistent issue remains: visual defect from vision-language\nmisalignment, creating a bottleneck in visual processing capacity. To address\nthis challenge, we develop Complementary Adaptive Token-level Contrastive\nDecoding to Mitigate Hallucinations in LVLMs (CATCH), based on the Information\nBottleneck theory. CATCH introduces Complementary Visual Decoupling (CVD) for\nvisual information separation, Non-Visual Screening (NVS) for hallucination\ndetection, and Adaptive Token-level Contrastive Decoding (ATCD) for\nhallucination mitigation. CATCH addresses issues related to visual defects that\ncause diminished fine-grained feature perception and cumulative hallucinations\nin open-ended scenarios. It is applicable to various visual question-answering\ntasks without requiring any specific data or prior knowledge, and generalizes\nrobustly to new tasks without additional training, opening new possibilities\nfor advancing LVLM in various challenging applications.", "journal": ""}
{"doi": "10.48550/arXiv.2411.15060", "date": "2024-11-22", "title": "Detecting Hallucinations in Virtual Histology with Neural Precursors", "authors": "Ji-Hun Oh, Kianoush Falahkheirkhah, Rohit Bhargava", "abstract": "Significant biomedical research and clinical care rely on the histopathologic\nexamination of tissue structure using microscopy of stained tissue. Virtual\nstaining (VS) offers a promising alternative with the potential to reduce cost\nand eliminate the use of toxic reagents. However, the critical challenge of\nhallucinations limits confidence in its use, necessitating a VS co-pilot to\ndetect these hallucinations. Here, we first formally establish the problem of\nhallucination detection in VS. Next, we introduce a scalable, post-hoc\nhallucination detection method that identifies a Neural Hallucination Precursor\n(NHP) from VS model embeddings for test-time detection. We report extensive\nvalidation across diverse and challenging VS settings to demonstrate NHP's\neffectiveness and robustness. Furthermore, we show that VS models with fewer\nhallucinations do not necessarily disclose them better, risking a false sense\nof security when reporting just the former metric. This highlights the need for\na reassessment of current VS evaluation practices.", "journal": ""}
{"doi": "10.48550/arXiv.2411.16771", "date": "2024-11-25", "title": "VidHal: Benchmarking Temporal Hallucinations in Vision LLMs", "authors": "Wey Yeh Choong, Yangyang Guo, Mohan Kankanhalli", "abstract": "Vision Large Language Models (VLLMs) are widely acknowledged to be prone to\nhallucinations. Existing research addressing this problem has primarily been\nconfined to image inputs, with limited exploration of video-based\nhallucinations. Furthermore, current evaluation methods fail to capture nuanced\nerrors in generated responses, which are often exacerbated by the rich\nspatiotemporal dynamics of videos. To address this, we introduce VidHal, a\nbenchmark specially designed to evaluate video-based hallucinations in VLLMs.\nVidHal is constructed by bootstrapping video instances across a wide range of\ncommon temporal aspects. A defining feature of our benchmark lies in the\ncareful creation of captions which represent varying levels of hallucination\nassociated with each video. To enable fine-grained evaluation, we propose a\nnovel caption ordering task requiring VLLMs to rank captions by hallucinatory\nextent. We conduct extensive experiments on VidHal and comprehensively evaluate\na broad selection of models. Our results uncover significant limitations in\nexisting VLLMs regarding hallucination generation. Through our benchmark, we\naim to inspire further research on 1) holistic understanding of VLLM\ncapabilities, particularly regarding hallucination, and 2) extensive\ndevelopment of advanced VLLMs to alleviate this problem.", "journal": ""}
{"doi": "10.48550/arXiv.2412.04141", "date": "2024-12-05", "title": "Reducing Tool Hallucination via Reliability Alignment", "authors": "Hongshen Xu, Zichen Zhu, Lei Pan, Zihan Wang, Su Zhu, Da Ma, Ruisheng Cao, Lu Chen, Kai Yu", "abstract": "Large Language Models (LLMs) have expanded their capabilities beyond language\ngeneration to interact with external tools, enabling automation and real-world\napplications. However, tool hallucinations, where models either select\ninappropriate tools or misuse them, pose significant challenges, leading to\nerroneous task execution, increased computational costs, and reduced system\nreliability. To systematically address this issue, we define and categorize\ntool hallucinations into two main types, tool selection hallucination and tool\nusage hallucination. To evaluate and mitigate these issues, we introduce\nRelyToolBench, which integrates specialized test cases and novel metrics to\nassess hallucination-aware task success and efficiency. Finally, we propose\nRelign, a reliability alignment framework that expands the tool-use action\nspace to include indecisive actions, allowing LLMs to defer tool use, seek\nclarification, or adjust tool selection dynamically. Through extensive\nexperiments, we demonstrate that Relign significantly reduces tool\nhallucinations, improves task reliability, and enhances the efficiency of LLM\ntool interactions.", "journal": ""}
{"doi": "10.48550/arXiv.2501.02518", "date": "2025-01-05", "title": "CHAIR -- Classifier of Hallucination as Improver", "authors": "Ao Sun", "abstract": "In this work, we introduce CHAIR (Classifier of Hallucination As ImproveR), a\nsupervised framework for detecting hallucinations by analyzing internal logits\nfrom each layer of every token. Our method extracts a compact set of features\nsuch as maximum, minimum, mean, standard deviation, and slope-from the token\nlogits across all layers, enabling effective hallucination detection without\noverfitting. Experiments on TruthfulQA and MMLU datasets demonstrate that CHAIR\nsignificantly improves detection accuracy, particularly in zero-shot scenarios,\nshowcasing its robustness and generalizability. Beyond hallucination detection,\nCHAIR highlights the potential of using internal representations for designing\nadvanced decoding strategies. By leveraging patterns in logits, we suggest that\nmore sophisticated models and adaptive decoding methods could further reduce\nhallucinations and enhance text completion quality. CHAIR not only offers a\npractical solution for detecting hallucinations but also lays the groundwork\nfor exploring richer representations in LLMs to improve their factuality and\ncoherence.", "journal": ""}
{"doi": "10.48550/arXiv.2501.08292", "date": "2025-01-14", "title": "HALoGEN: Fantastic LLM Hallucinations and Where to Find Them", "authors": "Abhilasha Ravichander, Shrusti Ghela, David Wadden, Yejin Choi", "abstract": "Despite their impressive ability to generate high-quality and fluent text,\ngenerative large language models (LLMs) also produce hallucinations: statements\nthat are misaligned with established world knowledge or provided input context.\nHowever, measuring hallucination can be challenging, as having humans verify\nmodel generations on-the-fly is both expensive and time-consuming. In this\nwork, we release HALoGEN, a comprehensive hallucination benchmark consisting\nof: (1) 10,923 prompts for generative models spanning nine domains including\nprogramming, scientific attribution, and summarization, and (2) automatic\nhigh-precision verifiers for each use case that decompose LLM generations into\natomic units, and verify each unit against a high-quality knowledge source. We\nuse this framework to evaluate ~150,000 generations from 14 language models,\nfinding that even the best-performing models are riddled with hallucinations\n(sometimes up to 86% of generated atomic facts depending on the domain). We\nfurther define a novel error classification for LLM hallucinations based on\nwhether they likely stem from incorrect recollection of training data (Type A\nerrors), or incorrect knowledge in training data (Type B errors), or are\nfabrication (Type C errors). We hope our framework provides a foundation to\nenable the principled study of why generative models hallucinate, and advances\nthe development of trustworthy large language models.", "journal": ""}
{"doi": "10.48550/arXiv.2501.09997", "date": "2025-01-17", "title": "Attention-guided Self-reflection for Zero-shot Hallucination Detection in Large Language Models", "authors": "Qiang Liu, Xinlong Chen, Yue Ding, Shizhen Xu, Shu Wu, Liang Wang", "abstract": "Hallucination has emerged as a significant barrier to the effective\napplication of Large Language Models (LLMs). In this work, we introduce a novel\nAttention-Guided SElf-Reflection (AGSER) approach for zero-shot hallucination\ndetection in LLMs. The AGSER method utilizes attention contributions to\ncategorize the input query into attentive and non-attentive queries. Each query\nis then processed separately through the LLMs, allowing us to compute\nconsistency scores between the generated responses and the original answer. The\ndifference between the two consistency scores serves as a hallucination\nestimator. In addition to its efficacy in detecting hallucinations, AGSER\nnotably reduces computational overhead, requiring only three passes through the\nLLM and utilizing two sets of tokens. We have conducted extensive experiments\nwith four widely-used LLMs across three different hallucination benchmarks,\ndemonstrating that our approach significantly outperforms existing methods in\nzero-shot hallucination detection.", "journal": ""}
{"doi": "10.48550/arXiv.2501.13824", "date": "2025-01-23", "title": "Hallucinations Can Improve Large Language Models in Drug Discovery", "authors": "Shuzhou Yuan, Michael F\u00e4rber", "abstract": "Concerns about hallucinations in Large Language Models (LLMs) have been\nraised by researchers, yet their potential in areas where creativity is vital,\nsuch as drug discovery, merits exploration. In this paper, we come up with the\nhypothesis that hallucinations can improve LLMs in drug discovery. To verify\nthis hypothesis, we use LLMs to describe the SMILES string of molecules in\nnatural language and then incorporate these descriptions as part of the prompt\nto address specific tasks in drug discovery. Evaluated on seven LLMs and five\nclassification tasks, our findings confirm the hypothesis: LLMs can achieve\nbetter performance with text containing hallucinations. Notably, Llama-3.1-8B\nachieves an 18.35% gain in ROC-AUC compared to the baseline without\nhallucination. Furthermore, hallucinations generated by GPT-4o provide the most\nconsistent improvements across models. Additionally, we conduct empirical\nanalyses and a case study to investigate key factors affecting performance and\nthe underlying reasons. Our research sheds light on the potential use of\nhallucinations for LLMs and offers new perspectives for future research\nleveraging LLMs in drug discovery.", "journal": ""}
{"doi": "10.48550/arXiv.2502.01056", "date": "2025-02-03", "title": "Mitigating Hallucinations in Large Vision-Language Models with Internal Fact-based Contrastive Decoding", "authors": "Chao Wang, Xuancheng Zhou, Weiwei Fu, Yang Zhou", "abstract": "Large Visual Language Models (LVLMs) integrate visual and linguistic\nmodalities, exhibiting exceptional performance across various multimodal tasks.\nNevertheless, LVLMs remain vulnerable to the issue of object hallucinations.\nPrevious efforts to mitigate this issue focus on supervised fine-tuning (SFT)\nor incorporating external knowledge, both of which entail significant costs\nrelated to training and the acquisition of external data. To address these\nchallenges, we propose a novel model-agnostic approach termed Internal\nFact-based Contrastive Decoding (IFCD), designed to mitigate and suppress\nhallucinations during the inference process of LVLMs by exploiting the LVLMs'\nown hallucinations. IFCD is grounded in experimental observations that\nalterations to the LVLMs' internal representations tend to amplify\nhallucinations caused by language bias. By contrasting disturbed distribution,\nIFCD calibrates the LVLMs' output and effectively removes the hallucinatory\nlogits from the final predictions. Experimental results validate that IFCD\nsignificantly alleviates both object-level and attribute-level hallucinations\nwhile achieving an average 9% accuracy improvement on POPE and 8% accuracy\nimprovement on MME object hallucinations subset compared with direct decoding,\nrespectively.", "journal": ""}
{"doi": "10.48550/arXiv.2502.01812", "date": "2025-02-03", "title": "SelfCheckAgent: Zero-Resource Hallucination Detection in Generative Large Language Models", "authors": "Diyana Muhammed, Gollam Rabby, S\u00f6ren Auer", "abstract": "Detecting hallucinations in Large Language Models (LLMs) remains a critical\nchallenge for their reliable deployment in real-world applications. To address\nthis, we introduce SelfCheckAgent, a novel framework integrating three\ndifferent agents: the Symbolic Agent, the Specialized Detection Agent, and the\nContextual Consistency Agent. These agents provide a robust multi-dimensional\napproach to hallucination detection. Notable results include the Contextual\nConsistency Agent leveraging Llama 3.1 with Chain-of-Thought (CoT) to achieve\noutstanding performance on the WikiBio dataset, with NonFactual hallucination\ndetection scoring 93.64%, Factual 70.26%, and Ranking 78.48% respectively. On\nthe AIME dataset, GPT-4o with CoT excels in NonFactual detection with 94.89%\nbut reveals trade-offs in Factual with 30.58% and Ranking with 30.68%,\nunderscoring the complexity of hallucination detection in the complex\nmathematical domains. The framework also incorporates a triangulation strategy,\nwhich increases the strengths of the SelfCheckAgent, yielding significant\nimprovements in real-world hallucination identification. The comparative\nanalysis demonstrates SelfCheckAgent's applicability across diverse domains,\npositioning it as a crucial advancement for trustworthy LLMs. These findings\nhighlight the potentiality of consistency-driven methodologies in detecting\nhallucinations in LLMs.", "journal": ""}
{"doi": "10.48550/arXiv.2502.08663", "date": "2025-02-10", "title": "Hallucination Detection: A Probabilistic Framework Using Embeddings Distance Analysis", "authors": "Emanuele Ricco, Lorenzo Cima, Roberto Di Pietro", "abstract": "Hallucinations are one of the major issues affecting LLMs, hindering their\nwide adoption in production systems. While current research solutions for\ndetecting hallucinations are mainly based on heuristics, in this paper we\nintroduce a mathematically sound methodology to reason about hallucination, and\nleverage it to build a tool to detect hallucinations. To the best of our\nknowledge, we are the first to show that hallucinated content has structural\ndifferences with respect to correct content. To prove this result, we resort to\nthe Minkowski distances in the embedding space. Our findings demonstrate\nstatistically significant differences in the embedding distance distributions,\nthat are also scale free -- they qualitatively hold regardless of the distance\nnorm used and the number of keywords, questions, or responses. We leverage\nthese structural differences to develop a tool to detect hallucinated\nresponses, achieving an accuracy of 66\\% for a specific configuration of system\nparameters -- comparable with the best results in the field. In conclusion, the\nsuggested methodology is promising and novel, possibly paving the way for\nfurther research in the domain, also along the directions highlighted in our\nfuture work.", "journal": ""}
{"doi": "10.48550/arXiv.2502.08666", "date": "2025-02-11", "title": "Hallucination, Monofacts, and Miscalibration: An Empirical Investigation", "authors": "Miranda Muqing Miao, Michael Kearns", "abstract": "Hallucinated facts in large language models (LLMs) have recently been shown\nto obey a statistical lower bound determined by the monofact rate (related to\nthe classical Good-Turing missing mass estimator) minus model miscalibration\n(Kalai & Vempala, 2024). We present the first empirical investigation of this\nthree-way relationship in classical n-gram models and fine-tuned\nencoder-decoder Transformers. By generating training data from Pareto\ndistributions with varying shape parameters, we systematically control the\nmonofact rates and establish its positive relationship with hallucination. To\nbridge theory and practice, we derive an empirical analog of the hallucination\nbound by replacing the population miscalibration term (Section 2.1) with an\nempirical bin-wise KL divergence and confirm its practical viability. We then\nintroduce selective upweighting -- a simple yet effective technique that\nstrategically repeats as little as 5% of training examples -- to deliberately\ninject miscalibration into the model. This intervention reduces hallucination\nby up to 40%, challenging universal deduplication policies. Our experiments\nreveal a critical trade-off: selective upweighting maintains pre-injection\nlevels of accuracy while substantially reducing hallucination, whereas standard\ntraining gradually improves accuracy but fails to address persistently high\nhallucination, indicating an inherent tension in optimization objectives.", "journal": ""}
{"doi": "10.48550/arXiv.2502.11306", "date": "2025-02-16", "title": "Smoothing Out Hallucinations: Mitigating LLM Hallucination with Smoothed Knowledge Distillation", "authors": "Hieu Nguyen, Zihao He, Shoumik Atul Gandre, Ujjwal Pasupulety, Sharanya Kumari Shivakumar, Kristina Lerman", "abstract": "Large language models (LLMs) often suffer from hallucination, generating\nfactually incorrect or ungrounded content, which limits their reliability in\nhigh-stakes applications. A key factor contributing to hallucination is the use\nof hard labels during training, which enforce deterministic supervision,\nencourage overconfidence, and disregard the uncertainty inherent in natural\nlanguage. To address this, we propose mitigating hallucination through\nknowledge distillation (KD), where a teacher model provides smoothed soft\nlabels to a student model, reducing overconfidence and improving factual\ngrounding. We apply KD during supervised finetuning on instructional data,\nevaluating its effectiveness across LLMs from different families. Experimental\nresults on summarization benchmarks demonstrate that KD reduces hallucination\ncompared to standard finetuning while preserving performance on general NLP\ntasks. These findings highlight KD as a promising approach for mitigating\nhallucination in LLMs and improving model reliability.", "journal": ""}
{"doi": "10.48550/arXiv.2502.13490", "date": "2025-02-19", "title": "What are Models Thinking about? Understanding Large Language Model Hallucinations \"Psychology\" through Model Inner State Analysis", "authors": "Peiran Wang, Yang Liu, Yunfei Lu, Jue Hong, Ye Wu", "abstract": "Large language model (LLM) systems suffer from the models' unstable ability\nto generate valid and factual content, resulting in hallucination generation.\nCurrent hallucination detection methods heavily rely on out-of-model\ninformation sources, such as RAG to assist the detection, thus bringing heavy\nadditional latency. Recently, internal states of LLMs' inference have been\nwidely used in numerous research works, such as prompt injection detection,\netc. Considering the interpretability of LLM internal states and the fact that\nthey do not require external information sources, we introduce such states into\nLLM hallucination detection. In this paper, we systematically analyze different\ninternal states' revealing features during inference forward and\ncomprehensively evaluate their ability in hallucination detection.\nSpecifically, we cut the forward process of a large language model into three\nstages: understanding, query, generation, and extracting the internal state\nfrom these stages. By analyzing these states, we provide a deep understanding\nof why the hallucinated content is generated and what happened in the internal\nstate of the models. Then, we introduce these internal states into\nhallucination detection and conduct comprehensive experiments to discuss the\nadvantages and limitations.", "journal": ""}
{"doi": "10.48550/arXiv.2502.13622", "date": "2025-02-19", "title": "REFIND at SemEval-2025 Task 3: Retrieval-Augmented Factuality Hallucination Detection in Large Language Models", "authors": "DongGeon Lee, Hwanjo Yu", "abstract": "Hallucinations in large language model (LLM) outputs severely limit their\nreliability in knowledge-intensive tasks such as question answering. To address\nthis challenge, we introduce REFIND (Retrieval-augmented Factuality\nhallucINation Detection), a novel framework that detects hallucinated spans\nwithin LLM outputs by directly leveraging retrieved documents. As part of the\nREFIND, we propose the Context Sensitivity Ratio (CSR), a novel metric that\nquantifies the sensitivity of LLM outputs to retrieved evidence. This\ninnovative approach enables REFIND to efficiently and accurately detect\nhallucinations, setting it apart from existing methods. In the evaluation,\nREFIND demonstrated robustness across nine languages, including low-resource\nsettings, and significantly outperformed baseline models, achieving superior\nIoU scores in identifying hallucinated spans. This work highlights the\neffectiveness of quantifying context sensitivity for hallucination detection,\nthereby paving the way for more reliable and trustworthy LLM applications\nacross diverse languages. Our code is available at\nhttps://github.com/oneonlee/REFIND.", "journal": ""}
{"doi": "10.48550/arXiv.2502.14302", "date": "2025-02-20", "title": "MedHallu: A Comprehensive Benchmark for Detecting Medical Hallucinations in Large Language Models", "authors": "Shrey Pandit, Jiawei Xu, Junyuan Hong, Zhangyang Wang, Tianlong Chen, Kaidi Xu, Ying Ding", "abstract": "Advancements in Large Language Models (LLMs) and their increasing use in\nmedical question-answering necessitate rigorous evaluation of their\nreliability. A critical challenge lies in hallucination, where models generate\nplausible yet factually incorrect outputs. In the medical domain, this poses\nserious risks to patient safety and clinical decision-making. To address this,\nwe introduce MedHallu, the first benchmark specifically designed for medical\nhallucination detection. MedHallu comprises 10,000 high-quality question-answer\npairs derived from PubMedQA, with hallucinated answers systematically generated\nthrough a controlled pipeline. Our experiments show that state-of-the-art LLMs,\nincluding GPT-4o, Llama-3.1, and the medically fine-tuned UltraMedical,\nstruggle with this binary hallucination detection task, with the best model\nachieving an F1 score as low as 0.625 for detecting \"hard\" category\nhallucinations. Using bidirectional entailment clustering, we show that\nharder-to-detect hallucinations are semantically closer to ground truth.\nThrough experiments, we also show incorporating domain-specific knowledge and\nintroducing a \"not sure\" category as one of the answer categories improves the\nprecision and F1 scores by up to 38% relative to baselines.", "journal": ""}
{"doi": "10.48550/arXiv.2502.20750", "date": "2025-02-28", "title": "Mitigating Hallucinations in Large Vision-Language Models by Adaptively Constraining Information Flow", "authors": "Jiaqi Bai, Hongcheng Guo, Zhongyuan Peng, Jian Yang, Zhoujun Li, Mohan Li, Zhihong Tian", "abstract": "Large vision-language models show tremendous potential in understanding\nvisual information through human languages. However, they are prone to suffer\nfrom object hallucination, i.e., the generated image descriptions contain\nobjects that do not exist in the image. In this paper, we reveal that object\nhallucination can be attributed to overconfidence in irrelevant visual features\nwhen soft visual tokens map to the LLM's word embedding space. Specifically, by\nfiguring out the semantic similarity between visual tokens and LLM's word\nembedding, we observe that the smoothness of similarity distribution strongly\ncorrelates with the emergence of object hallucinations. To mitigate\nhallucinations, we propose using the Variational Information Bottleneck (VIB)\nto alleviate overconfidence by introducing stochastic noise, facilitating the\nconstraining of irrelevant information. Furthermore, we propose an\nentropy-based noise-controlling strategy to enable the injected noise to be\nadaptively constrained regarding the smoothness of the similarity distribution.\nWe adapt the proposed AdaVIB across distinct model architectures. Experimental\nresults demonstrate that the proposed AdaVIB mitigates object hallucinations by\neffectively alleviating the overconfidence in irrelevant visual features, with\nconsistent improvements on two object hallucination benchmarks.", "journal": ""}
{"doi": "10.48550/arXiv.2502.20780", "date": "2025-02-28", "title": "MedHallTune: An Instruction-Tuning Benchmark for Mitigating Medical Hallucination in Vision-Language Models", "authors": "Qiao Yan, Yuchen Yuan, Xiaowei Hu, Yihan Wang, Jiaqi Xu, Jinpeng Li, Chi-Wing Fu, Pheng-Ann Heng", "abstract": "The increasing use of vision-language models (VLMs) in healthcare\napplications presents great challenges related to hallucinations, in which the\nmodels may generate seemingly plausible results that are in fact incorrect.\nSuch hallucinations can jeopardize clinical decision making, potentially\nharming the diagnosis and treatments. In this work, we propose MedHallTune, a\nlarge-scale benchmark designed specifically to evaluate and mitigate\nhallucinations in medical VLMs. Comprising over 100,000 images and 1,000,000\ninstruction pairs, MedHallTune includes both hallucination and\nnon-hallucination samples, each with ground-truth annotations. We conduct a\ncomprehensive evaluation of current medical and general VLMs using MedHallTune,\nassessing their performance across key metrics, including clinical accuracy,\nrelevance, detail level, and risk level. The experimental results show that\nfine-tuning with MedHallTune successfully improves the ability of several\nexisting models to manage hallucinations and boost their zero-shot performance\non downstream visual-question-answering (VQA) tasks, making them more reliable\nfor practical medical applications. Our work contributes to the development of\nmore trustworthy VLMs. Codes and dataset will be available at\n\\href{https://github.com/russellyq/MedHallTune}{MedHallTune}.", "journal": ""}
{"doi": "10.48550/arXiv.2503.03032", "date": "2025-03-04", "title": "SAFE: A Sparse Autoencoder-Based Framework for Robust Query Enrichment and Hallucination Mitigation in LLMs", "authors": "Samir Abdaljalil, Filippo Pallucchini, Andrea Seveso, Hasan Kurban, Fabio Mercorio, Erchin Serpedin", "abstract": "Despite the state-of-the-art performance of Large Language Models (LLMs),\nthese models often suffer from hallucinations, which can undermine their\nperformance in critical applications. In this work, we propose SAFE, a novel\nmethod for detecting and mitigating hallucinations by leveraging Sparse\nAutoencoders (SAEs). While hallucination detection techniques and SAEs have\nbeen explored independently, their synergistic application in a comprehensive\nsystem, particularly for hallucination-aware query enrichment, has not been\nfully investigated. To validate the effectiveness of SAFE, we evaluate it on\ntwo models with available SAEs across three diverse cross-domain datasets\ndesigned to assess hallucination problems. Empirical results demonstrate that\nSAFE consistently improves query generation accuracy and mitigates\nhallucinations across all datasets, achieving accuracy improvements of up to\n29.45%.", "journal": ""}
{"doi": "10.48550/arXiv.2503.07772", "date": "2025-03-10", "title": "EAZY: Eliminating Hallucinations in LVLMs by Zeroing out Hallucinatory Image Tokens", "authors": "Liwei Che, Tony Qingze Liu, Jing Jia, Weiyi Qin, Ruixiang Tang, Vladimir Pavlovic", "abstract": "Despite their remarkable potential, Large Vision-Language Models (LVLMs)\nstill face challenges with object hallucination, a problem where their\ngenerated outputs mistakenly incorporate objects that do not actually exist.\nAlthough most works focus on addressing this issue within the language-model\nbackbone, our work shifts the focus to the image input source, investigating\nhow specific image tokens contribute to hallucinations. Our analysis reveals a\nstriking finding: a small subset of image tokens with high attention scores are\nthe primary drivers of object hallucination. By removing these hallucinatory\nimage tokens (only 1.5% of all image tokens), the issue can be effectively\nmitigated. This finding holds consistently across different models and\ndatasets. Building on this insight, we introduce EAZY, a novel, training-free\nmethod that automatically identifies and Eliminates hAllucinations by Zeroing\nout hallucinatorY image tokens. We utilize EAZY for unsupervised object\nhallucination detection, achieving 15% improvement compared to previous\nmethods. Additionally, EAZY demonstrates remarkable effectiveness in mitigating\nhallucinations while preserving model utility and seamlessly adapting to\nvarious LVLM architectures.", "journal": ""}
{"doi": "10.48550/arXiv.2504.12691", "date": "2025-04-17", "title": "Why and How LLMs Hallucinate: Connecting the Dots with Subsequence Associations", "authors": "Yiyou Sun, Yu Gai, Lijie Chen, Abhilasha Ravichander, Yejin Choi, Dawn Song", "abstract": "Large language models (LLMs) frequently generate hallucinations-content that\ndeviates from factual accuracy or provided context-posing challenges for\ndiagnosis due to the complex interplay of underlying causes. This paper\nintroduces a subsequence association framework to systematically trace and\nunderstand hallucinations. Our key insight is that hallucinations arise when\ndominant hallucinatory associations outweigh faithful ones. Through theoretical\nand empirical analyses, we demonstrate that decoder-only transformers\neffectively function as subsequence embedding models, with linear layers\nencoding input-output associations. We propose a tracing algorithm that\nidentifies causal subsequences by analyzing hallucination probabilities across\nrandomized input contexts. Experiments show our method outperforms standard\nattribution techniques in identifying hallucination causes and aligns with\nevidence from the model's training corpus. This work provides a unified\nperspective on hallucinations and a robust framework for their tracing and\nanalysis.", "journal": ""}
{"doi": "10.48550/arXiv.2504.14429", "date": "2025-04-20", "title": "ResNetVLLM-2: Addressing ResNetVLLM's Multi-Modal Hallucinations", "authors": "Ahmad Khalil, Mahmoud Khalil, Alioune Ngom", "abstract": "Large Language Models (LLMs) have transformed natural language processing\n(NLP) tasks, but they suffer from hallucination, generating plausible yet\nfactually incorrect content. This issue extends to Video-Language Models\n(VideoLLMs), where textual descriptions may inaccurately represent visual\ncontent, resulting in multi-modal hallucinations. In this paper, we address\nhallucination in ResNetVLLM, a video-language model combining ResNet visual\nencoders with LLMs. We introduce a two-step protocol: (1) a faithfulness\ndetection strategy that uses a modified Lynx model to assess semantic alignment\nbetween generated captions and ground-truth video references, and (2) a\nhallucination mitigation strategy using Retrieval-Augmented Generation (RAG)\nwith an ad-hoc knowledge base dynamically constructed during inference. Our\nenhanced model, ResNetVLLM-2, reduces multi-modal hallucinations by\ncross-verifying generated content against external knowledge, improving factual\nconsistency. Evaluation on the ActivityNet-QA benchmark demonstrates a\nsubstantial accuracy increase from 54.8% to 65.3%, highlighting the\neffectiveness of our hallucination detection and mitigation strategies in\nenhancing video-language model reliability.", "journal": ""}
{"doi": "10.48550/arXiv.2505.01958", "date": "2025-05-04", "title": "A Comprehensive Analysis for Visual Object Hallucination in Large Vision-Language Models", "authors": "Liqiang Jing, Guiming Hardy Chen, Ehsan Aghazadeh, Xin Eric Wang, Xinya Du", "abstract": "Large Vision-Language Models (LVLMs) demonstrate remarkable capabilities in\nmultimodal tasks, but visual object hallucination remains a persistent issue.\nIt refers to scenarios where models generate inaccurate visual object-related\ninformation based on the query input, potentially leading to misinformation and\nconcerns about safety and reliability. Previous works focus on the evaluation\nand mitigation of visual hallucinations, but the underlying causes have not\nbeen comprehensively investigated. In this paper, we analyze each component of\nLLaVA-like LVLMs -- the large language model, the vision backbone, and the\nprojector -- to identify potential sources of error and their impact. Based on\nour observations, we propose methods to mitigate hallucination for each\nproblematic component. Additionally, we developed two hallucination benchmarks:\nQA-VisualGenome, which emphasizes attribute and relation hallucinations, and\nQA-FB15k, which focuses on cognition-based hallucinations.", "journal": ""}
{"doi": "10.48550/arXiv.2505.15386", "date": "2025-05-21", "title": "RePPL: Recalibrating Perplexity by Uncertainty in Semantic Propagation and Language Generation for Explainable QA Hallucination Detection", "authors": "Yiming Huang, Junyan Zhang, Zihao Wang, Biquan Bie, Xuming Hu, Yi R., Fung, Xinlei He", "abstract": "Large Language Models (LLMs) have become powerful, but hallucinations remain\na vital obstacle to their trustworthy use. While previous works improved the\ncapability of hallucination detection by measuring uncertainty, they all lack\nthe ability to explain the provenance behind why hallucinations occur, i.e.,\nwhich part of the inputs tends to trigger hallucinations. Recent works on the\nprompt attack indicate that uncertainty exists in semantic propagation, where\nattention mechanisms gradually fuse local token information into high-level\nsemantics across layers. Meanwhile, uncertainty also emerges in language\ngeneration, due to its probability-based selection of high-level semantics for\nsampled generations. Based on that, we propose RePPL to recalibrate uncertainty\nmeasurement by these two aspects, which dispatches explainable uncertainty\nscores to each token and aggregates in Perplexity-style Log-Average form as\ntotal score. Experiments show that our method achieves the best comprehensive\ndetection performance across various QA datasets on advanced models (average\nAUC of 0.833), and our method is capable of producing token-level uncertainty\nscores as explanations for the hallucination. Leveraging these scores, we\npreliminarily find the chaotic pattern of hallucination and showcase its\npromising usage.", "journal": ""}
{"doi": "10.48550/arXiv.1905.06537", "date": "2019-05-16", "title": "FH-GAN: Face Hallucination and Recognition using Generative Adversarial Network", "authors": "Bayram Bayramli, Usman Ali, Te Qi, Hongtao Lu", "abstract": "There are many factors affecting visual face recognition, such as low\nresolution images, aging, illumination and pose variance, etc. One of the most\nimportant problem is low resolution face images which can result in bad\nperformance on face recognition. Most of the general face recognition\nalgorithms usually assume a sufficient resolution for the face images. However,\nin practice many applications often do not have sufficient image resolutions.\nThe modern face hallucination models demonstrate reasonable performance to\nreconstruct high-resolution images from its corresponding low resolution\nimages. However, they do not consider identity level information during\nhallucination which directly affects results of the recognition of low\nresolution faces. To address this issue, we propose a Face Hallucination\nGenerative Adversarial Network (FH-GAN) which improves the quality of low\nresolution face images and accurately recognize those low quality images.\nConcretely, we make the following contributions: 1) we propose FH-GAN network,\nan end-to-end system, that improves both face hallucination and face\nrecognition simultaneously. The novelty of this proposed network depends on\nincorporating identity information in a GAN-based face hallucination algorithm\nvia combining a face recognition network for identity preserving. 2) We also\npropose a new face hallucination network, namely Dense Sparse Network (DSNet),\nwhich improves upon the state-of-art in face hallucination. 3) We demonstrate\nbenefits of training the face recognition and GAN-based DSNet jointly by\nreporting good result on face hallucination and recognition.", "journal": ""}
{"doi": "10.48550/arXiv.2304.11049", "date": "2023-04-20", "title": "Using Mobile Data and Deep Models to Assess Auditory Verbal Hallucinations", "authors": "Shayan Mirjafari, Subigya Nepal, Weichen Wang, Andrew T. Campbell", "abstract": "Hallucination is an apparent perception in the absence of real external\nsensory stimuli. An auditory hallucination is a perception of hearing sounds\nthat are not real. A common form of auditory hallucination is hearing voices in\nthe absence of any speakers which is known as Auditory Verbal Hallucination\n(AVH). AVH is fragments of the mind's creation that mostly occur in people\ndiagnosed with mental illnesses such as bipolar disorder and schizophrenia.\nAssessing the valence of hallucinated voices (i.e., how negative or positive\nvoices are) can help measure the severity of a mental illness. We study N=435\nindividuals, who experience hearing voices, to assess auditory verbal\nhallucination. Participants report the valence of voices they hear four times a\nday for a month through ecological momentary assessments with questions that\nhave four answering scales from ``not at all'' to ``extremely''. We collect\nthese self-reports as the valence supervision of AVH events via a mobile\napplication. Using the application, participants also record audio diaries to\ndescribe the content of hallucinated voices verbally. In addition, we passively\ncollect mobile sensing data as contextual signals. We then experiment with how\npredictive these linguistic and contextual cues from the audio diary and mobile\nsensing data are of an auditory verbal hallucination event. Finally, using\ntransfer learning and data fusion techniques, we train a neural net model that\npredicts the valance of AVH with a performance of 54\\% top-1 and 72\\% top-2 F1\nscore.", "journal": ""}
{"doi": "10.48550/arXiv.2305.18248", "date": "2023-05-29", "title": "Do Language Models Know When They're Hallucinating References?", "authors": "Ayush Agrawal, Mirac Suzgun, Lester Mackey, Adam Tauman Kalai", "abstract": "State-of-the-art language models (LMs) are notoriously susceptible to\ngenerating hallucinated information. Such inaccurate outputs not only undermine\nthe reliability of these models but also limit their use and raise serious\nconcerns about misinformation and propaganda. In this work, we focus on\nhallucinated book and article references and present them as the \"model\norganism\" of language model hallucination research, due to their frequent and\neasy-to-discern nature. We posit that if a language model cites a particular\nreference in its output, then it should ideally possess sufficient information\nabout its authors and content, among other relevant details. Using this basic\ninsight, we illustrate that one can identify hallucinated references without\never consulting any external resources, by asking a set of direct or indirect\nqueries to the language model about the references. These queries can be\nconsidered as \"consistency checks.\" Our findings highlight that while LMs,\nincluding GPT-4, often produce inconsistent author lists for hallucinated\nreferences, they also often accurately recall the authors of real references.\nIn this sense, the LM can be said to \"know\" when it is hallucinating\nreferences. Furthermore, these findings show how hallucinated references can be\ndissected to shed light on their nature. Replication code and results can be\nfound at https://github.com/microsoft/hallucinated-references.", "journal": ""}
{"doi": "10.48550/arXiv.2307.03987", "date": "2023-07-08", "title": "A Stitch in Time Saves Nine: Detecting and Mitigating Hallucinations of LLMs by Validating Low-Confidence Generation", "authors": "Neeraj Varshney, Wenlin Yao, Hongming Zhang, Jianshu Chen, Dong Yu", "abstract": "Recently developed large language models have achieved remarkable success in\ngenerating fluent and coherent text. However, these models often tend to\n'hallucinate' which critically hampers their reliability. In this work, we\naddress this crucial problem and propose an approach that actively detects and\nmitigates hallucinations during the generation process. Specifically, we first\nidentify the candidates of potential hallucination leveraging the model's logit\noutput values, check their correctness through a validation procedure, mitigate\nthe detected hallucinations, and then continue with the generation process.\nThrough extensive experiments with GPT-3.5 (text-davinci-003) on the 'article\ngeneration task', we first demonstrate the individual efficacy of our detection\nand mitigation techniques. Specifically, the detection technique achieves a\nrecall of ~88% and the mitigation technique successfully mitigates 57.6% of the\ncorrectly detected hallucinations. Importantly, our mitigation technique does\nnot introduce new hallucinations even in the case of incorrectly detected\nhallucinations, i.e., false positives. Then, we show that the proposed active\ndetection and mitigation approach successfully reduces the hallucinations of\nthe GPT-3.5 model from 47.5% to 14.5% on average. We further demonstrate the\neffectiveness and wide applicability of our approach through additional studies\nincluding performance on different types of questions (multi-hop and false\npremise questions) and with another LLM from a different model family (Vicuna).\nIn summary, our work contributes to improving the reliability and\ntrustworthiness of large language models, a crucial step en route to enabling\ntheir widespread adoption in real-world applications.", "journal": ""}
{"doi": "10.48550/arXiv.2310.06827", "date": "2023-10-10", "title": "Teaching Language Models to Hallucinate Less with Synthetic Tasks", "authors": "Erik Jones, Hamid Palangi, Clarisse Sim\u00f5es, Varun Chandrasekaran, Subhabrata Mukherjee, Arindam Mitra, Ahmed Awadallah, Ece Kamar", "abstract": "Large language models (LLMs) frequently hallucinate on abstractive\nsummarization tasks such as document-based question-answering, meeting\nsummarization, and clinical report generation, even though all necessary\ninformation is included in context. However, optimizing LLMs to hallucinate\nless on these tasks is challenging, as hallucination is hard to efficiently\nevaluate at each optimization step. In this work, we show that reducing\nhallucination on a synthetic task can also reduce hallucination on real-world\ndownstream tasks. Our method, SynTra, first designs a synthetic task where\nhallucinations are easy to elicit and measure. It next optimizes the LLM's\nsystem message via prefix-tuning on the synthetic task, and finally transfers\nthe system message to realistic, hard-to-optimize tasks. Across three realistic\nabstractive summarization tasks, SynTra reduces hallucination for two\n13B-parameter LLMs using only a synthetic retrieval task for supervision. We\nalso find that optimizing the system message rather than the model weights can\nbe critical; fine-tuning the entire model on the synthetic task can\ncounterintuitively increase hallucination. Overall, SynTra demonstrates that\nthe extra flexibility of working with synthetic data can help mitigate\nundesired behaviors in practice.", "journal": ""}
{"doi": "10.48550/arXiv.2311.13614", "date": "2023-11-22", "title": "HalluciDoctor: Mitigating Hallucinatory Toxicity in Visual Instruction Data", "authors": "Qifan Yu, Juncheng Li, Longhui Wei, Liang Pang, Wentao Ye, Bosheng Qin, Siliang Tang, Qi Tian, Yueting Zhuang", "abstract": "Multi-modal Large Language Models (MLLMs) tuned on machine-generated\ninstruction-following data have demonstrated remarkable performance in various\nmulti-modal understanding and generation tasks. However, the hallucinations\ninherent in machine-generated data, which could lead to hallucinatory outputs\nin MLLMs, remain under-explored. This work aims to investigate various\nhallucinations (i.e., object, relation, attribute hallucinations) and mitigate\nthose hallucinatory toxicities in large-scale machine-generated visual\ninstruction datasets. Drawing on the human ability to identify factual errors,\nwe present a novel hallucination detection and elimination framework,\nHalluciDoctor, based on the cross-checking paradigm. We use our framework to\nidentify and eliminate hallucinations in the training data automatically.\nInterestingly, HalluciDoctor also indicates that spurious correlations arising\nfrom long-tail object co-occurrences contribute to hallucinations. Based on\nthat, we execute counterfactual visual instruction expansion to balance data\ndistribution, thereby enhancing MLLMs' resistance to hallucinations.\nComprehensive experiments on hallucination evaluation benchmarks show that our\nmethod successfully mitigates 44.6% hallucinations relatively and maintains\ncompetitive performance compared to LLaVA. The data and code for this paper are\npublicly available. \\url{https://github.com/Yuqifan1117/HalluciDoctor}.", "journal": ""}
{"doi": "10.48550/arXiv.2311.14648", "date": "2023-11-24", "title": "Calibrated Language Models Must Hallucinate", "authors": "Adam Tauman Kalai, Santosh S. Vempala", "abstract": "Recent language models generate false but plausible-sounding text with\nsurprising frequency. Such \"hallucinations\" are an obstacle to the usability of\nlanguage-based AI systems and can harm people who rely upon their outputs. This\nwork shows that there is an inherent statistical lower-bound on the rate that\npretrained language models hallucinate certain types of facts, having nothing\nto do with the transformer LM architecture or data quality. For \"arbitrary\"\nfacts whose veracity cannot be determined from the training data, we show that\nhallucinations must occur at a certain rate for language models that satisfy a\nstatistical calibration condition appropriate for generative language models.\nSpecifically, if the maximum probability of any fact is bounded, we show that\nthe probability of generating a hallucination is close to the fraction of facts\nthat occur exactly once in the training data (a \"Good-Turing\" estimate), even\nassuming ideal training data without errors.\n  One conclusion is that models pretrained to be sufficiently good predictors\n(i.e., calibrated) may require post-training to mitigate hallucinations on the\ntype of arbitrary facts that tend to appear once in the training set. However,\nour analysis also suggests that there is no statistical reason that pretraining\nwill lead to hallucination on facts that tend to appear more than once in the\ntraining data (like references to publications such as articles and books,\nwhose hallucinations have been particularly notable and problematic) or on\nsystematic facts (like arithmetic calculations). Therefore, different\narchitectures and learning algorithms may mitigate these latter types of\nhallucinations.", "journal": ""}
{"doi": "10.48550/arXiv.2401.00396", "date": "2023-12-31", "title": "RAGTruth: A Hallucination Corpus for Developing Trustworthy Retrieval-Augmented Language Models", "authors": "Cheng Niu, Yuanhao Wu, Juno Zhu, Siliang Xu, Kashun Shum, Randy Zhong, Juntong Song, Tong Zhang", "abstract": "Retrieval-augmented generation (RAG) has become a main technique for\nalleviating hallucinations in large language models (LLMs). Despite the\nintegration of RAG, LLMs may still present unsupported or contradictory claims\nto the retrieved contents. In order to develop effective hallucination\nprevention strategies under RAG, it is important to create benchmark datasets\nthat can measure the extent of hallucination. This paper presents RAGTruth, a\ncorpus tailored for analyzing word-level hallucinations in various domains and\ntasks within the standard RAG frameworks for LLM applications. RAGTruth\ncomprises nearly 18,000 naturally generated responses from diverse LLMs using\nRAG. These responses have undergone meticulous manual annotations at both the\nindividual cases and word levels, incorporating evaluations of hallucination\nintensity. We not only benchmark hallucination frequencies across different\nLLMs, but also critically assess the effectiveness of several existing\nhallucination detection methodologies. Furthermore, we show that using a\nhigh-quality dataset such as RAGTruth, it is possible to finetune a relatively\nsmall LLM and achieve a competitive level of performance in hallucination\ndetection when compared to the existing prompt-based approaches using\nstate-of-the-art large language models such as GPT-4.", "journal": ""}
{"doi": "10.48550/arXiv.2402.03190", "date": "2024-02-05", "title": "Unified Hallucination Detection for Multimodal Large Language Models", "authors": "Xiang Chen, Chenxi Wang, Yida Xue, Ningyu Zhang, Xiaoyan Yang, Qiang Li, Yue Shen, Lei Liang, Jinjie Gu, Huajun Chen", "abstract": "Despite significant strides in multimodal tasks, Multimodal Large Language\nModels (MLLMs) are plagued by the critical issue of hallucination. The reliable\ndetection of such hallucinations in MLLMs has, therefore, become a vital aspect\nof model evaluation and the safeguarding of practical application deployment.\nPrior research in this domain has been constrained by a narrow focus on\nsingular tasks, an inadequate range of hallucination categories addressed, and\na lack of detailed granularity. In response to these challenges, our work\nexpands the investigative horizons of hallucination detection. We present a\nnovel meta-evaluation benchmark, MHaluBench, meticulously crafted to facilitate\nthe evaluation of advancements in hallucination detection methods.\nAdditionally, we unveil a novel unified multimodal hallucination detection\nframework, UNIHD, which leverages a suite of auxiliary tools to validate the\noccurrence of hallucinations robustly. We demonstrate the effectiveness of\nUNIHD through meticulous evaluation and comprehensive analysis. We also provide\nstrategic insights on the application of specific tools for addressing various\ncategories of hallucinations.", "journal": ""}
{"doi": "10.48550/arXiv.2402.08021", "date": "2024-02-12", "title": "Careless Whisper: Speech-to-Text Hallucination Harms", "authors": "Allison Koenecke, Anna Seo Gyeong Choi, Katelyn X. Mei, Hilke Schellmann, Mona Sloane", "abstract": "Speech-to-text services aim to transcribe input audio as accurately as\npossible. They increasingly play a role in everyday life, for example in\npersonal voice assistants or in customer-company interactions. We evaluate Open\nAI's Whisper, a state-of-the-art automated speech recognition service\noutperforming industry competitors, as of 2023. While many of Whisper's\ntranscriptions were highly accurate, we find that roughly 1\\% of audio\ntranscriptions contained entire hallucinated phrases or sentences which did not\nexist in any form in the underlying audio. We thematically analyze the\nWhisper-hallucinated content, finding that 38\\% of hallucinations include\nexplicit harms such as perpetuating violence, making up inaccurate\nassociations, or implying false authority. We then study why hallucinations\noccur by observing the disparities in hallucination rates between speakers with\naphasia (who have a lowered ability to express themselves using speech and\nvoice) and a control group. We find that hallucinations disproportionately\noccur for individuals who speak with longer shares of non-vocal durations -- a\ncommon symptom of aphasia. We call on industry practitioners to ameliorate\nthese language-model-based hallucinations in Whisper, and to raise awareness of\npotential biases amplified by hallucinations in downstream applications of\nspeech-to-text models.", "journal": ""}
{"doi": "10.48550/arXiv.2402.10612", "date": "2024-02-16", "title": "Retrieve Only When It Needs: Adaptive Retrieval Augmentation for Hallucination Mitigation in Large Language Models", "authors": "Hanxing Ding, Liang Pang, Zihao Wei, Huawei Shen, Xueqi Cheng", "abstract": "Hallucinations pose a significant challenge for the practical implementation\nof large language models (LLMs). The utilization of parametric knowledge in\ngenerating factual content is constrained by the limited knowledge of LLMs,\npotentially resulting in internal hallucinations. While incorporating external\ninformation can help fill knowledge gaps, it also introduces the risk of\nirrelevant information, thereby increasing the likelihood of external\nhallucinations. A careful and balanced integration of the parametric knowledge\nwithin LLMs with external information is crucial to alleviate hallucinations.\nIn this study, we present Rowen, a novel approach that enhances LLMs with a\nselective retrieval augmentation process tailored to address hallucinated\noutputs. This process is governed by a multilingual semantic-aware detection\nmodule, which evaluates the consistency of the perturbed responses across\nvarious languages for the same queries. Upon detecting inconsistencies\nindicative of hallucinations, Rowen activates the retrieval of external\ninformation to rectify the model outputs. Rowen adeptly harmonizes the\nintrinsic parameters in LLMs with external knowledge sources, effectively\nmitigating hallucinations by ensuring a balanced integration of internal\nreasoning and external evidence. Through a comprehensive empirical analysis, we\ndemonstrate that Rowen surpasses the current state-of-the-art in both detecting\nand mitigating hallucinated content within the outputs of LLMs.", "journal": ""}
{"doi": "10.48550/arXiv.2403.00896", "date": "2024-03-01", "title": "DiaHalu: A Dialogue-level Hallucination Evaluation Benchmark for Large Language Models", "authors": "Kedi Chen, Qin Chen, Jie Zhou, Yishen He, Liang He", "abstract": "Since large language models (LLMs) achieve significant success in recent\nyears, the hallucination issue remains a challenge, numerous benchmarks are\nproposed to detect the hallucination. Nevertheless, some of these benchmarks\nare not naturally generated by LLMs but are intentionally induced. Also, many\nmerely focus on the factuality hallucination while ignoring the faithfulness\nhallucination. Additionally, although dialogue pattern is more widely utilized\nin the era of LLMs, current benchmarks only concentrate on sentence-level and\npassage-level hallucination. In this study, we propose DiaHalu, the first\ndialogue-level hallucination evaluation benchmark to our knowledge. Initially,\nwe integrate the collected topics into system prompts and facilitate a dialogue\nbetween two ChatGPT3.5. Subsequently, we manually modify the contents that do\nnot adhere to human language conventions and then have LLMs re-generate,\nsimulating authentic human-machine interaction scenarios. Finally, professional\nscholars annotate all the samples in the dataset. DiaHalu covers four common\nmulti-turn dialogue domains and five hallucination subtypes, extended from\nfactuality and faithfulness hallucination. Experiments through some well-known\nLLMs and detection methods on the dataset show that DiaHalu is a challenging\nbenchmark, holding significant value for further research.", "journal": ""}
{"doi": "10.48550/arXiv.2403.20009", "date": "2024-03-29", "title": "On Large Language Models' Hallucination with Regard to Known Facts", "authors": "Che Jiang, Biqing Qi, Xiangyu Hong, Dayuan Fu, Yang Cheng, Fandong Meng, Mo Yu, Bowen Zhou, Jie Zhou", "abstract": "Large language models are successful in answering factoid questions but are\nalso prone to hallucination. We investigate the phenomenon of LLMs possessing\ncorrect answer knowledge yet still hallucinating from the perspective of\ninference dynamics, an area not previously covered in studies on\nhallucinations. We are able to conduct this analysis via two key ideas. First,\nwe identify the factual questions that query the same triplet knowledge but\nresult in different answers. The difference between the model behaviors on the\ncorrect and incorrect outputs hence suggests the patterns when hallucinations\nhappen. Second, to measure the pattern, we utilize mappings from the residual\nstreams to vocabulary space. We reveal the different dynamics of the output\ntoken probabilities along the depths of layers between the correct and\nhallucinated cases. In hallucinated cases, the output token's information\nrarely demonstrates abrupt increases and consistent superiority in the later\nstages of the model. Leveraging the dynamic curve as a feature, we build a\nclassifier capable of accurately detecting hallucinatory predictions with an\n88\\% success rate. Our study shed light on understanding the reasons for LLMs'\nhallucinations on their known facts, and more importantly, on accurately\npredicting when they are hallucinating.", "journal": ""}
{"doi": "10.48550/arXiv.2405.04180", "date": "2024-05-07", "title": "Sora Detector: A Unified Hallucination Detection for Large Text-to-Video Models", "authors": "Zhixuan Chu, Lei Zhang, Yichen Sun, Siqiao Xue, Zhibo Wang, Zhan Qin, Kui Ren", "abstract": "The rapid advancement in text-to-video (T2V) generative models has enabled\nthe synthesis of high-fidelity video content guided by textual descriptions.\nDespite this significant progress, these models are often susceptible to\nhallucination, generating contents that contradict the input text, which poses\na challenge to their reliability and practical deployment. To address this\ncritical issue, we introduce the SoraDetector, a novel unified framework\ndesigned to detect hallucinations across diverse large T2V models, including\nthe cutting-edge Sora model. Our framework is built upon a comprehensive\nanalysis of hallucination phenomena, categorizing them based on their\nmanifestation in the video content. Leveraging the state-of-the-art keyframe\nextraction techniques and multimodal large language models, SoraDetector first\nevaluates the consistency between extracted video content summary and textual\nprompts, then constructs static and dynamic knowledge graphs (KGs) from frames\nto detect hallucination both in single frames and across frames. Sora Detector\nprovides a robust and quantifiable measure of consistency, static and dynamic\nhallucination. In addition, we have developed the Sora Detector Agent to\nautomate the hallucination detection process and generate a complete video\nquality report for each input video. Lastly, we present a novel meta-evaluation\nbenchmark, T2VHaluBench, meticulously crafted to facilitate the evaluation of\nadvancements in T2V hallucination detection. Through extensive experiments on\nvideos generated by Sora and other large T2V models, we demonstrate the\nefficacy of our approach in accurately detecting hallucinations. The code and\ndataset can be accessed via GitHub.", "journal": ""}
{"doi": "10.48550/arXiv.2405.15356", "date": "2024-05-24", "title": "Alleviating Hallucinations in Large Vision-Language Models through Hallucination-Induced Optimization", "authors": "Xinyu Lyu, Beitao Chen, Lianli Gao, Jingkuan Song, Heng Tao Shen", "abstract": "Although Large Visual Language Models (LVLMs) have demonstrated exceptional\nabilities in understanding multimodal data, they invariably suffer from\nhallucinations, leading to a disconnect between the generated text and the\ncorresponding images. Almost all current visual contrastive decoding methods\nattempt to mitigate these hallucinations by introducing visual uncertainty\ninformation that appropriately widens the contrastive logits gap between\nhallucinatory and targeted ones. However, due to uncontrollable nature of the\nglobal visual uncertainty, they struggle to precisely induce the hallucinatory\ntokens, which severely limits their effectiveness in mitigating hallucinations\nand may even lead to the generation of undesired hallucinations. To tackle this\nissue, we conducted the theoretical analysis to promote the effectiveness of\ncontrast decoding. Building on this insight, we introduce a novel optimization\nstrategy named Hallucination-Induced Optimization (HIO). This strategy seeks to\namplify the contrast between hallucinatory and targeted tokens relying on a\nfine-tuned theoretical preference model (i.e., Contrary Bradley-Terry Model),\nthereby facilitating efficient contrast decoding to alleviate hallucinations in\nLVLMs. Extensive experimental research demonstrates that our HIO strategy can\neffectively reduce hallucinations in LVLMs, outperforming state-of-the-art\nmethods across various benchmarks.", "journal": ""}
{"doi": "10.48550/arXiv.2406.17115", "date": "2024-06-24", "title": "Evaluating the Quality of Hallucination Benchmarks for Large Vision-Language Models", "authors": "Bei Yan, Jie Zhang, Zheng Yuan, Shiguang Shan, Xilin Chen", "abstract": "Despite the rapid progress and outstanding performance of Large\nVision-Language Models (LVLMs) in recent years, LVLMs have been plagued by the\nissue of hallucination, i.e., LVLMs tend to generate responses that are\ninconsistent with the corresponding visual inputs. To evaluate the degree of\nhallucination in LVLMs, previous works have proposed a series of benchmarks\nfeaturing different types of tasks and evaluation metrics. However, we find\nthat the quality of the existing hallucination benchmarks varies, with some\nsuffering from problems, e.g., inconsistent evaluation results under repeated\ntests, and misalignment with human evaluation. To this end, we propose a\nHallucination benchmark Quality Measurement framework (HQM), which leverages\nvarious indicators to assess the reliability and validity of existing\nhallucination benchmarks separately. Specifically, for reliability we explore\ntest-retest reliability and parallel-forms reliability, while for validity we\nexamine criterion validity and coverage of hallucination types. Furthermore,\nbased on the results of our quality measurement, we construct a High-Quality\nHallucination Benchmark (HQH) for LVLMs, which demonstrates superior\nreliability and validity under our HQM framework. We conduct an extensive\nevaluation of over 10 representative LVLMs, including GPT-4o and\nGemini-1.5-Pro, to provide an in-depth analysis of the hallucination issues in\nexisting models. Our benchmark is publicly available at\nhttps://github.com/HQHBench/HQHBench.", "journal": ""}
{"doi": "10.48550/arXiv.2407.08039", "date": "2024-07-10", "title": "Knowledge Overshadowing Causes Amalgamated Hallucination in Large Language Models", "authors": "Yuji Zhang, Sha Li, Jiateng Liu, Pengfei Yu, Yi R. Fung, Jing Li, Manling Li, Heng Ji", "abstract": "Hallucination is often regarded as a major impediment for using large\nlanguage models (LLMs), especially for knowledge-intensive tasks. Even when the\ntraining corpus consists solely of true statements, language models still\ngenerate hallucinations in the form of amalgamations of multiple facts. We coin\nthis phenomenon as ``knowledge overshadowing'': when we query knowledge from a\nlanguage model with multiple conditions, some conditions overshadow others,\nleading to hallucinated outputs. This phenomenon partially stems from training\ndata imbalance, which we verify on both pretrained models and fine-tuned\nmodels, over a wide range of LM model families and sizes.From a theoretical\npoint of view, knowledge overshadowing can be interpreted as\nover-generalization of the dominant conditions (patterns). We show that the\nhallucination rate grows with both the imbalance ratio (between the popular and\nunpopular condition) and the length of dominant condition description,\nconsistent with our derived generalization bound. Finally, we propose to\nutilize overshadowing conditions as a signal to catch hallucination before it\nis produced, along with a training-free self-contrastive decoding method to\nalleviate hallucination during inference. Our proposed approach showcases up to\n82% F1 for hallucination anticipation and 11.2% to 39.4% hallucination control,\nwith different models and datasets.", "journal": ""}
{"doi": "10.48550/arXiv.2410.09962", "date": "2024-10-13", "title": "LongHalQA: Long-Context Hallucination Evaluation for MultiModal Large Language Models", "authors": "Han Qiu, Jiaxing Huang, Peng Gao, Qin Qi, Xiaoqin Zhang, Ling Shao, Shijian Lu", "abstract": "Hallucination, a phenomenon where multimodal large language models~(MLLMs)\ntend to generate textual responses that are plausible but unaligned with the\nimage, has become one major hurdle in various MLLM-related applications.\nSeveral benchmarks have been created to gauge the hallucination levels of\nMLLMs, by either raising discriminative questions about the existence of\nobjects or introducing LLM evaluators to score the generated text from MLLMs.\nHowever, the discriminative data largely involve simple questions that are not\naligned with real-world text, while the generative data involve LLM evaluators\nthat are computationally intensive and unstable due to their inherent\nrandomness. We propose LongHalQA, an LLM-free hallucination benchmark that\ncomprises 6K long and complex hallucination text. LongHalQA is featured by\nGPT4V-generated hallucinatory data that are well aligned with real-world\nscenarios, including object/image descriptions and multi-round conversations\nwith 14/130 words and 189 words, respectively, on average. It introduces two\nnew tasks, hallucination discrimination and hallucination completion, unifying\nboth discriminative and generative evaluations in a single\nmultiple-choice-question form and leading to more reliable and efficient\nevaluations without the need for LLM evaluators. Further, we propose an\nadvanced pipeline that greatly facilitates the construction of future\nhallucination benchmarks with long and complex questions and descriptions.\nExtensive experiments over multiple recent MLLMs reveal various new challenges\nwhen they are handling hallucinations with long and complex textual data.\nDataset and evaluation code are available at\nhttps://github.com/hanqiu-hq/LongHalQA.", "journal": ""}
{"doi": "10.48550/arXiv.2410.16251", "date": "2024-10-21", "title": "Can Knowledge Editing Really Correct Hallucinations?", "authors": "Baixiang Huang, Canyu Chen, Xiongxiao Xu, Ali Payani, Kai Shu", "abstract": "Large Language Models (LLMs) suffer from hallucinations, referring to the\nnon-factual information in generated content, despite their superior capacities\nacross tasks. Meanwhile, knowledge editing has been developed as a new popular\nparadigm to correct erroneous factual knowledge encoded in LLMs with the\nadvantage of avoiding retraining from scratch. However, a common issue of\nexisting evaluation datasets for knowledge editing is that they do not ensure\nthat LLMs actually generate hallucinated answers to the evaluation questions\nbefore editing. When LLMs are evaluated on such datasets after being edited by\ndifferent techniques, it is hard to directly adopt the performance to assess\nthe effectiveness of different knowledge editing methods in correcting\nhallucinations. Thus, the fundamental question remains insufficiently\nvalidated: Can knowledge editing really correct hallucinations in LLMs? We\nproposed HalluEditBench to holistically benchmark knowledge editing methods in\ncorrecting real-world hallucinations. First, we rigorously construct a massive\nhallucination dataset with 9 domains, 26 topics and more than 6,000\nhallucinations. Then, we assess the performance of knowledge editing methods in\na holistic way on five dimensions including Efficacy, Generalization,\nPortability, Locality, and Robustness. Through HalluEditBench, we have provided\nnew insights into the potentials and limitations of different knowledge editing\nmethods in correcting hallucinations, which could inspire future improvements\nand facilitate progress in the field of knowledge editing.", "journal": ""}
{"doi": "10.48550/arXiv.2412.17056", "date": "2024-12-22", "title": "The HalluRAG Dataset: Detecting Closed-Domain Hallucinations in RAG Applications Using an LLM's Internal States", "authors": "Fabian Ridder, Malte Schilling", "abstract": "Detecting hallucinations in large language models (LLMs) is critical for\nenhancing their reliability and trustworthiness. Most research focuses on\nhallucinations as deviations from information seen during training. However,\nthe opaque nature of an LLM's parametric knowledge complicates the\nunderstanding of why generated texts appear ungrounded: The LLM might not have\npicked up the necessary knowledge from large and often inaccessible datasets,\nor the information might have been changed or contradicted during further\ntraining. Our focus is on hallucinations involving information not used in\ntraining, which we determine by using recency to ensure the information emerged\nafter a cut-off date. This study investigates these hallucinations by detecting\nthem at sentence level using different internal states of various LLMs. We\npresent HalluRAG, a dataset designed to train classifiers on these\nhallucinations. Depending on the model and quantization, MLPs trained on\nHalluRAG detect hallucinations with test accuracies ranging up to 75 %, with\nMistral-7B-Instruct-v0.1 achieving the highest test accuracies. Our results\nshow that IAVs detect hallucinations as effectively as CEVs and reveal that\nanswerable and unanswerable prompts are encoded differently as separate\nclassifiers for these categories improved accuracy. However, HalluRAG showed\nsome limited generalizability, advocating for more diversity in datasets on\nhallucinations.", "journal": ""}
{"doi": "10.48550/arXiv.2502.08109", "date": "2025-02-12", "title": "HuDEx: Integrating Hallucination Detection and Explainability for Enhancing the Reliability of LLM responses", "authors": "Sujeong Lee, Hayoung Lee, Seongsoo Heo, Wonik Choi", "abstract": "Recent advances in large language models (LLMs) have shown promising\nimprovements, often surpassing existing methods across a wide range of\ndownstream tasks in natural language processing. However, these models still\nface challenges, which may hinder their practical applicability. For example,\nthe phenomenon of hallucination is known to compromise the reliability of LLMs,\nespecially in fields that demand high factual precision. Current benchmarks\nprimarily focus on hallucination detection and factuality evaluation but do not\nextend beyond identification. This paper proposes an explanation enhanced\nhallucination-detection model, coined as HuDEx, aimed at enhancing the\nreliability of LLM-generated responses by both detecting hallucinations and\nproviding detailed explanations. The proposed model provides a novel approach\nto integrate detection with explanations, and enable both users and the LLM\nitself to understand and reduce errors. Our measurement results demonstrate\nthat the proposed model surpasses larger LLMs, such as Llama3 70B and GPT-4, in\nhallucination detection accuracy, while maintaining reliable explanations.\nFurthermore, the proposed model performs well in both zero-shot and other test\nenvironments, showcasing its adaptability across diverse benchmark datasets.\nThe proposed approach further enhances the hallucination detection research by\nintroducing a novel approach to integrating interpretability with hallucination\ndetection, which further enhances the performance and reliability of evaluating\nhallucinations in language models.", "journal": ""}
{"doi": "10.48550/arXiv.2503.19650", "date": "2025-03-25", "title": "HausaNLP at SemEval-2025 Task 3: Towards a Fine-Grained Model-Aware Hallucination Detection", "authors": "Maryam Bala, Amina Imam Abubakar, Abdulhamid Abubakar, Abdulkadir Shehu Bichi, Hafsa Kabir Ahmad, Sani Abdullahi Sani, Idris Abdulmumin, Shamsuddeen Hassan Muhamad, Ibrahim Said Ahmad", "abstract": "This paper presents our findings of the Multilingual Shared Task on\nHallucinations and Related Observable Overgeneration Mistakes, MU-SHROOM, which\nfocuses on identifying hallucinations and related overgeneration errors in\nlarge language models (LLMs). The shared task involves detecting specific text\nspans that constitute hallucinations in the outputs generated by LLMs in 14\nlanguages. To address this task, we aim to provide a nuanced, model-aware\nunderstanding of hallucination occurrences and severity in English. We used\nnatural language inference and fine-tuned a ModernBERT model using a synthetic\ndataset of 400 samples, achieving an Intersection over Union (IoU) score of\n0.032 and a correlation score of 0.422. These results indicate a moderately\npositive correlation between the model's confidence scores and the actual\npresence of hallucinations. The IoU score indicates that our model has a\nrelatively low overlap between the predicted hallucination span and the truth\nannotation. The performance is unsurprising, given the intricate nature of\nhallucination detection. Hallucinations often manifest subtly, relying on\ncontext, making pinpointing their exact boundaries formidable.", "journal": ""}
{"doi": "10.48550/arXiv.2504.08809", "date": "2025-04-09", "title": "Decoupling Contrastive Decoding: Robust Hallucination Mitigation in Multimodal Large Language Models", "authors": "Wei Chen, Xin Yan, Bin Wen, Fan Yang, Tingting Gao, Di Zhang, Long Chen", "abstract": "Although multimodal large language models (MLLMs) exhibit remarkable\nreasoning capabilities on complex multimodal understanding tasks, they still\nsuffer from the notorious hallucination issue: generating outputs misaligned\nwith obvious visual or factual evidence. Currently, training-based solutions,\nlike direct preference optimization (DPO), leverage paired preference data to\nsuppress hallucinations. However, they risk sacrificing general reasoning\ncapabilities due to the likelihood displacement. Meanwhile, training-free\nsolutions, like contrastive decoding, achieve this goal by subtracting the\nestimated hallucination pattern from a distorted input. Yet, these handcrafted\nperturbations (e.g., add noise to images) may poorly capture authentic\nhallucination patterns. To avoid these weaknesses of existing methods, and\nrealize robust hallucination mitigation (i.e., maintaining general reasoning\nperformance), we propose a novel framework: Decoupling Contrastive Decoding\n(DCD). Specifically, DCD decouples the learning of positive and negative\nsamples in preference datasets, and trains separate positive and negative image\nprojections within the MLLM. The negative projection implicitly models real\nhallucination patterns, which enables vision-aware negative images in the\ncontrastive decoding inference stage. Our DCD alleviates likelihood\ndisplacement by avoiding pairwise optimization and generalizes robustly without\nhandcrafted degradation. Extensive ablations across hallucination benchmarks\nand general reasoning tasks demonstrate the effectiveness of DCD, i.e., it\nmatches DPO's hallucination suppression while preserving general capabilities\nand outperforms the handcrafted contrastive decoding methods.", "journal": ""}
{"doi": "10.48550/arXiv.2505.13143", "date": "2025-05-19", "title": "Auditing Meta-Cognitive Hallucinations in Reasoning Large Language Models", "authors": "Haolang Lu, Yilian Liu, Jingxin Xu, Guoshun Nan, Yuanlong Yu, Zhican Chen, Kun Wang", "abstract": "The development of Reasoning Large Language Models (RLLMs) has significantly\nimproved multi-step reasoning capabilities, but it has also made hallucination\nproblems more frequent and harder to eliminate. While existing approaches\nmitigate hallucinations through external knowledge integration, model parameter\nanalysis, or self-verification, they often fail to capture how hallucinations\nemerge and evolve across the reasoning chain. In this work, we study the\ncausality of hallucinations under constrained knowledge domains by auditing the\nChain-of-Thought (CoT) trajectory and assessing the model's cognitive\nconfidence in potentially erroneous or biased claims. Our analysis reveals that\nin long-CoT settings, RLLMs can iteratively reinforce biases and errors through\nflawed reflective reasoning, eventually leading to hallucinated reasoning\npaths. Surprisingly, even direct interventions at the origin of hallucinations\noften fail to reverse their effects, as reasoning chains exhibit 'chain\ndisloyalty' -- a resistance to correction and a tendency to preserve flawed\nlogic. Furthermore, we show that existing hallucination detection methods are\nless reliable and interpretable than previously assumed in complex reasoning\nscenarios. Unlike methods such as circuit tracing that require access to model\ninternals, our black-box auditing approach supports interpretable long-chain\nhallucination attribution, offering better generalizability and practical\nutility. Code and data are available at:\nhttps://anonymous.4open.science/r/repo_for_meta_hallucination", "journal": ""}
{"doi": "10.48550/arXiv.2506.05551", "date": "2025-06-05", "title": "When Semantics Mislead Vision: Mitigating Large Multimodal Models Hallucinations in Scene Text Spotting and Understanding", "authors": "Yan Shu, Hangui Lin, Yexin Liu, Yan Zhang, Gangyan Zeng, Yan Li, Yu Zhou, Ser-Nam Lim, Harry Yang, Nicu Sebe", "abstract": "Large Multimodal Models (LMMs) have achieved impressive progress in visual\nperception and reasoning. However, when confronted with visually ambiguous or\nnon-semantic scene text, they often struggle to accurately spot and understand\nthe content, frequently generating semantically plausible yet visually\nincorrect answers, which we refer to as semantic hallucination. In this work,\nwe investigate the underlying causes of semantic hallucination and identify a\nkey finding: Transformer layers in LLM with stronger attention focus on scene\ntext regions are less prone to producing semantic hallucinations. Thus, we\npropose a training-free semantic hallucination mitigation framework comprising\ntwo key components: (1) ZoomText, a coarse-to-fine strategy that identifies\npotential text regions without external detectors; and (2) Grounded Layer\nCorrection, which adaptively leverages the internal representations from layers\nless prone to hallucination to guide decoding, correcting hallucinated outputs\nfor non-semantic samples while preserving the semantics of meaningful ones. To\nenable rigorous evaluation, we introduce TextHalu-Bench, a benchmark of over\n1,730 samples spanning both semantic and non-semantic cases, with manually\ncurated question-answer pairs designed to probe model hallucinations. Extensive\nexperiments demonstrate that our method not only effectively mitigates semantic\nhallucination but also achieves strong performance on public benchmarks for\nscene text spotting and understanding.", "journal": ""}
{"doi": "10.48550/arXiv.1304.7153", "date": "2013-04-26", "title": "A Convex Approach for Image Hallucination", "authors": "Peter Innerhofer, Thomas Pock", "abstract": "In this paper we propose a global convex approach for image hallucination.\nAltering the idea of classical multi image super resolution (SU) systems to\nsingle image SU, we incorporate aligned images to hallucinate the output. Our\nwork is based on the paper of Tappen et al. where they use a non-convex model\nfor image hallucination. In comparison we formulate a convex primal\noptimization problem and derive a fast converging primal-dual algorithm with a\nglobal optimal solution. We use a database with face images to incorporate\nhigh-frequency details to the high-resolution output. We show that we can\nachieve state-of-the-art results by using a convex approach.", "journal": ""}
{"doi": "10.48550/arXiv.2102.08209", "date": "2021-02-09", "title": "Modeling the Hallucinating Brain: A Generative Adversarial Framework", "authors": "Masoumeh Zareh, Mohammad Hossein Manshaei, Sayed Jalal Zahabi", "abstract": "This paper looks into the modeling of hallucination in the human's brain.\nHallucinations are known to be causally associated with some malfunctions\nwithin the interaction of different areas of the brain involved in perception.\nFocusing on visual hallucination and its underlying causes, we identify an\nadversarial mechanism between different parts of the brain which are\nresponsible in the process of visual perception. We then show how the\ncharacterized adversarial interactions in the brain can be modeled by a\ngenerative adversarial network.", "journal": ""}
{"doi": "10.48550/arXiv.2310.15319", "date": "2023-10-23", "title": "Hallucination Detection for Grounded Instruction Generation", "authors": "Lingjun Zhao, Khanh Nguyen, Hal Daum\u00e9 III", "abstract": "We investigate the problem of generating instructions to guide humans to\nnavigate in simulated residential environments. A major issue with current\nmodels is hallucination: they generate references to actions or objects that\nare inconsistent with what a human follower would perform or encounter along\nthe described path. We develop a model that detects these hallucinated\nreferences by adopting a model pre-trained on a large corpus of image-text\npairs, and fine-tuning it with a contrastive loss that separates correct\ninstructions from instructions containing synthesized hallucinations. Our final\nmodel outperforms several baselines, including using word probability estimated\nby the instruction-generation model, and supervised models based on LSTM and\nTransformer.", "journal": ""}
{"doi": "10.48550/arXiv.2411.04077", "date": "2024-11-06", "title": "H-POPE: Hierarchical Polling-based Probing Evaluation of Hallucinations in Large Vision-Language Models", "authors": "Nhi Pham, Michael Schott", "abstract": "By leveraging both texts and images, large vision language models (LVLMs)\nhave shown significant progress in various multi-modal tasks. Nevertheless,\nthese models often suffer from hallucinations, e.g., they exhibit\ninconsistencies between the visual input and the textual output. To address\nthis, we propose H-POPE, a coarse-to-fine-grained benchmark that systematically\nassesses hallucination in object existence and attributes. Our evaluation shows\nthat models are prone to hallucinations on object existence, and even more so\non fine-grained attributes. We further investigate whether these models rely on\nvisual input to formulate the output texts.", "journal": ""}
{"doi": "10.48550/arXiv.2502.15389", "date": "2025-02-21", "title": "The Role of Background Information in Reducing Object Hallucination in Vision-Language Models: Insights from Cutoff API Prompting", "authors": "Masayo Tomita, Katsuhiko Hayashi, Tomoyuki Kaneko", "abstract": "Vision-Language Models (VLMs) occasionally generate outputs that contradict\ninput images, constraining their reliability in real-world applications. While\nvisual prompting is reported to suppress hallucinations by augmenting prompts\nwith relevant area inside an image, the effectiveness in terms of the area\nremains uncertain. This study analyzes success and failure cases of\nAttention-driven visual prompting in object hallucination, revealing that\npreserving background context is crucial for mitigating object hallucination.", "journal": ""}
{"doi": "10.48550/arXiv.2504.10168", "date": "2025-04-14", "title": "HalluSearch at SemEval-2025 Task 3: A Search-Enhanced RAG Pipeline for Hallucination Detection", "authors": "Mohamed A. Abdallah, Samhaa R. El-Beltagy", "abstract": "In this paper, we present HalluSearch, a multilingual pipeline designed to\ndetect fabricated text spans in Large Language Model (LLM) outputs. Developed\nas part of Mu-SHROOM, the Multilingual Shared-task on Hallucinations and\nRelated Observable Overgeneration Mistakes, HalluSearch couples\nretrieval-augmented verification with fine-grained factual splitting to\nidentify and localize hallucinations in fourteen different languages. Empirical\nevaluations show that HalluSearch performs competitively, placing fourth in\nboth English (within the top ten percent) and Czech. While the system's\nretrieval-based strategy generally proves robust, it faces challenges in\nlanguages with limited online coverage, underscoring the need for further\nresearch to ensure consistent hallucination detection across diverse linguistic\ncontexts.", "journal": ""}
{"doi": "10.48550/arXiv.2310.04988", "date": "2023-10-08", "title": "The Troubling Emergence of Hallucination in Large Language Models -- An Extensive Definition, Quantification, and Prescriptive Remediations", "authors": "Vipula Rawte, Swagata Chakraborty, Agnibh Pathak, Anubhav Sarkar, S. M Towhidul Islam Tonmoy, Aman Chadha, Amit P. Sheth, Amitava Das", "abstract": "The recent advancements in Large Language Models (LLMs) have garnered\nwidespread acclaim for their remarkable emerging capabilities. However, the\nissue of hallucination has parallelly emerged as a by-product, posing\nsignificant concerns. While some recent endeavors have been made to identify\nand mitigate different types of hallucination, there has been a limited\nemphasis on the nuanced categorization of hallucination and associated\nmitigation methods. To address this gap, we offer a fine-grained discourse on\nprofiling hallucination based on its degree, orientation, and category, along\nwith offering strategies for alleviation. As such, we define two overarching\norientations of hallucination: (i) factual mirage (FM) and (ii) silver lining\n(SL). To provide a more comprehensive understanding, both orientations are\nfurther sub-categorized into intrinsic and extrinsic, with three degrees of\nseverity - (i) mild, (ii) moderate, and (iii) alarming. We also meticulously\ncategorize hallucination into six types: (i) acronym ambiguity, (ii) numeric\nnuisance, (iii) generated golem, (iv) virtual voice, (v) geographic erratum,\nand (vi) time wrap. Furthermore, we curate HallucInation eLiciTation (HILT), a\npublicly available dataset comprising of 75,000 samples generated using 15\ncontemporary LLMs along with human annotations for the aforementioned\ncategories. Finally, to establish a method for quantifying and to offer a\ncomparative spectrum that allows us to evaluate and rank LLMs based on their\nvulnerability to producing hallucinations, we propose Hallucination\nVulnerability Index (HVI). We firmly believe that HVI holds significant value\nas a tool for the wider NLP community, with the potential to serve as a rubric\nin AI-related policy-making. In conclusion, we propose two solution strategies\nfor mitigating hallucinations.", "journal": ""}
{"doi": "10.48550/arXiv.2503.05777", "date": "2025-02-26", "title": "Medical Hallucinations in Foundation Models and Their Impact on Healthcare", "authors": "Yubin Kim, Hyewon Jeong, Shan Chen, Shuyue Stella Li, Mingyu Lu, Kumail Alhamoud, Jimin Mun, Cristina Grau, Minseok Jung, Rodrigo Gameiro, Lizhou Fan, Eugene Park, Tristan Lin, Joonsik Yoon, Wonjin Yoon, Maarten Sap, Yulia Tsvetkov, Paul Liang, Xuhai Xu, Xin Liu, Daniel McDuff, Hyeonhoon Lee, Hae Won Park, Samir Tulebaev, Cynthia Breazeal", "abstract": "Foundation Models that are capable of processing and generating multi-modal\ndata have transformed AI's role in medicine. However, a key limitation of their\nreliability is hallucination, where inaccurate or fabricated information can\nimpact clinical decisions and patient safety. We define medical hallucination\nas any instance in which a model generates misleading medical content. This\npaper examines the unique characteristics, causes, and implications of medical\nhallucinations, with a particular focus on how these errors manifest themselves\nin real-world clinical scenarios. Our contributions include (1) a taxonomy for\nunderstanding and addressing medical hallucinations, (2) benchmarking models\nusing medical hallucination dataset and physician-annotated LLM responses to\nreal medical cases, providing direct insight into the clinical impact of\nhallucinations, and (3) a multi-national clinician survey on their experiences\nwith medical hallucinations. Our results reveal that inference techniques such\nas Chain-of-Thought (CoT) and Search Augmented Generation can effectively\nreduce hallucination rates. However, despite these improvements, non-trivial\nlevels of hallucination persist. These findings underscore the ethical and\npractical imperative for robust detection and mitigation strategies,\nestablishing a foundation for regulatory policies that prioritize patient\nsafety and maintain clinical integrity as AI becomes more integrated into\nhealthcare. The feedback from clinicians highlights the urgent need for not\nonly technical advances but also for clearer ethical and regulatory guidelines\nto ensure patient safety. A repository organizing the paper resources,\nsummaries, and additional information is available at\nhttps://github.com/mitmedialab/medical hallucination.", "journal": ""}
{"doi": "10.48550/arXiv.1611.08091", "date": "2016-11-24", "title": "Deep Joint Face Hallucination and Recognition", "authors": "Junyu Wu, Shengyong Ding, Wei Xu, Hongyang Chao", "abstract": "Deep models have achieved impressive performance for face hallucination\ntasks. However, we observe that directly feeding the hallucinated facial images\ninto recog- nition models can even degrade the recognition performance despite\nthe much better visualization quality. In this paper, we address this problem\nby jointly learning a deep model for two tasks, i.e. face hallucination and\nrecognition. In particular, we design an end-to-end deep convolution network\nwith hallucination sub-network cascaded by recognition sub-network. The\nrecognition sub- network are responsible for producing discriminative feature\nrepresentations using the hallucinated images as inputs generated by\nhallucination sub-network. During training, we feed LR facial images into the\nnetwork and optimize the parameters by minimizing two loss items, i.e. 1) face\nhallucination loss measured by the pixel wise difference between the ground\ntruth HR images and network-generated images; and 2) verification loss which is\nmeasured by the classification error and intra-class distance. We extensively\nevaluate our method on LFW and YTF datasets. The experimental results show that\nour method can achieve recognition accuracy 97.95% on 4x down-sampled LFW\ntesting set, outperforming the accuracy 96.35% of conventional face recognition\nmodel. And on the more challenging YTF dataset, we achieve recognition accuracy\n90.65%, a margin over the recognition accuracy 89.45% obtained by conventional\nface recognition model on the 4x down-sampled version.", "journal": ""}
{"doi": "10.48550/arXiv.2108.09793", "date": "2021-08-22", "title": "From Agile Ground to Aerial Navigation: Learning from Learned Hallucination", "authors": "Zizhao Wang, Xuesu Xiao, Alexander J Nettekoven, Kadhiravan Umasankar, Anika Singh, Sriram Bommakanti, Ufuk Topcu, Peter Stone", "abstract": "This paper presents a self-supervised Learning from Learned Hallucination\n(LfLH) method to learn fast and reactive motion planners for ground and aerial\nrobots to navigate through highly constrained environments. The recent Learning\nfrom Hallucination (LfH) paradigm for autonomous navigation executes motion\nplans by random exploration in completely safe obstacle-free spaces, uses\nhand-crafted hallucination techniques to add imaginary obstacles to the robot's\nperception, and then learns motion planners to navigate in realistic,\nhighly-constrained, dangerous spaces. However, current hand-crafted\nhallucination techniques need to be tailored for specific robot types (e.g., a\ndifferential drive ground vehicle), and use approximations heavily dependent on\ncertain assumptions (e.g., a short planning horizon). In this work, instead of\nmanually designing hallucination functions, LfLH learns to hallucinate obstacle\nconfigurations, where the motion plans from random exploration in open space\nare optimal, in a self-supervised manner. LfLH is robust to different robot\ntypes and does not make assumptions about the planning horizon. Evaluated in\nboth simulated and physical environments with a ground and an aerial robot,\nLfLH outperforms or performs comparably to previous hallucination approaches,\nalong with sampling- and optimization-based classical methods.", "journal": ""}
{"doi": "10.48550/arXiv.2209.13853", "date": "2022-09-28", "title": "Thinking Hallucination for Video Captioning", "authors": "Nasib Ullah, Partha Pratim Mohanta", "abstract": "With the advent of rich visual representations and pre-trained language\nmodels, video captioning has seen continuous improvement over time. Despite the\nperformance improvement, video captioning models are prone to hallucination.\nHallucination refers to the generation of highly pathological descriptions that\nare detached from the source material. In video captioning, there are two kinds\nof hallucination: object and action hallucination. Instead of endeavoring to\nlearn better representations of a video, in this work, we investigate the\nfundamental sources of the hallucination problem. We identify three main\nfactors: (i) inadequate visual features extracted from pre-trained models, (ii)\nimproper influences of source and target contexts during multi-modal fusion,\nand (iii) exposure bias in the training strategy. To alleviate these problems,\nwe propose two robust solutions: (a) the introduction of auxiliary heads\ntrained in multi-label settings on top of the extracted visual features and (b)\nthe addition of context gates, which dynamically select the features during\nfusion. The standard evaluation metrics for video captioning measures\nsimilarity with ground truth captions and do not adequately capture object and\naction relevance. To this end, we propose a new metric, COAHA (caption object\nand action hallucination assessment), which assesses the degree of\nhallucination. Our method achieves state-of-the-art performance on the\nMSR-Video to Text (MSR-VTT) and the Microsoft Research Video Description Corpus\n(MSVD) datasets, especially by a massive margin in CIDEr score.", "journal": ""}
{"doi": "10.48550/arXiv.2305.10355", "date": "2023-05-17", "title": "Evaluating Object Hallucination in Large Vision-Language Models", "authors": "Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Zhao, Ji-Rong Wen", "abstract": "Inspired by the superior language abilities of large language models (LLM),\nlarge vision-language models (LVLM) have been recently explored by integrating\npowerful LLMs for improving the performance on complex multimodal tasks.\nDespite the promising progress on LVLMs, we find that LVLMs suffer from the\nhallucination problem, i.e. they tend to generate objects that are inconsistent\nwith the target images in the descriptions. To investigate it, this work\npresents the first systematic study on object hallucination of LVLMs. We\nconduct the evaluation experiments on several representative LVLMs, and show\nthat they mostly suffer from severe object hallucination issue. We further\ndiscuss that the visual instructions may influence the hallucination, and find\nthat: objects that frequently occur in the visual instructions or co-occur with\nthe image objects, are obviously prone to be hallucinated by LVLMs. Besides, we\nfind that existing evaluation methods might be affected by the input\ninstructions and generation styles of LVLMs. Thus, we further design an\nimproved evaluation method for object hallucination by proposing a\npolling-based query method called POPE. Experiment results demonstrate that our\nPOPE can evaluate the object hallucination in a more stable and flexible way.\nOur codes and data are publicly available at https://github.com/RUCAIBox/POPE.", "journal": ""}
{"doi": "10.48550/arXiv.2311.15296", "date": "2023-11-26", "title": "UHGEval: Benchmarking the Hallucination of Chinese Large Language Models via Unconstrained Generation", "authors": "Xun Liang, Shichao Song, Simin Niu, Zhiyu Li, Feiyu Xiong, Bo Tang, Yezhaohui Wang, Dawei He, Peng Cheng, Zhonghao Wang, Haiying Deng", "abstract": "Large language models (LLMs) have emerged as pivotal contributors in\ncontemporary natural language processing and are increasingly being applied\nacross a diverse range of industries. However, these large-scale probabilistic\nstatistical models cannot currently ensure the requisite quality in\nprofessional content generation. These models often produce hallucinated text,\ncompromising their practical utility in professional contexts. To assess the\nauthentic reliability of LLMs in text generation, numerous initiatives have\ndeveloped benchmark evaluations for hallucination phenomena. Nevertheless,\nthese benchmarks frequently utilize constrained generation techniques due to\ncost and temporal constraints. These techniques encompass the use of directed\nhallucination induction and strategies that deliberately alter authentic text\nto produce hallucinations. These approaches are not congruent with the\nunrestricted text generation demanded by real-world applications. Furthermore,\na well-established Chinese-language dataset dedicated to the evaluation of\nhallucinations in text generation is presently lacking. Consequently, we have\ndeveloped an Unconstrained Hallucination Generation Evaluation (UHGEval)\nbenchmark, designed to compile outputs produced with minimal restrictions by\nLLMs. Concurrently, we have established a comprehensive benchmark evaluation\nframework to aid subsequent researchers in undertaking scalable and\nreproducible experiments. We have also executed extensive experiments,\nevaluating prominent Chinese language models and the GPT series models to\nderive professional performance insights regarding hallucination challenges.", "journal": ""}
{"doi": "10.48550/arXiv.2402.11622", "date": "2024-02-18", "title": "Logical Closed Loop: Uncovering Object Hallucinations in Large Vision-Language Models", "authors": "Junfei Wu, Qiang Liu, Ding Wang, Jinghao Zhang, Shu Wu, Liang Wang, Tieniu Tan", "abstract": "Object hallucination has been an Achilles' heel which hinders the broader\napplications of large vision-language models (LVLMs). Object hallucination\nrefers to the phenomenon that the LVLMs claim non-existent objects in the\nimage. To mitigate the object hallucinations, instruction tuning and external\nmodel-based detection methods have been proposed, which either require\nlarge-scare computational resources or depend on the detection result of\nexternal models. However, there remains an under-explored field to utilize the\nLVLM itself to alleviate object hallucinations. In this work, we adopt the\nintuition that the LVLM tends to respond logically consistently for existent\nobjects but inconsistently for hallucinated objects. Therefore, we propose a\nLogical Closed Loop-based framework for Object Hallucination Detection and\nMitigation, namely LogicCheckGPT. In specific, we devise logical consistency\nprobing to raise questions with logical correlations, inquiring about\nattributes from objects and vice versa. Whether their responses can form a\nlogical closed loop serves as an indicator of object hallucination. As a\nplug-and-play method, it can be seamlessly applied to all existing LVLMs.\nComprehensive experiments conducted on three benchmarks across four LVLMs have\ndemonstrated significant improvements brought by our method, indicating its\neffectiveness and generality.", "journal": ""}
{"doi": "10.48550/arXiv.2405.00253", "date": "2024-04-30", "title": "CodeHalu: Investigating Code Hallucinations in LLMs via Execution-based Verification", "authors": "Yuchen Tian, Weixiang Yan, Qian Yang, Xuandong Zhao, Qian Chen, Wen Wang, Ziyang Luo, Lei Ma, Dawn Song", "abstract": "Large Language Models (LLMs) have made significant progress in code\ngeneration, offering developers groundbreaking automated programming support.\nHowever, LLMs often generate code that is syntactically correct and even\nsemantically plausible, but may not execute as expected or fulfill specified\nrequirements. This phenomenon of hallucinations in the code domain has not been\nsystematically explored. To advance the community's understanding and research\non this issue, we introduce the concept of code hallucinations and propose a\nclassification method for code hallucination based on execution verification.\nWe categorize code hallucinations into four main types: mapping, naming,\nresource, and logic hallucinations, with each category further divided into\ndifferent subcategories to understand and address the unique challenges faced\nby LLMs in code generation with finer granularity. Additionally, we present a\ndynamic detection algorithm called CodeHalu designed to detect and quantify\ncode hallucinations. We also introduce the CodeHaluEval benchmark, which\nincludes 8,883 samples from 699 tasks, to systematically and quantitatively\nevaluate code hallucinations. By evaluating 17 popular LLMs using this\nbenchmark, we reveal significant differences in their accuracy and reliability\nin code generation, offering detailed insights for further improving the code\ngeneration capabilities of LLMs. The CodeHalu benchmark and code are publicly\navailable at https://github.com/yuchen814/CodeHalu.", "journal": ""}
{"doi": "10.48550/arXiv.2405.00648", "date": "2024-05-01", "title": "Drowzee: Metamorphic Testing for Fact-Conflicting Hallucination Detection in Large Language Models", "authors": "Ningke Li, Yuekang Li, Yi Liu, Ling Shi, Kailong Wang, Haoyu Wang", "abstract": "Large language models (LLMs) have transformed the landscape of language\nprocessing, yet struggle with significant challenges in terms of security,\nprivacy, and the generation of seemingly coherent but factually inaccurate\noutputs, commonly referred to as hallucinations. Among these challenges, one\nparticularly pressing issue is Fact-Conflicting Hallucination (FCH), where LLMs\ngenerate content that directly contradicts established facts. Tackling FCH\nposes a formidable task due to two primary obstacles: Firstly, automating the\nconstruction and updating of benchmark datasets is challenging, as current\nmethods rely on static benchmarks that don't cover the diverse range of FCH\nscenarios. Secondly, validating LLM outputs' reasoning process is inherently\ncomplex, especially with intricate logical relations involved.\n  In addressing these obstacles, we propose an innovative approach leveraging\nlogic programming to enhance metamorphic testing for detecting Fact-Conflicting\nHallucinations (FCH). Our method gathers data from sources like Wikipedia,\nexpands it with logical reasoning to create diverse test cases, assesses LLMs\nthrough structured prompts, and validates their coherence using semantic-aware\nassessment mechanisms. Our method generates test cases and detects\nhallucinations across six different LLMs spanning nine domains, revealing\nhallucination rates ranging from 24.7% to 59.8%. Key observations indicate that\nLLMs encounter challenges, particularly with temporal concepts, handling\nout-of-distribution knowledge, and exhibiting deficiencies in logical reasoning\ncapabilities. The outcomes underscore the efficacy of logic-based test cases\ngenerated by our tool in both triggering and identifying hallucinations. These\nfindings underscore the imperative for ongoing collaborative endeavors within\nthe community to detect and address LLM hallucinations.", "journal": ""}
{"doi": "10.48550/arXiv.2405.17821", "date": "2024-05-28", "title": "RITUAL: Random Image Transformations as a Universal Anti-hallucination Lever in Large Vision Language Models", "authors": "Sangmin Woo, Jaehyuk Jang, Donguk Kim, Yubin Choi, Changick Kim", "abstract": "Recent advancements in Large Vision Language Models (LVLMs) have\nrevolutionized how machines understand and generate textual responses based on\nvisual inputs, yet they often produce \"hallucinatory\" outputs that misinterpret\nvisual information, posing challenges in reliability and trustworthiness. We\npropose RITUAL, a simple decoding method that reduces hallucinations by\nleveraging randomly transformed images as complementary inputs during decoding,\nadjusting the output probability distribution without additional training or\nexternal models. Our key insight is that random transformations expose the\nmodel to diverse visual perspectives, enabling it to correct misinterpretations\nthat lead to hallucinations. Specifically, when a model hallucinates based on\nthe original image, the transformed images -- altered in aspects such as\norientation, scale, or color -- provide alternative viewpoints that help\nrecalibrate the model's predictions. By integrating the probability\ndistributions from both the original and transformed images, RITUAL effectively\nreduces hallucinations. To further improve reliability and address potential\ninstability from arbitrary transformations, we introduce RITUAL+, an extension\nthat selects image transformations based on self-feedback from the LVLM.\nInstead of applying transformations randomly, RITUAL+ uses the LVLM to evaluate\nand choose transformations that are most beneficial for reducing hallucinations\nin a given context. This self-adaptive approach mitigates the potential\nnegative impact of certain transformations on specific tasks, ensuring more\nconsistent performance across different scenarios. Experiments demonstrate that\nRITUAL and RITUAL+ significantly reduce hallucinations across several object\nhallucination benchmarks.", "journal": ""}
{"doi": "10.48550/arXiv.2405.20315", "date": "2024-05-30", "title": "ANAH: Analytical Annotation of Hallucinations in Large Language Models", "authors": "Ziwei Ji, Yuzhe Gu, Wenwei Zhang, Chengqi Lyu, Dahua Lin, Kai Chen", "abstract": "Reducing the `$\\textit{hallucination}$' problem of Large Language Models\n(LLMs) is crucial for their wide applications. A comprehensive and fine-grained\nmeasurement of the hallucination is the first key step for the governance of\nthis issue but is under-explored in the community. Thus, we present\n$\\textbf{ANAH}$, a bilingual dataset that offers $\\textbf{AN}$alytical\n$\\textbf{A}$nnotation of $\\textbf{H}$allucinations in LLMs within Generative\nQuestion Answering. Each answer sentence in our dataset undergoes rigorous\nannotation, involving the retrieval of a reference fragment, the judgment of\nthe hallucination type, and the correction of hallucinated content. ANAH\nconsists of ~12k sentence-level annotations for ~4.3k LLM responses covering\nover 700 topics, constructed by a human-in-the-loop pipeline. Thanks to the\nfine granularity of the hallucination annotations, we can quantitatively\nconfirm that the hallucinations of LLMs progressively accumulate in the answer\nand use ANAH to train and evaluate hallucination annotators. We conduct\nextensive experiments on studying generative and discriminative annotators and\nshow that, although current open-source LLMs have difficulties in fine-grained\nhallucination annotation, the generative annotator trained with ANAH can\nsurpass all open-source LLMs and GPT-3.5, obtain performance competitive with\nGPT-4, and exhibits better generalization ability on unseen questions.", "journal": ""}
{"doi": "10.48550/arXiv.2407.00569", "date": "2024-06-30", "title": "Investigating and Mitigating the Multimodal Hallucination Snowballing in Large Vision-Language Models", "authors": "Weihong Zhong, Xiaocheng Feng, Liang Zhao, Qiming Li, Lei Huang, Yuxuan Gu, Weitao Ma, Yuan Xu, Bing Qin", "abstract": "Though advanced in understanding visual information with human languages,\nLarge Vision-Language Models (LVLMs) still suffer from multimodal\nhallucinations. A natural concern is that during multimodal interaction, the\ngenerated hallucinations could influence the LVLMs' subsequent generation.\nThus, we raise a question: When presented with a query relevant to the\npreviously generated hallucination, will LVLMs be misled and respond\nincorrectly, even though the ground visual information exists? To answer this,\nwe propose a framework called MMHalSnowball to evaluate LVLMs' behaviors when\nencountering generated hallucinations, where LVLMs are required to answer\nspecific visual questions within a curated hallucinatory conversation.\nCrucially, our experiment shows that the performance of open-source LVLMs drops\nby at least $31\\%$, indicating that LVLMs are prone to accept the generated\nhallucinations and make false claims that they would not have supported without\ndistractions. We term this phenomenon Multimodal Hallucination Snowballing. To\nmitigate this, we further propose a training-free method called Residual Visual\nDecoding, where we revise the output distribution of LVLMs with the one derived\nfrom the residual visual input, providing models with direct access to the\nvisual information. Experiments show that our method can mitigate more than\n$24\\%$ of the snowballed multimodal hallucination while maintaining\ncapabilities.", "journal": ""}
{"doi": "10.48550/arXiv.2407.10793", "date": "2024-07-15", "title": "GraphEval: A Knowledge-Graph Based LLM Hallucination Evaluation Framework", "authors": "Hannah Sansford, Nicholas Richardson, Hermina Petric Maretic, Juba Nait Saada", "abstract": "Methods to evaluate Large Language Model (LLM) responses and detect\ninconsistencies, also known as hallucinations, with respect to the provided\nknowledge, are becoming increasingly important for LLM applications. Current\nmetrics fall short in their ability to provide explainable decisions,\nsystematically check all pieces of information in the response, and are often\ntoo computationally expensive to be used in practice. We present GraphEval: a\nhallucination evaluation framework based on representing information in\nKnowledge Graph (KG) structures. Our method identifies the specific triples in\nthe KG that are prone to hallucinations and hence provides more insight into\nwhere in the response a hallucination has occurred, if at all, than previous\nmethods. Furthermore, using our approach in conjunction with state-of-the-art\nnatural language inference (NLI) models leads to an improvement in balanced\naccuracy on various hallucination benchmarks, compared to using the raw NLI\nmodels. Lastly, we explore the use of GraphEval for hallucination correction by\nleveraging the structure of the KG, a method we name GraphCorrect, and\ndemonstrate that the majority of hallucinations can indeed be rectified.", "journal": ""}
{"doi": "10.48550/arXiv.2408.15205", "date": "2024-08-27", "title": "Leveraging Hallucinations to Reduce Manual Prompt Dependency in Promptable Segmentation", "authors": "Jian Hu, Jiayi Lin, Junchi Yan, Shaogang Gong", "abstract": "Promptable segmentation typically requires instance-specific manual prompts\nto guide the segmentation of each desired object. To minimize such a need,\ntask-generic promptable segmentation has been introduced, which employs a\nsingle task-generic prompt to segment various images of different objects in\nthe same task. Current methods use Multimodal Large Language Models (MLLMs) to\nreason detailed instance-specific prompts from a task-generic prompt for\nimproving segmentation accuracy. The effectiveness of this segmentation heavily\ndepends on the precision of these derived prompts. However, MLLMs often suffer\nhallucinations during reasoning, resulting in inaccurate prompting. While\nexisting methods focus on eliminating hallucinations to improve a model, we\nargue that MLLM hallucinations can reveal valuable contextual insights when\nleveraged correctly, as they represent pre-trained large-scale knowledge beyond\nindividual images. In this paper, we utilize hallucinations to mine\ntask-related information from images and verify its accuracy for enhancing\nprecision of the generated prompts. Specifically, we introduce an iterative\nPrompt-Mask Cycle generation framework (ProMaC) with a prompt generator and a\nmask generator.The prompt generator uses a multi-scale chain of thought\nprompting, initially exploring hallucinations for extracting extended\ncontextual knowledge on a test image.These hallucinations are then reduced to\nformulate precise instance-specific prompts, directing the mask generator to\nproduce masks that are consistent with task semantics by mask semantic\nalignment. The generated masks iteratively induce the prompt generator to focus\nmore on task-relevant image areas and reduce irrelevant hallucinations,\nresulting jointly in better prompts and masks. Experiments on 5 benchmarks\ndemonstrate the effectiveness of ProMaC. Code given in\nhttps://lwpyh.github.io/ProMaC/.", "journal": ""}
{"doi": "10.48550/arXiv.2409.09318", "date": "2024-09-14", "title": "ODE: Open-Set Evaluation of Hallucinations in Multimodal Large Language Models", "authors": "Yahan Tu, Rui Hu, Jitao Sang", "abstract": "Hallucination poses a persistent challenge for multimodal large language\nmodels (MLLMs). However, existing benchmarks for evaluating hallucinations are\ngenerally static, which may overlook the potential risk of data contamination.\nTo address this issue, we propose ODE, an open-set, dynamic protocol designed\nto evaluate object hallucinations in MLLMs at both the existence and attribute\nlevels. ODE employs a graph-based structure to represent real-world object\nconcepts, their attributes, and the distributional associations between them.\nThis structure facilitates the extraction of concept combinations based on\ndiverse distributional criteria, generating varied samples for structured\nqueries that evaluate hallucinations in both generative and discriminative\ntasks. Through the generation of new samples, dynamic concept combinations, and\nvaried distribution frequencies, ODE mitigates the risk of data contamination\nand broadens the scope of evaluation. This protocol is applicable to both\ngeneral and specialized scenarios, including those with limited data.\nExperimental results demonstrate the effectiveness of our protocol, revealing\nthat MLLMs exhibit higher hallucination rates when evaluated with ODE-generated\nsamples, which indicates potential data contamination. Furthermore, these\ngenerated samples aid in analyzing hallucination patterns and fine-tuning\nmodels, offering an effective approach to mitigating hallucinations in MLLMs.", "journal": ""}
{"doi": "10.48550/arXiv.2409.11283", "date": "2024-09-17", "title": "Zero-resource Hallucination Detection for Text Generation via Graph-based Contextual Knowledge Triples Modeling", "authors": "Xinyue Fang, Zhen Huang, Zhiliang Tian, Minghui Fang, Ziyi Pan, Quntian Fang, Zhihua Wen, Hengyue Pan, Dongsheng Li", "abstract": "LLMs obtain remarkable performance but suffer from hallucinations. Most\nresearch on detecting hallucination focuses on the questions with short and\nconcrete correct answers that are easy to check the faithfulness. Hallucination\ndetections for text generation with open-ended answers are more challenging.\nSome researchers use external knowledge to detect hallucinations in generated\ntexts, but external resources for specific scenarios are hard to access. Recent\nstudies on detecting hallucinations in long text without external resources\nconduct consistency comparison among multiple sampled outputs. To handle long\ntexts, researchers split long texts into multiple facts and individually\ncompare the consistency of each pairs of facts. However, these methods (1)\nhardly achieve alignment among multiple facts; (2) overlook dependencies\nbetween multiple contextual facts. In this paper, we propose a graph-based\ncontext-aware (GCA) hallucination detection for text generations, which aligns\nknowledge facts and considers the dependencies between contextual knowledge\ntriples in consistency comparison. Particularly, to align multiple facts, we\nconduct a triple-oriented response segmentation to extract multiple knowledge\ntriples. To model dependencies among contextual knowledge triple (facts), we\nconstruct contextual triple into a graph and enhance triples' interactions via\nmessage passing and aggregating via RGCN. To avoid the omission of knowledge\ntriples in long text, we conduct a LLM-based reverse verification via\nreconstructing the knowledge triples. Experiments show that our model enhances\nhallucination detection and excels all baselines.", "journal": ""}
{"doi": "10.48550/arXiv.2410.02899", "date": "2024-10-03", "title": "FactCheckmate: Preemptively Detecting and Mitigating Hallucinations in LMs", "authors": "Deema Alnuhait, Neeraja Kirtane, Muhammad Khalifa, Hao Peng", "abstract": "Language models (LMs) hallucinate. We inquire: Can we detect and mitigate\nhallucinations before they happen? This work answers this research question in\nthe positive, by showing that the internal representations of LMs provide rich\nsignals that can be used for this purpose. We introduce FactCheckMate, which\npreemptively detects hallucinations by learning a classifier that predicts\nwhether the LM will hallucinate, based on the model's hidden states produced\nover the inputs, before decoding begins. If a hallucination is detected,\nFactCheckMate then intervenes, by adjusting the LM's hidden states such that\nthe model will produce more factual outputs. FactCheckMate provides fresh\ninsights that the inner workings of LMs can be revealed by their hidden states.\nPractically, both the detection and mitigation models in FactCheckMate are\nlightweight, adding little inference overhead; FactCheckMate proves a more\nefficient approach for mitigating hallucinations compared to many post-hoc\nalternatives. We evaluate FactCheckMate over LMs of different scales and model\nfamilies (including Llama, Mistral, and Gemma), across a variety of QA datasets\nfrom different domains. Our results demonstrate the effectiveness of leveraging\ninternal representations for early hallucination detection and mitigation,\nachieving over 70% preemptive detection accuracy. On average, outputs generated\nby LMs with intervention are 34.4% more factual compared to those without\nintervention. The average overhead difference in the inference time introduced\nby FactCheckMate is around 3.16 seconds.", "journal": ""}
{"doi": "10.48550/arXiv.2410.15460", "date": "2024-10-20", "title": "Hallucination Detox: Sensitivity Dropout (SenD) for Large Language Model Training", "authors": "Shahrad Mohammadzadeh, Juan David Guerra, Marco Bonizzato, Reihaneh Rabbany, Golnoosh Farnadi", "abstract": "As large language models (LLMs) are increasingly deployed across various\nindustries, concerns regarding their reliability, particularly due to\nhallucinations - outputs that are factually inaccurate or irrelevant to user\ninput - have grown. Our research investigates the relationship between the\ntraining process and the emergence of hallucinations to address a key gap in\nexisting research that focuses primarily on post hoc detection and mitigation\nstrategies. Using models from the Pythia suite (70M - 12B parameters) and\nseveral hallucination detection metrics, we analyze hallucination trends\nthroughout training and explore LLM internal dynamics. We introduce Sensitivity\nDropout (SenD), a novel training protocol designed to mitigate hallucinations\nby reducing variance during training. SenD achieves this by deterministically\ndropping embedding indices with significant variability, referred to as\nSensitive Embedding Indices. In addition, we develop an unsupervised\nhallucination detection metric, Efficient EigenScore (EES), which approximates\nthe traditional EigenScore at 2x speed. This efficient metric is integrated\ninto our protocol, allowing SenD to be both computationally scalable and\neffective at reducing hallucinations. Our empirical evaluation demonstrates\nthat our approach improves LLM reliability at test time by up to 40% compared\nto normal training while also providing an efficient method to improve factual\naccuracy when adapting LLMs to Wikipedia, Medical, and LegalBench domains.", "journal": ""}
{"doi": "10.48550/arXiv.2411.16724", "date": "2024-11-23", "title": "Devils in Middle Layers of Large Vision-Language Models: Interpreting, Detecting and Mitigating Object Hallucinations via Attention Lens", "authors": "Zhangqi Jiang, Junkai Chen, Beier Zhu, Tingjin Luo, Yankun Shen, Xu Yang", "abstract": "Hallucinations in Large Vision-Language Models (LVLMs) significantly\nundermine their reliability, motivating researchers to explore the causes of\nhallucination. However, most studies primarily focus on the language aspect\nrather than the visual. In this paper, we address how LVLMs process visual\ninformation and whether this process causes hallucination. Firstly, we use the\nattention lens to identify the stages at which LVLMs handle visual data,\ndiscovering that the middle layers are crucial. Moreover, we find that these\nlayers can be further divided into two stages: ''visual information\nenrichment'' and ''semantic refinement'' which respectively propagate visual\ndata to object tokens and interpret it through text. By analyzing attention\npatterns during the visual information enrichment stage, we find that real\ntokens consistently receive higher attention weights than hallucinated ones,\nserving as a strong indicator of hallucination. Further examination of\nmulti-head attention maps reveals that hallucination tokens often result from\nheads interacting with inconsistent objects. Based on these insights, we\npropose a simple inference-time method that adjusts visual attention by\nintegrating information across various heads. Extensive experiments demonstrate\nthat this approach effectively mitigates hallucinations in mainstream LVLMs\nwithout additional training costs. Code is available at\nhttps://github.com/ZhangqiJiang07/middle_layers_indicating_hallucinations.", "journal": ""}
{"doi": "10.48550/arXiv.2412.11124", "date": "2024-12-15", "title": "Combating Multimodal LLM Hallucination via Bottom-Up Holistic Reasoning", "authors": "Shengqiong Wu, Hao Fei, Liangming Pan, William Yang Wang, Shuicheng Yan, Tat-Seng Chua", "abstract": "Recent advancements in multimodal large language models (MLLMs) have shown\nunprecedented capabilities in advancing various vision-language tasks. However,\nMLLMs face significant challenges with hallucinations, and misleading outputs\nthat do not align with the input data. While existing efforts are paid to\ncombat MLLM hallucinations, several pivotal challenges are still unsolved.\nFirst, while current approaches aggressively focus on addressing errors at the\nperception level, another important type at the cognition level requiring\nfactual commonsense can be overlooked. In addition, existing methods might fall\nshort in finding a more effective way to represent visual input, which is yet a\nkey bottleneck that triggers visual hallucinations. Moreover, MLLMs can\nfrequently be misled by faulty textual inputs and cause hallucinations, while\nunfortunately, this type of issue has long been overlooked by existing studies.\nInspired by human intuition in handling hallucinations, this paper introduces a\nnovel bottom-up reasoning framework. Our framework systematically addresses\npotential issues in both visual and textual inputs by verifying and integrating\nperception-level information with cognition-level commonsense knowledge,\nensuring more reliable outputs. Extensive experiments demonstrate significant\nimprovements in multiple hallucination benchmarks after integrating MLLMs with\nthe proposed framework. In-depth analyses reveal the great potential of our\nmethods in addressing perception- and cognition-level hallucinations.", "journal": ""}
{"doi": "10.48550/arXiv.2412.20622", "date": "2024-12-29", "title": "Towards a Systematic Evaluation of Hallucinations in Large-Vision Language Models", "authors": "Ashish Seth, Dinesh Manocha, Chirag Agarwal", "abstract": "Large Vision-Language Models (LVLMs) have demonstrated remarkable performance\nin complex multimodal tasks. However, these models still suffer from\nhallucinations, particularly when required to implicitly recognize or infer\ndiverse visual entities from images for complex vision-language tasks. To\naddress this challenge, we propose HALLUCINOGEN, a novel visual question\nanswering (VQA) benchmark that employs contextual reasoning prompts as\nhallucination attacks to evaluate the extent of hallucination in\nstate-of-the-art LVLMs. Our benchmark provides a comprehensive study of the\nimplicit reasoning capabilities of these models by first categorizing visual\nentities based on the ease of recognition in an image as either salient\n(prominent, visibly recognizable objects such as a car) or latent entities\n(such as identifying a disease from a chest X-ray), which are not readily\nvisible and require domain knowledge or contextual reasoning for accurate\ninference. Next, we design hallucination attacks for both types of entities to\nassess hallucinations in LVLMs while performing various vision-language tasks,\nsuch as locating or reasoning about specific entities within an image, where\nmodels must perform implicit reasoning by verifying the existence of the\nqueried entity within the image before generating responses. Finally, our\nextensive evaluations of eleven LVLMs, including powerful open-source models\n(like LLaMA-3.2 and DeepSeek-V2), commercial models like Gemini, and two\nhallucination mitigation strategies across multiple datasets, demonstrate that\ncurrent LVLMs remain susceptible to hallucination attacks.", "journal": ""}
{"doi": "10.48550/arXiv.2501.02020", "date": "2025-01-02", "title": "Enhancing Uncertainty Modeling with Semantic Graph for Hallucination Detection", "authors": "Kedi Chen, Qin Chen, Jie Zhou, Xinqi Tao, Bowen Ding, Jingwen Xie, Mingchen Xie, Peilong Li, Feng Zheng, Liang He", "abstract": "Large Language Models (LLMs) are prone to hallucination with non-factual or\nunfaithful statements, which undermines the applications in real-world\nscenarios. Recent researches focus on uncertainty-based hallucination\ndetection, which utilizes the output probability of LLMs for uncertainty\ncalculation and does not rely on external knowledge or frequent sampling from\nLLMs. Whereas, most approaches merely consider the uncertainty of each\nindependent token, while the intricate semantic relations among tokens and\nsentences are not well studied, which limits the detection of hallucination\nthat spans over multiple tokens and sentences in the passage. In this paper, we\npropose a method to enhance uncertainty modeling with semantic graph for\nhallucination detection. Specifically, we first construct a semantic graph that\nwell captures the relations among entity tokens and sentences. Then, we\nincorporate the relations between two entities for uncertainty propagation to\nenhance sentence-level hallucination detection. Given that hallucination occurs\ndue to the conflict between sentences, we further present a graph-based\nuncertainty calibration method that integrates the contradiction probability of\nthe sentence with its neighbors in the semantic graph for uncertainty\ncalculation. Extensive experiments on two datasets show the great advantages of\nour proposed approach. In particular, we obtain substantial improvements with\n19.78% in passage-level hallucination detection.", "journal": ""}
{"doi": "10.48550/arXiv.2501.13946", "date": "2025-01-19", "title": "Hallucination Mitigation using Agentic AI Natural Language-Based Frameworks", "authors": "Diego Gosmar, Deborah A. Dahl", "abstract": "Hallucinations remain a significant challenge in current Generative AI\nmodels, undermining trust in AI systems and their reliability. This study\ninvestigates how orchestrating multiple specialized Artificial Intelligent\nAgents can help mitigate such hallucinations, with a focus on systems\nleveraging Natural Language Processing (NLP) to facilitate seamless agent\ninteractions. To achieve this, we design a pipeline that introduces over three\nhundred prompts, purposefully crafted to induce hallucinations, into a\nfront-end agent. The outputs are then systematically reviewed and refined by\nsecond- and third-level agents, each employing distinct large language models\nand tailored strategies to detect unverified claims, incorporate explicit\ndisclaimers, and clarify speculative content. Additionally, we introduce a set\nof novel Key Performance Indicators (KPIs) specifically designed to evaluate\nhallucination score levels. A dedicated fourth-level AI agent is employed to\nevaluate these KPIs, providing detailed assessments and ensuring accurate\nquantification of shifts in hallucination-related behaviors. A core component\nof this investigation is the use of the OVON (Open Voice Network) framework,\nwhich relies on universal NLP-based interfaces to transfer contextual\ninformation among agents. Through structured JSON messages, each agent\ncommunicates its assessment of the hallucination likelihood and the reasons\nunderlying questionable content, thereby enabling the subsequent stage to\nrefine the text without losing context. The results demonstrate that employing\nmultiple specialized agents capable of interoperating with each other through\nNLP-based agentic frameworks can yield promising outcomes in hallucination\nmitigation, ultimately bolstering trust within the AI community.", "journal": ""}
{"doi": "10.48550/arXiv.2502.17305", "date": "2025-02-24", "title": "`Generalization is hallucination' through the lens of tensor completions", "authors": "Liang Ze Wong", "abstract": "In this short position paper, we introduce tensor completions and artifacts\nand make the case that they are a useful theoretical framework for\nunderstanding certain types of hallucinations and generalizations in language\nmodels.", "journal": ""}
{"doi": "10.48550/arXiv.2504.05324", "date": "2025-02-28", "title": "Hybrid Retrieval for Hallucination Mitigation in Large Language Models: A Comparative Analysis", "authors": "Chandana Sree Mala, Gizem Gezici, Fosca Giannotti", "abstract": "Large Language Models (LLMs) excel in language comprehension and generation\nbut are prone to hallucinations, producing factually incorrect or unsupported\noutputs. Retrieval Augmented Generation (RAG) systems address this issue by\ngrounding LLM responses with external knowledge. This study evaluates the\nrelationship between retriever effectiveness and hallucination reduction in\nLLMs using three retrieval approaches: sparse retrieval based on BM25 keyword\nsearch, dense retrieval using semantic search with Sentence Transformers, and a\nproposed hybrid retrieval module. The hybrid module incorporates query\nexpansion and combines the results of sparse and dense retrievers through a\ndynamically weighted Reciprocal Rank Fusion score. Using the HaluBench dataset,\na benchmark for hallucinations in question answering tasks, we assess retrieval\nperformance with metrics such as mean average precision and normalised\ndiscounted cumulative gain, focusing on the relevance of the top three\nretrieved documents. Results show that the hybrid retriever achieves better\nrelevance scores, outperforming both sparse and dense retrievers. Further\nevaluation of LLM-generated answers against ground truth using metrics such as\naccuracy, hallucination rate, and rejection rate reveals that the hybrid\nretriever achieves the highest accuracy on fails, the lowest hallucination\nrate, and the lowest rejection rate. These findings highlight the hybrid\nretriever's ability to enhance retrieval relevance, reduce hallucination rates,\nand improve LLM reliability, emphasising the importance of advanced retrieval\ntechniques in mitigating hallucinations and improving response accuracy.", "journal": ""}
{"doi": "10.48550/arXiv.2504.13169", "date": "2025-04-17", "title": "Generate, but Verify: Reducing Hallucination in Vision-Language Models with Retrospective Resampling", "authors": "Tsung-Han Wu, Heekyung Lee, Jiaxin Ge, Joseph E. Gonzalez, Trevor Darrell, David M. Chan", "abstract": "Vision-Language Models (VLMs) excel at visual understanding but often suffer\nfrom visual hallucinations, where they generate descriptions of nonexistent\nobjects, actions, or concepts, posing significant risks in safety-critical\napplications. Existing hallucination mitigation methods typically follow one of\ntwo paradigms: generation adjustment, which modifies decoding behavior to align\ntext with visual inputs, and post-hoc verification, where external models\nassess and correct outputs. While effective, generation adjustment methods\noften rely on heuristics and lack correction mechanisms, while post-hoc\nverification is complicated, typically requiring multiple models and tending to\nreject outputs rather than refine them. In this work, we introduce REVERSE, a\nunified framework that integrates hallucination-aware training with on-the-fly\nself-verification. By leveraging a new hallucination-verification dataset\ncontaining over 1.3M semi-synthetic samples, along with a novel inference-time\nretrospective resampling technique, our approach enables VLMs to both detect\nhallucinations during generation and dynamically revise those hallucinations.\nOur evaluations show that REVERSE achieves state-of-the-art hallucination\nreduction, outperforming the best existing methods by up to 12% on CHAIR-MSCOCO\nand 34% on HaloQuest. Our dataset, model, and code are available at:\nhttps://reverse-vlm.github.io.", "journal": ""}
{"doi": "10.48550/arXiv.2504.20799", "date": "2025-04-29", "title": "Hallucination by Code Generation LLMs: Taxonomy, Benchmarks, Mitigation, and Challenges", "authors": "Yunseo Lee, John Youngeun Song, Dongsun Kim, Jindae Kim, Mijung Kim, Jaechang Nam", "abstract": "Recent technical breakthroughs in large language models (LLMs) have enabled\nthem to fluently generate source code. Software developers often leverage both\ngeneral-purpose and code-specialized LLMs to revise existing code or even\ngenerate a whole function from scratch. These capabilities are also beneficial\nin no-code or low-code contexts, in which one can write programs without a\ntechnical background. However, due to their internal design, LLMs are prone to\ngenerating hallucinations, which are incorrect, nonsensical, and not\njustifiable information but difficult to identify its presence. This problem\nalso occurs when generating source code. Once hallucinated code is produced, it\nis often challenging for users to identify and fix it, especially when such\nhallucinations can be identified under specific execution paths. As a result,\nthe hallucinated code may remain unnoticed within the codebase. This survey\ninvestigates recent studies and techniques relevant to hallucinations generated\nby CodeLLMs. We categorize the types of hallucinations in the code generated by\nCodeLLMs, review existing benchmarks and mitigation strategies, and identify\nopen challenges. Based on these findings, this survey outlines further research\ndirections in the detection and removal of hallucinations produced by CodeLLMs.", "journal": ""}
{"doi": "10.48550/arXiv.2505.05057", "date": "2025-05-08", "title": "Towards Mitigating API Hallucination in Code Generated by LLMs with Hierarchical Dependency Aware", "authors": "Yujia Chen, Mingyu Chen, Cuiyun Gao, Zhihan Jiang, Zhongqi Li, Yuchi Ma", "abstract": "Application Programming Interfaces (APIs) are crucial in modern software\ndevelopment. Large Language Models (LLMs) assist in automated code generation\nbut often struggle with API hallucination, including invoking non-existent APIs\nand misusing existing ones in practical development scenarios. Existing studies\nresort to Retrieval-Augmented Generation (RAG) methods for mitigating the\nhallucination issue, but tend to fail since they generally ignore the\nstructural dependencies in practical projects and do not indeed validate\nwhether the generated APIs are available or not. To address these limitations,\nwe propose MARIN, a framework for mitigating API hallucination in code\ngenerated by LLMs with hierarchical dependency aware. MARIN consists of two\nphases: Hierarchical Dependency Mining, which analyzes local and global\ndependencies of the current function, aiming to supplement comprehensive\nproject context in LLMs input, and Dependency Constrained Decoding, which\nutilizes mined dependencies to adaptively constrain the generation process,\naiming to ensure the generated APIs align with the projects specifications. To\nfacilitate the evaluation of the degree of API hallucination, we introduce a\nnew benchmark APIHulBench and two new metrics including Micro Hallucination\nNumber (MiHN) and Macro Hallucination Rate (MaHR). Experiments on six\nstate-of-the-art LLMs demonstrate that MARIN effectively reduces API\nhallucinations, achieving an average decrease of 67.52% in MiHN and 73.56% in\nMaHR compared to the RAG approach. Applied to Huaweis internal projects and two\nproprietary LLMs, MARIN achieves average decreases of 57.33% in MiHN and 59.41%\nin MaHR.", "journal": ""}
{"doi": "10.48550/arXiv.2505.12886", "date": "2025-05-19", "title": "Detection and Mitigation of Hallucination in Large Reasoning Models: A Mechanistic Perspective", "authors": "Zhongxiang Sun, Qipeng Wang, Haoyu Wang, Xiao Zhang, Jun Xu", "abstract": "Large Reasoning Models (LRMs) have shown impressive capabilities in\nmulti-step reasoning tasks. However, alongside these successes, a more\ndeceptive form of model error has emerged--Reasoning Hallucination--where\nlogically coherent but factually incorrect reasoning traces lead to persuasive\nyet faulty conclusions. Unlike traditional hallucinations, these errors are\nembedded within structured reasoning, making them more difficult to detect and\npotentially more harmful. In this work, we investigate reasoning hallucinations\nfrom a mechanistic perspective. We propose the Reasoning Score, which\nquantifies the depth of reasoning by measuring the divergence between logits\nobtained from projecting late layers of LRMs to the vocabulary space,\neffectively distinguishing shallow pattern-matching from genuine deep\nreasoning. Using this score, we conduct an in-depth analysis on the ReTruthQA\ndataset and identify two key reasoning hallucination patterns: early-stage\nfluctuation in reasoning depth and incorrect backtracking to flawed prior\nsteps. These insights motivate our Reasoning Hallucination Detection (RHD)\nframework, which achieves state-of-the-art performance across multiple domains.\nTo mitigate reasoning hallucinations, we further introduce GRPO-R, an enhanced\nreinforcement learning algorithm that incorporates step-level deep reasoning\nrewards via potential-based shaping. Our theoretical analysis establishes\nstronger generalization guarantees, and experiments demonstrate improved\nreasoning quality and reduced hallucination rates.", "journal": ""}
{"doi": "10.48550/arXiv.2505.16146", "date": "2025-05-22", "title": "Steering LVLMs via Sparse Autoencoder for Hallucination Mitigation", "authors": "Zhenglin Hua, Jinghan He, Zijun Yao, Tianxu Han, Haiyun Guo, Yuheng Jia, Junfeng Fang", "abstract": "Large vision-language models (LVLMs) have achieved remarkable performance on\nmultimodal tasks such as visual question answering (VQA) and image captioning.\nHowever, they still suffer from hallucinations, generating text inconsistent\nwith visual input, posing significant risks in real-world applications.\nExisting approaches to address this issue focus on incorporating external\nknowledge bases, alignment training, or decoding strategies, all of which\nrequire substantial computational cost and time. Recent works try to explore\nmore efficient alternatives by adjusting LVLMs' internal representations.\nAlthough promising, these methods may cause hallucinations to be insufficiently\nsuppressed or lead to excessive interventions that negatively affect normal\nsemantics. In this work, we leverage sparse autoencoders (SAEs) to identify\nsemantic directions closely associated with either hallucinations or actuality,\nrealizing more precise and direct hallucination-related representations. Our\nanalysis demonstrates that interventions along the faithful direction we\nidentified can mitigate hallucinations, while those along the hallucinatory\ndirection can exacerbate them. Building on these insights, we propose Steering\nLVLMs via SAE Latent Directions (SSL), a training-free method based on\nSAE-derived latent directions to mitigate hallucinations in LVLMs. Extensive\nexperiments demonstrate that SSL significantly outperforms existing decoding\napproaches in mitigating hallucinations, while maintaining transferability\nacross different model architectures with negligible additional time overhead.", "journal": ""}
{"doi": "10.48550/arXiv.2506.17748", "date": "2025-06-21", "title": "HIDE and Seek: Detecting Hallucinations in Language Models via Decoupled Representations", "authors": "Anwoy Chatterjee, Yash Goel, Tanmoy Chakraborty", "abstract": "Contemporary Language Models (LMs), while impressively fluent, often generate\ncontent that is factually incorrect or unfaithful to the input context - a\ncritical issue commonly referred to as 'hallucination'. This tendency of LMs to\ngenerate hallucinated content undermines their reliability, especially because\nthese fabrications are often highly convincing and therefore difficult to\ndetect. While several existing methods attempt to detect hallucinations, most\nrely on analyzing multiple generations per input, leading to increased\ncomputational cost and latency. To address this, we propose a single-pass,\ntraining-free approach for effective Hallucination detectIon via Decoupled\nrEpresentations (HIDE). Our approach leverages the hypothesis that\nhallucinations result from a statistical decoupling between an LM's internal\nrepresentations of input context and its generated output. We quantify this\ndecoupling using the Hilbert-Schmidt Independence Criterion (HSIC) applied to\nhidden-state representations extracted while generating the output sequence. We\nconduct extensive experiments on four diverse question answering datasets,\nevaluating both faithfulness and factuality hallucinations across six\nopen-source LMs of varying scales and properties. Our results demonstrate that\nHIDE outperforms other single-pass methods in almost all settings, achieving an\naverage relative improvement of ~29% in AUC-ROC over the best-performing\nsingle-pass strategy across various models and datasets. Additionally, HIDE\nshows competitive and often superior performance with multi-pass\nstate-of-the-art methods, obtaining an average relative improvement of ~3% in\nAUC-ROC while consuming ~51% less computation time. Our findings highlight the\neffectiveness of exploiting internal representation decoupling in LMs for\nefficient and practical hallucination detection.", "journal": ""}
{"doi": "10.48550/arXiv.1906.07008", "date": "2019-06-17", "title": "Hallucinated Adversarial Learning for Robust Visual Tracking", "authors": "Qiangqiang Wu, Zhihui Chen, Lin Cheng, Yan Yan, Bo Li, Hanzi Wang", "abstract": "Humans can easily learn new concepts from just a single exemplar, mainly due\nto their remarkable ability to imagine or hallucinate what the unseen exemplar\nmay look like in different settings. Incorporating such an ability to\nhallucinate diverse new samples of the tracked instance can help the trackers\nalleviate the over-fitting problem in the low-data tracking regime. To achieve\nthis, we propose an effective adversarial approach, denoted as adversarial\n\"hallucinator\" (AH), for robust visual tracking. The proposed AH is designed to\nfirstly learn transferable non-linear deformations between a pair of\nsame-identity instances, and then apply these deformations to an unseen tracked\ninstance in order to generate diverse positive training samples. By\nincorporating AH into an online tracking-by-detection framework, we propose the\nhallucinated adversarial tracker (HAT), which jointly optimizes AH with an\nonline classifier (e.g., MDNet) in an end-to-end manner. In addition, a novel\nselective deformation transfer (SDT) method is presented to better select the\ndeformations which are more suitable for transfer. Extensive experiments on 3\npopular benchmarks demonstrate that our HAT achieves the state-of-the-art\nperformance.", "journal": ""}
{"doi": "10.48550/arXiv.2010.08098", "date": "2020-10-16", "title": "Agile Robot Navigation through Hallucinated Learning and Sober Deployment", "authors": "Xuesu Xiao, Bo Liu, Peter Stone", "abstract": "Learning from Hallucination (LfH) is a recent machine learning paradigm for\nautonomous navigation, which uses training data collected in completely safe\nenvironments and adds numerous imaginary obstacles to make the environment\ndensely constrained, to learn navigation planners that produce feasible\nnavigation even in highly constrained (more dangerous) spaces. However, LfH\nrequires hallucinating the robot perception during deployment to match with the\nhallucinated training data, which creates a need for sometimes-infeasible prior\nknowledge and tends to generate very conservative planning. In this work, we\npropose a new LfH paradigm that does not require runtime hallucination -- a\nfeature we call \"sober deployment\" -- and can therefore adapt to more realistic\nnavigation scenarios. This novel Hallucinated Learning and Sober Deployment\n(HLSD) paradigm is tested in a benchmark testbed of 300 simulated navigation\nenvironments with a wide range of difficulty levels, and in the real-world. In\nmost cases, HLSD outperforms both the original LfH method and a classical\nnavigation planner.", "journal": ""}
{"doi": "10.48550/arXiv.2110.05031", "date": "2021-10-11", "title": "EDFace-Celeb-1M: Benchmarking Face Hallucination with a Million-scale Dataset", "authors": "Kaihao Zhang, Dongxu Li, Wenhan Luo, Jingyu Liu, Jiankang Deng, Wei Liu, Stefanos Zafeiriou", "abstract": "Recent deep face hallucination methods show stunning performance in\nsuper-resolving severely degraded facial images, even surpassing human ability.\nHowever, these algorithms are mainly evaluated on non-public synthetic\ndatasets. It is thus unclear how these algorithms perform on public face\nhallucination datasets. Meanwhile, most of the existing datasets do not well\nconsider the distribution of races, which makes face hallucination methods\ntrained on these datasets biased toward some specific races. To address the\nabove two problems, in this paper, we build a public Ethnically Diverse Face\ndataset, EDFace-Celeb-1M, and design a benchmark task for face hallucination.\nOur dataset includes 1.7 million photos that cover different countries, with\nbalanced race composition. To the best of our knowledge, it is the largest and\npublicly available face hallucination dataset in the wild. Associated with this\ndataset, this paper also contributes various evaluation protocols and provides\ncomprehensive analysis to benchmark the existing state-of-the-art methods. The\nbenchmark evaluations demonstrate the performance and limitations of\nstate-of-the-art algorithms.", "journal": ""}
{"doi": "10.48550/arXiv.2204.07931", "date": "2022-04-17", "title": "On the Origin of Hallucinations in Conversational Models: Is it the Datasets or the Models?", "authors": "Nouha Dziri, Sivan Milton, Mo Yu, Osmar Zaiane, Siva Reddy", "abstract": "Knowledge-grounded conversational models are known to suffer from producing\nfactually invalid statements, a phenomenon commonly called hallucination. In\nthis work, we investigate the underlying causes of this phenomenon: is\nhallucination due to the training data, or to the models? We conduct a\ncomprehensive human study on both existing knowledge-grounded conversational\nbenchmarks and several state-of-the-art models. Our study reveals that the\nstandard benchmarks consist of >60% hallucinated responses, leading to models\nthat not only hallucinate but even amplify hallucinations. Our findings raise\nimportant questions on the quality of existing datasets and models trained\nusing them. We make our annotations publicly available for future research.", "journal": ""}
{"doi": "10.48550/arXiv.2210.13210", "date": "2022-10-24", "title": "Mutual Information Alleviates Hallucinations in Abstractive Summarization", "authors": "Liam van der Poel, Ryan Cotterell, Clara Meister", "abstract": "Despite significant progress in the quality of language generated from\nabstractive summarization models, these models still exhibit the tendency to\nhallucinate, i.e., output content not supported by the source document. A\nnumber of works have tried to fix--or at least uncover the source of--the\nproblem with limited success. In this paper, we identify a simple criterion\nunder which models are significantly more likely to assign more probability to\nhallucinated content during generation: high model uncertainty. This finding\noffers a potential explanation for hallucinations: models default to favoring\ntext with high marginal probability, i.e., high-frequency occurrences in the\ntraining set, when uncertain about a continuation. It also motivates possible\nroutes for real-time intervention during decoding to prevent such\nhallucinations. We propose a decoding strategy that switches to optimizing for\npointwise mutual information of the source and target token--rather than purely\nthe probability of the target token--when the model exhibits uncertainty.\nExperiments on the XSum dataset show that our method decreases the probability\nof hallucinated tokens while maintaining the Rouge and BertS scores of\ntop-performing decoding strategies.", "journal": ""}
{"doi": "10.48550/arXiv.2301.04449", "date": "2023-01-11", "title": "Diving Deep into Modes of Fact Hallucinations in Dialogue Systems", "authors": "Souvik Das, Sougata Saha, Rohini K. Srihari", "abstract": "Knowledge Graph(KG) grounded conversations often use large pre-trained models\nand usually suffer from fact hallucination. Frequently entities with no\nreferences in knowledge sources and conversation history are introduced into\nresponses, thus hindering the flow of the conversation -- existing work attempt\nto overcome this issue by tweaking the training procedure or using a multi-step\nrefining method. However, minimal effort is put into constructing an\nentity-level hallucination detection system, which would provide fine-grained\nsignals that control fallacious content while generating responses. As a first\nstep to address this issue, we dive deep to identify various modes of\nhallucination in KG-grounded chatbots through human feedback analysis.\nSecondly, we propose a series of perturbation strategies to create a synthetic\ndataset named FADE (FActual Dialogue Hallucination DEtection Dataset). Finally,\nwe conduct comprehensive data analyses and create multiple baseline models for\nhallucination detection to compare against human-verified data and already\nestablished benchmarks.", "journal": ""}
{"doi": "10.48550/arXiv.2306.06085", "date": "2023-06-09", "title": "Trapping LLM Hallucinations Using Tagged Context Prompts", "authors": "Philip Feldman, James R. Foulds, Shimei Pan", "abstract": "Recent advances in large language models (LLMs), such as ChatGPT, have led to\nhighly sophisticated conversation agents. However, these models suffer from\n\"hallucinations,\" where the model generates false or fabricated information.\nAddressing this challenge is crucial, particularly with AI-driven platforms\nbeing adopted across various sectors. In this paper, we propose a novel method\nto recognize and flag instances when LLMs perform outside their domain\nknowledge, and ensuring users receive accurate information.\n  We find that the use of context combined with embedded tags can successfully\ncombat hallucinations within generative language models. To do this, we\nbaseline hallucination frequency in no-context prompt-response pairs using\ngenerated URLs as easily-tested indicators of fabricated data. We observed a\nsignificant reduction in overall hallucination when context was supplied along\nwith question prompts for tested generative engines. Lastly, we evaluated how\nplacing tags within contexts impacted model responses and were able to\neliminate hallucinations in responses with 98.88% effectiveness.", "journal": ""}
{"doi": "10.48550/arXiv.2307.15343", "date": "2023-07-28", "title": "Med-HALT: Medical Domain Hallucination Test for Large Language Models", "authors": "Ankit Pal, Logesh Kumar Umapathi, Malaikannan Sankarasubbu", "abstract": "This research paper focuses on the challenges posed by hallucinations in\nlarge language models (LLMs), particularly in the context of the medical\ndomain. Hallucination, wherein these models generate plausible yet unverified\nor incorrect information, can have serious consequences in healthcare\napplications. We propose a new benchmark and dataset, Med-HALT (Medical Domain\nHallucination Test), designed specifically to evaluate and reduce\nhallucinations. Med-HALT provides a diverse multinational dataset derived from\nmedical examinations across various countries and includes multiple innovative\ntesting modalities. Med-HALT includes two categories of tests reasoning and\nmemory-based hallucination tests, designed to assess LLMs's problem-solving and\ninformation retrieval abilities.\n  Our study evaluated leading LLMs, including Text Davinci, GPT-3.5, LlaMa-2,\nMPT, and Falcon, revealing significant differences in their performance. The\npaper provides detailed insights into the dataset, promoting transparency and\nreproducibility. Through this work, we aim to contribute to the development of\nsafer and more reliable language models in healthcare. Our benchmark can be\nfound at medhalt.github.io", "journal": ""}
{"doi": "10.48550/arXiv.2308.11764", "date": "2023-08-22", "title": "Halo: Estimation and Reduction of Hallucinations in Open-Source Weak Large Language Models", "authors": "Mohamed Elaraby, Mengyin Lu, Jacob Dunn, Xueying Zhang, Yu Wang, Shizhu Liu, Pingchuan Tian, Yuping Wang, Yuxuan Wang", "abstract": "Large Language Models (LLMs) have revolutionized Natural Language Processing\n(NLP). Although convenient for research and practical applications, open-source\nLLMs with fewer parameters often suffer from severe hallucinations compared to\ntheir larger counterparts. This paper focuses on measuring and reducing\nhallucinations in BLOOM 7B, a representative of such weaker open-source LLMs\nthat are publicly available for research and commercial applications. We\nintroduce HaloCheck, a lightweight BlackBox knowledge-free framework designed\nto quantify the severity of hallucinations in LLMs. Additionally, we explore\ntechniques like knowledge injection and teacher-student approaches to alleviate\nhallucinations in low-parameter LLMs. Our experiments effectively demonstrate\nthe reduction of hallucinations in challenging domains for these LLMs.", "journal": ""}
{"doi": "10.48550/arXiv.2309.02301", "date": "2023-09-05", "title": "CIEM: Contrastive Instruction Evaluation Method for Better Instruction Tuning", "authors": "Hongyu Hu, Jiyuan Zhang, Minyi Zhao, Zhenbang Sun", "abstract": "Nowadays, the research on Large Vision-Language Models (LVLMs) has been\nsignificantly promoted thanks to the success of Large Language Models (LLM).\nNevertheless, these Vision-Language Models (VLMs) are suffering from the\ndrawback of hallucination -- due to insufficient understanding of vision and\nlanguage modalities, VLMs may generate incorrect perception information when\ndoing downstream applications, for example, captioning a non-existent entity.\nTo address the hallucination phenomenon, on the one hand, we introduce a\nContrastive Instruction Evaluation Method (CIEM), which is an automatic\npipeline that leverages an annotated image-text dataset coupled with an LLM to\ngenerate factual/contrastive question-answer pairs for the evaluation of the\nhallucination of VLMs. On the other hand, based on CIEM, we further propose a\nnew instruction tuning method called CIT (the abbreviation of Contrastive\nInstruction Tuning) to alleviate the hallucination of VLMs by automatically\nproducing high-quality factual/contrastive question-answer pairs and\ncorresponding justifications for model tuning. Through extensive experiments on\nCIEM and CIT, we pinpoint the hallucination issues commonly present in existing\nVLMs, the disability of the current instruction-tuning dataset to handle the\nhallucination phenomenon and the superiority of CIT-tuned VLMs over both CIEM\nand public datasets.", "journal": ""}
{"doi": "10.48550/arXiv.2309.02654", "date": "2023-09-06", "title": "Zero-Resource Hallucination Prevention for Large Language Models", "authors": "Junyu Luo, Cao Xiao, Fenglong Ma", "abstract": "The prevalent use of large language models (LLMs) in various domains has\ndrawn attention to the issue of \"hallucination,\" which refers to instances\nwhere LLMs generate factually inaccurate or ungrounded information. Existing\ntechniques for hallucination detection in language assistants rely on intricate\nfuzzy, specific free-language-based chain of thought (CoT) techniques or\nparameter-based methods that suffer from interpretability issues. Additionally,\nthe methods that identify hallucinations post-generation could not prevent\ntheir occurrence and suffer from inconsistent performance due to the influence\nof the instruction format and model style. In this paper, we introduce a novel\npre-detection self-evaluation technique, referred to as SELF-FAMILIARITY, which\nfocuses on evaluating the model's familiarity with the concepts present in the\ninput instruction and withholding the generation of response in case of\nunfamiliar concepts. This approach emulates the human ability to refrain from\nresponding to unfamiliar topics, thus reducing hallucinations. We validate\nSELF-FAMILIARITY across four different large language models, demonstrating\nconsistently superior performance compared to existing techniques. Our findings\npropose a significant shift towards preemptive strategies for hallucination\nmitigation in LLM assistants, promising improvements in reliability,\napplicability, and interpretability.", "journal": ""}
{"doi": "10.48550/arXiv.2310.00259", "date": "2023-09-30", "title": "AutoHall: Automated Hallucination Dataset Generation for Large Language Models", "authors": "Zouying Cao, Yifei Yang, Hai Zhao", "abstract": "While Large language models (LLMs) have garnered widespread applications\nacross various domains due to their powerful language understanding and\ngeneration capabilities, the detection of non-factual or hallucinatory content\ngenerated by LLMs remains scarce. Currently, one significant challenge in\nhallucination detection is the laborious task of time-consuming and expensive\nmanual annotation of the hallucinatory generation. To address this issue, this\npaper first introduces a method for automatically constructing model-specific\nhallucination datasets based on existing fact-checking datasets called\nAutoHall. Furthermore, we propose a zero-resource and black-box hallucination\ndetection method based on self-contradiction. We conduct experiments towards\nprevalent open-/closed-source LLMs, achieving superior hallucination detection\nperformance compared to extant baselines. Moreover, our experiments reveal\nvariations in hallucination proportions and types among different models.", "journal": ""}
{"doi": "10.48550/arXiv.2311.07397", "date": "2023-11-13", "title": "AMBER: An LLM-free Multi-dimensional Benchmark for MLLMs Hallucination Evaluation", "authors": "Junyang Wang, Yuhang Wang, Guohai Xu, Jing Zhang, Yukai Gu, Haitao Jia, Jiaqi Wang, Haiyang Xu, Ming Yan, Ji Zhang, Jitao Sang", "abstract": "Despite making significant progress in multi-modal tasks, current Multi-modal\nLarge Language Models (MLLMs) encounter the significant challenge of\nhallucinations, which may lead to harmful consequences. Therefore, evaluating\nMLLMs' hallucinations is becoming increasingly important in model improvement\nand practical application deployment. Previous works are limited in high\nevaluation costs (e.g., relying on humans or advanced LLMs) and insufficient\nevaluation dimensions (e.g., types of tasks and hallucinations). In this paper,\nwe propose an LLM-free multi-dimensional benchmark AMBER, which can be used to\nevaluate both generative task and discriminative task including existence,\nattribute and relation hallucination. Based on AMBER, we design a low-cost and\nefficient evaluation pipeline. Additionally, we conduct a comprehensive\nevaluation and detailed analysis of mainstream MLLMs including GPT-4V(ision),\nand also give guideline suggestions for mitigating hallucinations. The data and\ncode of AMBER are available at https://github.com/junyangwang0410/AMBER.", "journal": ""}
{"doi": "10.48550/arXiv.2311.09114", "date": "2023-11-15", "title": "Ever: Mitigating Hallucination in Large Language Models through Real-Time Verification and Rectification", "authors": "Haoqiang Kang, Juntong Ni, Huaxiu Yao", "abstract": "Large Language Models (LLMs) have demonstrated remarkable proficiency in\ngenerating fluent text. However, they often encounter the challenge of\ngenerating inaccurate or hallucinated content. This issue is common in both\nnon-retrieval-based generation and retrieval-augmented generation approaches,\nand existing post-hoc rectification methods may not address the accumulated\nhallucination errors that may be caused by the \"snowballing\" issue, especially\nin reasoning tasks. To tackle these challenges, we introduce a novel approach\ncalled Real-time Verification and Rectification (Ever). Instead of waiting\nuntil the end of the generation process to rectify hallucinations, Ever employs\na real-time, step-wise generation and hallucination rectification strategy. The\nprimary objective is to detect and rectify hallucinations as they occur during\nthe text generation process. When compared to both retrieval-based and\nnon-retrieval-based baselines, Ever demonstrates a significant improvement in\ngenerating trustworthy and factually accurate text across a diverse range of\ntasks, including short-form QA, biography generation, and multi-hop reasoning.", "journal": ""}
{"doi": "10.48550/arXiv.2311.15548", "date": "2023-11-27", "title": "Deficiency of Large Language Models in Finance: An Empirical Examination of Hallucination", "authors": "Haoqiang Kang, Xiao-Yang Liu", "abstract": "The hallucination issue is recognized as a fundamental deficiency of large\nlanguage models (LLMs), especially when applied to fields such as finance,\neducation, and law. Despite the growing concerns, there has been a lack of\nempirical investigation. In this paper, we provide an empirical examination of\nLLMs' hallucination behaviors in financial tasks. First, we empirically\ninvestigate LLM model's ability of explaining financial concepts and\nterminologies. Second, we assess LLM models' capacity of querying historical\nstock prices. Third, to alleviate the hallucination issue, we evaluate the\nefficacy of four practical methods, including few-shot learning, Decoding by\nContrasting Layers (DoLa), the Retrieval Augmentation Generation (RAG) method\nand the prompt-based tool learning method for a function to generate a query\ncommand. Finally, our major finding is that off-the-shelf LLMs experience\nserious hallucination behaviors in financial tasks. Therefore, there is an\nurgent need to call for research efforts in mitigating LLMs' hallucination.", "journal": ""}
{"doi": "10.48550/arXiv.2312.01701", "date": "2023-12-04", "title": "Mitigating Fine-Grained Hallucination by Fine-Tuning Large Vision-Language Models with Caption Rewrites", "authors": "Lei Wang, Jiabang He, Shenshen Li, Ning Liu, Ee-Peng Lim", "abstract": "Large language models (LLMs) have shown remarkable performance in natural\nlanguage processing (NLP) tasks. To comprehend and execute diverse human\ninstructions over image data, instruction-tuned large vision-language models\n(LVLMs) have been introduced. However, LVLMs may suffer from different types of\nobject hallucinations. Nevertheless, LVLMs are evaluated for coarse-grained\nobject hallucinations only (i.e., generated objects non-existent in the input\nimage). The fine-grained object attributes and behaviors non-existent in the\nimage may still be generated but not measured by the current evaluation\nmethods. In this paper, we thus focus on reducing fine-grained hallucinations\nof LVLMs. We propose \\textit{ReCaption}, a framework that consists of two\ncomponents: rewriting captions using ChatGPT and fine-tuning the\ninstruction-tuned LVLMs on the rewritten captions. We also propose a\nfine-grained probing-based evaluation method named \\textit{Fine-Grained Object\nHallucination Evaluation} (\\textit{FGHE}). Our experiment results demonstrate\nthat ReCaption effectively reduces fine-grained object hallucination for\ndifferent LVLM options and improves their text generation quality. The code can\nbe found at https://github.com/Anonymousanoy/FOHE.", "journal": ""}
{"doi": "10.48550/arXiv.2401.08358", "date": "2024-01-16", "title": "Hallucination Detection and Hallucination Mitigation: An Investigation", "authors": "Junliang Luo, Tianyu Li, Di Wu, Michael Jenkin, Steve Liu, Gregory Dudek", "abstract": "Large language models (LLMs), including ChatGPT, Bard, and Llama, have\nachieved remarkable successes over the last two years in a range of different\napplications. In spite of these successes, there exist concerns that limit the\nwide application of LLMs. A key problem is the problem of hallucination.\nHallucination refers to the fact that in addition to correct responses, LLMs\ncan also generate seemingly correct but factually incorrect responses. This\nreport aims to present a comprehensive review of the current literature on both\nhallucination detection and hallucination mitigation. We hope that this report\ncan serve as a good reference for both engineers and researchers who are\ninterested in LLMs and applying them to real world tasks.", "journal": ""}
{"doi": "10.48550/arXiv.2402.09717", "date": "2024-02-15", "title": "Visually Dehallucinative Instruction Generation: Know What You Don't Know", "authors": "Sungguk Cha, Jusung Lee, Younghyun Lee, Cheoljong Yang", "abstract": "\"When did the emperor Napoleon invented iPhone?\" Such hallucination-inducing\nquestion is well known challenge in generative language modeling. In this\nstudy, we present an innovative concept of visual hallucination, referred to as\n\"I Know (IK)\" hallucination, to address scenarios where \"I Don't Know\" is the\ndesired response. To effectively tackle this issue, we propose the VQAv2-IDK\nbenchmark, the subset of VQAv2 comprising unanswerable image-question pairs as\ndetermined by human annotators. Stepping further, we present the visually\ndehallucinative instruction generation method for IK hallucination and\nintroduce the IDK-Instructions visual instruction database. Our experiments\nshow that current methods struggle with IK hallucination. Yet, our approach\neffectively reduces these hallucinations, proving its versatility across\ndifferent frameworks and datasets.", "journal": ""}
{"doi": "10.48550/arXiv.2402.09801", "date": "2024-02-15", "title": "EFUF: Efficient Fine-grained Unlearning Framework for Mitigating Hallucinations in Multimodal Large Language Models", "authors": "Shangyu Xing, Fei Zhao, Zhen Wu, Tuo An, Weihao Chen, Chunhui Li, Jianbing Zhang, Xinyu Dai", "abstract": "Multimodal large language models (MLLMs) have attracted increasing attention\nin the past few years, but they may still generate descriptions that include\nobjects not present in the corresponding images, a phenomenon known as object\nhallucination. To eliminate hallucinations, existing methods manually annotate\npaired responses with and without hallucinations, and then employ various\nalignment algorithms to improve the alignment capability between images and\ntext. However, they not only demand considerable computation resources during\nthe finetuning stage but also require expensive human annotation to construct\npaired data needed by the alignment algorithms. To address these issues, we\nborrow the idea of unlearning and propose an efficient fine-grained unlearning\nframework (EFUF), which can eliminate hallucinations without the need for\npaired data. Extensive experiments show that our method consistently reduces\nhallucinations while preserving the generation quality with modest\ncomputational overhead. Our code and datasets will be publicly available.", "journal": ""}
{"doi": "10.48550/arXiv.2402.14545", "date": "2024-02-22", "title": "Less is More: Mitigating Multimodal Hallucination from an EOS Decision Perspective", "authors": "Zihao Yue, Liang Zhang, Qin Jin", "abstract": "Large Multimodal Models (LMMs) often suffer from multimodal hallucinations,\nwherein they may create content that is not present in the visual inputs. In\nthis paper, we explore a new angle of this issue: overly detailed training data\nhinders the model's ability to timely terminate generation, leading to\ncontinued outputs beyond visual perception limits. By investigating how the\nmodel decides to terminate generation with EOS, the special end-of-sentence\ntoken, we find that the model assesses the completeness of the entire sequence\nby comparing the generated text with the image. This observation suggests that\nthe model possesses an inherent potential of making proper EOS decisions based\non its visual perception to avoid overly lengthy outputs. To take advantage of\nsuch potential, we explore two methods to mitigate multimodal hallucinations: a\ntraining objective that enables the model to reduce hallucinations by learning\nfrom regular instruction data, and a data filtering strategy to prevent harmful\ntraining data from exacerbating model hallucinations. Both methods\nsignificantly improve the hallucination performance of LMMs, without requiring\nany additional data or knowledge.", "journal": ""}
{"doi": "10.48550/arXiv.2402.15300", "date": "2024-02-23", "title": "Seeing is Believing: Mitigating Hallucination in Large Vision-Language Models via CLIP-Guided Decoding", "authors": "Ailin Deng, Zhirui Chen, Bryan Hooi", "abstract": "Large Vision-Language Models (LVLMs) are susceptible to object\nhallucinations, an issue in which their generated text contains non-existent\nobjects, greatly limiting their reliability and practicality. Current\napproaches often rely on the model's token likelihoods or other internal\ninformation, instruction tuning on additional datasets, or incorporating\ncomplex external tools. We first perform empirical analysis on sentence-level\nLVLM hallucination, finding that CLIP similarity to the image acts as a\nstronger and more robust indicator of hallucination compared to token\nlikelihoods. Motivated by this, we introduce our CLIP-Guided Decoding (CGD)\napproach, a straightforward but effective training-free approach to reduce\nobject hallucination at decoding time. CGD uses CLIP to guide the model's\ndecoding process by enhancing visual grounding of generated text with the\nimage. Experiments demonstrate that CGD effectively mitigates object\nhallucination across multiple LVLM families while preserving the utility of\ntext generation. Codes are available at\nhttps://github.com/d-ailin/CLIP-Guided-Decoding.", "journal": ""}
{"doi": "10.48550/arXiv.2402.15422", "date": "2024-02-23", "title": "A Data-Centric Approach To Generate Faithful and High Quality Patient Summaries with Large Language Models", "authors": "Stefan Hegselmann, Shannon Zejiang Shen, Florian Gierse, Monica Agrawal, David Sontag, Xiaoyi Jiang", "abstract": "Patients often face difficulties in understanding their hospitalizations,\nwhile healthcare workers have limited resources to provide explanations. In\nthis work, we investigate the potential of large language models to generate\npatient summaries based on doctors' notes and study the effect of training data\non the faithfulness and quality of the generated summaries. To this end, we\nrelease (i) a rigorous labeling protocol for errors in medical texts and (ii) a\npublicly available dataset of annotated hallucinations in 100 doctor-written\nand 100 generated summaries. We show that fine-tuning on hallucination-free\ndata effectively reduces hallucinations from 2.60 to 1.55 per summary for Llama\n2, while preserving relevant information. We observe a similar effect on GPT-4\n(0.70 to 0.40), when the few-shot examples are hallucination-free. We also\nconduct a qualitative evaluation using hallucination-free and improved training\ndata. We find that common quantitative metrics do not correlate well with\nfaithfulness and quality. Finally, we test GPT-4 for automatic hallucination\ndetection, which clearly outperforms common baselines.", "journal": ""}
{"doi": "10.48550/arXiv.2403.04307", "date": "2024-03-07", "title": "HaluEval-Wild: Evaluating Hallucinations of Language Models in the Wild", "authors": "Zhiying Zhu, Yiming Yang, Zhiqing Sun", "abstract": "Hallucinations pose a significant challenge to the reliability of large\nlanguage models (LLMs) in critical domains. Recent benchmarks designed to\nassess LLM hallucinations within conventional NLP tasks, such as\nknowledge-intensive question answering (QA) and summarization, are insufficient\nfor capturing the complexities of user-LLM interactions in dynamic, real-world\nsettings. To address this gap, we introduce HaluEval-Wild, the first benchmark\nspecifically designed to evaluate LLM hallucinations in the wild. We\nmeticulously collect challenging (adversarially filtered by Alpaca) user\nqueries from ShareGPT, an existing real-world user-LLM interaction datasets, to\nevaluate the hallucination rates of various LLMs. Upon analyzing the collected\nqueries, we categorize them into five distinct types, which enables a\nfine-grained analysis of the types of hallucinations LLMs exhibit, and\nsynthesize the reference answers with the powerful GPT-4 model and\nretrieval-augmented generation (RAG). Our benchmark offers a novel approach\ntowards enhancing our comprehension of and improving LLM reliability in\nscenarios reflective of real-world interactions. Our benchmark is available at\nhttps://github.com/HaluEval-Wild/HaluEval-Wild.", "journal": ""}
{"doi": "10.48550/arXiv.2403.06710", "date": "2024-03-11", "title": "HILL: A Hallucination Identifier for Large Language Models", "authors": "Florian Leiser, Sven Eckhardt, Valentin Leuthe, Merlin Knaeble, Alexander Maedche, Gerhard Schwabe, Ali Sunyaev", "abstract": "Large language models (LLMs) are prone to hallucinations, i.e., nonsensical,\nunfaithful, and undesirable text. Users tend to overrely on LLMs and\ncorresponding hallucinations which can lead to misinterpretations and errors.\nTo tackle the problem of overreliance, we propose HILL, the \"Hallucination\nIdentifier for Large Language Models\". First, we identified design features for\nHILL with a Wizard of Oz approach with nine participants. Subsequently, we\nimplemented HILL based on the identified design features and evaluated HILL's\ninterface design by surveying 17 participants. Further, we investigated HILL's\nfunctionality to identify hallucinations based on an existing\nquestion-answering dataset and five user interviews. We find that HILL can\ncorrectly identify and highlight hallucinations in LLM responses which enables\nusers to handle LLM responses with more caution. With that, we propose an\neasy-to-implement adaptation to existing LLMs and demonstrate the relevance of\nuser-centered designs of AI artifacts.", "journal": ""}
{"doi": "10.48550/arXiv.2403.14401", "date": "2024-03-21", "title": "Pensieve: Retrospect-then-Compare Mitigates Visual Hallucination", "authors": "Dingchen Yang, Bowen Cao, Guang Chen, Changjun Jiang", "abstract": "Multi-modal Large Language Models (MLLMs) demonstrate remarkable success\nacross various vision-language tasks. However, they suffer from visual\nhallucination, where the generated responses diverge from the provided image.\nAre MLLMs oblivious to the accurate visual cues when they hallucinate? Our\ninvestigation reveals that the visual branch may equally advocate both accurate\nand erroneous content. To address this issue, we propose Pensieve, a\ntraining-free method that leverages the analogous visual hallucinations, which\nare induced by images sharing common semantic and appearance characteristics,\nto mitigate hallucination. Specifically, Pensieve enables MLLMs to retrospect\nrelevant images as references and compare their visual content with the test\nimage via confidence score subtraction. Moreover, our paradigm balances the\neffects of addressing errors from both the visual and textual branches by\nadaptively scaling the subtracted scores. Experiments on Whoops, LLaVA Bench,\nPOPE, and MME demonstrate the efficacy of Pensieve in mitigating visual\nhallucination, surpassing other advanced decoding strategies. Pensieve also\naids MLLMs in identifying visual details and enhance the specificity of\ngenerated image descriptions.", "journal": ""}
{"doi": "10.48550/arXiv.2404.05904", "date": "2024-04-08", "title": "The Hallucinations Leaderboard -- An Open Effort to Measure Hallucinations in Large Language Models", "authors": "Giwon Hong, Aryo Pradipta Gema, Rohit Saxena, Xiaotang Du, Ping Nie, Yu Zhao, Laura Perez-Beltrachini, Max Ryabinin, Xuanli He, Cl\u00e9mentine Fourrier, Pasquale Minervini", "abstract": "Large Language Models (LLMs) have transformed the Natural Language Processing\n(NLP) landscape with their remarkable ability to understand and generate\nhuman-like text. However, these models are prone to ``hallucinations'' --\noutputs that do not align with factual reality or the input context. This paper\nintroduces the Hallucinations Leaderboard, an open initiative to quantitatively\nmeasure and compare the tendency of each model to produce hallucinations. The\nleaderboard uses a comprehensive set of benchmarks focusing on different\naspects of hallucinations, such as factuality and faithfulness, across various\ntasks, including question-answering, summarisation, and reading comprehension.\nOur analysis provides insights into the performance of different models,\nguiding researchers and practitioners in choosing the most reliable models for\ntheir applications.", "journal": ""}
{"doi": "10.48550/arXiv.2404.07461", "date": "2024-04-11", "title": "An Audit on the Perspectives and Challenges of Hallucinations in NLP", "authors": "Pranav Narayanan Venkit, Tatiana Chakravorti, Vipul Gupta, Heidi Biggs, Mukund Srinath, Koustava Goswami, Sarah Rajtmajer, Shomir Wilson", "abstract": "We audit how hallucination in large language models (LLMs) is characterized\nin peer-reviewed literature, using a critical examination of 103 publications\nacross NLP research. Through the examination of the literature, we identify a\nlack of agreement with the term `hallucination' in the field of NLP.\nAdditionally, to compliment our audit, we conduct a survey with 171\npractitioners from the field of NLP and AI to capture varying perspectives on\nhallucination. Our analysis calls for the necessity of explicit definitions and\nframeworks outlining hallucination within NLP, highlighting potential\nchallenges, and our survey inputs provide a thematic understanding of the\ninfluence and ramifications of hallucination in society.", "journal": ""}
{"doi": "10.48550/arXiv.2405.18027", "date": "2024-05-28", "title": "TimeChara: Evaluating Point-in-Time Character Hallucination of Role-Playing Large Language Models", "authors": "Jaewoo Ahn, Taehyun Lee, Junyoung Lim, Jin-Hwa Kim, Sangdoo Yun, Hwaran Lee, Gunhee Kim", "abstract": "While Large Language Models (LLMs) can serve as agents to simulate human\nbehaviors (i.e., role-playing agents), we emphasize the importance of\npoint-in-time role-playing. This situates characters at specific moments in the\nnarrative progression for three main reasons: (i) enhancing users' narrative\nimmersion, (ii) avoiding spoilers, and (iii) fostering engagement in fandom\nrole-playing. To accurately represent characters at specific time points,\nagents must avoid character hallucination, where they display knowledge that\ncontradicts their characters' identities and historical timelines. We introduce\nTimeChara, a new benchmark designed to evaluate point-in-time character\nhallucination in role-playing LLMs. Comprising 10,895 instances generated\nthrough an automated pipeline, this benchmark reveals significant hallucination\nissues in current state-of-the-art LLMs (e.g., GPT-4o). To counter this\nchallenge, we propose Narrative-Experts, a method that decomposes the reasoning\nsteps and utilizes narrative experts to reduce point-in-time character\nhallucinations effectively. Still, our findings with TimeChara highlight the\nongoing challenges of point-in-time character hallucination, calling for\nfurther study.", "journal": ""}
{"doi": "10.48550/arXiv.2405.19186", "date": "2024-05-29", "title": "MetaToken: Detecting Hallucination in Image Descriptions by Meta Classification", "authors": "Laura Fieback, Jakob Spiegelberg, Hanno Gottschalk", "abstract": "Large Vision Language Models (LVLMs) have shown remarkable capabilities in\nmultimodal tasks like visual question answering or image captioning. However,\ninconsistencies between the visual information and the generated text, a\nphenomenon referred to as hallucinations, remain an unsolved problem with\nregard to the trustworthiness of LVLMs. To address this problem, recent works\nproposed to incorporate computationally costly Large (Vision) Language Models\nin order to detect hallucinations on a sentence- or subsentence-level. In this\nwork, we introduce MetaToken, a lightweight binary classifier to detect\nhallucinations on the token-level at negligible cost. Based on a statistical\nanalysis, we reveal key factors of hallucinations in LVLMs. MetaToken can be\napplied to any open-source LVLM without any knowledge about ground truth data\nproviding a calibrated detection of hallucinations. We evaluate our method on\nfour state-of-the-art LVLMs demonstrating the effectiveness of our approach.", "journal": ""}
{"doi": "10.48550/arXiv.2406.11267", "date": "2024-06-17", "title": "Mitigating Large Language Model Hallucination with Faithful Finetuning", "authors": "Minda Hu, Bowei He, Yufei Wang, Liangyou Li, Chen Ma, Irwin King", "abstract": "Large language models (LLMs) have demonstrated remarkable performance on\nvarious natural language processing tasks. However, they are prone to\ngenerating fluent yet untruthful responses, known as \"hallucinations\".\nHallucinations can lead to the spread of misinformation and cause harm in\ncritical applications. Mitigating hallucinations is challenging as they arise\nfrom factors such as noisy data, model overconfidence, lack of knowledge, and\nthe generation process itself. Recent efforts have attempted to address this\nissue through representation editing and decoding algorithms, reducing\nhallucinations without major structural changes or retraining. However, these\napproaches either implicitly edit LLMs' behavior in latent space or suppress\nthe tendency to output unfaithful results during decoding instead of explicitly\nmodeling on hallucination. In this work, we introduce Faithful Finetuning (F2),\na novel method that explicitly models the process of faithful question\nanswering through carefully designed loss functions during fine-tuning. We\nconduct extensive experiments on popular datasets and demonstrate that F2\nachieves significant improvements over vanilla models and baselines.", "journal": ""}
{"doi": "10.48550/arXiv.2406.16449", "date": "2024-06-24", "title": "Evaluating and Analyzing Relationship Hallucinations in Large Vision-Language Models", "authors": "Mingrui Wu, Jiayi Ji, Oucheng Huang, Jiale Li, Yuhang Wu, Xiaoshuai Sun, Rongrong Ji", "abstract": "The issue of hallucinations is a prevalent concern in existing Large\nVision-Language Models (LVLMs). Previous efforts have primarily focused on\ninvestigating object hallucinations, which can be easily alleviated by\nintroducing object detectors. However, these efforts neglect hallucinations in\ninter-object relationships, which is essential for visual comprehension. In\nthis work, we introduce R-Bench, a novel benchmark for evaluating Vision\nRelationship Hallucination. R-Bench features image-level questions that focus\non the existence of relationships and instance-level questions that assess\nlocal visual comprehension. We identify three types of relationship\nco-occurrences that lead to hallucinations: relationship-relationship,\nsubject-relationship, and relationship-object. The visual instruction tuning\ndataset's long-tail distribution significantly impacts LVLMs' understanding of\nvisual relationships. Furthermore, our analysis reveals that current LVLMs tend\nto disregard visual content and overly rely on the common sense knowledge of\nLarge Language Models. They also struggle with reasoning about spatial\nrelationships based on contextual information.", "journal": ""}
{"doi": "10.48550/arXiv.2407.03282", "date": "2024-07-03", "title": "LLM Internal States Reveal Hallucination Risk Faced With a Query", "authors": "Ziwei Ji, Delong Chen, Etsuko Ishii, Samuel Cahyawijaya, Yejin Bang, Bryan Wilie, Pascale Fung", "abstract": "The hallucination problem of Large Language Models (LLMs) significantly\nlimits their reliability and trustworthiness. Humans have a self-awareness\nprocess that allows us to recognize what we don't know when faced with queries.\nInspired by this, our paper investigates whether LLMs can estimate their own\nhallucination risk before response generation. We analyze the internal\nmechanisms of LLMs broadly both in terms of training data sources and across 15\ndiverse Natural Language Generation (NLG) tasks, spanning over 700 datasets.\nOur empirical analysis reveals two key insights: (1) LLM internal states\nindicate whether they have seen the query in training data or not; and (2) LLM\ninternal states show they are likely to hallucinate or not regarding the query.\nOur study explores particular neurons, activation layers, and tokens that play\na crucial role in the LLM perception of uncertainty and hallucination risk. By\na probing estimator, we leverage LLM self-assessment, achieving an average\nhallucination estimation accuracy of 84.32\\% at run time.", "journal": ""}
{"doi": "10.48550/arXiv.2408.13906", "date": "2024-08-25", "title": "ConVis: Contrastive Decoding with Hallucination Visualization for Mitigating Hallucinations in Multimodal Large Language Models", "authors": "Yeji Park, Deokyeong Lee, Junsuk Choe, Buru Chang", "abstract": "Hallucinations in Multimodal Large Language Models (MLLMs) where generated\nresponses fail to accurately reflect the given image pose a significant\nchallenge to their reliability. To address this, we introduce ConVis, a novel\ntraining-free contrastive decoding method. ConVis leverages a text-to-image\n(T2I) generation model to semantically reconstruct the given image from\nhallucinated captions. By comparing the contrasting probability distributions\nproduced by the original and reconstructed images, ConVis enables MLLMs to\ncapture visual contrastive signals that penalize hallucination generation.\nNotably, this method operates purely within the decoding process, eliminating\nthe need for additional data or model updates. Our extensive experiments on\nfive popular benchmarks demonstrate that ConVis effectively reduces\nhallucinations across various MLLMs, highlighting its potential to enhance\nmodel reliability.", "journal": ""}
{"doi": "10.48550/arXiv.2408.15533", "date": "2024-08-28", "title": "LRP4RAG: Detecting Hallucinations in Retrieval-Augmented Generation via Layer-wise Relevance Propagation", "authors": "Haichuan Hu, Yuhan Sun, Quanjun Zhang", "abstract": "Retrieval-Augmented Generation (RAG) has become a primary technique for\nmitigating hallucinations in large language models (LLMs). However, incomplete\nknowledge extraction and insufficient understanding can still mislead LLMs to\nproduce irrelevant or even contradictory responses, which means hallucinations\npersist in RAG. In this paper, we propose LRP4RAG, a method based on the\nLayer-wise Relevance Propagation (LRP) algorithm for detecting hallucinations\nin RAG. Specifically, we first utilize LRP to compute the relevance between the\ninput and output of the RAG generator. We then apply further extraction and\nresampling to the relevance matrix. The processed relevance data are input into\nmultiple classifiers to determine whether the output contains hallucinations.\nTo the best of our knowledge, this is the first time that LRP has been used for\ndetecting RAG hallucinations, and extensive experiments demonstrate that\nLRP4RAG outperforms existing baselines.", "journal": ""}
{"doi": "10.48550/arXiv.2409.05746", "date": "2024-09-09", "title": "LLMs Will Always Hallucinate, and We Need to Live With This", "authors": "Sourav Banerjee, Ayushi Agarwal, Saloni Singla", "abstract": "As Large Language Models become more ubiquitous across domains, it becomes\nimportant to examine their inherent limitations critically. This work argues\nthat hallucinations in language models are not just occasional errors but an\ninevitable feature of these systems. We demonstrate that hallucinations stem\nfrom the fundamental mathematical and logical structure of LLMs. It is,\ntherefore, impossible to eliminate them through architectural improvements,\ndataset enhancements, or fact-checking mechanisms. Our analysis draws on\ncomputational theory and Godel's First Incompleteness Theorem, which references\nthe undecidability of problems like the Halting, Emptiness, and Acceptance\nProblems. We demonstrate that every stage of the LLM process-from training data\ncompilation to fact retrieval, intent classification, and text generation-will\nhave a non-zero probability of producing hallucinations. This work introduces\nthe concept of Structural Hallucination as an intrinsic nature of these\nsystems. By establishing the mathematical certainty of hallucinations, we\nchallenge the prevailing notion that they can be fully mitigated.", "journal": ""}
{"doi": "10.48550/arXiv.2409.13612", "date": "2024-09-20", "title": "FIHA: Autonomous Hallucination Evaluation in Vision-Language Models with Davidson Scene Graphs", "authors": "Bowen Yan, Zhengsong Zhang, Liqiang Jing, Eftekhar Hossain, Xinya Du", "abstract": "The rapid development of Large Vision-Language Models (LVLMs) often comes\nwith widespread hallucination issues, making cost-effective and comprehensive\nassessments increasingly vital. Current approaches mainly rely on costly\nannotations and are not comprehensive -- in terms of evaluating all aspects\nsuch as relations, attributes, and dependencies between aspects. Therefore, we\nintroduce the FIHA (autonomous Fine-graIned Hallucination evAluation evaluation\nin LVLMs), which could access hallucination LVLMs in the LLM-free and\nannotation-free way and model the dependency between different types of\nhallucinations. FIHA can generate Q&A pairs on any image dataset at minimal\ncost, enabling hallucination assessment from both image and caption. Based on\nthis approach, we introduce a benchmark called FIHA-v1, which consists of\ndiverse questions on various images from MSCOCO and Foggy. Furthermore, we use\nthe Davidson Scene Graph (DSG) to organize the structure among Q&A pairs, in\nwhich we can increase the reliability of the evaluation. We evaluate\nrepresentative models using FIHA-v1, highlighting their limitations and\nchallenges. We released our code and data.", "journal": ""}
{"doi": "10.48550/arXiv.2410.02762", "date": "2024-10-03", "title": "Interpreting and Editing Vision-Language Representations to Mitigate Hallucinations", "authors": "Nick Jiang, Anish Kachinthaya, Suzie Petryk, Yossi Gandelsman", "abstract": "We investigate the internal representations of vision-language models (VLMs)\nto address hallucinations, a persistent challenge despite advances in model\nsize and training. We project VLMs' internal image representations to their\nlanguage vocabulary and observe more confident output probabilities on real\nobjects than hallucinated objects. We additionally use these output\nprobabilities to spatially localize real objects. Building on this approach, we\nintroduce a knowledge erasure algorithm that removes hallucinations by linearly\northogonalizing image features with respect to hallucinated object features. We\nshow that targeted edits to a model's latent representations can reduce\nhallucinations by up to 25.7% on the COCO2014 dataset while preserving\nperformance. Our findings demonstrate how a deeper understanding of VLMs'\nlatent representations can enhance reliability and enable novel capabilities,\nsuch as zero-shot segmentation.", "journal": ""}
{"doi": "10.48550/arXiv.2410.12787", "date": "2024-10-16", "title": "The Curse of Multi-Modalities: Evaluating Hallucinations of Large Multimodal Models across Language, Visual, and Audio", "authors": "Sicong Leng, Yun Xing, Zesen Cheng, Yang Zhou, Hang Zhang, Xin Li, Deli Zhao, Shijian Lu, Chunyan Miao, Lidong Bing", "abstract": "Recent advancements in large multimodal models (LMMs) have significantly\nenhanced performance across diverse tasks, with ongoing efforts to further\nintegrate additional modalities such as video and audio. However, most existing\nLMMs remain vulnerable to hallucinations, the discrepancy between the factual\nmultimodal input and the generated textual output, which has limited their\napplicability in various real-world scenarios. This paper presents the first\nsystematic investigation of hallucinations in LMMs involving the three most\ncommon modalities: language, visual, and audio. Our study reveals two key\ncontributors to hallucinations: overreliance on unimodal priors and spurious\ninter-modality correlations. To address these challenges, we introduce the\nbenchmark The Curse of Multi-Modalities (CMM), which comprehensively evaluates\nhallucinations in LMMs, providing a detailed analysis of their underlying\nissues. Our findings highlight key vulnerabilities, including imbalances in\nmodality integration and biases from training data, underscoring the need for\nbalanced cross-modal learning and enhanced hallucination mitigation strategies.\nBased on our observations and findings, we suggest potential research\ndirections that could enhance the reliability of LMMs.", "journal": ""}
{"doi": "10.48550/arXiv.2411.02712", "date": "2024-11-05", "title": "V-DPO: Mitigating Hallucination in Large Vision Language Models via Vision-Guided Direct Preference Optimization", "authors": "Yuxi Xie, Guanzhen Li, Xiao Xu, Min-Yen Kan", "abstract": "Large vision-language models (LVLMs) suffer from hallucination, resulting in\nmisalignment between the output textual response and the input visual content.\nRecent research indicates that the over-reliance on the Large Language Model\n(LLM) backbone, as one cause of the LVLM hallucination, inherently introduces\nbias from language priors, leading to insufficient context attention to the\nvisual inputs.\n  We tackle this issue of hallucination by mitigating such over-reliance\nthrough preference learning. We propose Vision-guided Direct Preference\nOptimization (V-DPO) to enhance visual context learning at training time. To\ninterpret the effectiveness and generalizability of V-DPO on different types of\ntraining data, we construct a synthetic dataset containing both response- and\nimage-contrast preference pairs, compared against existing human-annotated\nhallucination samples. Our approach achieves significant improvements compared\nwith baseline methods across various hallucination benchmarks. Our analysis\nindicates that V-DPO excels in learning from image-contrast preference data,\ndemonstrating its superior ability to elicit and understand nuances of visual\ncontext. Our code is publicly available at https://github.com/YuxiXie/V-DPO.", "journal": ""}
{"doi": "10.48550/arXiv.2411.04847", "date": "2024-11-07", "title": "Prompt-Guided Internal States for Hallucination Detection of Large Language Models", "authors": "Fujie Zhang, Peiqi Yu, Biao Yi, Baolei Zhang, Tong Li, Zheli Liu", "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\na variety of tasks in different domains. However, they sometimes generate\nresponses that are logically coherent but factually incorrect or misleading,\nwhich is known as LLM hallucinations. Data-driven supervised methods train\nhallucination detectors by leveraging the internal states of LLMs, but\ndetectors trained on specific domains often struggle to generalize well to\nother domains. In this paper, we aim to enhance the cross-domain performance of\nsupervised detectors with only in-domain data. We propose a novel framework,\nprompt-guided internal states for hallucination detection of LLMs, namely\nPRISM. By utilizing appropriate prompts to guide changes to the structure\nrelated to text truthfulness in LLMs' internal states, we make this structure\nmore salient and consistent across texts from different domains. We integrated\nour framework with existing hallucination detection methods and conducted\nexperiments on datasets from different domains. The experimental results\nindicate that our framework significantly enhances the cross-domain\ngeneralization of existing hallucination detection methods.", "journal": ""}
{"doi": "10.48550/arXiv.2411.07457", "date": "2024-11-12", "title": "DecoPrompt : Decoding Prompts Reduces Hallucinations when Large Language Models Meet False Premises", "authors": "Nan Xu, Xuezhe Ma", "abstract": "While large language models (LLMs) have demonstrated increasing power, they\nhave also called upon studies on their hallucinated outputs that deviate from\nfactually correct statements. In this paper, we focus on one important scenario\nof false premises, where LLMs are distracted by misaligned claims although the\nmodel possesses the required factual knowledge to answer original questions\naccurately. Inspired by the observation that entropy of the false-premise\nprompt is closely related to its likelihood to elicit hallucination generation,\nwe propose a new prompting algorithm, named DecoPrompt, to mitigate\nhallucination. DecoPrompt leverages LLMs to \"decode\" the false-premise prompts\nwithout really eliciting hallucination output from LLMs. We perform experiments\non two datasets, demonstrating that DecoPrompt can reduce hallucinations\neffectively on outputs from different LLMs. Moreover, DecoPrompt exhibits\ncross-model transferability, which facilitates its applications to scenarios\nsuch as LLMs of large sizes or unavailable model logits.", "journal": ""}
{"doi": "10.48550/arXiv.2411.15839", "date": "2024-11-24", "title": "VaLiD: Mitigating the Hallucination of Large Vision Language Models by Visual Layer Fusion Contrastive Decoding", "authors": "Jiaqi Wang, Yifei Gao, Jitao Sang", "abstract": "Large Vision-Language Models (LVLMs) have demonstrated remarkable\ncapabilities in multimodal task reasoning. However, they often generate\nresponses that appear plausible yet do not accurately reflect the visual\ncontent, a phenomenon known as hallucination. Recent approaches have introduced\ntraining-free methods to mitigate hallucinations by adjusting the decoding\nstrategy during the inference stage, typically attributing hallucinations to\nthe language model itself. Our analysis, however, reveals that distortions in\nthe visual encoding process significantly affect the model's reasoning\ncapabilities. Specifically, earlier visual layers may retain key features but\ngradually distort as the information propagates toward the output layer.\nBuilding on these insights, we propose a novel hallucination-mitigation method\nfrom the visual encoding perspective: \\textbf{V}isu\\textbf{a}l \\textbf{L}ayer\nFus\\textbf{i}on Contrastive \\textbf{D}ecoding (\\textbf{VaLiD}). This method\nutilizes uncertainty to guide the visual layer selection, correcting\ndistortions in the visual encoding process and thereby enhancing the\nreliability of the generated content. Experimental results demonstrate the\neffectiveness of VaLiD in mitigating hallucinations across various benchmarks,\nachieving state-of-the-art performance when compared to baseline methods. Codes\nare available at\n\\href{https://github.com/RicardoLuL/VaLiD_LVLMs_hallucinations}{Github}.", "journal": ""}
{"doi": "10.48550/arXiv.2501.19164", "date": "2025-01-31", "title": "Poison as Cure: Visual Noise for Mitigating Object Hallucinations in LVMs", "authors": "Kejia Zhang, Keda Tao, Jiasheng Tang, Huan Wang", "abstract": "Large vision-language models (LVMs) extend large language models (LLMs) with\nvisual perception capabilities, enabling them to process and interpret visual\ninformation. A major challenge compromising their reliability is object\nhallucination that LVMs may generate plausible but factually inaccurate\ninformation. We propose a novel visual adversarial perturbation (VAP) method to\nmitigate this hallucination issue. VAP alleviates LVM hallucination by applying\nstrategically optimized visual noise without altering the base model. Our\napproach formulates hallucination suppression as an optimization problem,\nleveraging adversarial strategies to generate beneficial visual perturbations\nthat enhance the model's factual grounding and reduce parametric knowledge\nbias. Extensive experimental results demonstrate that our method consistently\nreduces object hallucinations across 8 state-of-the-art LVMs, validating its\nefficacy across diverse evaluations.", "journal": ""}
{"doi": "10.48550/arXiv.2502.03799", "date": "2025-02-06", "title": "Enhancing Hallucination Detection through Noise Injection", "authors": "Litian Liu, Reza Pourreza, Sunny Panchal, Apratim Bhattacharyya, Yao Qin, Roland Memisevic", "abstract": "Large Language Models (LLMs) are prone to generating plausible yet incorrect\nresponses, known as hallucinations. Effectively detecting hallucinations is\ntherefore crucial for the safe deployment of LLMs. Recent research has linked\nhallucinations to model uncertainty, suggesting that hallucinations can be\ndetected by measuring dispersion over answer distributions obtained from a set\nof samples drawn from a model. While drawing from the distribution over tokens\ndefined by the model is a natural way to obtain samples, in this work, we argue\nthat it is sub-optimal for the purpose of detecting hallucinations. We show\nthat detection can be improved significantly by taking into account model\nuncertainty in the Bayesian sense. To this end, we propose a very simple and\nefficient approach that perturbs an appropriate subset of model parameters, or\nequivalently hidden unit activations, during sampling. We demonstrate its\neffectiveness across a wide range of datasets and model architectures.", "journal": ""}
{"doi": "10.48550/arXiv.2502.15079", "date": "2025-02-20", "title": "Can Hallucination Correction Improve Video-Language Alignment?", "authors": "Lingjun Zhao, Mingyang Xie, Paola Cascante-Bonilla, Hal Daum\u00e9 III, Kwonjoon Lee", "abstract": "Large Vision-Language Models often generate hallucinated content that is not\ngrounded in its visual inputs. While prior work focuses on mitigating\nhallucinations, we instead explore leveraging hallucination correction as a\ntraining objective to improve video-language alignment. We introduce HACA, a\nself-training framework learning to correct hallucinations in descriptions that\ndo not align with the video content. By identifying and correcting\ninconsistencies, HACA enhances the model's ability to align video and textual\nrepresentations for spatio-temporal reasoning. Our experimental results show\nconsistent gains in video-caption binding and text-to-video retrieval tasks,\ndemonstrating that hallucination correction-inspired tasks serve as an\neffective strategy for improving vision and language alignment.", "journal": ""}
{"doi": "10.48550/arXiv.2502.16842", "date": "2025-02-24", "title": "Exploring Causes and Mitigation of Hallucinations in Large Vision Language Models", "authors": "Yaqi Sun, Kyohei Atarashi, Koh Takeuchi, Hisashi Kashima", "abstract": "Large Vision-Language Models (LVLMs) integrate image encoders with Large\nLanguage Models (LLMs) to process multi-modal inputs and perform complex visual\ntasks. However, they often generate hallucinations by describing non-existent\nobjects or attributes, compromising their reliability. This study analyzes\nhallucination patterns in image captioning, showing that not all tokens in the\ngeneration process are influenced by image input and that image dependency can\nserve as a useful signal for hallucination detection. To address this, we\ndevelop an automated pipeline to identify hallucinated objects and train a\ntoken-level classifier using hidden representations from parallel inference\npasses-with and without image input. Leveraging this classifier, we introduce a\ndecoding strategy that effectively controls hallucination rates in image\ncaptioning at inference time.", "journal": ""}
{"doi": "10.48550/arXiv.2502.16872", "date": "2025-02-24", "title": "Mitigating Hallucinations in Diffusion Models through Adaptive Attention Modulation", "authors": "Trevine Oorloff, Yaser Yacoob, Abhinav Shrivastava", "abstract": "Diffusion models, while increasingly adept at generating realistic images,\nare notably hindered by hallucinations -- unrealistic or incorrect features\ninconsistent with the trained data distribution. In this work, we propose\nAdaptive Attention Modulation (AAM), a novel approach to mitigate\nhallucinations by analyzing and modulating the self-attention mechanism in\ndiffusion models. We hypothesize that self-attention during early denoising\nsteps may inadvertently amplify or suppress features, contributing to\nhallucinations. To counter this, AAM introduces a temperature scaling mechanism\nwithin the softmax operation of the self-attention layers, dynamically\nmodulating the attention distribution during inference. Additionally, AAM\nemploys a masked perturbation technique to disrupt early-stage noise that may\notherwise propagate into later stages as hallucinations. Extensive experiments\ndemonstrate that AAM effectively reduces hallucinatory artifacts, enhancing\nboth the fidelity and reliability of generated images. For instance, the\nproposed approach improves the FID score by 20.8% and reduces the percentage of\nhallucinated images by 12.9% (in absolute terms) on the Hands dataset.", "journal": ""}
{"doi": "10.48550/arXiv.2502.17598", "date": "2025-02-24", "title": "Hallucination Detection in LLMs Using Spectral Features of Attention Maps", "authors": "Jakub Binkowski, Denis Janiak, Albert Sawczyn, Bogdan Gabrys, Tomasz Kajdanowicz", "abstract": "Large Language Models (LLMs) have demonstrated remarkable performance across\nvarious tasks but remain prone to hallucinations. Detecting hallucinations is\nessential for safety-critical applications, and recent methods leverage\nattention map properties to this end, though their effectiveness remains\nlimited. In this work, we investigate the spectral features of attention maps\nby interpreting them as adjacency matrices of graph structures. We propose the\n$\\text{LapEigvals}$ method, which utilises the top-$k$ eigenvalues of the\nLaplacian matrix derived from the attention maps as an input to hallucination\ndetection probes. Empirical evaluations demonstrate that our approach achieves\nstate-of-the-art hallucination detection performance among attention-based\nmethods. Extensive ablation studies further highlight the robustness and\ngeneralisation of $\\text{LapEigvals}$, paving the way for future advancements\nin the hallucination detection domain.", "journal": ""}
{"doi": "10.48550/arXiv.2502.18342", "date": "2025-02-25", "title": "BRIDO: Bringing Democratic Order to Abstractive Summarization", "authors": "Junhyun Lee, Harshith Goka, Hyeonmok Ko", "abstract": "Hallucination refers to the inaccurate, irrelevant, and inconsistent text\ngenerated from large language models (LLMs). While the LLMs have shown great\npromise in a variety of tasks, the issue of hallucination still remains a major\nchallenge for many practical uses. In this paper, we tackle the issue of\nhallucination in abstract text summarization by mitigating exposure bias.\nExisting models targeted for exposure bias mitigation, namely BRIO, aim for\nbetter summarization quality in the ROUGE score. We propose a model that uses a\nsimilar exposure bias mitigation strategy but with a goal that is aligned with\nless hallucination. We conjecture that among a group of candidate outputs, ones\nwith hallucinations will comprise the minority of the whole group. That is,\ncandidates with less similarity with others will have a higher chance of\ncontaining hallucinated content. Our method uses this aspect and utilizes\ncontrastive learning, incentivizing candidates with high inter-candidate ROUGE\nscores. We performed experiments on the XSum and CNN/DM summarization datasets,\nand our method showed 6.25% and 3.82% improvement, respectively, on the\nconsistency G-Eval score over BRIO.", "journal": ""}
{"doi": "10.48550/arXiv.2503.00361", "date": "2025-03-01", "title": "Octopus: Alleviating Hallucination via Dynamic Contrastive Decoding", "authors": "Wei Suo, Lijun Zhang, Mengyang Sun, Lin Yuanbo Wu, Peng Wang, Yanning Zhang", "abstract": "Large Vision-Language Models (LVLMs) have obtained impressive performance in\nvisual content understanding and multi-modal reasoning. Unfortunately, these\nlarge models suffer from serious hallucination problems and tend to generate\nfabricated responses. Recently, several Contrastive Decoding (CD) strategies\nhave been proposed to alleviate hallucination by introducing disturbed inputs.\nAlthough great progress has been made, these CD strategies mostly apply a\none-size-fits-all approach for all input conditions. In this paper, we revisit\nthis process through extensive experiments. Related results show that\nhallucination causes are hybrid and each generative step faces a unique\nhallucination challenge. Leveraging these meaningful insights, we introduce a\nsimple yet effective Octopus-like framework that enables the model to\nadaptively identify hallucination types and create a dynamic CD workflow. Our\nOctopus framework not only outperforms existing methods across four benchmarks\nbut also demonstrates excellent deployability and expansibility. Code is\navailable at https://github.com/LijunZhang01/Octopus.", "journal": ""}
{"doi": "10.48550/arXiv.2503.02851", "date": "2025-03-04", "title": "Shakespearean Sparks: The Dance of Hallucination and Creativity in LLMs' Decoding Layers", "authors": "Zicong He, Boxuan Zhang, Lu Cheng", "abstract": "Large language models (LLMs) are known to hallucinate, a phenomenon often\nlinked to creativity. While previous research has primarily explored this\nconnection through theoretical or qualitative lenses, our work takes a\nquantitative approach to systematically examine the relationship between\nhallucination and creativity in LLMs. Given the complex nature of creativity,\nwe propose a narrow definition tailored to LLMs and introduce an evaluation\nframework, HCL, which quantifies Hallucination and Creativity across different\nLayers of LLMs during decoding. Our empirical analysis reveals a tradeoff\nbetween hallucination and creativity that is consistent across layer depth,\nmodel type, and model size. Notably, across different model architectures, we\nidentify a specific layer at each model size that optimally balances this\ntradeoff. Additionally, the optimal layer tends to appear in the early layers\nof larger models, and the confidence of the model is also significantly higher\nat this layer. These findings provide a quantitative perspective that offers\nnew insights into the interplay between LLM creativity and hallucination. The\ncode and data for our experiments are available at\nhttps://github.com/ZicongHe2002/HCL-Spark.", "journal": ""}
{"doi": "10.48550/arXiv.2503.05757", "date": "2025-02-22", "title": "Uncertainty-Aware Fusion: An Ensemble Framework for Mitigating Hallucinations in Large Language Models", "authors": "Prasenjit Dey, Srujana Merugu, Sivaramakrishnan Kaveri", "abstract": "Large Language Models (LLMs) are known to hallucinate and generate\nnon-factual outputs which can undermine user trust. Traditional methods to\ndirectly mitigate hallucinations, such as representation editing and\ncontrastive decoding, often require additional training data and involve high\nimplementation complexity. While ensemble-based approaches harness multiple\nLLMs to tap into the \"wisdom of crowds\", these methods overlook uncertainties\nin individual model responses. Recent studies reveal that uncertainty\nestimation can enable LLMs to self-assess the likelihood of generating\nhallucinations. In this work, we focus on factoid question answering (QA) and\nobserve that LLMs accuracy and self-assessment capabilities vary widely with\ndifferent models excelling in different scenarios. Leveraging this insight, we\npropose Uncertainty-Aware Fusion (UAF), an ensemble framework to reduces\nhallucinations by strategically combining multiple LLM based on their accuracy\nand self-assessment abilities. Empirical results on several public benchmark\ndatasets show that UAF outperforms state-of-the-art hallucination mitigation\nmethods by $8\\%$ in factual accuracy, while either narrowing or surpassing the\nperformance gap with GPT-4.", "journal": ""}
{"doi": "10.48550/arXiv.2503.08963", "date": "2025-03-11", "title": "Gradient-guided Attention Map Editing: Towards Efficient Contextual Hallucination Mitigation", "authors": "Yu Wang, Jiaxin Zhang, Xiang Gao, Wendi Cui, Peng Li, Kamalika Das", "abstract": "In tasks like summarization and open-book question answering (QA), Large\nLanguage Models (LLMs) often encounter \"contextual hallucination\", where they\nproduce irrelevant or incorrect responses despite having access to accurate\nsource information. This typically occurs because these models tend to\nprioritize self-generated content over the input context, causing them to\ndisregard pertinent details. To address this challenge, we introduce a novel\nmethod called \"Guided Attention Map Editing\" (GAME), which dynamically adjusts\nattention maps to improve contextual relevance. During inference, GAME employs\na trained classifier to identify attention maps prone to inducing\nhallucinations and executes targeted interventions. These interventions, guided\nby gradient-informed \"edit directions'', strategically redistribute attention\nweights across various heads to effectively reduce hallucination. Comprehensive\nevaluations on challenging summarization and open-book QA tasks show that GAME\nconsistently reduces hallucinations across a variety of open-source models.\nSpecifically, GAME reduces hallucinations by 10% in the XSum summarization task\nwhile achieving a 7X speed-up in computational efficiency compared to the\nstate-of-the-art baselines.", "journal": ""}
{"doi": "10.48550/arXiv.2503.14392", "date": "2025-03-18", "title": "From \"Hallucination\" to \"Suture\": Insights from Language Philosophy to Enhance Large Language Models", "authors": "Qiantong Wang", "abstract": "This paper explores hallucination phenomena in large language models (LLMs)\nthrough the lens of language philosophy and psychoanalysis. By incorporating\nLacan's concepts of the \"chain of signifiers\" and \"suture points,\" we propose\nthe Anchor-RAG framework as a novel approach to mitigate hallucinations. In\ncontrast to the predominant reliance on trial-and-error experiments, constant\nadjustments of mathematical formulas, or resource-intensive methods that\nemphasize quantity over quality, our approach returns to the fundamental\nprinciples of linguistics to analyze the root causes of hallucinations in LLMs.\nDrawing from robust theoretical foundations, we derive algorithms and models\nthat are not only effective in reducing hallucinations but also enhance LLM\nperformance and improve output quality. This paper seeks to establish a\ncomprehensive theoretical framework for understanding hallucinations in LLMs\nand aims to challenge the prevalent \"guess-and-test\" approach and rat race\nmentality in the field. We aspire to pave the way for a new era of\ninterpretable LLMs, offering deeper insights into the inner workings of\nlanguage-based AI systems.", "journal": ""}
{"doi": "10.48550/arXiv.2503.17229", "date": "2025-03-21", "title": "FactSelfCheck: Fact-Level Black-Box Hallucination Detection for LLMs", "authors": "Albert Sawczyn, Jakub Binkowski, Denis Janiak, Bogdan Gabrys, Tomasz Kajdanowicz", "abstract": "Large Language Models (LLMs) frequently generate hallucinated content, posing\nsignificant challenges for applications where factuality is crucial. While\nexisting hallucination detection methods typically operate at the sentence\nlevel or passage level, we propose FactSelfCheck, a novel black-box\nsampling-based method that enables fine-grained fact-level detection. Our\napproach represents text as knowledge graphs consisting of facts in the form of\ntriples. Through analyzing factual consistency across multiple LLM responses,\nwe compute fine-grained hallucination scores without requiring external\nresources or training data. Our evaluation demonstrates that FactSelfCheck\nperforms competitively with leading sentence-level sampling-based methods while\nproviding more detailed insights. Most notably, our fact-level approach\nsignificantly improves hallucination correction, achieving a 35.5% increase in\nfactual content compared to the baseline, while sentence-level SelfCheckGPT\nyields only a 10.6% improvement. The granular nature of our detection enables\nmore precise identification and correction of hallucinated content.\nAdditionally, we contribute a new dataset for evaluating sampling-based methods\n- FavaMultiSamples.", "journal": ""}
{"doi": "10.48550/arXiv.2503.18242", "date": "2025-03-23", "title": "ShED-HD: A Shannon Entropy Distribution Framework for Lightweight Hallucination Detection on Edge Devices", "authors": "Aneesh Vathul, Daniel Lee, Sheryl Chen, Arthi Tasmia", "abstract": "Large Language Models (LLMs) have demonstrated impressive capabilities on a\nbroad array of NLP tasks, but their tendency to produce\nhallucinations$\\unicode{x2013}$plausible-sounding but factually incorrect\ncontent$\\unicode{x2013}$poses severe challenges in high-stakes domains.\nExisting hallucination detection methods either bear the computational cost of\nmultiple inference passes or sacrifice accuracy for efficiency with single-pass\napproaches, neither of which is ideal in resource-constrained environments such\nas edge devices. We propose the Shannon Entropy Distribution Hallucination\nDetector (ShED-HD), a novel hallucination detection framework that bridges this\ngap by classifying sequence-level entropy patterns using a lightweight BiLSTM\narchitecture with single-headed attention. In contrast to prior approaches,\nShED-HD efficiently detects distinctive uncertainty patterns across entire\noutput sequences, preserving contextual awareness. Through in-depth evaluation\non three datasets (BioASQ, TriviaQA, and Jeopardy Questions), we show that\nShED-HD significantly outperforms other computationally efficient approaches in\nthe out-of-distribution setting, while achieving comparable performance in the\nin-distribution setting. ShED-HD facilitates hallucination detection that is\nlow-cost, accurate, and generalizable, improving the credibility of content\ngenerated by LLMs in resource-constrained environments where trustworthy AI\nfunctionality is crucial.", "journal": ""}
{"doi": "10.48550/arXiv.2504.07069", "date": "2025-04-09", "title": "HalluciNot: Hallucination Detection Through Context and Common Knowledge Verification", "authors": "Bibek Paudel, Alexander Lyzhov, Preetam Joshi, Puneet Anand", "abstract": "This paper introduces a comprehensive system for detecting hallucinations in\nlarge language model (LLM) outputs in enterprise settings. We present a novel\ntaxonomy of LLM responses specific to hallucination in enterprise applications,\ncategorizing them into context-based, common knowledge, enterprise-specific,\nand innocuous statements. Our hallucination detection model HDM-2 validates LLM\nresponses with respect to both context and generally known facts (common\nknowledge). It provides both hallucination scores and word-level annotations,\nenabling precise identification of problematic content. To evaluate it on\ncontext-based and common-knowledge hallucinations, we introduce a new dataset\nHDMBench. Experimental results demonstrate that HDM-2 out-performs existing\napproaches across RagTruth, TruthfulQA, and HDMBench datasets. This work\naddresses the specific challenges of enterprise deployment, including\ncomputational efficiency, domain specialization, and fine-grained error\nidentification. Our evaluation dataset, model weights, and inference code are\npublicly available.", "journal": ""}
{"doi": "10.48550/arXiv.2504.07680", "date": "2025-04-10", "title": "Synthetic Fluency: Hallucinations, Confabulations, and the Creation of Irish Words in LLM-Generated Translations", "authors": "Sheila Castilho, Zoe Fitzsimmons, Claire Holton, Aoife Mc Donagh", "abstract": "This study examines hallucinations in Large Language Model (LLM) translations\ninto Irish, specifically focusing on instances where the models generate novel,\nnon-existent words. We classify these hallucinations within verb and noun\ncategories, identifying six distinct patterns among the latter. Additionally,\nwe analyse whether these hallucinations adhere to Irish morphological rules and\nwhat linguistic tendencies they exhibit. Our findings show that while both\nGPT-4.o and GPT-4.o Mini produce similar types of hallucinations, the Mini\nmodel generates them at a significantly higher frequency. Beyond\nclassification, the discussion raises speculative questions about the\nimplications of these hallucinations for the Irish language. Rather than\nseeking definitive answers, we offer food for thought regarding the increasing\nuse of LLMs and their potential role in shaping Irish vocabulary and linguistic\nevolution. We aim to prompt discussion on how such technologies might influence\nlanguage over time, particularly in the context of low-resource,\nmorphologically rich languages.", "journal": ""}
{"doi": "10.48550/arXiv.2504.08526", "date": "2025-04-11", "title": "Hallucination, reliability, and the role of generative AI in science", "authors": "Charles Rathkopf", "abstract": "Generative AI is increasingly used in scientific domains, from protein\nfolding to climate modeling. But these models produce distinctive errors known\nas hallucinations - outputs that are incorrect yet superficially plausible.\nWorse, some arguments suggest that hallucinations are an inevitable consequence\nof the mechanisms underlying generative inference. Fortunately, such arguments\nrely on a conception of hallucination defined solely with respect to internal\nproperties of the model, rather than in reference to the empirical target\nsystem. This conception fails to distinguish epistemically benign errors from\nthose that threaten scientific inference. I introduce the concept of corrosive\nhallucination to capture the epistemically troubling subclass:\nmisrepresentations that are substantively misleading and resistant to\nsystematic anticipation. I argue that although corrosive hallucinations do pose\na threat to scientific reliability, they are not inevitable. Scientific\nworkflows such as those surrounding AlphaFold and GenCast, both of which serve\nas case studies, can neutralize their effects by imposing theoretical\nconstraints during training, and by strategically screening for errors at\ninference time. When embedded in such workflows, generative AI can reliably\ncontribute to scientific knowledge.", "journal": ""}
{"doi": "10.48550/arXiv.2504.08596", "date": "2025-04-11", "title": "MedHal: An Evaluation Dataset for Medical Hallucination Detection", "authors": "Gaya Mehenni, Amal Zouaq", "abstract": "We present MedHal, a novel large-scale dataset specifically designed to\nevaluate if models can detect hallucinations in medical texts. Current\nhallucination detection methods face significant limitations when applied to\nspecialized domains like medicine, where they can have disastrous consequences.\nExisting medical datasets are either too small, containing only a few hundred\nsamples, or focus on a single task like Question Answering or Natural Language\nInference. MedHal addresses these gaps by: (1) incorporating diverse medical\ntext sources and tasks; (2) providing a substantial volume of annotated samples\nsuitable for training medical hallucination detection models; and (3) including\nexplanations for factual inconsistencies to guide model learning. We\ndemonstrate MedHal's utility by training and evaluating a baseline medical\nhallucination detection model, showing improvements over general-purpose\nhallucination detection approaches. This resource enables more efficient\nevaluation of medical text generation systems while reducing reliance on costly\nexpert review, potentially accelerating the development of medical AI research.", "journal": ""}
{"doi": "10.48550/arXiv.2504.10020", "date": "2025-04-14", "title": "The Mirage of Performance Gains: Why Contrastive Decoding Fails to Address Multimodal Hallucination", "authors": "Hao Yin, Guangzong Si, Zilei Wang", "abstract": "Contrastive decoding strategies are widely used to reduce hallucinations in\nmultimodal large language models (MLLMs). These methods work by constructing\ncontrastive samples to induce hallucinations and then suppressing them in the\noutput distribution. However, this paper demonstrates that such approaches fail\nto effectively mitigate the hallucination problem. The performance improvements\nobserved on POPE Benchmark are largely driven by two misleading factors: (1)\ncrude, unidirectional adjustments to the model's output distribution and (2)\nthe adaptive plausibility constraint, which reduces the sampling strategy to\ngreedy search. To further illustrate these issues, we introduce a series of\nspurious improvement methods and evaluate their performance against contrastive\ndecoding techniques. Experimental results reveal that the observed performance\ngains in contrastive decoding are entirely unrelated to its intended goal of\nmitigating hallucinations. Our findings challenge common assumptions about the\neffectiveness of contrastive decoding strategies and pave the way for\ndeveloping genuinely effective solutions to hallucinations in MLLMs.", "journal": ""}
{"doi": "10.48550/arXiv.2504.10167", "date": "2025-04-14", "title": "C-FAITH: A Chinese Fine-Grained Benchmark for Automated Hallucination Evaluation", "authors": "Xu Zhang, Zhifei Liu, Jiahao Wang, Huixuan Zhang, Fan Xu, Junzhe Zhang, Xiaojun Wan", "abstract": "Despite the rapid advancement of large language models, they remain highly\nsusceptible to generating hallucinations, which significantly hinders their\nwidespread application. Hallucination research requires dynamic and\nfine-grained evaluation. However, most existing hallucination benchmarks\n(especially in Chinese language) rely on human annotations, making automatical\nand cost-effective hallucination evaluation challenging. To address this, we\nintroduce HaluAgent, an agentic framework that automatically constructs\nfine-grained QA dataset based on some knowledge documents. Our experiments\ndemonstrate that the manually designed rules and prompt optimization can\nimprove the quality of generated data. Using HaluAgent, we construct C-FAITH, a\nChinese QA hallucination benchmark created from 1,399 knowledge documents\nobtained from web scraping, totaling 60,702 entries. We comprehensively\nevaluate 16 mainstream LLMs with our proposed C-FAITH, providing detailed\nexperimental results and analysis.", "journal": ""}
{"doi": "10.48550/arXiv.2504.18114", "date": "2025-04-25", "title": "Evaluating Evaluation Metrics -- The Mirage of Hallucination Detection", "authors": "Atharva Kulkarni, Yuan Zhang, Joel Ruben Antony Moniz, Xiou Ge, Bo-Hsiang Tseng, Dhivya Piraviperumal, Swabha Swayamdipta, Hong Yu", "abstract": "Hallucinations pose a significant obstacle to the reliability and widespread\nadoption of language models, yet their accurate measurement remains a\npersistent challenge. While many task- and domain-specific metrics have been\nproposed to assess faithfulness and factuality concerns, the robustness and\ngeneralization of these metrics are still untested. In this paper, we conduct a\nlarge-scale empirical evaluation of 6 diverse sets of hallucination detection\nmetrics across 4 datasets, 37 language models from 5 families, and 5 decoding\nmethods. Our extensive investigation reveals concerning gaps in current\nhallucination evaluation: metrics often fail to align with human judgments,\ntake an overtly myopic view of the problem, and show inconsistent gains with\nparameter scaling. Encouragingly, LLM-based evaluation, particularly with\nGPT-4, yields the best overall results, and mode-seeking decoding methods seem\nto reduce hallucinations, especially in knowledge-grounded settings. These\nfindings underscore the need for more robust metrics to understand and quantify\nhallucinations, and better strategies to mitigate them.", "journal": ""}
{"doi": "10.48550/arXiv.2504.18639", "date": "2025-04-25", "title": "Span-Level Hallucination Detection for LLM-Generated Answers", "authors": "Passant Elchafei, Mervet Abu-Elkheir", "abstract": "Detecting spans of hallucination in LLM-generated answers is crucial for\nimproving factual consistency. This paper presents a span-level hallucination\ndetection framework for the SemEval-2025 Shared Task, focusing on English and\nArabic texts. Our approach integrates Semantic Role Labeling (SRL) to decompose\nthe answer into atomic roles, which are then compared with a retrieved\nreference context obtained via question-based LLM prompting. Using a\nDeBERTa-based textual entailment model, we evaluate each role semantic\nalignment with the retrieved context. The entailment scores are further refined\nthrough token-level confidence measures derived from output logits, and the\ncombined scores are used to detect hallucinated spans. Experiments on the\nMu-SHROOM dataset demonstrate competitive performance. Additionally,\nhallucinated spans have been verified through fact-checking by prompting GPT-4\nand LLaMA. Our findings contribute to improving hallucination detection in\nLLM-generated responses.", "journal": ""}
{"doi": "10.48550/arXiv.2505.03420", "date": "2025-05-06", "title": "Mitigating Image Captioning Hallucinations in Vision-Language Models", "authors": "Fei Zhao, Chengcui Zhang, Runlin Zhang, Tianyang Wang, Xi Li", "abstract": "Hallucinations in vision-language models (VLMs) hinder reliability and\nreal-world applicability, usually stemming from distribution shifts between\npretraining data and test samples. Existing solutions, such as retraining or\nfine-tuning on additional data, demand significant computational resources and\nlabor-intensive data collection, while ensemble-based methods incur additional\ncosts by introducing auxiliary VLMs. To address these challenges, we propose a\nnovel test-time adaptation framework using reinforcement learning to mitigate\nhallucinations during inference without retraining or any auxiliary VLMs. By\nupdating only the learnable parameters in the layer normalization of the\nlanguage model (approximately 0.003% of the model parameters), our method\nreduces distribution shifts between test samples and pretraining samples. A\nCLIP-based hallucination evaluation model is proposed to provide dual rewards\nto VLMs. Experimental results demonstrate a 15.4% and 17.3% reduction in\nhallucination rates on LLaVA and InstructBLIP, respectively. Our approach\noutperforms state-of-the-art baselines with a 68.3% improvement in\nhallucination mitigation, demonstrating its effectiveness.", "journal": "IEEE International Conference on Multimedia Information Processing\n  and Retrieval (MIPR) 2025"}
{"doi": "10.48550/arXiv.2505.04844", "date": "2025-05-07", "title": "Osiris: A Lightweight Open-Source Hallucination Detection System", "authors": "Alex Shan, John Bauer, Christopher D. Manning", "abstract": "Retrieval-Augmented Generation (RAG) systems have gained widespread adoption\nby application builders because they leverage sources of truth to enable Large\nLanguage Models (LLMs) to generate more factually sound responses. However,\nhallucinations, instances of LLM responses that are unfaithful to the provided\ncontext, often prevent these systems from being deployed in production\nenvironments. Current hallucination detection methods typically involve human\nevaluation or the use of closed-source models to review RAG system outputs for\nhallucinations. Both human evaluators and closed-source models suffer from\nscaling issues due to their high costs and slow inference speeds. In this work,\nwe introduce a perturbed multi-hop QA dataset with induced hallucinations. Via\nsupervised fine-tuning on our dataset, we achieve better recall with a 7B model\nthan GPT-4o on the RAGTruth hallucination detection benchmark and offer\ncompetitive performance on precision and accuracy, all while using a fraction\nof the parameters. Code is released at our repository.", "journal": ""}
{"doi": "10.48550/arXiv.2505.12343", "date": "2025-05-18", "title": "Mitigating Hallucinations via Inter-Layer Consistency Aggregation in Large Vision-Language Models", "authors": "Kai Tang, Jinhao You, Xiuqi Ge, Hanze Li, Yichen Guo, Xiande Huang", "abstract": "Despite the impressive capabilities of Large Vision-Language Models (LVLMs),\nthey remain susceptible to hallucinations-generating content that is\ninconsistent with the input image. Existing training-free hallucination\nmitigation methods often suffer from unstable performance and high sensitivity\nto hyperparameter settings, limiting their practicality and broader adoption.\nIn this paper, we propose a novel decoding mechanism, Decoding with Inter-layer\nConsistency via Layer Aggregation (DCLA), which requires no retraining,\nfine-tuning, or access to external knowledge bases. Specifically, our approach\nconstructs a dynamic semantic reference by aggregating representations from\nprevious layers, and corrects semantically deviated layers to enforce\ninter-layer consistency. The method allows DCLA to robustly mitigate\nhallucinations across multiple LVLMs. Experiments on hallucination benchmarks\nsuch as MME and POPE demonstrate that DCLA effectively reduces hallucinations\nwhile enhancing the reliability and performance of LVLMs.", "journal": ""}
{"doi": "10.48550/arXiv.2505.12969", "date": "2025-05-19", "title": "Calm-Whisper: Reduce Whisper Hallucination On Non-Speech By Calming Crazy Heads Down", "authors": "Yingzhi Wang, Anas Alhmoud, Saad Alsahly, Muhammad Alqurishi, Mirco Ravanelli", "abstract": "OpenAI's Whisper has achieved significant success in Automatic Speech\nRecognition. However, it has consistently been found to exhibit hallucination\nissues, particularly in non-speech segments, which limits its broader\napplication in complex industrial settings.\n  In this paper, we introduce a novel method to reduce Whisper's hallucination\non non-speech segments without using any pre- or post-possessing techniques.\nSpecifically, we benchmark the contribution of each self-attentional head in\nthe Whisper-large-v3 decoder to the hallucination problem by performing a\nhead-wise mask. Our findings reveal that only 3 of the 20 heads account for\nover 75% of the hallucinations on the UrbanSound dataset. We then fine-tune\nthese three crazy heads using a collection of non-speech data. The results show\nthat our best fine-tuned model, namely Calm-Whisper, achieves over 80%\nreduction in non-speech hallucination with only less than 0.1% WER degradation\non LibriSpeech test-clean and test-other.", "journal": ""}
{"doi": "10.48550/arXiv.2505.15291", "date": "2025-05-21", "title": "Hallucinate at the Last in Long Response Generation: A Case Study on Long Document Summarization", "authors": "Joonho Yang, Seunghyun Yoon, Hwan Chang, Byeongjeong Kim, Hwanhee Lee", "abstract": "Large Language Models (LLMs) have significantly advanced text generation\ncapabilities, including tasks like summarization, often producing coherent and\nfluent outputs. However, faithfulness to source material remains a significant\nchallenge due to the generation of hallucinations. While extensive research\nfocuses on detecting and reducing these inaccuracies, less attention has been\npaid to the positional distribution of hallucination within generated text,\nparticularly in long outputs. In this work, we investigate where hallucinations\noccur in LLM-based long response generation, using long document summarization\nas a key case study. Focusing on the challenging setting of long context-aware\nlong response generation, we find a consistent and concerning phenomenon:\nhallucinations tend to concentrate disproportionately in the latter parts of\nthe generated long response. To understand this bias, we explore potential\ncontributing factors related to the dynamics of attention and decoding over\nlong sequences. Furthermore, we investigate methods to mitigate this positional\nhallucination, aiming to improve faithfulness specifically in the concluding\nsegments of long outputs.", "journal": ""}
{"doi": "10.48550/arXiv.2505.15963", "date": "2025-05-21", "title": "OViP: Online Vision-Language Preference Learning", "authors": "Shujun Liu, Siyuan Wang, Zejun Li, Jianxiang Wang, Cheng Zeng, Zhongyu Wei", "abstract": "Large vision-language models (LVLMs) remain vulnerable to hallucination,\noften generating content misaligned with visual inputs. While recent approaches\nadvance multi-modal Direct Preference Optimization (DPO) to mitigate\nhallucination, they typically rely on predefined or randomly edited negative\nsamples that fail to reflect actual model errors, limiting training efficacy.\nIn this work, we propose an Online Vision-language Preference Learning (OViP)\nframework that dynamically constructs contrastive training data based on the\nmodel's own hallucinated outputs. By identifying semantic differences between\nsampled response pairs and synthesizing negative images using a diffusion\nmodel, OViP generates more relevant supervision signals in real time. This\nfailure-driven training enables adaptive alignment of both textual and visual\npreferences. Moreover, we refine existing evaluation protocols to better\ncapture the trade-off between hallucination suppression and expressiveness.\nExperiments on hallucination and general benchmarks demonstrate that OViP\neffectively reduces hallucinations while preserving core multi-modal\ncapabilities.", "journal": ""}
{"doi": "10.48550/arXiv.2505.16411", "date": "2025-05-22", "title": "Mitigating Hallucinations in Vision-Language Models through Image-Guided Head Suppression", "authors": "Sreetama Sarkar, Yue Che, Alex Gavin, Peter A. Beerel, Souvik Kundu", "abstract": "Despite their remarkable progress in multimodal understanding tasks, large\nvision language models (LVLMs) often suffer from \"hallucinations\", generating\ntexts misaligned with the visual context. Existing methods aimed at reducing\nhallucinations through inference time intervention incur a significant increase\nin latency. To mitigate this, we present SPIN, a task-agnostic attention-guided\nhead suppression strategy that can be seamlessly integrated during inference,\nwithout incurring any significant compute or latency overhead. We investigate\nwhether hallucination in LVLMs can be linked to specific model components. Our\nanalysis suggests that hallucinations can be attributed to a dynamic subset of\nattention heads in each layer. Leveraging this insight, for each text query\ntoken, we selectively suppress attention heads that exhibit low attention to\nimage tokens, keeping the top-K attention heads intact. Extensive evaluations\non visual question answering and image description tasks demonstrate the\nefficacy of SPIN in reducing hallucination scores up to 2.7x while maintaining\nF1, and improving throughput by 1.8x compared to existing alternatives. Code is\navailable at https://github.com/YUECHE77/SPIN.", "journal": ""}
{"doi": "10.48550/arXiv.2505.17485", "date": "2025-05-23", "title": "keepitsimple at SemEval-2025 Task 3: LLM-Uncertainty based Approach for Multilingual Hallucination Span Detection", "authors": "Saketh Reddy Vemula, Parameswari Krishnamurthy", "abstract": "Identification of hallucination spans in black-box language model generated\ntext is essential for applications in the real world. A recent attempt at this\ndirection is SemEval-2025 Task 3, Mu-SHROOM-a Multilingual Shared Task on\nHallucinations and Related Observable Over-generation Errors. In this work, we\npresent our solution to this problem, which capitalizes on the variability of\nstochastically-sampled responses in order to identify hallucinated spans. Our\nhypothesis is that if a language model is certain of a fact, its sampled\nresponses will be uniform, while hallucinated facts will yield different and\nconflicting results. We measure this divergence through entropy-based analysis,\nallowing for accurate identification of hallucinated segments. Our method is\nnot dependent on additional training and hence is cost-effective and adaptable.\nIn addition, we conduct extensive hyperparameter tuning and perform error\nanalysis, giving us crucial insights into model behavior.", "journal": ""}
{"doi": "10.48550/arXiv.2505.18581", "date": "2025-05-24", "title": "Removal of Hallucination on Hallucination: Debate-Augmented RAG", "authors": "Wentao Hu, Wengyu Zhang, Yiyang Jiang, Chen Jason Zhang, Xiaoyong Wei, Qing Li", "abstract": "Retrieval-Augmented Generation (RAG) enhances factual accuracy by integrating\nexternal knowledge, yet it introduces a critical issue: erroneous or biased\nretrieval can mislead generation, compounding hallucinations, a phenomenon we\nterm Hallucination on Hallucination. To address this, we propose\nDebate-Augmented RAG (DRAG), a training-free framework that integrates\nMulti-Agent Debate (MAD) mechanisms into both retrieval and generation stages.\nIn retrieval, DRAG employs structured debates among proponents, opponents, and\njudges to refine retrieval quality and ensure factual reliability. In\ngeneration, DRAG introduces asymmetric information roles and adversarial\ndebates, enhancing reasoning robustness and mitigating factual inconsistencies.\nEvaluations across multiple tasks demonstrate that DRAG improves retrieval\nreliability, reduces RAG-induced hallucinations, and significantly enhances\noverall factual accuracy. Our code is available at\nhttps://github.com/Huenao/Debate-Augmented-RAG.", "journal": ""}
{"doi": "10.48550/arXiv.2506.04039", "date": "2025-06-04", "title": "Mitigating Hallucinations in Large Vision-Language Models via Entity-Centric Multimodal Preference Optimization", "authors": "Jiulong Wu, Zhengliang Shi, Shuaiqiang Wang, Jizhou Huang, Dawei Yin, Lingyong Yan, Min Cao, Min Zhang", "abstract": "Large Visual Language Models (LVLMs) have demonstrated impressive\ncapabilities across multiple tasks. However, their trustworthiness is often\nchallenged by hallucinations, which can be attributed to the modality\nmisalignment and the inherent hallucinations of their underlying Large Language\nModels (LLMs) backbone. Existing preference alignment methods focus on aligning\nmodel responses with human preferences while neglecting image-text modality\nalignment, resulting in over-reliance on LLMs and hallucinations. In this\npaper, we propose Entity-centric Multimodal Preference Optimization (EMPO),\nwhich achieves enhanced modality alignment than existing human preference\nalignment methods. Besides, to overcome the scarcity of high-quality multimodal\npreference data, we utilize open-source instruction datasets to automatically\nconstruct high-quality preference data across three aspects: image,\ninstruction, and response. Experiments on two human preference datasets and\nfive multimodal hallucination benchmarks demonstrate the effectiveness of EMPO,\ne.g., reducing hallucination rates by 85.9% on Object-HalBench and 49.8% on\nMM-HalBench.", "journal": ""}
{"doi": "10.48550/arXiv.2506.09886", "date": "2025-06-11", "title": "Attention Head Embeddings with Trainable Deep Kernels for Hallucination Detection in LLMs", "authors": "Rodion Oblovatny, Alexandra Bazarova, Alexey Zaytsev", "abstract": "We present a novel approach for detecting hallucinations in large language\nmodels (LLMs) by analyzing the probabilistic divergence between prompt and\nresponse hidden-state distributions. Counterintuitively, we find that\nhallucinated responses exhibit smaller deviations from their prompts compared\nto grounded responses, suggesting that hallucinations often arise from\nsuperficial rephrasing rather than substantive reasoning. Leveraging this\ninsight, we propose a model-intrinsic detection method that uses distributional\ndistances as principled hallucination scores, eliminating the need for external\nknowledge or auxiliary models. To enhance sensitivity, we employ deep learnable\nkernels that automatically adapt to capture nuanced geometric differences\nbetween distributions. Our approach outperforms existing baselines,\ndemonstrating state-of-the-art performance on several benchmarks. The method\nremains competitive even without kernel training, offering a robust, scalable\nsolution for hallucination detection.", "journal": ""}
{"doi": "10.48550/arXiv.2506.11129", "date": "2025-06-10", "title": "Trustworthy AI for Medicine: Continuous Hallucination Detection and Elimination with CHECK", "authors": "Carlos Garcia-Fernandez, Luis Felipe, Monique Shotande, Muntasir Zitu, Aakash Tripathi, Ghulam Rasool, Issam El Naqa, Vivek Rudrapatna, Gilmer Valdes", "abstract": "Large language models (LLMs) show promise in healthcare, but hallucinations\nremain a major barrier to clinical use. We present CHECK, a continuous-learning\nframework that integrates structured clinical databases with a classifier\ngrounded in information theory to detect both factual and reasoning-based\nhallucinations. Evaluated on 1500 questions from 100 pivotal clinical trials,\nCHECK reduced LLama3.3-70B-Instruct hallucination rates from 31% to 0.3% -\nmaking an open source model state of the art. Its classifier generalized across\nmedical benchmarks, achieving AUCs of 0.95-0.96, including on the MedQA (USMLE)\nbenchmark and HealthBench realistic multi-turn medical questioning. By\nleveraging hallucination probabilities to guide GPT-4o's refinement and\njudiciously escalate compute, CHECK boosted its USMLE passing rate by 5\npercentage points, achieving a state-of-the-art 92.1%. By suppressing\nhallucinations below accepted clinical error thresholds, CHECK offers a\nscalable foundation for safe LLM deployment in medicine and other high-stakes\ndomains.", "journal": ""}
{"doi": "10.48550/arXiv.2506.13130", "date": "2025-06-16", "title": "ZINA: Multimodal Fine-grained Hallucination Detection and Editing", "authors": "Yuiga Wada, Kazuki Matsuda, Komei Sugiura, Graham Neubig", "abstract": "Multimodal Large Language Models (MLLMs) often generate hallucinations, where\nthe output deviates from the visual content. Given that these hallucinations\ncan take diverse forms, detecting hallucinations at a fine-grained level is\nessential for comprehensive evaluation and analysis. To this end, we propose a\nnovel task of multimodal fine-grained hallucination detection and editing for\nMLLMs. Moreover, we propose ZINA, a novel method that identifies hallucinated\nspans at a fine-grained level, classifies their error types into six\ncategories, and suggests appropriate refinements. To train and evaluate models\nfor this task, we constructed VisionHall, a dataset comprising 6.9k outputs\nfrom twelve MLLMs manually annotated by 211 annotators, and 20k synthetic\nsamples generated using a graph-based method that captures dependencies among\nerror types. We demonstrated that ZINA outperformed existing methods, including\nGPT-4o and LLama-3.2, in both detection and editing tasks.", "journal": ""}
{"doi": "10.48550/arXiv.2506.15065", "date": "2025-06-18", "title": "HEAL: An Empirical Study on Hallucinations in Embodied Agents Driven by Large Language Models", "authors": "Trishna Chakraborty, Udita Ghosh, Xiaopan Zhang, Fahim Faisal Niloy, Yue Dong, Jiachen Li, Amit K. Roy-Chowdhury, Chengyu Song", "abstract": "Large language models (LLMs) are increasingly being adopted as the cognitive\ncore of embodied agents. However, inherited hallucinations, which stem from\nfailures to ground user instructions in the observed physical environment, can\nlead to navigation errors, such as searching for a refrigerator that does not\nexist. In this paper, we present the first systematic study of hallucinations\nin LLM-based embodied agents performing long-horizon tasks under scene-task\ninconsistencies. Our goal is to understand to what extent hallucinations occur,\nwhat types of inconsistencies trigger them, and how current models respond. To\nachieve these goals, we construct a hallucination probing set by building on an\nexisting benchmark, capable of inducing hallucination rates up to 40x higher\nthan base prompts. Evaluating 12 models across two simulation environments, we\nfind that while models exhibit reasoning, they fail to resolve scene-task\ninconsistencies-highlighting fundamental limitations in handling infeasible\ntasks. We also provide actionable insights on ideal model behavior for each\nscenario, offering guidance for developing more robust and reliable planning\nstrategies.", "journal": ""}
{"doi": "10.48550/arXiv.2506.17088", "date": "2025-06-20", "title": "Chain-of-Thought Prompting Obscures Hallucination Cues in Large Language Models: An Empirical Evaluation", "authors": "Jiahao Cheng, Tiancheng Su, Jia Yuan, Guoxiu He, Jiawei Liu, Xinqi Tao, Jingwen Xie, Huaxia Li", "abstract": "Large Language Models (LLMs) often exhibit \\textit{hallucinations},\ngenerating factually incorrect or semantically irrelevant content in response\nto prompts. Chain-of-Thought (CoT) prompting can mitigate hallucinations by\nencouraging step-by-step reasoning, but its impact on hallucination detection\nremains underexplored. To bridge this gap, we conduct a systematic empirical\nevaluation. We begin with a pilot experiment, revealing that CoT reasoning\nsignificantly affects the LLM's internal states and token probability\ndistributions. Building on this, we evaluate the impact of various CoT\nprompting methods on mainstream hallucination detection methods across both\ninstruction-tuned and reasoning-oriented LLMs. Specifically, we examine three\nkey dimensions: changes in hallucination score distributions, variations in\ndetection accuracy, and shifts in detection confidence. Our findings show that\nwhile CoT prompting helps reduce hallucination frequency, it also tends to\nobscure critical signals used for detection, impairing the effectiveness of\nvarious detection methods. Our study highlights an overlooked trade-off in the\nuse of reasoning. Code is publicly available at:\nhttps://anonymous.4open.science/r/cot-hallu-detect.", "journal": ""}
{"doi": "10.48550/arXiv.2506.17664", "date": "2025-06-21", "title": "MDSAM:Memory-Driven Sparse Attention Matrix for LVLMs Hallucination Mitigation", "authors": "Shuaiye Lu, Linjiang Zhou, Xiaochuan Shi", "abstract": "Hallucinations in large vision-language models (LVLMs) often stem from the\nmodel's sensitivity to image tokens during decoding, as evidenced by attention\npeaks observed when generating both real and hallucinated entities. To address\nthis, we propose Memory-Driven Sparse Attention Matrix (MDSAM) , a novel\ntraining-free approach that dynamically captures and refines the attention\nallocated to image tokens at each layer. MDSAM memorizes attention patterns and\nactivates updates through alignment during decoding, enhancing focus on\nrelevant image tokens while effectively reducing hallucinations. We evaluate\nMDSAM on multiple benchmarks for tasks such as image captioning and visual\nquestion answering, demonstrating its ability to consistently reduce\nhallucinations and improve reliability. Compatible with various LVLM\narchitectures, MDSAM highlights its adaptability and effectiveness in\nmitigating hallucinations without requiring additional training or external\ntools.", "journal": ""}
{"doi": "10.48550/arXiv.2403.15048", "date": "2024-03-22", "title": "Make VLM Recognize Visual Hallucination on Cartoon Character Image with Pose Information", "authors": "Bumsoo Kim, Wonseop Shin, Kyuchul Lee, Yonghoon Jung, Sanghyun Seo", "abstract": "Leveraging large-scale Text-to-Image (TTI) models have become a common\ntechnique for generating exemplar or training dataset in the fields of image\nsynthesis, video editing, 3D reconstruction. However, semantic structural\nvisual hallucinations involving perceptually severe defects remain a concern,\nespecially in the domain of non-photorealistic rendering (NPR) such as cartoons\nand pixelization-style character. To detect these hallucinations in NPR, We\npropose a novel semantic structural hallucination detection system using\nVision-Language Model (VLM). Our approach is to leverage the emerging\ncapability of large language model, in-context learning which denotes that VLM\nhas seen some examples by user for specific downstream task, here hallucination\ndetection. Based on in-context learning, we introduce pose-aware in-context\nvisual learning (PA-ICVL) which improve the overall performance of VLM by\nfurther inputting visual data beyond prompts, RGB images and pose information.\nBy incorporating pose guidance, we enable VLMs to make more accurate decisions.\nExperimental results demonstrate significant improvements in identifying visual\nhallucinations compared to baseline methods relying solely on RGB images.\nWithin selected two VLMs, GPT-4v, Gemini pro vision, our proposed PA-ICVL\nimproves the hallucination detection with 50% to 78%, 57% to 80%, respectively.\nThis research advances a capability of TTI models toward real-world\napplications by mitigating visual hallucinations via in-context visual\nlearning, expanding their potential in non-photorealistic domains. In addition,\nit showcase how users can boost the downstream-specialized capability of open\nVLM by harnessing additional conditions. We collect synthetic\ncartoon-hallucination dataset with TTI models, this dataset and final tuned VLM\nwill be publicly available.", "journal": ""}
{"doi": "10.48550/arXiv.2406.10279", "date": "2024-06-12", "title": "We Have a Package for You! A Comprehensive Analysis of Package Hallucinations by Code Generating LLMs", "authors": "Joseph Spracklen, Raveen Wijewickrama, A H M Nazmus Sakib, Anindya Maiti, Bimal Viswanath, Murtuza Jadliwala", "abstract": "The reliance of popular programming languages such as Python and JavaScript\non centralized package repositories and open-source software, combined with the\nemergence of code-generating Large Language Models (LLMs), has created a new\ntype of threat to the software supply chain: package hallucinations. These\nhallucinations, which arise from fact-conflicting errors when generating code\nusing LLMs, represent a novel form of package confusion attack that poses a\ncritical threat to the integrity of the software supply chain. This paper\nconducts a rigorous and comprehensive evaluation of package hallucinations\nacross different programming languages, settings, and parameters, exploring how\na diverse set of models and configurations affect the likelihood of generating\nerroneous package recommendations and identifying the root causes of this\nphenomenon. Using 16 popular LLMs for code generation and two unique prompt\ndatasets, we generate 576,000 code samples in two programming languages that we\nanalyze for package hallucinations. Our findings reveal that that the average\npercentage of hallucinated packages is at least 5.2% for commercial models and\n21.7% for open-source models, including a staggering 205,474 unique examples of\nhallucinated package names, further underscoring the severity and pervasiveness\nof this threat. To overcome this problem, we implement several hallucination\nmitigation strategies and show that they are able to significantly reduce the\nnumber of package hallucinations while maintaining code quality. Our\nexperiments and findings highlight package hallucinations as a persistent and\nsystemic phenomenon while using state-of-the-art LLMs for code generation, and\na significant challenge which deserves the research community's urgent\nattention.", "journal": ""}
{"doi": "10.48550/arXiv.2011.02593", "date": "2020-11-05", "title": "Detecting Hallucinated Content in Conditional Neural Sequence Generation", "authors": "Chunting Zhou, Graham Neubig, Jiatao Gu, Mona Diab, Paco Guzman, Luke Zettlemoyer, Marjan Ghazvininejad", "abstract": "Neural sequence models can generate highly fluent sentences, but recent\nstudies have also shown that they are also prone to hallucinate additional\ncontent not supported by the input. These variety of fluent but wrong outputs\nare particularly problematic, as it will not be possible for users to tell they\nare being presented incorrect content. To detect these errors, we propose a\ntask to predict whether each token in the output sequence is hallucinated (not\ncontained in the input) and collect new manually annotated evaluation sets for\nthis task. We also introduce a method for learning to detect hallucinations\nusing pretrained language models fine tuned on synthetic data that includes\nautomatically inserted hallucinations Experiments on machine translation (MT)\nand abstractive summarization demonstrate that our proposed approach\nconsistently outperforms strong baselines on all benchmark datasets. We further\ndemonstrate how to use the token-level hallucination labels to define a\nfine-grained loss over the target sequence in low-resource MT and achieve\nsignificant improvements over strong baseline methods. We also apply our method\nto word-level quality estimation for MT and show its effectiveness in both\nsupervised and unsupervised settings. Codes and data available at\nhttps://github.com/violet-zct/fairseq-detect-hallucination.", "journal": ""}
{"doi": "10.48550/arXiv.2302.05852", "date": "2023-02-12", "title": "\"Why is this misleading?\": Detecting News Headline Hallucinations with Explanations", "authors": "Jiaming Shen, Jialu Liu, Dan Finnie, Negar Rahmati, Michael Bendersky, Marc Najork", "abstract": "Automatic headline generation enables users to comprehend ongoing news events\npromptly and has recently become an important task in web mining and natural\nlanguage processing. With the growing need for news headline generation, we\nargue that the hallucination issue, namely the generated headlines being not\nsupported by the original news stories, is a critical challenge for the\ndeployment of this feature in web-scale systems Meanwhile, due to the\ninfrequency of hallucination cases and the requirement of careful reading for\nraters to reach the correct consensus, it is difficult to acquire a large\ndataset for training a model to detect such hallucinations through human\ncuration. In this work, we present a new framework named ExHalder to address\nthis challenge for headline hallucination detection. ExHalder adapts the\nknowledge from public natural language inference datasets into the news domain\nand learns to generate natural language sentences to explain the hallucination\ndetection results. To evaluate the model performance, we carefully collect a\ndataset with more than six thousand labeled <article, headline> pairs.\nExtensive experiments on this dataset and another six public ones demonstrate\nthat ExHalder can identify hallucinated headlines accurately and justifies its\npredictions with human-readable natural language explanations.", "journal": ""}
{"doi": "10.48550/arXiv.2305.06978", "date": "2023-05-11", "title": "Meta-hallucinator: Towards Few-Shot Cross-Modality Cardiac Image Segmentation", "authors": "Ziyuan Zhao, Fangcheng Zhou, Zeng Zeng, Cuntai Guan, S. Kevin Zhou", "abstract": "Domain shift and label scarcity heavily limit deep learning applications to\nvarious medical image analysis tasks. Unsupervised domain adaptation (UDA)\ntechniques have recently achieved promising cross-modality medical image\nsegmentation by transferring knowledge from a label-rich source domain to an\nunlabeled target domain. However, it is also difficult to collect annotations\nfrom the source domain in many clinical applications, rendering most prior\nworks suboptimal with the label-scarce source domain, particularly for few-shot\nscenarios, where only a few source labels are accessible. To achieve efficient\nfew-shot cross-modality segmentation, we propose a novel\ntransformation-consistent meta-hallucination framework, meta-hallucinator, with\nthe goal of learning to diversify data distributions and generate useful\nexamples for enhancing cross-modality performance. In our framework,\nhallucination and segmentation models are jointly trained with the\ngradient-based meta-learning strategy to synthesize examples that lead to good\nsegmentation performance on the target domain. To further facilitate data\nhallucination and cross-domain knowledge transfer, we develop a self-ensembling\nmodel with a hallucination-consistent property. Our meta-hallucinator can\nseamlessly collaborate with the meta-segmenter for learning to hallucinate with\nmutual benefits from a combined view of meta-learning and self-ensembling\nlearning. Extensive studies on MM-WHS 2017 dataset for cross-modality cardiac\nsegmentation demonstrate that our method performs favorably against various\napproaches by a lot in the few-shot UDA scenario.", "journal": "Medical Image Computing and Computer Assisted Intervention, MICCAI\n  2022. Lecture Notes in Computer Science, vol 13435. Springer, Cham"}
{"doi": "10.48550/arXiv.2308.06394", "date": "2023-08-11", "title": "Detecting and Preventing Hallucinations in Large Vision Language Models", "authors": "Anisha Gunjal, Jihan Yin, Erhan Bas", "abstract": "Instruction tuned Large Vision Language Models (LVLMs) have significantly\nadvanced in generalizing across a diverse set of multi-modal tasks, especially\nfor Visual Question Answering (VQA). However, generating detailed responses\nthat are visually grounded is still a challenging task for these models. We\nfind that even the current state-of-the-art LVLMs (InstructBLIP) still contain\na staggering 30 percent of the hallucinatory text in the form of non-existent\nobjects, unfaithful descriptions, and inaccurate relationships. To address\nthis, we introduce M-HalDetect, a (M)ultimodal (Hal)lucination (Detect)ion\nDataset that can be used to train and benchmark models for hallucination\ndetection and prevention. M-HalDetect consists of 16k fine-grained annotations\non VQA examples, making it the first comprehensive multi-modal hallucination\ndetection dataset for detailed image descriptions. Unlike previous work that\nonly consider object hallucination, we additionally annotate both entity\ndescriptions and relationships that are unfaithful. To demonstrate the\npotential of this dataset for hallucination prevention, we optimize\nInstructBLIP through our novel Fine-grained Direct Preference Optimization\n(FDPO). We also train fine-grained multi-modal reward models from InstructBLIP\nand evaluate their effectiveness with best-of-n rejection sampling. We perform\nhuman evaluation on both FDPO and rejection sampling, and find that they reduce\nhallucination rates in InstructBLIP by 41% and 55% respectively. We also find\nthat our reward model generalizes to other multi-modal models, reducing\nhallucinations in LLaVA and mPLUG-OWL by 15% and 57% respectively, and has\nstrong correlation with human evaluated accuracy scores.", "journal": ""}
{"doi": "10.48550/arXiv.2310.00754", "date": "2023-10-01", "title": "Analyzing and Mitigating Object Hallucination in Large Vision-Language Models", "authors": "Yiyang Zhou, Chenhang Cui, Jaehong Yoon, Linjun Zhang, Zhun Deng, Chelsea Finn, Mohit Bansal, Huaxiu Yao", "abstract": "Large vision-language models (LVLMs) have shown remarkable abilities in\nunderstanding visual information with human languages. However, LVLMs still\nsuffer from object hallucination, which is the problem of generating\ndescriptions that include objects that do not actually exist in the images.\nThis can negatively impact many vision-language tasks, such as visual\nsummarization and reasoning. To address this issue, we propose a simple yet\npowerful algorithm, LVLM Hallucination Revisor (LURE), to post-hoc rectify\nobject hallucination in LVLMs by reconstructing less hallucinatory\ndescriptions. LURE is grounded in a rigorous statistical analysis of the key\nfactors underlying object hallucination, including co-occurrence (the frequent\nappearance of certain objects alongside others in images), uncertainty (objects\nwith higher uncertainty during LVLM decoding), and object position\n(hallucination often appears in the later part of the generated text). LURE can\nalso be seamlessly integrated with any LVLMs. We evaluate LURE on six\nopen-source LVLMs, achieving a 23% improvement in general object hallucination\nevaluation metrics over the previous best approach. In both GPT and human\nevaluations, LURE consistently ranks at the top. Our data and code are\navailable at https://github.com/YiyangZhou/LURE.", "journal": ""}
{"doi": "10.48550/arXiv.2310.01779", "date": "2023-10-03", "title": "HallE-Control: Controlling Object Hallucination in Large Multimodal Models", "authors": "Bohan Zhai, Shijia Yang, Chenfeng Xu, Sheng Shen, Kurt Keutzer, Chunyuan Li, Manling Li", "abstract": "Current Large Multimodal Models (LMMs) achieve remarkable progress, yet there\nremains significant uncertainty regarding their ability to accurately apprehend\nvisual details, that is, in performing detailed captioning. To address this, we\nintroduce $\\textit{CCEval}$, a GPT-4 assisted evaluation method for detailed\ncaptioning. Interestingly, while LMMs demonstrate minimal object existence\nhallucination in existing VQA benchmarks, our proposed evaluation reveals\ncontinued susceptibility to such hallucinations. In this paper, we make the\nfirst attempt to investigate such hallucination from different aspects,\nincluding image resolution, the language decoder size, and instruction data\namount, quality, granularity. Our findings underscore the unwarranted inference\nwhen the language description includes details at a finer object granularity\nthan what the vision module can ground or verify, thus inducing hallucination.\nTo control such hallucinations, we further attribute the reliability of\ncaptioning to contextual knowledge (involving only contextually grounded\nobjects) and parametric knowledge (containing inferred objects by the model).\nThus, we introduce $\\textit{HallE-Control}$, a controllable LMM in terms of\n$\\textbf{Hall}$ucination in object $\\textbf{E}$xistence. HallE-Control can\ncondition the captioning to shift between (i) exclusively depicting contextual\nknowledge for grounded objects and (ii) blending it with parametric knowledge\nto imagine inferred objects. Our method reduces hallucination by 44% compared\nto LLaVA$_{7B}$ and maintains the object coverage.", "journal": ""}
{"doi": "10.48550/arXiv.2311.16839", "date": "2023-11-28", "title": "Beyond Hallucinations: Enhancing LVLMs through Hallucination-Aware Direct Preference Optimization", "authors": "Zhiyuan Zhao, Bin Wang, Linke Ouyang, Xiaoyi Dong, Jiaqi Wang, Conghui He", "abstract": "Multimodal large language models have made significant advancements in recent\nyears, yet they still suffer from a common issue known as the \"hallucination\nproblem\", in which the models generate textual descriptions that inaccurately\ndepict or entirely fabricate content from associated images. This paper\nintroduces a novel solution, Hallucination-Aware Direct Preference Optimization\n(HA-DPO), which reframes the hallucination problem as a preference selection\ntask. The model is trained to favor the non-hallucinating response when\npresented with two responses of the same image (one accurate and one\nhallucinatory). Furthermore, this paper proposes an efficient pipeline for\nconstructing positive~(non-hallucinatory) and negative~(hallucinatory) sample\npairs, ensuring a high-quality, style-consistent dataset for robust preference\nlearning. When applied to three mainstream multimodal models, HA-DPO\nsignificantly reduced hallucination issues and amplified the models'\ngeneralization capabilities. Notably, the MiniGPT-4 model, when enhanced with\nHA-DPO, demonstrated a substantial improvement: POPE accuracy rose from 51.13%\nto 86.13% (an absolute improvement of 35%), and the MME score surged from\n932.00 to 1326.46 (a relative improvement of 42.32%). The codes, models, and\ndatasets are made accessible at https://opendatalab.github.io/HA-DPO.", "journal": ""}
{"doi": "10.48550/arXiv.2312.03631", "date": "2023-12-06", "title": "Mitigating Open-Vocabulary Caption Hallucinations", "authors": "Assaf Ben-Kish, Moran Yanuka, Morris Alper, Raja Giryes, Hadar Averbuch-Elor", "abstract": "While recent years have seen rapid progress in image-conditioned text\ngeneration, image captioning still suffers from the fundamental issue of\nhallucinations, namely, the generation of spurious details that cannot be\ninferred from the given image. Existing methods largely use closed-vocabulary\nobject lists to mitigate or evaluate hallucinations in image captioning,\nignoring the long-tailed nature of hallucinations that occur in practice. To\nthis end, we propose a framework for addressing hallucinations in image\ncaptioning in the open-vocabulary setting. Our framework includes a new\nbenchmark, OpenCHAIR, that leverages generative foundation models to evaluate\nopen-vocabulary object hallucinations for image captioning, surpassing the\npopular and similarly-sized CHAIR benchmark in both diversity and accuracy.\nFurthermore, to mitigate open-vocabulary hallucinations without using a closed\nobject list, we propose MOCHa, an approach harnessing advancements in\nreinforcement learning. Our multi-objective reward function explicitly targets\nthe trade-off between fidelity and adequacy in generations without requiring\nany strong supervision. MOCHa improves a large variety of image captioning\nmodels, as captured by our OpenCHAIR benchmark and other existing metrics. Code\nand models can be found at: https://github.com/assafbk/mocha_code", "journal": ""}
{"doi": "10.48550/arXiv.2312.05209", "date": "2023-12-08", "title": "HALO: An Ontology for Representing and Categorizing Hallucinations in Large Language Models", "authors": "Navapat Nananukul, Mayank Kejriwal", "abstract": "Recent progress in generative AI, including large language models (LLMs) like\nChatGPT, has opened up significant opportunities in fields ranging from natural\nlanguage processing to knowledge discovery and data mining. However, there is\nalso a growing awareness that the models can be prone to problems such as\nmaking information up or `hallucinations', and faulty reasoning on seemingly\nsimple problems. Because of the popularity of models like ChatGPT, both\nacademic scholars and citizen scientists have documented hallucinations of\nseveral different types and severity. Despite this body of work, a formal model\nfor describing and representing these hallucinations (with relevant meta-data)\nat a fine-grained level, is still lacking. In this paper, we address this gap\nby presenting the Hallucination Ontology or HALO, a formal, extensible ontology\nwritten in OWL that currently offers support for six different types of\nhallucinations known to arise in LLMs, along with support for provenance and\nexperimental metadata. We also collect and publish a dataset containing\nhallucinations that we inductively gathered across multiple independent Web\nsources, and show that HALO can be successfully used to model this dataset and\nanswer competency questions.", "journal": ""}
{"doi": "10.48550/arXiv.2401.01301", "date": "2024-01-02", "title": "Large Legal Fictions: Profiling Legal Hallucinations in Large Language Models", "authors": "Matthew Dahl, Varun Magesh, Mirac Suzgun, Daniel E. Ho", "abstract": "Do large language models (LLMs) know the law? These models are increasingly\nbeing used to augment legal practice, education, and research, yet their\nrevolutionary potential is threatened by the presence of hallucinations --\ntextual output that is not consistent with legal facts. We present the first\nsystematic evidence of these hallucinations, documenting LLMs' varying\nperformance across jurisdictions, courts, time periods, and cases. Our work\nmakes four key contributions. First, we develop a typology of legal\nhallucinations, providing a conceptual framework for future research in this\narea. Second, we find that legal hallucinations are alarmingly prevalent,\noccurring between 58% of the time with ChatGPT 4 and 88% with Llama 2, when\nthese models are asked specific, verifiable questions about random federal\ncourt cases. Third, we illustrate that LLMs often fail to correct a user's\nincorrect legal assumptions in a contra-factual question setup. Fourth, we\nprovide evidence that LLMs cannot always predict, or do not always know, when\nthey are producing legal hallucinations. Taken together, our findings caution\nagainst the rapid and unsupervised integration of popular LLMs into legal\ntasks. Even experienced lawyers must remain wary of legal hallucinations, and\nthe risks are highest for those who stand to benefit from LLMs the most -- pro\nse litigants or those without access to traditional legal resources.", "journal": "Journal of Legal Analysis 16, no. 1 (2024): 64-93"}
{"doi": "10.48550/arXiv.2401.01572", "date": "2024-01-03", "title": "Hallucinations in Neural Automatic Speech Recognition: Identifying Errors and Hallucinatory Models", "authors": "Rita Frieske, Bertram E. Shi", "abstract": "Hallucinations are a type of output error produced by deep neural networks.\nWhile this has been studied in natural language processing, they have not been\nresearched previously in automatic speech recognition. Here, we define\nhallucinations in ASR as transcriptions generated by a model that are\nsemantically unrelated to the source utterance, yet still fluent and coherent.\nThe similarity of hallucinations to probable natural language outputs of the\nmodel creates a danger of deception and impacts the credibility of the system.\nWe show that commonly used metrics, such as word error rates, cannot\ndifferentiate between hallucinatory and non-hallucinatory models. To address\nthis, we propose a perturbation-based method for assessing the susceptibility\nof an automatic speech recognition (ASR) model to hallucination at test time,\nwhich does not require access to the training dataset. We demonstrate that this\nmethod helps to distinguish between hallucinatory and non-hallucinatory models\nthat have similar baseline word error rates. We further explore the\nrelationship between the types of ASR errors and the types of dataset noise to\ndetermine what types of noise are most likely to create hallucinatory outputs.\nWe devise a framework for identifying hallucinations by analysing their\nsemantic connection with the ground truth and their fluency. Finally, we\ndiscover how to induce hallucinations with a random noise injection to the\nutterance.", "journal": ""}
{"doi": "10.48550/arXiv.2403.05612", "date": "2024-03-08", "title": "Unfamiliar Finetuning Examples Control How Language Models Hallucinate", "authors": "Katie Kang, Eric Wallace, Claire Tomlin, Aviral Kumar, Sergey Levine", "abstract": "Large language models are known to hallucinate when faced with unfamiliar\nqueries, but the underlying mechanism that govern how models hallucinate are\nnot yet fully understood. In this work, we find that unfamiliar examples in the\nmodels' finetuning data -- those that introduce concepts beyond the base\nmodel's scope of knowledge -- are crucial in shaping these errors. In\nparticular, we find that an LLM's hallucinated predictions tend to mirror the\nresponses associated with its unfamiliar finetuning examples. This suggests\nthat by modifying how unfamiliar finetuning examples are supervised, we can\ninfluence a model's responses to unfamiliar queries (e.g., say ``I don't\nknow''). We empirically validate this observation in a series of controlled\nexperiments involving SFT, RL, and reward model finetuning on TriviaQA and\nMMLU. Our work further investigates RL finetuning strategies for improving the\nfactuality of long-form model generations. We find that, while hallucinations\nfrom the reward model can significantly undermine the effectiveness of RL\nfactuality finetuning, strategically controlling how reward models hallucinate\ncan minimize these negative effects. Leveraging our previous observations on\ncontrolling hallucinations, we propose an approach for learning more reliable\nreward models, and show that they improve the efficacy of RL factuality\nfinetuning in long-form biography and book/movie plot generation tasks.", "journal": ""}
{"doi": "10.48550/arXiv.2404.09971", "date": "2024-04-15", "title": "Constructing Benchmarks and Interventions for Combating Hallucinations in LLMs", "authors": "Adi Simhi, Jonathan Herzig, Idan Szpektor, Yonatan Belinkov", "abstract": "Large language models (LLMs) are prone to hallucinations, which sparked a\nwidespread effort to detect and prevent them. Recent work attempts to mitigate\nhallucinations by intervening in the model's generation, typically computing\nrepresentative vectors of hallucinations vs. grounded generations, for steering\nthe model's hidden states away from a hallucinatory state. However, common\nstudies employ different setups and do not properly separate different possible\ncauses of hallucinations, making interventions misguided. In this work, we\nintroduce a method for categorizing examples based on the model's prior\nknowledge, named WACK. We construct WACK benchmarks that support interventions\nin two settings: open-book and closed-book question answering. Using the\nbenchmarks, we perform an extensive investigation of the effect of different\nchoices for intervention, such as the intervened components, and how often and\nhow strongly to intervene. We find that intervention success varies depending\non the component, with the attention blocks performing well and the residual\nstream proving detrimental to language modeling capabilities. We also show that\ninterventions can benefit from representative vectors collected before, rather\nthan after, a hallucination occurs. Finally, we introduce a new dynamic\nintervention, which intervenes only if needed, and thus is more robust than\nstandard static interventions. The code is available at\nhttps://github.com/technion-cs-nlp/hallucination-mitigation .", "journal": ""}
{"doi": "10.48550/arXiv.2404.18930", "date": "2024-04-29", "title": "Hallucination of Multimodal Large Language Models: A Survey", "authors": "Zechen Bai, Pichao Wang, Tianjun Xiao, Tong He, Zongbo Han, Zheng Zhang, Mike Zheng Shou", "abstract": "This survey presents a comprehensive analysis of the phenomenon of\nhallucination in multimodal large language models (MLLMs), also known as Large\nVision-Language Models (LVLMs), which have demonstrated significant\nadvancements and remarkable abilities in multimodal tasks. Despite these\npromising developments, MLLMs often generate outputs that are inconsistent with\nthe visual content, a challenge known as hallucination, which poses substantial\nobstacles to their practical deployment and raises concerns regarding their\nreliability in real-world applications. This problem has attracted increasing\nattention, prompting efforts to detect and mitigate such inaccuracies. We\nreview recent advances in identifying, evaluating, and mitigating these\nhallucinations, offering a detailed overview of the underlying causes,\nevaluation benchmarks, metrics, and strategies developed to address this issue.\nAdditionally, we analyze the current challenges and limitations, formulating\nopen questions that delineate potential pathways for future research. By\ndrawing the granular classification and landscapes of hallucination causes,\nevaluation benchmarks, and mitigation methods, this survey aims to deepen the\nunderstanding of hallucinations in MLLMs and inspire further advancements in\nthe field. Through our thorough and in-depth review, we contribute to the\nongoing dialogue on enhancing the robustness and reliability of MLLMs,\nproviding valuable insights and resources for researchers and practitioners\nalike. Resources are available at:\nhttps://github.com/showlab/Awesome-MLLM-Hallucination.", "journal": ""}
{"doi": "10.48550/arXiv.2406.10900", "date": "2024-06-16", "title": "AutoHallusion: Automatic Generation of Hallucination Benchmarks for Vision-Language Models", "authors": "Xiyang Wu, Tianrui Guan, Dianqi Li, Shuaiyi Huang, Xiaoyu Liu, Xijun Wang, Ruiqi Xian, Abhinav Shrivastava, Furong Huang, Jordan Lee Boyd-Graber, Tianyi Zhou, Dinesh Manocha", "abstract": "Large vision-language models (LVLMs) are prone to hallucinations, where\ncertain contextual cues in an image can trigger the language module to produce\noverconfident and incorrect reasoning about abnormal or hypothetical objects.\nWhile some benchmarks have been developed to investigate LVLM hallucinations,\nthey often rely on hand-crafted corner cases whose failure patterns may not\ngeneralize well. Additionally, fine-tuning on these examples could undermine\ntheir validity. To address this, we aim to scale up the number of cases through\nan automated approach, reducing human bias in crafting such corner cases. This\nmotivates the development of AutoHallusion, the first automated benchmark\ngeneration approach that employs several key strategies to create a diverse\nrange of hallucination examples. Our generated visual-question pairs pose\nsignificant challenges to LVLMs, requiring them to overcome contextual biases\nand distractions to arrive at correct answers. AutoHallusion enables us to\ncreate new benchmarks at the minimum cost and thus overcomes the fragility of\nhand-crafted benchmarks. It also reveals common failure patterns and reasons,\nproviding key insights to detect, avoid, or control hallucinations.\nComprehensive evaluations of top-tier LVLMs, e.g., GPT-4V(ision), Gemini Pro\nVision, Claude 3, and LLaVA-1.5, show a 97.7% and 98.7% success rate of\nhallucination induction on synthetic and real-world datasets of AutoHallusion,\npaving the way for a long battle against hallucinations. The codebase and data\ncan be accessed at https://github.com/wuxiyang1996/AutoHallusion.", "journal": ""}
{"doi": "10.48550/arXiv.2406.12718", "date": "2024-06-18", "title": "Mitigating Object Hallucinations in Large Vision-Language Models with Assembly of Global and Local Attention", "authors": "Wenbin An, Feng Tian, Sicong Leng, Jiahao Nie, Haonan Lin, QianYing Wang, Ping Chen, Xiaoqin Zhang, Shijian Lu", "abstract": "Despite great success across various multimodal tasks, Large Vision-Language\nModels (LVLMs) often encounter object hallucinations with generated textual\nresponses being inconsistent with the actual objects in images. We examine\ndifferent LVLMs and pinpoint that one root cause of object hallucinations lies\nwith deficient attention on discriminative image features. Specifically, LVLMs\noften predominantly attend to prompt-irrelevant global features instead of\nprompt-relevant local features, undermining their visual grounding capacity and\nleading to object hallucinations. We propose Assembly of Global and Local\nAttention (AGLA), a training-free and plug-and-play approach that mitigates\nhallucinations by assembling global features for response generation and local\nfeatures for visual discrimination simultaneously. Specifically, we introduce\nan image-prompt matching scheme that captures prompt-relevant local features\nfrom images, leading to an augmented view of the input image where\nprompt-relevant content is highlighted while irrelevant distractions are\nsuppressed. Hallucinations can thus be mitigated with a calibrated logit\ndistribution that is from generative global features of the original image and\ndiscriminative local features of the augmented image. Extensive experiments\nshow the superiority of AGLA in LVLM hallucination mitigation, demonstrating\nits wide applicability across both discriminative and generative tasks. Our\ncode is available at https://github.com/Lackel/AGLA.", "journal": ""}
{"doi": "10.48550/arXiv.2407.06192", "date": "2024-07-08", "title": "Multi-Object Hallucination in Vision-Language Models", "authors": "Xuweiyi Chen, Ziqiao Ma, Xuejun Zhang, Sihan Xu, Shengyi Qian, Jianing Yang, David F. Fouhey, Joyce Chai", "abstract": "Large vision language models (LVLMs) often suffer from object hallucination,\nproducing objects not present in the given images. While current benchmarks for\nobject hallucination primarily concentrate on the presence of a single object\nclass rather than individual entities, this work systematically investigates\nmulti-object hallucination, examining how models misperceive (e.g., invent\nnonexistent objects or become distracted) when tasked with focusing on multiple\nobjects simultaneously. We introduce Recognition-based Object Probing\nEvaluation (ROPE), an automated evaluation protocol that considers the\ndistribution of object classes within a single image during testing and uses\nvisual referring prompts to eliminate ambiguity. With comprehensive empirical\nstudies and analysis of potential factors leading to multi-object\nhallucination, we found that (1). LVLMs suffer more hallucinations when\nfocusing on multiple objects compared to a single object. (2). The tested\nobject class distribution affects hallucination behaviors, indicating that\nLVLMs may follow shortcuts and spurious correlations. (3). Hallucinatory\nbehaviors are influenced by data-specific factors, salience and frequency, and\nmodel intrinsic behaviors. We hope to enable LVLMs to recognize and reason\nabout multiple objects that often occur in realistic visual scenes, provide\ninsights, and quantify our progress towards mitigating the issues.", "journal": ""}
{"doi": "10.48550/arXiv.2407.07071", "date": "2024-07-09", "title": "Lookback Lens: Detecting and Mitigating Contextual Hallucinations in Large Language Models Using Only Attention Maps", "authors": "Yung-Sung Chuang, Linlu Qiu, Cheng-Yu Hsieh, Ranjay Krishna, Yoon Kim, James Glass", "abstract": "When asked to summarize articles or answer questions given a passage, large\nlanguage models (LLMs) can hallucinate details and respond with unsubstantiated\nanswers that are inaccurate with respect to the input context. This paper\ndescribes a simple approach for detecting such contextual hallucinations. We\nhypothesize that contextual hallucinations are related to the extent to which\nan LLM attends to information in the provided context versus its own\ngenerations. Based on this intuition, we propose a simple hallucination\ndetection model whose input features are given by the ratio of attention\nweights on the context versus newly generated tokens (for each attention head).\nWe find that a linear classifier based on these lookback ratio features is as\neffective as a richer detector that utilizes the entire hidden states of an LLM\nor a text-based entailment model. The lookback ratio-based detector -- Lookback\nLens -- is found to transfer across tasks and even models, allowing a detector\nthat is trained on a 7B model to be applied (without retraining) to a larger\n13B model. We further apply this detector to mitigate contextual\nhallucinations, and find that a simple classifier-guided decoding approach is\nable to reduce the amount of hallucination, for example by 9.6% in the XSum\nsummarization task.", "journal": ""}
{"doi": "10.48550/arXiv.2408.02032", "date": "2024-08-04", "title": "Self-Introspective Decoding: Alleviating Hallucinations for Large Vision-Language Models", "authors": "Fushuo Huo, Wenchao Xu, Zhong Zhang, Haozhao Wang, Zhicheng Chen, Peilin Zhao", "abstract": "While Large Vision-Language Models (LVLMs) have rapidly advanced in recent\nyears, the prevalent issue known as the `hallucination' problem has emerged as\na significant bottleneck, hindering their real-world deployments. Existing\nmethods mitigate this issue mainly from two perspectives: One approach\nleverages extra knowledge like robust instruction tuning LVLMs with curated\ndatasets or employing auxiliary analysis networks, which inevitable incur\nadditional costs. Another approach, known as contrastive decoding, induces\nhallucinations by manually disturbing the vision or instruction raw inputs and\nmitigates them by contrasting the outputs of the disturbed and original LVLMs.\nHowever, these approaches rely on empirical holistic input disturbances and\ndouble the inference cost. To avoid these issues, we propose a simple yet\neffective method named Self-Introspective Decoding (SID). Our empirical\ninvestigation reveals that pretrained LVLMs can introspectively assess the\nimportance of vision tokens based on preceding vision and text (both\ninstruction and generated) tokens. We develop the Context and Text-aware Token\nSelection (CT2S) strategy, which preserves only unimportant vision tokens after\nearly layers of LVLMs to adaptively amplify text-informed hallucination during\nthe auto-regressive decoding. This approach ensures that multimodal knowledge\nabsorbed in the early layers induces multimodal contextual rather than aimless\nhallucinations. Subsequently, the original token logits subtract the amplified\nvision-and-text association hallucinations, guiding LVLMs decoding faithfully.\nExtensive experiments illustrate SID generates less-hallucination and\nhigher-quality texts across various metrics, without extra knowledge and much\nadditional computation burdens.", "journal": ""}
{"doi": "10.48550/arXiv.2408.13184", "date": "2024-08-23", "title": "Can LLM be a Good Path Planner based on Prompt Engineering? Mitigating the Hallucination for Path Planning", "authors": "Hourui Deng, Hongjie Zhang, Jie Ou, Chaosheng Feng", "abstract": "Spatial reasoning in Large Language Models (LLMs) is the foundation for\nembodied intelligence. However, even in simple maze environments, LLMs still\nencounter challenges in long-term path-planning, primarily influenced by their\nspatial hallucination and context inconsistency hallucination by long-term\nreasoning. To address this challenge, this study proposes an innovative model,\nSpatial-to-Relational Transformation and Curriculum Q-Learning (S2RCQL). To\naddress the spatial hallucination of LLMs, we propose the Spatial-to-Relational\napproach, which transforms spatial prompts into entity relations and paths\nrepresenting entity relation chains. This approach fully taps the potential of\nLLMs in terms of sequential thinking. As a result, we design a path-planning\nalgorithm based on Q-learning to mitigate the context inconsistency\nhallucination, which enhances the reasoning ability of LLMs. Using the Q-value\nof state-action as auxiliary information for prompts, we correct the\nhallucinations of LLMs, thereby guiding LLMs to learn the optimal path.\nFinally, we propose a reverse curriculum learning technique based on LLMs to\nfurther mitigate the context inconsistency hallucination. LLMs can rapidly\naccumulate successful experiences by reducing task difficulty and leveraging\nthem to tackle more complex tasks. We performed comprehensive experiments based\non Baidu's self-developed LLM: ERNIE-Bot 4.0. The results showed that our\nS2RCQL achieved a 23%--40% improvement in both success and optimality rates\ncompared with advanced prompt engineering.", "journal": ""}
{"doi": "10.48550/arXiv.2408.17150", "date": "2024-08-30", "title": "Look, Compare, Decide: Alleviating Hallucination in Large Vision-Language Models via Multi-View Multi-Path Reasoning", "authors": "Xiaoye Qu, Jiashuo Sun, Wei Wei, Yu Cheng", "abstract": "Recently, Large Vision-Language Models (LVLMs) have demonstrated impressive\ncapabilities in multi-modal context comprehension. However, they still suffer\nfrom hallucination problems referring to generating inconsistent outputs with\nthe image content. To mitigate hallucinations, previous studies mainly focus on\nretraining LVLMs with custom datasets. Although effective, they inherently come\nwith additional computational costs. In this paper, we propose a training-free\nframework, \\textbf{MVP}, that aims to reduce hallucinations by making the most\nof the innate capabilities of the LVLMs via \\textbf{M}ulti-\\textbf{V}iew\nMulti-\\textbf{P}ath Reasoning. Specifically, we first devise a multi-view\ninformation-seeking strategy to thoroughly perceive the comprehensive\ninformation in the image, which enriches the general global information\ncaptured by the original vision encoder in LVLMs. Furthermore, during the\nanswer decoding, we observe that the occurrence of hallucinations has a strong\ncorrelation with the certainty of the answer tokens. Thus, we propose\nmulti-path reasoning for each information view to quantify and aggregate the\ncertainty scores for each potential answer among multiple decoding paths and\nfinally decide the output answer. By fully grasping the information in the\nimage and carefully considering the certainty of the potential answers when\ndecoding, our MVP can effectively reduce hallucinations in LVLMs.The extensive\nexperiments verify that our proposed MVP significantly mitigates the\nhallucination problem across four well-known LVLMs. The source code is\navailable at: \\url{https://github.com/GasolSun36/MVP}.", "journal": ""}
{"doi": "10.48550/arXiv.2409.00159", "date": "2024-08-30", "title": "LLMs Prompted for Graphs: Hallucinations and Generative Capabilities", "authors": "Gurvan Richardeau, Samy Chali, Erwan Le Merrer, Camilla Penzo, Gilles Tredan", "abstract": "Large Language Models (LLMs) are nowadays prompted for a wide variety of\ntasks. In this article, we investigate their ability in reciting and generating\ngraphs. We first study the ability of LLMs to regurgitate well known graphs\nfrom the literature (e.g. Karate club or the graph atlas)4. Secondly, we\nquestion the generative capabilities of LLMs by asking for Erdos-Renyi random\ngraphs. As opposed to the possibility that they could memorize some Erdos-Renyi\ngraphs included in their scraped training set, this second investigation aims\nat studying a possible emergent property of LLMs. For both tasks, we propose a\nmetric to assess their errors with the lens of hallucination (i.e. incorrect\ninformation returned as facts). We most notably find that the amplitude of\ngraph hallucinations can characterize the superiority of some LLMs. Indeed, for\nthe recitation task, we observe that graph hallucinations correlate with the\nHallucination Leaderboard, a hallucination rank that leverages 10, 000 times\nmore prompts to obtain its ranking. For the generation task, we find\nsurprisingly good and reproducible results in most of LLMs. We believe this to\nconstitute a starting point for more in-depth studies of this emergent\ncapability and a challenging benchmark for their improvements. Altogether,\nthese two aspects of LLMs capabilities bridge a gap between the network science\nand machine learning communities.", "journal": ""}
{"doi": "10.48550/arXiv.2409.12580", "date": "2024-09-19", "title": "LLMs Can Check Their Own Results to Mitigate Hallucinations in Traffic Understanding Tasks", "authors": "Malsha Ashani Mahawatta Dona, Beatriz Cabrero-Daniel, Yinan Yu, Christian Berger", "abstract": "Today's Large Language Models (LLMs) have showcased exemplary capabilities,\nranging from simple text generation to advanced image processing. Such models\nare currently being explored for in-vehicle services such as supporting\nperception tasks in Advanced Driver Assistance Systems (ADAS) or Autonomous\nDriving (AD) systems, given the LLMs' capabilities to process multi-modal data.\nHowever, LLMs often generate nonsensical or unfaithful information, known as\n``hallucinations'': a notable issue that needs to be mitigated. In this paper,\nwe systematically explore the adoption of SelfCheckGPT to spot hallucinations\nby three state-of-the-art LLMs (GPT-4o, LLaVA, and Llama3) when analysing\nvisual automotive data from two sources: Waymo Open Dataset, from the US, and\nPREPER CITY dataset, from Sweden. Our results show that GPT-4o is better at\ngenerating faithful image captions than LLaVA, whereas the former demonstrated\nleniency in mislabeling non-hallucinated content as hallucinations compared to\nthe latter. Furthermore, the analysis of the performance metrics revealed that\nthe dataset type (Waymo or PREPER CITY) did not significantly affect the\nquality of the captions or the effectiveness of hallucination detection.\nHowever, the models showed better performance rates over images captured during\ndaytime, compared to during dawn, dusk or night. Overall, the results show that\nSelfCheckGPT and its adaptation can be used to filter hallucinations in\ngenerated traffic-related image captions for state-of-the-art LLMs.", "journal": ""}
{"doi": "10.48550/arXiv.2410.12130", "date": "2024-10-16", "title": "Iter-AHMCL: Alleviate Hallucination for Large Language Model via Iterative Model-level Contrastive Learning", "authors": "Huiwen Wu, Xiaohan Li, Xiaogang Xu, Jiafei Wu, Deyi Zhang, Zhe Liu", "abstract": "The development of Large Language Models (LLMs) has significantly advanced\nvarious AI applications in commercial and scientific research fields, such as\nscientific literature summarization, writing assistance, and knowledge graph\nconstruction. However, a significant challenge is the high risk of\nhallucination during LLM inference, which can lead to security concerns like\nfactual inaccuracies, inconsistent information, and fabricated content. To\ntackle this issue, it is essential to develop effective methods for reducing\nhallucination while maintaining the original capabilities of the LLM. This\npaper introduces a novel approach called Iterative Model-level Contrastive\nLearning (Iter-AHMCL) to address hallucination. This method modifies the\nrepresentation layers of pre-trained LLMs by using contrastive `positive' and\n`negative' models, trained on data with and without hallucinations. By\nleveraging the differences between these two models, we create a more\nstraightforward pathway to eliminate hallucinations, and the iterative nature\nof contrastive learning further enhances performance. Experimental validation\non four pre-trained foundation LLMs (LLaMA2, Alpaca, LLaMA3, and Qwen)\nfinetuning with a specially designed dataset shows that our approach achieves\nan average improvement of 10.1 points on the TruthfulQA benchmark.\nComprehensive experiments demonstrate the effectiveness of Iter-AHMCL in\nreducing hallucination while maintaining the general capabilities of LLMs.", "journal": ""}
{"doi": "10.48550/arXiv.2410.15926", "date": "2024-10-21", "title": "Mitigating Object Hallucination via Concentric Causal Attention", "authors": "Yun Xing, Yiheng Li, Ivan Laptev, Shijian Lu", "abstract": "Recent Large Vision Language Models (LVLMs) present remarkable zero-shot\nconversational and reasoning capabilities given multimodal queries.\nNevertheless, they suffer from object hallucination, a phenomenon where LVLMs\nare prone to generate textual responses not factually aligned with image\ninputs. Our pilot study reveals that object hallucination is closely tied with\nRotary Position Encoding (RoPE), a widely adopted positional dependency\nmodeling design in existing LVLMs. Due to the long-term decay in RoPE, LVLMs\ntend to hallucinate more when relevant visual cues are distant from instruction\ntokens in the multimodal input sequence. Additionally, we observe a similar\neffect when reversing the sequential order of visual tokens during multimodal\nalignment. Our tests indicate that long-term decay in RoPE poses challenges to\nLVLMs while capturing visual-instruction interactions across long distances. We\npropose Concentric Causal Attention (CCA), a simple yet effective positional\nalignment strategy that mitigates the impact of RoPE long-term decay in LVLMs\nby naturally reducing relative distance between visual and instruction tokens.\nWith CCA, visual tokens can better interact with instruction tokens, thereby\nenhancing model's perception capability and alleviating object hallucination.\nWithout bells and whistles, our positional alignment method surpasses existing\nhallucination mitigation strategies by large margins on multiple object\nhallucination benchmarks.", "journal": ""}
{"doi": "10.48550/arXiv.2410.17477", "date": "2024-10-22", "title": "Do Robot Snakes Dream like Electric Sheep? Investigating the Effects of Architectural Inductive Biases on Hallucination", "authors": "Jerry Huang, Prasanna Parthasarathi, Mehdi Rezagholizadeh, Boxing Chen, Sarath Chandar", "abstract": "The growth in prominence of large language models (LLMs) in everyday life can\nbe largely attributed to their generative abilities, yet some of this is also\nowed to the risks and costs associated with their use. On one front is their\ntendency to hallucinate false or misleading information, limiting their\nreliability. On another is the increasing focus on the computational\nlimitations associated with traditional self-attention based LLMs, which has\nbrought about new alternatives, in particular recurrent models, meant to\novercome them. Yet it remains uncommon to consider these two concerns\nsimultaneously. Do changes in architecture exacerbate/alleviate existing\nconcerns about hallucinations? Do they affect how and where they occur? Through\nan extensive evaluation, we study how these architecture-based inductive biases\naffect the propensity to hallucinate. While hallucination remains a general\nphenomenon not limited to specific architectures, the situations in which they\noccur and the ease with which specific types of hallucinations can be induced\ncan significantly differ based on the model architecture. These findings\nhighlight the need for better understanding both these problems in conjunction\nwith each other, as well as consider how to design more universal techniques\nfor handling hallucinations.", "journal": ""}
{"doi": "10.48550/arXiv.2411.10867", "date": "2024-11-16", "title": "ViBe: A Text-to-Video Benchmark for Evaluating Hallucination in Large Multimodal Models", "authors": "Vipula Rawte, Sarthak Jain, Aarush Sinha, Garv Kaushik, Aman Bansal, Prathiksha Rumale Vishwanath, Samyak Rajesh Jain, Aishwarya Naresh Reganti, Vinija Jain, Aman Chadha, Amit P. Sheth, Amitava Das", "abstract": "Recent advances in Large Multimodal Models (LMMs) have expanded their\ncapabilities to video understanding, with Text-to-Video (T2V) models excelling\nin generating videos from textual prompts. However, they still frequently\nproduce hallucinated content, revealing AI-generated inconsistencies. We\nintroduce ViBe (https://vibe-t2v-bench.github.io/): a large-scale dataset of\nhallucinated videos from open-source T2V models. We identify five major\nhallucination types: Vanishing Subject, Omission Error, Numeric Variability,\nSubject Dysmorphia, and Visual Incongruity. Using ten T2V models, we generated\nand manually annotated 3,782 videos from 837 diverse MS COCO captions. Our\nproposed benchmark includes a dataset of hallucinated videos and a\nclassification framework using video embeddings. ViBe serves as a critical\nresource for evaluating T2V reliability and advancing hallucination detection.\nWe establish classification as a baseline, with the TimeSFormer + CNN ensemble\nachieving the best performance (0.345 accuracy, 0.342 F1 score). While initial\nbaselines proposed achieve modest accuracy, this highlights the difficulty of\nautomated hallucination detection and the need for improved methods. Our\nresearch aims to drive the development of more robust T2V models and evaluate\ntheir outputs based on user preferences.", "journal": ""}
{"doi": "10.48550/arXiv.2412.18947", "date": "2024-12-25", "title": "MedHallBench: A New Benchmark for Assessing Hallucination in Medical Large Language Models", "authors": "Kaiwen Zuo, Yirui Jiang", "abstract": "Medical Large Language Models (MLLMs) have demonstrated potential in\nhealthcare applications, yet their propensity for hallucinations -- generating\nmedically implausible or inaccurate information -- presents substantial risks\nto patient care. This paper introduces MedHallBench, a comprehensive benchmark\nframework for evaluating and mitigating hallucinations in MLLMs. Our\nmethodology integrates expert-validated medical case scenarios with established\nmedical databases to create a robust evaluation dataset. The framework employs\na sophisticated measurement system that combines automated ACHMI (Automatic\nCaption Hallucination Measurement in Medical Imaging) scoring with rigorous\nclinical expert evaluations and utilizes reinforcement learning methods to\nachieve automatic annotation. Through an optimized reinforcement learning from\nhuman feedback (RLHF) training pipeline specifically designed for medical\napplications, MedHallBench enables thorough evaluation of MLLMs across diverse\nclinical contexts while maintaining stringent accuracy standards. We conducted\ncomparative experiments involving various models, utilizing the benchmark to\nestablish a baseline for widely adopted large language models (LLMs). Our\nfindings indicate that ACHMI provides a more nuanced understanding of the\neffects of hallucinations compared to traditional metrics, thereby highlighting\nits advantages in hallucination assessment. This research establishes a\nfoundational framework for enhancing MLLMs' reliability in healthcare settings\nand presents actionable strategies for addressing the critical challenge of AI\nhallucinations in medical applications.", "journal": ""}
{"doi": "10.48550/arXiv.2501.01926", "date": "2025-01-03", "title": "Mitigating Hallucination for Large Vision Language Model by Inter-Modality Correlation Calibration Decoding", "authors": "Jiaming Li, Jiacheng Zhang, Zequn Jie, Lin Ma, Guanbin Li", "abstract": "Large vision-language models (LVLMs) have shown remarkable capabilities in\nvisual-language understanding for downstream multi-modal tasks. Despite their\nsuccess, LVLMs still suffer from generating hallucinations in complex\ngeneration tasks, leading to inconsistencies between visual inputs and\ngenerated content. To address this issue, some approaches have introduced\ninference-time interventions, such as contrastive decoding and attention\nrectification, to reduce overreliance on language priors. However, these\napproaches overlook hallucinations stemming from spurious inter-modality\ncorrelations. In this paper, we propose an Inter-Modality Correlation\nCalibration Decoding (IMCCD) method to mitigate hallucinations in LVLMs in a\ntraining-free manner. In this method, we design a Cross-Modal Value-Enhanced\nDecoding(CMVED) module to alleviate hallucination by a novel contrastive\ndecoding mechanism. During the estimation of distorted distribution, CMVED\nmasks the value vectors associated with significant cross-modal attention\nweights, which address both uni-modality overreliance and misleading\ninter-modality correlations. Additionally, a Content-Driven Attention\nRefinement(CDAR) module refines cross-modal attention weights, guiding LVLMs to\nfocus on important visual content. Experimental results on diverse\nhallucination benchmarks validate the superiority of our method over existing\nstate-of-the-art techniques in reducing hallucinations in LVLM text generation.\nOur code will be available at https://github.com/lijm48/IMCCD.", "journal": ""}
{"doi": "10.48550/arXiv.2501.09349", "date": "2025-01-16", "title": "ChartInsighter: An Approach for Mitigating Hallucination in Time-series Chart Summary Generation with A Benchmark Dataset", "authors": "Fen Wang, Bomiao Wang, Xueli Shu, Zhen Liu, Zekai Shao, Chao Liu, Siming Chen", "abstract": "Effective chart summary can significantly reduce the time and effort decision\nmakers spend interpreting charts, enabling precise and efficient communication\nof data insights. Previous studies have faced challenges in generating accurate\nand semantically rich summaries of time-series data charts. In this paper, we\nidentify summary elements and common hallucination types in the generation of\ntime-series chart summaries, which serve as our guidelines for automatic\ngeneration. We introduce ChartInsighter, which automatically generates chart\nsummaries of time-series data, effectively reducing hallucinations in chart\nsummary generation. Specifically, we assign multiple agents to generate the\ninitial chart summary and collaborate iteratively, during which they invoke\nexternal data analysis modules to extract insights and compile them into a\ncoherent summary. Additionally, we implement a self-consistency test method to\nvalidate and correct our summary. We create a high-quality benchmark of charts\nand summaries, with hallucination types annotated on a sentence-by-sentence\nbasis, facilitating the evaluation of the effectiveness of reducing\nhallucinations. Our evaluations using our benchmark show that our method\nsurpasses state-of-the-art models, and that our summary hallucination rate is\nthe lowest, which effectively reduces various hallucinations and improves\nsummary quality. The benchmark is available at\nhttps://github.com/wangfen01/ChartInsighter.", "journal": ""}
{"doi": "10.48550/arXiv.2501.09695", "date": "2025-01-16", "title": "Mitigating Hallucinations in Large Vision-Language Models via DPO: On-Policy Data Hold the Key", "authors": "Zhihe Yang, Xufang Luo, Dongqi Han, Yunjian Xu, Dongsheng Li", "abstract": "Hallucination remains a major challenge for Large Vision-Language Models\n(LVLMs). Direct Preference Optimization (DPO) has gained increasing attention\nas a simple solution to hallucination issues. It directly learns from\nconstructed preference pairs that reflect the severity of hallucinations in\nresponses to the same prompt and image. Nonetheless, different data\nconstruction methods in existing works bring notable performance variations. We\nidentify a crucial factor here: outcomes are largely contingent on whether the\nconstructed data aligns on-policy w.r.t the initial (reference) policy of DPO.\nTheoretical analysis suggests that learning from off-policy data is impeded by\nthe presence of KL-divergence between the updated policy and the reference\npolicy. From the perspective of dataset distribution, we systematically\nsummarize the inherent flaws in existing algorithms that employ DPO to address\nhallucination issues. To alleviate the problems, we propose On-Policy Alignment\n(OPA)-DPO framework, which uniquely leverages expert feedback to correct\nhallucinated responses and aligns both the original and expert-revised\nresponses in an on-policy manner. Notably, with only 4.8k data, OPA-DPO\nachieves an additional reduction in the hallucination rate of LLaVA-1.5-7B:\n13.26% on the AMBER benchmark and 5.39% on the Object-Hal benchmark, compared\nto the previous SOTA algorithm trained with 16k samples. Our implementation is\navailable at https://github.com/zhyang2226/OPA-DPO.", "journal": ""}
{"doi": "10.48550/arXiv.2502.12187", "date": "2025-02-15", "title": "Hallucinations are inevitable but can be made statistically negligible. The \"innate\" inevitability of hallucinations cannot explain practical LLM issues", "authors": "Atsushi Suzuki, Yulan He, Feng Tian, Zhongyuan Wang", "abstract": "Hallucinations, a phenomenon where a language model (LM) generates nonfactual\ncontent, pose a significant challenge to the practical deployment of LMs. While\nmany empirical methods have been proposed to mitigate hallucinations, recent\nstudies established a computability-theoretic result showing that any LM will\ninevitably generate hallucinations on an infinite set of inputs, regardless of\nthe quality and quantity of training datasets and the choice of the language\nmodel architecture and training and inference algorithms. Although the\ncomputability-theoretic result may seem pessimistic, its significance in\npractical viewpoints has remained unclear. This paper claims that those\n\"innate\" inevitability results from computability theory and diagonal argument,\nin principle, cannot explain practical issues of LLMs. We demonstrate this\nclaim by presenting a positive theoretical result from a probabilistic\nperspective. Specifically, we prove that hallucinations can be made\nstatistically negligible, provided that the quality and quantity of the\ntraining data are sufficient. Interestingly, our positive result coexists with\nthe computability-theoretic result, implying that while hallucinations on an\ninfinite set of inputs cannot be entirely eliminated, their probability can\nalways be reduced by improving algorithms and training data. By evaluating the\ntwo seemingly contradictory results through the lens of information theory, we\nargue that our probability-theoretic positive result better reflects practical\nconsiderations than the computability-theoretic negative result.", "journal": ""}
{"doi": "10.48550/arXiv.2502.12414", "date": "2025-02-18", "title": "Lost in Transcription, Found in Distribution Shift: Demystifying Hallucination in Speech Foundation Models", "authors": "Hanin Atwany, Abdul Waheed, Rita Singh, Monojit Choudhury, Bhiksha Raj", "abstract": "Speech foundation models trained at a massive scale, both in terms of model\nand data size, result in robust systems capable of performing multiple speech\ntasks, including automatic speech recognition (ASR). These models transcend\nlanguage and domain barriers, yet effectively measuring their performance\nremains a challenge. Traditional metrics like word error rate (WER) and\ncharacter error rate (CER) are commonly used to evaluate ASR performance but\noften fail to reflect transcription quality in critical contexts, particularly\nwhen detecting fabricated outputs. This phenomenon, known as hallucination, is\nespecially concerning in high-stakes domains such as healthcare, legal, and\naviation, where errors can have severe consequences. In our work, we address\nthis gap by investigating hallucination in ASR models. We examine how factors\nsuch as distribution shifts, model size, and model architecture influence the\nhallucination error rate (HER), a metric we introduce to quantify\nhallucinations. Our analysis of over 20 ASR models reveals \\numinsights~key\ninsights: (1) High WERs can mask low hallucination rates, while low WERs may\nconceal dangerous hallucinations. (2) Synthetic noise, both adversarial and\ncommon perturbations like white noise, pitch shift, and time stretching,\nincrease HER. (3) Distribution shift correlates strongly with HER ($\\alpha =\n0.91$). Our findings highlight the importance of incorporating HER alongside\ntraditional metrics like WER to better assess ASR model performance,\nparticularly in high-stakes domains.", "journal": ""}
{"doi": "10.48550/arXiv.2502.13416", "date": "2025-02-19", "title": "Detecting LLM Fact-conflicting Hallucinations Enhanced by Temporal-logic-based Reasoning", "authors": "Ningke Li, Yahui Song, Kailong Wang, Yuekang Li, Ling Shi, Yi Liu, Haoyu Wang", "abstract": "Large language models (LLMs) face the challenge of hallucinations -- outputs\nthat seem coherent but are actually incorrect. A particularly damaging type is\nfact-conflicting hallucination (FCH), where generated content contradicts\nestablished facts. Addressing FCH presents three main challenges: 1)\nAutomatically constructing and maintaining large-scale benchmark datasets is\ndifficult and resource-intensive; 2) Generating complex and efficient test\ncases that the LLM has not been trained on -- especially those involving\nintricate temporal features -- is challenging, yet crucial for eliciting\nhallucinations; and 3) Validating the reasoning behind LLM outputs is\ninherently difficult, particularly with complex logical relationships, as it\nrequires transparency in the model's decision-making process.\n  This paper presents Drowzee, an innovative end-to-end metamorphic testing\nframework that utilizes temporal logic to identify fact-conflicting\nhallucinations (FCH) in large language models (LLMs). Drowzee builds a\ncomprehensive factual knowledge base by crawling sources like Wikipedia and\nuses automated temporal-logic reasoning to convert this knowledge into a large,\nextensible set of test cases with ground truth answers. LLMs are tested using\nthese cases through template-based prompts, which require them to generate both\nanswers and reasoning steps. To validate the reasoning, we propose two\nsemantic-aware oracles that compare the semantic structure of LLM outputs to\nthe ground truths. Across nine LLMs in nine different knowledge domains,\nexperimental results show that Drowzee effectively identifies rates of\nnon-temporal-related hallucinations ranging from 24.7% to 59.8%, and rates of\ntemporal-related hallucinations ranging from 16.7% to 39.2%.", "journal": ""}
{"doi": "10.48550/arXiv.2502.16143", "date": "2025-02-22", "title": "The Law of Knowledge Overshadowing: Towards Understanding, Predicting, and Preventing LLM Hallucination", "authors": "Yuji Zhang, Sha Li, Cheng Qian, Jiateng Liu, Pengfei Yu, Chi Han, Yi R. Fung, Kathleen McKeown, Chengxiang Zhai, Manling Li, Heng Ji", "abstract": "Hallucination is a persistent challenge in large language models (LLMs),\nwhere even with rigorous quality control, models often generate distorted\nfacts. This paradox, in which error generation continues despite high-quality\ntraining data, calls for a deeper understanding of the underlying LLM\nmechanisms. To address it, we propose a novel concept: knowledge overshadowing,\nwhere model's dominant knowledge can obscure less prominent knowledge during\ntext generation, causing the model to fabricate inaccurate details. Building on\nthis idea, we introduce a novel framework to quantify factual hallucinations by\nmodeling knowledge overshadowing. Central to our approach is the log-linear\nlaw, which predicts that the rate of factual hallucination increases linearly\nwith the logarithmic scale of (1) Knowledge Popularity, (2) Knowledge Length,\nand (3) Model Size. The law provides a means to preemptively quantify\nhallucinations, offering foresight into their occurrence even before model\ntraining or inference. Built on overshadowing effect, we propose a new decoding\nstrategy CoDa, to mitigate hallucinations, which notably enhance model\nfactuality on Overshadow (27.9%), MemoTrap (13.1%) and NQ-Swap (18.3%). Our\nfindings not only deepen understandings of the underlying mechanisms behind\nhallucinations but also provide actionable insights for developing more\npredictable and controllable language models.", "journal": ""}
{"doi": "10.48550/arXiv.2503.01075", "date": "2025-03-03", "title": "Tackling Hallucination from Conditional Models for Medical Image Reconstruction with DynamicDPS", "authors": "Seunghoi Kim, Henry F. J. Tregidgo, Matteo Figini, Chen Jin, Sarang Joshi, Daniel C. Alexander", "abstract": "Hallucinations are spurious structures not present in the ground truth,\nposing a critical challenge in medical image reconstruction, especially for\ndata-driven conditional models. We hypothesize that combining an unconditional\ndiffusion model with data consistency, trained on a diverse dataset, can reduce\nthese hallucinations. Based on this, we propose DynamicDPS, a diffusion-based\nframework that integrates conditional and unconditional diffusion models to\nenhance low-quality medical images while systematically reducing\nhallucinations. Our approach first generates an initial reconstruction using a\nconditional model, then refines it with an adaptive diffusion-based inverse\nproblem solver. DynamicDPS skips early stage in the reverse process by\nselecting an optimal starting time point per sample and applies Wolfe's line\nsearch for adaptive step sizes, improving both efficiency and image fidelity.\nUsing diffusion priors and data consistency, our method effectively reduces\nhallucinations from any conditional model output. We validate its effectiveness\nin Image Quality Transfer for low-field MRI enhancement. Extensive evaluations\non synthetic and real MR scans, including a downstream task for tissue volume\nestimation, show that DynamicDPS reduces hallucinations, improving relative\nvolume estimation by over 15% for critical tissues while using only 5% of the\nsampling steps required by baseline diffusion models. As a model-agnostic and\nfine-tuning-free approach, DynamicDPS offers a robust solution for\nhallucination reduction in medical imaging. The code will be made publicly\navailable upon publication.", "journal": ""}
{"doi": "10.48550/arXiv.2503.06149", "date": "2025-03-08", "title": "Wireless Hallucination in Generative AI-enabled Communications: Concepts, Issues, and Solutions", "authors": "Xudong Wang, Jiacheng Wang, Lei Feng, Dusit Niyato, Ruichen Zhang, Jiawen Kang, Zehui Xiong, Hongyang Du, Shiwen Mao", "abstract": "Generative AI (GenAI) is driving the intelligence of wireless communications.\nDue to data limitations, random generation, and dynamic environments, GenAI may\ngenerate channel information or optimization strategies that violate physical\nlaws or deviate from actual real-world requirements. We refer to this\nphenomenon as wireless hallucination, which results in invalid channel\ninformation, spectrum wastage, and low communication reliability but remains\nunderexplored. To address this gap, this article provides a comprehensive\nconcept of wireless hallucinations in GenAI-driven communications, focusing on\nhallucination mitigation. Specifically, we first introduce the fundamental,\nanalyze its causes based on the GenAI workflow, and propose mitigation\nsolutions at the data, model, and post-generation levels. Then, we\nsystematically examines representative hallucination scenarios in GenAI-enabled\ncommunications and their corresponding solutions. Finally, we propose a novel\nintegrated mitigation solution for GenAI-based channel estimation. At the data\nlevel, we establish a channel estimation hallucination dataset and employ\ngenerative adversarial networks (GANs)-based data augmentation. Additionally,\nwe incorporate attention mechanisms and large language models (LLMs) to enhance\nboth training and inference performance. Experimental results demonstrate that\nthe proposed hybrid solutions reduce the normalized mean square error (NMSE) by\n0.19, effectively reducing wireless hallucinations.", "journal": ""}
{"doi": "10.48550/arXiv.2503.06169", "date": "2025-03-08", "title": "Treble Counterfactual VLMs: A Causal Approach to Hallucination", "authors": "Shawn Li, Jiashu Qu, Yuxiao Zhou, Yuehan Qin, Tiankai Yang, Yue Zhao", "abstract": "Vision-Language Models (VLMs) have advanced multi-modal tasks like image\ncaptioning, visual question answering, and reasoning. However, they often\ngenerate hallucinated outputs inconsistent with the visual context or prompt,\nlimiting reliability in critical applications like autonomous driving and\nmedical imaging. Existing studies link hallucination to statistical biases,\nlanguage priors, and biased feature learning but lack a structured causal\nunderstanding. In this work, we introduce a causal perspective to analyze and\nmitigate hallucination in VLMs. We hypothesize that hallucination arises from\nunintended direct influences of either the vision or text modality, bypassing\nproper multi-modal fusion. To address this, we construct a causal graph for\nVLMs and employ counterfactual analysis to estimate the Natural Direct Effect\n(NDE) of vision, text, and their cross-modal interaction on the output. We\nsystematically identify and mitigate these unintended direct effects to ensure\nthat responses are primarily driven by genuine multi-modal fusion. Our approach\nconsists of three steps: (1) designing structural causal graphs to distinguish\ncorrect fusion pathways from spurious modality shortcuts, (2) estimating\nmodality-specific and cross-modal NDE using perturbed image representations,\nhallucinated text embeddings, and degraded visual inputs, and (3) implementing\na test-time intervention module to dynamically adjust the model's dependence on\neach modality. Experimental results demonstrate that our method significantly\nreduces hallucination while preserving task performance, providing a robust and\ninterpretable framework for improving VLM reliability. To enhance accessibility\nand reproducibility, our code is publicly available at\nhttps://github.com/TREE985/Treble-Counterfactual-VLMs.", "journal": ""}
{"doi": "10.48550/arXiv.2503.10602", "date": "2025-03-13", "title": "TruthPrInt: Mitigating LVLM Object Hallucination Via Latent Truthful-Guided Pre-Intervention", "authors": "Jinhao Duan, Fei Kong, Hao Cheng, James Diffenderfer, Bhavya Kailkhura, Lichao Sun, Xiaofeng Zhu, Xiaoshuang Shi, Kaidi Xu", "abstract": "Object Hallucination (OH) has been acknowledged as one of the major\ntrustworthy challenges in Large Vision-Language Models (LVLMs). Recent\nadvancements in Large Language Models (LLMs) indicate that internal states,\nsuch as hidden states, encode the \"overall truthfulness\" of generated\nresponses. However, it remains under-explored how internal states in LVLMs\nfunction and whether they could serve as \"per-token\" hallucination indicators,\nwhich is essential for mitigating OH. In this paper, we first conduct an\nin-depth exploration of LVLM internal states in relation to OH issues and\ndiscover that (1) LVLM internal states are high-specificity per-token\nindicators of hallucination behaviors. Moreover, (2) different LVLMs encode\nuniversal patterns of hallucinations in common latent subspaces, indicating\nthat there exist \"generic truthful directions\" shared by various LVLMs. Based\non these discoveries, we propose Truthful-Guided Pre-Intervention (TruthPrInt)\nthat first learns the truthful direction of LVLM decoding and then applies\ntruthful-guided inference-time intervention during LVLM decoding. We further\npropose ComnHallu to enhance both cross-LVLM and cross-data hallucination\ndetection transferability by constructing and aligning hallucination latent\nsubspaces. We evaluate TruthPrInt in extensive experimental settings, including\nin-domain and out-of-domain scenarios, over popular LVLMs and OH benchmarks.\nExperimental results indicate that TruthPrInt significantly outperforms\nstate-of-the-art methods. Codes will be available at\nhttps://github.com/jinhaoduan/TruthPrInt.", "journal": ""}
{"doi": "10.48550/arXiv.2503.16541", "date": "2025-03-19", "title": "Poly-FEVER: A Multilingual Fact Verification Benchmark for Hallucination Detection in Large Language Models", "authors": "Hanzhi Zhang, Sumera Anjum, Heng Fan, Weijian Zheng, Yan Huang, Yunhe Feng", "abstract": "Hallucinations in generative AI, particularly in Large Language Models\n(LLMs), pose a significant challenge to the reliability of multilingual\napplications. Existing benchmarks for hallucination detection focus primarily\non English and a few widely spoken languages, lacking the breadth to assess\ninconsistencies in model performance across diverse linguistic contexts. To\naddress this gap, we introduce Poly-FEVER, a large-scale multilingual fact\nverification benchmark specifically designed for evaluating hallucination\ndetection in LLMs. Poly-FEVER comprises 77,973 labeled factual claims spanning\n11 languages, sourced from FEVER, Climate-FEVER, and SciFact. It provides the\nfirst large-scale dataset tailored for analyzing hallucination patterns across\nlanguages, enabling systematic evaluation of LLMs such as ChatGPT and the LLaMA\nseries. Our analysis reveals how topic distribution and web resource\navailability influence hallucination frequency, uncovering language-specific\nbiases that impact model accuracy. By offering a multilingual benchmark for\nfact verification, Poly-FEVER facilitates cross-linguistic comparisons of\nhallucination detection and contributes to the development of more reliable,\nlanguage-inclusive AI systems. The dataset is publicly available to advance\nresearch in responsible AI, fact-checking methodologies, and multilingual NLP,\npromoting greater transparency and robustness in LLM performance. The proposed\nPoly-FEVER is available at:\nhttps://huggingface.co/datasets/HanzhiZhang/Poly-FEVER.", "journal": ""}
{"doi": "10.48550/arXiv.2503.20504", "date": "2025-03-26", "title": "Vision-Amplified Semantic Entropy for Hallucination Detection in Medical Visual Question Answering", "authors": "Zehui Liao, Shishuai Hu, Ke Zou, Huazhu Fu, Liangli Zhen, Yong Xia", "abstract": "Multimodal large language models (MLLMs) have demonstrated significant\npotential in medical Visual Question Answering (VQA). Yet, they remain prone to\nhallucinations-incorrect responses that contradict input images, posing\nsubstantial risks in clinical decision-making. Detecting these hallucinations\nis essential for establishing trust in MLLMs among clinicians and patients,\nthereby enabling their real-world adoption. Current hallucination detection\nmethods, especially semantic entropy (SE), have demonstrated promising\nhallucination detection capacity for LLMs. However, adapting SE to medical\nMLLMs by incorporating visual perturbations presents a dilemma. Weak\nperturbations preserve image content and ensure clinical validity, but may be\noverlooked by medical MLLMs, which tend to over rely on language priors. In\ncontrast, strong perturbations can distort essential diagnostic features,\ncompromising clinical interpretation. To address this issue, we propose Vision\nAmplified Semantic Entropy (VASE), which incorporates weak image\ntransformations and amplifies the impact of visual input, to improve\nhallucination detection in medical VQA. We first estimate the semantic\npredictive distribution under weak visual transformations to preserve clinical\nvalidity, and then amplify visual influence by contrasting this distribution\nwith that derived from a distorted image. The entropy of the resulting\ndistribution is estimated as VASE. Experiments on two medical open-ended VQA\ndatasets demonstrate that VASE consistently outperforms existing hallucination\ndetection methods.", "journal": ""}
{"doi": "10.48550/arXiv.2503.20673", "date": "2025-03-26", "title": "Mitigating Low-Level Visual Hallucinations Requires Self-Awareness: Database, Model and Training Strategy", "authors": "Yinan Sun, Xiongkuo Min, Zicheng Zhang, Yixuan Gao, Yuqin Cao, Guangtao Zhai", "abstract": "The rapid development of multimodal large language models has resulted in\nremarkable advancements in visual perception and understanding, consolidating\nseveral tasks into a single visual question-answering framework. However, these\nmodels are prone to hallucinations, which limit their reliability as artificial\nintelligence systems. While this issue is extensively researched in natural\nlanguage processing and image captioning, there remains a lack of investigation\nof hallucinations in Low-level Visual Perception and Understanding (HLPU),\nespecially in the context of image quality assessment tasks. We consider that\nthese hallucinations arise from an absence of clear self-awareness within the\nmodels. To address this issue, we first introduce the HLPU instruction\ndatabase, the first instruction database specifically focused on hallucinations\nin low-level vision tasks. This database contains approximately 200K\nquestion-answer pairs and comprises four subsets, each covering different types\nof instructions. Subsequently, we propose the Self-Awareness Failure\nElimination (SAFEQA) model, which utilizes image features, salient region\nfeatures and quality features to improve the perception and comprehension\nabilities of the model in low-level vision tasks. Furthermore, we propose the\nEnhancing Self-Awareness Preference Optimization (ESA-PO) framework to increase\nthe model's awareness of knowledge boundaries, thereby mitigating the incidence\nof hallucination. Finally, we conduct comprehensive experiments on low-level\nvision tasks, with the results demonstrating that our proposed method\nsignificantly enhances self-awareness of the model in these tasks and reduces\nhallucinations. Notably, our proposed method improves both accuracy and\nself-awareness of the proposed model and outperforms close-source models in\nterms of various evaluation metrics.", "journal": ""}
{"doi": "10.48550/arXiv.2505.11741", "date": "2025-05-16", "title": "Diverging Towards Hallucination: Detection of Failures in Vision-Language Models via Multi-token Aggregation", "authors": "Geigh Zollicoffer, Minh Vu, Manish Bhattarai", "abstract": "Vision-language models (VLMs) now rival human performance on many multimodal\ntasks, yet they still hallucinate objects or generate unsafe text. Current\nhallucination detectors, e.g., single-token linear probing (SLP) and P(True),\ntypically analyze only the logit of the first generated token or just its\nhighest scoring component overlooking richer signals embedded within earlier\ntoken distributions. We demonstrate that analyzing the complete sequence of\nearly logits potentially provides substantially more diagnostic information. We\nemphasize that hallucinations may only emerge after several tokens, as subtle\ninconsistencies accumulate over time. By analyzing the Kullback-Leibler (KL)\ndivergence between logits corresponding to hallucinated and non-hallucinated\ntokens, we underscore the importance of incorporating later-token logits to\nmore accurately capture the reliability dynamics of VLMs. In response, we\nintroduce Multi-Token Reliability Estimation (MTRE), a lightweight, white-box\nmethod that aggregates logits from the first ten tokens using multi-token\nlog-likelihood ratios and self-attention. Despite the challenges posed by large\nvocabulary sizes and long logit sequences, MTRE remains efficient and\ntractable. On MAD-Bench, MM-SafetyBench, MathVista, and four\ncompositional-geometry benchmarks, MTRE improves AUROC by 9.4 +/- 1.3 points\nover SLP and by 12.1 +/- 1.7 points over P(True), setting a new\nstate-of-the-art in hallucination detection for open-source VLMs.", "journal": ""}
{"doi": "10.48550/arXiv.2505.19498", "date": "2025-05-26", "title": "Enhancing Visual Reliance in Text Generation: A Bayesian Perspective on Mitigating Hallucination in Large Vision-Language Models", "authors": "Nanxing Hu, Xiaoyue Duan, Jinchao Zhang, Guoliang Kang", "abstract": "Large Vision-Language Models (LVLMs) usually generate texts which satisfy\ncontext coherence but don't match the visual input. Such a hallucination issue\nhinders LVLMs' applicability in the real world. The key to solving\nhallucination in LVLM is to make the text generation rely more on the visual\ncontent. Most previous works choose to enhance/adjust the features/output of a\nspecific modality (i.e., visual or textual) to alleviate hallucinations in\nLVLM, which do not explicitly or systematically enhance the visual reliance. In\nthis paper, we comprehensively investigate the factors which may degenerate the\nvisual reliance in text generation of LVLM from a Bayesian perspective. Based\non our observations, we propose to mitigate hallucination in LVLM from three\naspects. Firstly, we observe that not all visual tokens are informative in\ngenerating meaningful texts. We propose to evaluate and remove redundant visual\ntokens to avoid their disturbance. Secondly, LVLM may encode inappropriate\nprior information, making it lean toward generating unexpected words. We\npropose a simple yet effective way to rectify the prior from a Bayesian\nperspective. Thirdly, we observe that starting from certain steps, the\nposterior of next-token prediction conditioned on visual tokens may collapse to\na prior distribution which does not depend on any informative visual tokens at\nall. Thus, we propose to stop further text generation to avoid hallucination.\nExtensive experiments on three benchmarks including POPE, CHAIR, and MME\ndemonstrate that our method can consistently mitigate the hallucination issue\nof LVLM and performs favorably against previous state-of-the-arts.", "journal": ""}
{"doi": "10.48550/arXiv.2505.21547", "date": "2025-05-24", "title": "Image Tokens Matter: Mitigating Hallucination in Discrete Tokenizer-based Large Vision-Language Models via Latent Editing", "authors": "Weixing Wang, Zifeng Ding, Jindong Gu, Rui Cao, Christoph Meinel, Gerard de Melo, Haojin Yang", "abstract": "Large Vision-Language Models (LVLMs) with discrete image tokenizers unify\nmultimodal representations by encoding visual inputs into a finite set of\ntokens. Despite their effectiveness, we find that these models still\nhallucinate non-existent objects. We hypothesize that this may be due to visual\npriors induced during training: When certain image tokens frequently co-occur\nin the same spatial regions and represent shared objects, they become strongly\nassociated with the verbalizations of those objects. As a result, the model may\nhallucinate by evoking visually absent tokens that often co-occur with present\nones. To test this assumption, we construct a co-occurrence graph of image\ntokens using a segmentation dataset and employ a Graph Neural Network (GNN)\nwith contrastive learning followed by a clustering method to group tokens that\nfrequently co-occur in similar visual contexts. We find that hallucinations\npredominantly correspond to clusters whose tokens dominate the input, and more\nspecifically, that the visually absent tokens in those clusters show much\nhigher correlation with hallucinated objects compared to tokens present in the\nimage. Based on this observation, we propose a hallucination mitigation method\nthat suppresses the influence of visually absent tokens by modifying latent\nimage embeddings during generation. Experiments show our method reduces\nhallucinations while preserving expressivity. Code is available at\nhttps://github.com/weixingW/CGC-VTD/tree/main", "journal": ""}
{"doi": "10.48550/arXiv.2402.13428", "date": "2024-02-20", "title": "Emergence and dynamics of delusions and hallucinations across stages in early psychosis", "authors": "Catalina Mourgues-Codern, David Benrimoh, Jay Gandhi, Emily A. Farina, Raina Vin, Tihare Zamorano, Deven Parekh, Ashok Malla, Ridha Joober, Martin Lepage, Srividya N. Iyer, Jean Addington, Carrie E. Bearden, Kristin S. Cadenhead, Barbara Cornblatt, Matcheri Keshavan, William S. Stone, Daniel H. Mathalon, Diana O. Perkins, Elaine F. Walker, Tyrone D. Cannon, Scott W. Woods, Jai L. Shah, Albert R. Powers", "abstract": "Hallucinations and delusions are often grouped together within the positive\nsymptoms of psychosis. However, recent evidence suggests they may be driven by\ndistinct computational and neural mechanisms. Examining the time course of\ntheir emergence may provide insights into the relationship between these\nunderlying mechanisms. Participants from the second (N = 719) and third (N =\n699) iterations of the North American Prodrome Longitudinal Study (NAPLS 2 and\n3) were assessed for timing of CHR-P-level delusion and hallucination onset.\nPre-onset symptom patterns in first-episode psychosis patients (FEP) from the\nPrevention and Early Intervention Program for Psychosis (PEPP-Montreal; N =\n694) were also assessed. Symptom onset was determined at baseline assessment\nand the evolution of symptom patterns examined over 24 months. In all three\nsamples, participants were more likely to report the onset of delusion-spectrum\nsymptoms prior to hallucination-spectrum symptoms (odds ratios (OR): NAPLS 2 =\n4.09; NAPLS 3 = 4.14; PEPP, Z = 7.01, P < 0.001) and to present with only\ndelusions compared to only hallucinations (OR: NAPLS 2 = 5.6; NAPLS 3 = 11.11;\nPEPP = 42.75). Re-emergence of delusions after remission was also more common\nthan re-emergence of hallucinations (Ps < 0.05), and hallucinations more often\nresolved first (Ps < 0.001). In both CHR-P samples, ratings of delusional\nideation fell with the onset of hallucinations (P = 0.007). Delusions tend to\nemerge before hallucinations and may play a role in their development. Further\nwork should examine the relationship between the mechanisms driving these\nsymptoms and its utility for diagnosis and treatment.", "journal": ""}
{"doi": "10.48550/arXiv.2505.07001", "date": "2025-05-11", "title": "Hallucination-Aware Multimodal Benchmark for Gastrointestinal Image Analysis with Large Vision-Language Models", "authors": "Bidur Khanal, Sandesh Pokhrel, Sanjay Bhandari, Ramesh Rana, Nikesh Shrestha, Ram Bahadur Gurung, Cristian Linte, Angus Watson, Yash Raj Shrestha, Binod Bhattarai", "abstract": "Vision-Language Models (VLMs) are becoming increasingly popular in the\nmedical domain, bridging the gap between medical images and clinical language.\nExisting VLMs demonstrate an impressive ability to comprehend medical images\nand text queries to generate detailed, descriptive diagnostic medical reports.\nHowever, hallucination--the tendency to generate descriptions that are\ninconsistent with the visual content--remains a significant issue in VLMs, with\nparticularly severe implications in the medical field. To facilitate VLM\nresearch on gastrointestinal (GI) image analysis and study hallucination, we\ncurate a multimodal image-text GI dataset: Gut-VLM. This dataset is created\nusing a two-stage pipeline: first, descriptive medical reports of Kvasir-v2\nimages are generated using ChatGPT, which introduces some hallucinated or\nincorrect texts. In the second stage, medical experts systematically review\nthese reports, and identify and correct potential inaccuracies to ensure\nhigh-quality, clinically reliable annotations. Unlike traditional datasets that\ncontain only descriptive texts, our dataset also features tags identifying\nhallucinated sentences and their corresponding corrections. A common approach\nto reducing hallucination in VLM is to finetune the model on a small-scale,\nproblem-specific dataset. However, we take a different strategy using our\ndataset. Instead of finetuning the VLM solely for generating textual reports,\nwe finetune it to detect and correct hallucinations, an approach we call\nhallucination-aware finetuning. Our results show that this approach is better\nthan simply finetuning for descriptive report generation. Additionally, we\nconduct an extensive evaluation of state-of-the-art VLMs across several\nmetrics, establishing a benchmark. GitHub Repo:\nhttps://github.com/bhattarailab/Hallucination-Aware-VLM.", "journal": ""}
{"doi": "10.48550/arXiv.1909.08130", "date": "2019-09-17", "title": "Identity-Aware Deep Face Hallucination via Adversarial Face Verification", "authors": "Hadi Kazemi, Fariborz Taherkhani, Nasser M. Nasrabadi", "abstract": "In this paper, we address the problem of face hallucination by proposing a\nnovel multi-scale generative adversarial network (GAN) architecture optimized\nfor face verification. First, we propose a multi-scale generator architecture\nfor face hallucination with a high up-scaling ratio factor, which has multiple\nintermediate outputs at different resolutions. The intermediate outputs have\nthe growing goal of synthesizing small to large images. Second, we incorporate\na face verifier with the original GAN discriminator and propose a novel\ndiscriminator which learns to discriminate different identities while\ndistinguishing fake generated HR face images from their ground truth images. In\nparticular, the learned generator cares for not only the visual quality of\nhallucinated face images but also preserving the discriminative features in the\nhallucination process. In addition, to capture perceptually relevant\ndifferences we employ a perceptual similarity loss, instead of similarity in\npixel space. We perform a quantitative and qualitative evaluation of our\nframework on the LFW and CelebA datasets. The experimental results show the\nadvantages of our proposed method against the state-of-the-art methods on the\n8x downsampled testing dataset.", "journal": ""}
{"doi": "10.48550/arXiv.2006.04363", "date": "2020-06-08", "title": "Hallucinating Value: A Pitfall of Dyna-style Planning with Imperfect Environment Models", "authors": "Taher Jafferjee, Ehsan Imani, Erin Talvitie, Martha White, Micheal Bowling", "abstract": "Dyna-style reinforcement learning (RL) agents improve sample efficiency over\nmodel-free RL agents by updating the value function with simulated experience\ngenerated by an environment model. However, it is often difficult to learn\naccurate models of environment dynamics, and even small errors may result in\nfailure of Dyna agents. In this paper, we investigate one type of model error:\nhallucinated states. These are states generated by the model, but that are not\nreal states of the environment. We present the Hallucinated Value Hypothesis\n(HVH): updating values of real states towards values of hallucinated states\nresults in misleading state-action values which adversely affect the control\npolicy. We discuss and evaluate four Dyna variants; three which update real\nstates toward simulated -- and therefore potentially hallucinated -- states and\none which does not. The experimental results provide evidence for the HVH thus\nsuggesting a fruitful direction toward developing Dyna algorithms robust to\nmodel error.", "journal": ""}
{"doi": "10.48550/arXiv.2111.15246", "date": "2021-11-30", "title": "Hallucinated Neural Radiance Fields in the Wild", "authors": "Xingyu Chen, Qi Zhang, Xiaoyu Li, Yue Chen, Ying Feng, Xuan Wang, Jue Wang", "abstract": "Neural Radiance Fields (NeRF) has recently gained popularity for its\nimpressive novel view synthesis ability. This paper studies the problem of\nhallucinated NeRF: i.e., recovering a realistic NeRF at a different time of day\nfrom a group of tourism images. Existing solutions adopt NeRF with a\ncontrollable appearance embedding to render novel views under various\nconditions, but they cannot render view-consistent images with an unseen\nappearance. To solve this problem, we present an end-to-end framework for\nconstructing a hallucinated NeRF, dubbed as Ha-NeRF. Specifically, we propose\nan appearance hallucination module to handle time-varying appearances and\ntransfer them to novel views. Considering the complex occlusions of tourism\nimages, we introduce an anti-occlusion module to decompose the static subjects\nfor visibility accurately. Experimental results on synthetic data and real\ntourism photo collections demonstrate that our method can hallucinate the\ndesired appearances and render occlusion-free images from different views. The\nproject and supplementary materials are available at\nhttps://rover-xingyu.github.io/Ha-NeRF/.", "journal": ""}
{"doi": "10.48550/arXiv.1811.02328", "date": "2018-11-06", "title": "Super-Identity Convolutional Neural Network for Face Hallucination", "authors": "Kaipeng Zhang, Zhanpeng Zhang, Chia-Wen Cheng, Winston H. Hsu, Yu Qiao, Wei Liu, Tong Zhang", "abstract": "Face hallucination is a generative task to super-resolve the facial image\nwith low resolution while human perception of face heavily relies on identity\ninformation. However, previous face hallucination approaches largely ignore\nfacial identity recovery. This paper proposes Super-Identity Convolutional\nNeural Network (SICNN) to recover identity information for generating faces\nclosed to the real identity. Specifically, we define a super-identity loss to\nmeasure the identity difference between a hallucinated face and its\ncorresponding high-resolution face within the hypersphere identity metric\nspace. However, directly using this loss will lead to a Dynamic Domain\nDivergence problem, which is caused by the large margin between the\nhigh-resolution domain and the hallucination domain. To overcome this\nchallenge, we present a domain-integrated training approach by constructing a\nrobust identity metric for faces from these two domains. Extensive experimental\nevaluations demonstrate that the proposed SICNN achieves superior visual\nquality over the state-of-the-art methods on a challenging task to\nsuper-resolve 12$\\times$14 faces with an 8$\\times$ upscaling factor. In\naddition, SICNN significantly improves the recognizability of\nultra-low-resolution faces.", "journal": ""}
{"doi": "10.48550/arXiv.2104.08704", "date": "2021-04-18", "title": "A Token-level Reference-free Hallucination Detection Benchmark for Free-form Text Generation", "authors": "Tianyu Liu, Yizhe Zhang, Chris Brockett, Yi Mao, Zhifang Sui, Weizhu Chen, Bill Dolan", "abstract": "Large pretrained generative models like GPT-3 often suffer from hallucinating\nnon-existent or incorrect content, which undermines their potential merits in\nreal applications. Existing work usually attempts to detect these\nhallucinations based on a corresponding oracle reference at a sentence or\ndocument level. However ground-truth references may not be readily available\nfor many free-form text generation applications, and sentence- or\ndocument-level detection may fail to provide the fine-grained signals that\nwould prevent fallacious content in real time. As a first step to addressing\nthese issues, we propose a novel token-level, reference-free hallucination\ndetection task and an associated annotated dataset named HaDes (HAllucination\nDEtection dataSet). To create this dataset, we first perturb a large number of\ntext segments extracted from English language Wikipedia, and then verify these\nwith crowd-sourced annotations. To mitigate label imbalance during annotation,\nwe utilize an iterative model-in-loop strategy. We conduct comprehensive data\nanalyses and create multiple baseline models.", "journal": ""}
{"doi": "10.48550/arXiv.2106.09486", "date": "2021-06-17", "title": "Deep HDR Hallucination for Inverse Tone Mapping", "authors": "Demetris Marnerides, Thomas Bashford-Rogers, Kurt Debattista", "abstract": "Inverse Tone Mapping (ITM) methods attempt to reconstruct High Dynamic Range\n(HDR) information from Low Dynamic Range (LDR) image content. The dynamic range\nof well-exposed areas must be expanded and any missing information due to\nover/under-exposure must be recovered (hallucinated). The majority of methods\nfocus on the former and are relatively successful, while most attempts on the\nlatter are not of sufficient quality, even ones based on Convolutional Neural\nNetworks (CNNs). A major factor for the reduced inpainting quality in some\nworks is the choice of loss function. Work based on Generative Adversarial\nNetworks (GANs) shows promising results for image synthesis and LDR inpainting,\nsuggesting that GAN losses can improve inverse tone mapping results. This work\npresents a GAN-based method that hallucinates missing information from badly\nexposed areas in LDR images and compares its efficacy with alternative\nvariations. The proposed method is quantitatively competitive with\nstate-of-the-art inverse tone mapping methods, providing good dynamic range\nexpansion for well-exposed areas and plausible hallucinations for saturated and\nunder-exposed areas. A density-based normalisation method, targeted for HDR\ncontent, is also proposed, as well as an HDR data augmentation method targeted\nfor HDR hallucination.", "journal": "Sensors 2021, 21, 4032"}
{"doi": "10.48550/arXiv.2109.09784", "date": "2021-08-30", "title": "Hallucinated but Factual! Inspecting the Factuality of Hallucinations in Abstractive Summarization", "authors": "Meng Cao, Yue Dong, Jackie Chi Kit Cheung", "abstract": "State-of-the-art abstractive summarization systems often generate\n\\emph{hallucinations}; i.e., content that is not directly inferable from the\nsource text. Despite being assumed incorrect, we find that much hallucinated\ncontent is factual, namely consistent with world knowledge. These factual\nhallucinations can be beneficial in a summary by providing useful background\ninformation. In this work, we propose a novel detection approach that separates\nfactual from non-factual hallucinations of entities. Our method utilizes an\nentity's prior and posterior probabilities according to pre-trained and\nfinetuned masked language models, respectively. Empirical results suggest that\nour approach vastly outperforms two baselines %in both accuracy and F1 scores\nand strongly correlates with human judgments. % on factuality classification\ntasks. Furthermore, we show that our detector, when used as a reward signal in\nan off-line reinforcement learning (RL) algorithm, significantly improves the\nfactuality of summaries while maintaining the level of abstractiveness.", "journal": ""}
{"doi": "10.48550/arXiv.2203.08436", "date": "2022-03-16", "title": "Don't Say What You Don't Know: Improving the Consistency of Abstractive Summarization by Constraining Beam Search", "authors": "Daniel King, Zejiang Shen, Nishant Subramani, Daniel S. Weld, Iz Beltagy, Doug Downey", "abstract": "Abstractive summarization systems today produce fluent and relevant output,\nbut often \"hallucinate\" statements not supported by the source text. We analyze\nthe connection between hallucinations and training data, and find evidence that\nmodels hallucinate because they train on target summaries that are unsupported\nby the source. Based on our findings, we present PINOCCHIO, a new decoding\nmethod that improves the consistency of a transformer-based abstractive\nsummarizer by constraining beam search to avoid hallucinations. Given the model\nstates and outputs at a given step, PINOCCHIO detects likely model\nhallucinations based on various measures of attribution to the source text.\nPINOCCHIO backtracks to find more consistent output, and can opt to produce no\nsummary at all when no consistent generation can be found. In experiments, we\nfind that PINOCCHIO improves the consistency of generation (in terms of F1) by\nan average of~67% on two abstractive summarization datasets.", "journal": ""}
{"doi": "10.48550/arXiv.2205.01307", "date": "2022-05-03", "title": "Embedding Hallucination for Few-Shot Language Fine-tuning", "authors": "Yiren Jian, Chongyang Gao, Soroush Vosoughi", "abstract": "Few-shot language learners adapt knowledge from a pre-trained model to\nrecognize novel classes from a few-labeled sentences. In such settings,\nfine-tuning a pre-trained language model can cause severe over-fitting. In this\npaper, we propose an Embedding Hallucination (EmbedHalluc) method, which\ngenerates auxiliary embedding-label pairs to expand the fine-tuning dataset.\nThe hallucinator is trained by playing an adversarial game with the\ndiscriminator, such that the hallucinated embedding is indiscriminative to the\nreal ones in the fine-tuning dataset. By training with the extended dataset,\nthe language learner effectively learns from the diverse hallucinated\nembeddings to overcome the over-fitting issue. Experiments demonstrate that our\nproposed method is effective in a wide range of language tasks, outperforming\ncurrent fine-tuning methods. Further, we show that EmbedHalluc outperforms\nother methods that address this over-fitting problem, such as common data\naugmentation, semi-supervised pseudo-labeling, and regularization. The code\nwill be made available at: https://github.com/yiren-jian/EmbedHalluc.", "journal": ""}
{"doi": "10.48550/arXiv.2211.09878", "date": "2022-11-17", "title": "Reducing Hallucinations in Neural Machine Translation with Feature Attribution", "authors": "Jo\u00ebl Tang, Marina Fomicheva, Lucia Specia", "abstract": "Neural conditional language generation models achieve the state-of-the-art in\nNeural Machine Translation (NMT) but are highly dependent on the quality of\nparallel training dataset. When trained on low-quality datasets, these models\nare prone to various error types, including hallucinations, i.e. outputs that\nare fluent, but unrelated to the source sentences. These errors are\nparticularly dangerous, because on the surface the translation can be perceived\nas a correct output, especially if the reader does not understand the source\nlanguage. We present a case study focusing on model understanding and\nregularisation to reduce hallucinations in NMT. We first use feature\nattribution methods to study the behaviour of an NMT model that produces\nhallucinations. We then leverage these methods to propose a novel loss function\nthat substantially helps reduce hallucinations and does not require retraining\nthe model from scratch.", "journal": ""}
{"doi": "10.48550/arXiv.2212.05765", "date": "2022-12-12", "title": "Information-Theoretic Text Hallucination Reduction for Video-grounded Dialogue", "authors": "Sunjae Yoon, Eunseop Yoon, Hee Suk Yoon, Junyeong Kim, Chang D. Yoo", "abstract": "Video-grounded Dialogue (VGD) aims to decode an answer sentence to a question\nregarding a given video and dialogue context. Despite the recent success of\nmulti-modal reasoning to generate answer sentences, existing dialogue systems\nstill suffer from a text hallucination problem, which denotes indiscriminate\ntext-copying from input texts without an understanding of the question. This is\ndue to learning spurious correlations from the fact that answer sentences in\nthe dataset usually include the words of input texts, thus the VGD system\nexcessively relies on copying words from input texts by hoping those words to\noverlap with ground-truth texts. Hence, we design Text Hallucination Mitigating\n(THAM) framework, which incorporates Text Hallucination Regularization (THR)\nloss derived from the proposed information-theoretic text hallucination\nmeasurement approach. Applying THAM with current dialogue systems validates the\neffectiveness on VGD benchmarks (i.e., AVSD@DSTC7 and AVSD@DSTC8) and shows\nenhanced interpretability.", "journal": ""}
{"doi": "10.48550/arXiv.2303.16104", "date": "2023-03-28", "title": "Hallucinations in Large Multilingual Translation Models", "authors": "Nuno M. Guerreiro, Duarte Alves, Jonas Waldendorf, Barry Haddow, Alexandra Birch, Pierre Colombo, Andr\u00e9 F. T. Martins", "abstract": "Large-scale multilingual machine translation systems have demonstrated\nremarkable ability to translate directly between numerous languages, making\nthem increasingly appealing for real-world applications. However, when deployed\nin the wild, these models may generate hallucinated translations which have the\npotential to severely undermine user trust and raise safety concerns. Existing\nresearch on hallucinations has primarily focused on small bilingual models\ntrained on high-resource languages, leaving a gap in our understanding of\nhallucinations in massively multilingual models across diverse translation\nscenarios. In this work, we fill this gap by conducting a comprehensive\nanalysis on both the M2M family of conventional neural machine translation\nmodels and ChatGPT, a general-purpose large language model~(LLM) that can be\nprompted for translation. Our investigation covers a broad spectrum of\nconditions, spanning over 100 translation directions across various resource\nlevels and going beyond English-centric language pairs. We provide key insights\nregarding the prevalence, properties, and mitigation of hallucinations, paving\nthe way towards more responsible and reliable machine translation systems.", "journal": ""}
{"doi": "10.48550/arXiv.2305.11746", "date": "2023-05-19", "title": "HalOmi: A Manually Annotated Benchmark for Multilingual Hallucination and Omission Detection in Machine Translation", "authors": "David Dale, Elena Voita, Janice Lam, Prangthip Hansanti, Christophe Ropers, Elahe Kalbassi, Cynthia Gao, Lo\u00efc Barrault, Marta R. Costa-juss\u00e0", "abstract": "Hallucinations in machine translation are translations that contain\ninformation completely unrelated to the input. Omissions are translations that\ndo not include some of the input information. While both cases tend to be\ncatastrophic errors undermining user trust, annotated data with these types of\npathologies is extremely scarce and is limited to a few high-resource\nlanguages. In this work, we release an annotated dataset for the hallucination\nand omission phenomena covering 18 translation directions with varying resource\nlevels and scripts. Our annotation covers different levels of partial and full\nhallucinations as well as omissions both at the sentence and at the word level.\nAdditionally, we revisit previous methods for hallucination and omission\ndetection, show that conclusions made based on a single language pair largely\ndo not hold for a large-scale evaluation, and establish new solid baselines.", "journal": "EMNLP 2023"}
{"doi": "10.48550/arXiv.2305.13534", "date": "2023-05-22", "title": "How Language Model Hallucinations Can Snowball", "authors": "Muru Zhang, Ofir Press, William Merrill, Alisa Liu, Noah A. Smith", "abstract": "A major risk of using language models in practical applications is their\ntendency to hallucinate incorrect statements. Hallucinations are often\nattributed to knowledge gaps in LMs, but we hypothesize that in some cases,\nwhen justifying previously generated hallucinations, LMs output false claims\nthat they can separately recognize as incorrect. We construct three\nquestion-answering datasets where ChatGPT and GPT-4 often state an incorrect\nanswer and offer an explanation with at least one incorrect claim. Crucially,\nwe find that ChatGPT and GPT-4 can identify 67% and 87% of their own mistakes,\nrespectively. We refer to this phenomenon as hallucination snowballing: an LM\nover-commits to early mistakes, leading to more mistakes that it otherwise\nwould not make.", "journal": ""}
{"doi": "10.48550/arXiv.2307.12168", "date": "2023-07-22", "title": "Hallucination Improves the Performance of Unsupervised Visual Representation Learning", "authors": "Jing Wu, Jennifer Hobbs, Naira Hovakimyan", "abstract": "Contrastive learning models based on Siamese structure have demonstrated\nremarkable performance in self-supervised learning. Such a success of\ncontrastive learning relies on two conditions, a sufficient number of positive\npairs and adequate variations between them. If the conditions are not met,\nthese frameworks will lack semantic contrast and be fragile on overfitting. To\naddress these two issues, we propose Hallucinator that could efficiently\ngenerate additional positive samples for further contrast. The Hallucinator is\ndifferentiable and creates new data in the feature space. Thus, it is optimized\ndirectly with the pre-training task and introduces nearly negligible\ncomputation. Moreover, we reduce the mutual information of hallucinated pairs\nand smooth them through non-linear operations. This process helps avoid\nover-confident contrastive learning models during the training and achieves\nmore transformation-invariant feature embeddings. Remarkably, we empirically\nprove that the proposed Hallucinator generalizes well to various contrastive\nlearning models, including MoCoV1&V2, SimCLR and SimSiam. Under the linear\nclassification protocol, a stable accuracy gain is achieved, ranging from 0.3%\nto 3.0% on CIFAR10&100, Tiny ImageNet, STL-10 and ImageNet. The improvement is\nalso observed in transferring pre-train encoders to the downstream tasks,\nincluding object detection and segmentation.", "journal": ""}
{"doi": "10.48550/arXiv.2309.01219", "date": "2023-09-03", "title": "Siren's Song in the AI Ocean: A Survey on Hallucination in Large Language Models", "authors": "Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu, Tingchen Fu, Xinting Huang, Enbo Zhao, Yu Zhang, Yulong Chen, Longyue Wang, Anh Tuan Luu, Wei Bi, Freda Shi, Shuming Shi", "abstract": "While large language models (LLMs) have demonstrated remarkable capabilities\nacross a range of downstream tasks, a significant concern revolves around their\npropensity to exhibit hallucinations: LLMs occasionally generate content that\ndiverges from the user input, contradicts previously generated context, or\nmisaligns with established world knowledge. This phenomenon poses a substantial\nchallenge to the reliability of LLMs in real-world scenarios. In this paper, we\nsurvey recent efforts on the detection, explanation, and mitigation of\nhallucination, with an emphasis on the unique challenges posed by LLMs. We\npresent taxonomies of the LLM hallucination phenomena and evaluation\nbenchmarks, analyze existing approaches aiming at mitigating LLM hallucination,\nand discuss potential directions for future research.", "journal": ""}
{"doi": "10.48550/arXiv.2309.01245", "date": "2023-09-03", "title": "Representations Matter: Embedding Modes of Large Language Models using Dynamic Mode Decomposition", "authors": "Mohamed Akrout", "abstract": "Existing large language models (LLMs) are known for generating \"hallucinated\"\ncontent, namely a fabricated text of plausibly looking, yet unfounded, facts.\nTo identify when these hallucination scenarios occur, we examine the properties\nof the generated text in the embedding space. Specifically, we draw inspiration\nfrom the dynamic mode decomposition (DMD) tool in analyzing the pattern\nevolution of text embeddings across sentences. We empirically demonstrate how\nthe spectrum of sentence embeddings over paragraphs is constantly low-rank for\nthe generated text, unlike that of the ground-truth text. Importantly, we find\nthat evaluation cases having LLM hallucinations correspond to ground-truth\nembedding patterns with a higher number of modes being poorly approximated by\nthe few modes associated with LLM embedding patterns. In analogy to near-field\nelectromagnetic evanescent waves, the embedding DMD eigenmodes of the generated\ntext with hallucinations vanishes quickly across sentences as opposed to those\nof the ground-truth text. This suggests that the hallucinations result from\nboth the generation techniques and the underlying representation.", "journal": ""}
{"doi": "10.48550/arXiv.2309.11064", "date": "2023-09-20", "title": "Exploring the Relationship between LLM Hallucinations and Prompt Linguistic Nuances: Readability, Formality, and Concreteness", "authors": "Vipula Rawte, Prachi Priya, S. M Towhidul Islam Tonmoy, S M Mehedi Zaman, Amit Sheth, Amitava Das", "abstract": "As Large Language Models (LLMs) have advanced, they have brought forth new\nchallenges, with one of the prominent issues being LLM hallucination. While\nvarious mitigation techniques are emerging to address hallucination, it is\nequally crucial to delve into its underlying causes. Consequently, in this\npreliminary exploratory investigation, we examine how linguistic factors in\nprompts, specifically readability, formality, and concreteness, influence the\noccurrence of hallucinations. Our experimental results suggest that prompts\ncharacterized by greater formality and concreteness tend to result in reduced\nhallucination. However, the outcomes pertaining to readability are somewhat\ninconclusive, showing a mixed pattern.", "journal": ""}
{"doi": "10.48550/arXiv.2310.06271", "date": "2023-10-10", "title": "Towards Mitigating Hallucination in Large Language Models via Self-Reflection", "authors": "Ziwei Ji, Tiezheng Yu, Yan Xu, Nayeon Lee, Etsuko Ishii, Pascale Fung", "abstract": "Large language models (LLMs) have shown promise for generative and\nknowledge-intensive tasks including question-answering (QA) tasks. However, the\npractical deployment still faces challenges, notably the issue of\n\"hallucination\", where models generate plausible-sounding but unfaithful or\nnonsensical information. This issue becomes particularly critical in the\nmedical domain due to the uncommon professional concepts and potential social\nrisks involved. This paper analyses the phenomenon of hallucination in medical\ngenerative QA systems using widely adopted LLMs and datasets. Our investigation\ncenters on the identification and comprehension of common problematic answers,\nwith a specific emphasis on hallucination. To tackle this challenge, we present\nan interactive self-reflection methodology that incorporates knowledge\nacquisition and answer generation. Through this feedback process, our approach\nsteadily enhances the factuality, consistency, and entailment of the generated\nanswers. Consequently, we harness the interactivity and multitasking ability of\nLLMs and produce progressively more precise and accurate answers. Experimental\nresults on both automatic and human evaluation demonstrate the superiority of\nour approach in hallucination reduction compared to baselines.", "journal": ""}
{"doi": "10.48550/arXiv.2310.06498", "date": "2023-10-10", "title": "A New Benchmark and Reverse Validation Method for Passage-level Hallucination Detection", "authors": "Shiping Yang, Renliang Sun, Xiaojun Wan", "abstract": "Large Language Models (LLMs) have shown their ability to collaborate\neffectively with humans in real-world scenarios. However, LLMs are apt to\ngenerate hallucinations, i.e., makeup incorrect text and unverified\ninformation, which can cause significant damage when deployed for\nmission-critical tasks. In this paper, we propose a self-check approach based\non reverse validation to detect factual errors automatically in a zero-resource\nfashion. To facilitate future studies and assess different methods, we\nconstruct a hallucination detection benchmark named PHD, which is generated by\nChatGPT and annotated by human annotators. Contrasting previous studies of\nzero-resource hallucination detection, our method and benchmark concentrate on\npassage-level detection instead of sentence-level. We empirically evaluate our\nmethod and existing zero-resource detection methods on two datasets. The\nexperimental results demonstrate that the proposed method considerably\noutperforms the baselines while costing fewer tokens and less time.\nFurthermore, we manually analyze some hallucination cases that LLM failed to\ncapture, revealing the shared limitation of zero-resource methods.", "journal": ""}
{"doi": "10.48550/arXiv.2310.16176", "date": "2023-10-24", "title": "Correction with Backtracking Reduces Hallucination in Summarization", "authors": "Zhenzhen Liu, Chao Wan, Varsha Kishore, Jin Peng Zhou, Minmin Chen, Kilian Q. Weinberger", "abstract": "Abstractive summarization aims at generating natural language summaries of a\nsource document that are succinct while preserving the important elements.\nDespite recent advances, neural text summarization models are known to be\nsusceptible to hallucinating (or more correctly confabulating), that is to\nproduce summaries with details that are not grounded in the source document. In\nthis paper, we introduce a simple yet efficient technique, CoBa, to reduce\nhallucination in abstractive summarization. The approach is based on two steps:\nhallucination detection and mitigation. We show that the former can be achieved\nthrough measuring simple statistics about conditional word probabilities and\ndistance to context words. Further, we demonstrate that straight-forward\nbacktracking is surprisingly effective at mitigation. We thoroughly evaluate\nthe proposed method with prior art on three benchmark datasets for text\nsummarization. The results show that CoBa is effective and efficient in\nreducing hallucination, and offers great adaptability and flexibility. Code can\nbe found at https://github.com/zhenzhel/CoBa.", "journal": ""}
{"doi": "10.48550/arXiv.2311.08117", "date": "2023-11-14", "title": "Insights into Classifying and Mitigating LLMs' Hallucinations", "authors": "Alessandro Bruno, Pier Luigi Mazzeo, Aladine Chetouani, Marouane Tliba, Mohamed Amine Kerkouri", "abstract": "The widespread adoption of large language models (LLMs) across diverse AI\napplications is proof of the outstanding achievements obtained in several\ntasks, such as text mining, text generation, and question answering. However,\nLLMs are not exempt from drawbacks. One of the most concerning aspects regards\nthe emerging problematic phenomena known as \"Hallucinations\". They manifest in\ntext generation systems, particularly in question-answering systems reliant on\nLLMs, potentially resulting in false or misleading information propagation.\nThis paper delves into the underlying causes of AI hallucination and elucidates\nits significance in artificial intelligence. In particular, Hallucination\nclassification is tackled over several tasks (Machine Translation, Question and\nAnswer, Dialog Systems, Summarisation Systems, Knowledge Graph with LLMs, and\nVisual Question Answer). Additionally, we explore potential strategies to\nmitigate hallucinations, aiming to enhance the overall reliability of LLMs. Our\nresearch addresses this critical issue within the HeReFaNMi (Health-Related\nFake News Mitigation) project, generously supported by NGI Search, dedicated to\ncombating Health-Related Fake News dissemination on the Internet. This\nendeavour represents a concerted effort to safeguard the integrity of\ninformation dissemination in an age of evolving AI technologies.", "journal": ""}
{"doi": "10.48550/arXiv.2311.10961", "date": "2023-11-18", "title": "Journey of Hallucination-minimized Generative AI Solutions for Financial Decision Makers", "authors": "Sohini Roychowdhury", "abstract": "Generative AI has significantly reduced the entry barrier to the domain of AI\nowing to the ease of use and core capabilities of automation, translation, and\nintelligent actions in our day to day lives. Currently, Large language models\n(LLMs) that power such chatbots are being utilized primarily for their\nautomation capabilities for software monitoring, report generation etc. and for\nspecific personalized question answering capabilities, on a limited scope and\nscale. One major limitation of the currently evolving family of LLMs is\n'hallucinations', wherein inaccurate responses are reported as factual.\nHallucinations are primarily caused by biased training data, ambiguous prompts\nand inaccurate LLM parameters, and they majorly occur while combining\nmathematical facts with language-based context. Thus, monitoring and\ncontrolling for hallucinations becomes necessary when designing solutions that\nare meant for decision makers. In this work we present the three major stages\nin the journey of designing hallucination-minimized LLM-based solutions that\nare specialized for the decision makers of the financial domain, namely:\nprototyping, scaling and LLM evolution using human feedback. These three stages\nand the novel data to answer generation modules presented in this work are\nnecessary to ensure that the Generative AI chatbots, autonomous reports and\nalerts are reliable and high-quality to aid key decision-making processes.", "journal": ""}
{"doi": "10.48550/arXiv.2311.13230", "date": "2023-11-22", "title": "Enhancing Uncertainty-Based Hallucination Detection with Stronger Focus", "authors": "Tianhang Zhang, Lin Qiu, Qipeng Guo, Cheng Deng, Yue Zhang, Zheng Zhang, Chenghu Zhou, Xinbing Wang, Luoyi Fu", "abstract": "Large Language Models (LLMs) have gained significant popularity for their\nimpressive performance across diverse fields. However, LLMs are prone to\nhallucinate untruthful or nonsensical outputs that fail to meet user\nexpectations in many real-world applications. Existing works for detecting\nhallucinations in LLMs either rely on external knowledge for reference\nretrieval or require sampling multiple responses from the LLM for consistency\nverification, making these methods costly and inefficient. In this paper, we\npropose a novel reference-free, uncertainty-based method for detecting\nhallucinations in LLMs. Our approach imitates human focus in factuality\nchecking from three aspects: 1) focus on the most informative and important\nkeywords in the given text; 2) focus on the unreliable tokens in historical\ncontext which may lead to a cascade of hallucinations; and 3) focus on the\ntoken properties such as token type and token frequency. Experimental results\non relevant datasets demonstrate the effectiveness of our proposed method,\nwhich achieves state-of-the-art performance across all the evaluation metrics\nand eliminates the need for additional information.", "journal": ""}
{"doi": "10.48550/arXiv.2311.13314", "date": "2023-11-22", "title": "Mitigating Large Language Model Hallucinations via Autonomous Knowledge Graph-based Retrofitting", "authors": "Xinyan Guan, Yanjiang Liu, Hongyu Lin, Yaojie Lu, Ben He, Xianpei Han, Le Sun", "abstract": "Incorporating factual knowledge in knowledge graph is regarded as a promising\napproach for mitigating the hallucination of large language models (LLMs).\nExisting methods usually only use the user's input to query the knowledge\ngraph, thus failing to address the factual hallucination generated by LLMs\nduring its reasoning process. To address this problem, this paper proposes\nKnowledge Graph-based Retrofitting (KGR), a new framework that incorporates\nLLMs with KGs to mitigate factual hallucination during the reasoning process by\nretrofitting the initial draft responses of LLMs based on the factual knowledge\nstored in KGs. Specifically, KGR leverages LLMs to extract, select, validate,\nand retrofit factual statements within the model-generated responses, which\nenables an autonomous knowledge verifying and refining procedure without any\nadditional manual efforts. Experiments show that KGR can significantly improve\nthe performance of LLMs on factual QA benchmarks especially when involving\ncomplex reasoning processes, which demonstrates the necessity and effectiveness\nof KGR in mitigating hallucination and enhancing the reliability of LLMs.", "journal": ""}
{"doi": "10.48550/arXiv.2311.16922", "date": "2023-11-28", "title": "Mitigating Object Hallucinations in Large Vision-Language Models through Visual Contrastive Decoding", "authors": "Sicong Leng, Hang Zhang, Guanzheng Chen, Xin Li, Shijian Lu, Chunyan Miao, Lidong Bing", "abstract": "Large Vision-Language Models (LVLMs) have advanced considerably, intertwining\nvisual recognition and language understanding to generate content that is not\nonly coherent but also contextually attuned. Despite their success, LVLMs still\nsuffer from the issue of object hallucinations, where models generate plausible\nyet incorrect outputs that include objects that do not exist in the images. To\nmitigate this issue, we introduce Visual Contrastive Decoding (VCD), a simple\nand training-free method that contrasts output distributions derived from\noriginal and distorted visual inputs. The proposed VCD effectively reduces the\nover-reliance on statistical bias and unimodal priors, two essential causes of\nobject hallucinations. This adjustment ensures the generated content is closely\ngrounded to visual inputs, resulting in contextually accurate outputs. Our\nexperiments show that VCD, without either additional training or the usage of\nexternal tools, significantly mitigates the object hallucination issue across\ndifferent LVLM families. Beyond mitigating object hallucinations, VCD also\nexcels in general LVLM benchmarks, highlighting its wide-ranging applicability.", "journal": ""}
{"doi": "10.48550/arXiv.2312.15576", "date": "2023-12-25", "title": "Reducing LLM Hallucinations using Epistemic Neural Networks", "authors": "Shreyas Verma, Kien Tran, Yusuf Ali, Guangyu Min", "abstract": "Reducing and detecting hallucinations in large language models is an open\nresearch problem. In this project, we attempt to leverage recent advances in\nthe field of uncertainty estimation to reduce hallucinations in frozen large\nlanguage models. Epistemic neural networks have recently been proposed to\nimprove output joint distributions for large pre-trained models. ENNs are small\nnetworks attached to large, frozen models to improve the model's joint\ndistributions and uncertainty estimates. In this work, we train an epistemic\nneural network on top of the Llama-2 7B model combined with a contrastive\ndecoding feature enhancement technique. We are the first to train an ENN for\nthe next token prediction task and explore the efficacy of this method in\nreducing hallucinations on the TruthfulQA dataset. In essence, we provide a\nmethod that leverages a pre-trained model's latent embeddings to reduce\nhallucinations.", "journal": ""}
{"doi": "10.48550/arXiv.2312.17249", "date": "2023-12-28", "title": "Do Androids Know They're Only Dreaming of Electric Sheep?", "authors": "Sky CH-Wang, Benjamin Van Durme, Jason Eisner, Chris Kedzie", "abstract": "We design probes trained on the internal representations of a transformer\nlanguage model to predict its hallucinatory behavior on three grounded\ngeneration tasks. To train the probes, we annotate for span-level hallucination\non both sampled (organic) and manually edited (synthetic) reference outputs.\nOur probes are narrowly trained and we find that they are sensitive to their\ntraining domain: they generalize poorly from one task to another or from\nsynthetic to organic hallucinations. However, on in-domain data, they can\nreliably detect hallucinations at many transformer layers, achieving 95% of\ntheir peak performance as early as layer 4. Here, probing proves accurate for\nevaluating hallucination, outperforming several contemporary baselines and even\nsurpassing an expert human annotator in response-level detection F1. Similarly,\non span-level labeling, probes are on par or better than the expert annotator\non two out of three generation tasks. Overall, we find that probing is a\nfeasible and efficient alternative to language model hallucination evaluation\nwhen model states are available.", "journal": ""}
{"doi": "10.48550/arXiv.2401.10768", "date": "2024-01-19", "title": "Knowledge Verification to Nip Hallucination in the Bud", "authors": "Fanqi Wan, Xinting Huang, Leyang Cui, Xiaojun Quan, Wei Bi, Shuming Shi", "abstract": "While large language models (LLMs) have demonstrated exceptional performance\nacross various tasks following human alignment, they may still generate\nresponses that sound plausible but contradict factual knowledge, a phenomenon\nknown as hallucination. In this paper, we demonstrate the feasibility of\nmitigating hallucinations by verifying and minimizing the inconsistency between\nexternal knowledge present in the alignment data and the intrinsic knowledge\nembedded within foundation LLMs. Specifically, we propose a novel approach\ncalled Knowledge Consistent Alignment (KCA), which employs a well-aligned LLM\nto automatically formulate assessments based on external knowledge to evaluate\nthe knowledge boundaries of foundation LLMs. To address knowledge\ninconsistencies in the alignment data, KCA implements several specific\nstrategies to deal with these data instances. We demonstrate the superior\nefficacy of KCA in reducing hallucinations across six benchmarks, utilizing\nfoundation LLMs of varying backbones and scales. This confirms the\neffectiveness of mitigating hallucinations by reducing knowledge inconsistency.\nOur code, model weights, and data are openly accessible at\n\\url{https://github.com/fanqiwan/KCA}.", "journal": ""}
{"doi": "10.48550/arXiv.2402.03744", "date": "2024-02-06", "title": "INSIDE: LLMs' Internal States Retain the Power of Hallucination Detection", "authors": "Chao Chen, Kai Liu, Ze Chen, Yi Gu, Yue Wu, Mingyuan Tao, Zhihang Fu, Jieping Ye", "abstract": "Knowledge hallucination have raised widespread concerns for the security and\nreliability of deployed LLMs. Previous efforts in detecting hallucinations have\nbeen employed at logit-level uncertainty estimation or language-level\nself-consistency evaluation, where the semantic information is inevitably lost\nduring the token-decoding procedure. Thus, we propose to explore the dense\nsemantic information retained within LLMs' \\textbf{IN}ternal \\textbf{S}tates\nfor halluc\\textbf{I}nation \\textbf{DE}tection (\\textbf{INSIDE}). In particular,\na simple yet effective \\textbf{EigenScore} metric is proposed to better\nevaluate responses' self-consistency, which exploits the eigenvalues of\nresponses' covariance matrix to measure the semantic consistency/diversity in\nthe dense embedding space. Furthermore, from the perspective of self-consistent\nhallucination detection, a test time feature clipping approach is explored to\ntruncate extreme activations in the internal states, which reduces\noverconfident generations and potentially benefits the detection of\noverconfident hallucinations. Extensive experiments and ablation studies are\nperformed on several popular LLMs and question-answering (QA) benchmarks,\nshowing the effectiveness of our proposal.", "journal": ""}
{"doi": "10.48550/arXiv.2402.11875", "date": "2024-02-19", "title": "M2K-VDG: Model-Adaptive Multimodal Knowledge Anchor Enhanced Video-grounded Dialogue Generation", "authors": "Hongcheng Liu, Pingjie Wang, Yu Wang, Yanfeng Wang", "abstract": "Video-grounded dialogue generation (VDG) requires the system to generate a\nfluent and accurate answer based on multimodal knowledge. However, the\ndifficulty in multimodal knowledge utilization brings serious hallucinations to\nVDG models in practice. Although previous works mitigate the hallucination in a\nvariety of ways, they hardly take notice of the importance of the multimodal\nknowledge anchor answer tokens. In this paper, we reveal via perplexity that\ndifferent VDG models experience varying hallucinations and exhibit diverse\nanchor tokens. Based on this observation, we propose M2K-VDG, a model-adaptive\nmultimodal knowledge anchor enhancement framework for hallucination reduction.\nFurthermore, we introduce the counterfactual effect for more accurate anchor\ntoken detection. The experimental results on three popular benchmarks exhibit\nthe superiority of our approach over state-of-the-art methods, demonstrating\nits effectiveness in reducing hallucinations.", "journal": ""}
{"doi": "10.48550/arXiv.2402.14488", "date": "2024-02-22", "title": "Does the Generator Mind its Contexts? An Analysis of Generative Model Faithfulness under Context Transfer", "authors": "Xinshuo Hu, Baotian Hu, Dongfang Li, Xiaoguang Li, Lifeng Shang", "abstract": "The present study introduces the knowledge-augmented generator, which is\nspecifically designed to produce information that remains grounded in\ncontextual knowledge, regardless of alterations in the context. Previous\nresearch has predominantly focused on examining hallucinations stemming from\nstatic input, such as in the domains of summarization or machine translation.\nHowever, our investigation delves into the faithfulness of generative question\nanswering in the presence of dynamic knowledge. Our objective is to explore the\nexistence of hallucinations arising from parametric memory when contextual\nknowledge undergoes changes, while also analyzing the underlying causes for\ntheir occurrence. In order to efficiently address this issue, we propose a\nstraightforward yet effective measure for detecting such hallucinations.\nIntriguingly, our investigation uncovers that all models exhibit a tendency to\ngenerate previous answers as hallucinations. To gain deeper insights into the\nunderlying causes of this phenomenon, we conduct a series of experiments that\nverify the critical role played by context in hallucination, both during\ntraining and testing, from various perspectives.", "journal": ""}
{"doi": "10.48550/arXiv.2403.02889", "date": "2024-03-05", "title": "InterrogateLLM: Zero-Resource Hallucination Detection in LLM-Generated Answers", "authors": "Yakir Yehuda, Itzik Malkiel, Oren Barkan, Jonathan Weill, Royi Ronen, Noam Koenigstein", "abstract": "Despite the many advances of Large Language Models (LLMs) and their\nunprecedented rapid evolution, their impact and integration into every facet of\nour daily lives is limited due to various reasons. One critical factor\nhindering their widespread adoption is the occurrence of hallucinations, where\nLLMs invent answers that sound realistic, yet drift away from factual truth. In\nthis paper, we present a novel method for detecting hallucinations in large\nlanguage models, which tackles a critical issue in the adoption of these models\nin various real-world scenarios. Through extensive evaluations across multiple\ndatasets and LLMs, including Llama-2, we study the hallucination levels of\nvarious recent LLMs and demonstrate the effectiveness of our method to\nautomatically detect them. Notably, we observe up to 87% hallucinations for\nLlama-2 in a specific experiment, where our method achieves a Balanced Accuracy\nof 81%, all without relying on external knowledge.", "journal": "https://aclanthology.org/2024.acl-long.506/"}
{"doi": "10.48550/arXiv.2403.03558", "date": "2024-03-06", "title": "Benchmarking Hallucination in Large Language Models based on Unanswerable Math Word Problem", "authors": "Yuhong Sun, Zhangyue Yin, Qipeng Guo, Jiawen Wu, Xipeng Qiu, Hui Zhao", "abstract": "Large language models (LLMs) are highly effective in various natural language\nprocessing (NLP) tasks. However, they are susceptible to producing unreliable\nconjectures in ambiguous contexts called hallucination. This paper presents a\nnew method for evaluating LLM hallucination in Question Answering (QA) based on\nthe unanswerable math word problem (MWP). To support this approach, we\ninnovatively develop a dataset called Unanswerable Math Word Problem (UMWP)\nwhich comprises 5200 questions across five categories. We developed an\nevaluation methodology combining text similarity and mathematical expression\ndetection to determine whether LLM considers the question unanswerable. The\nresults of extensive experiments conducted on 31 LLMs, including GPT-3,\nInstructGPT, LLaMA, and Claude, demonstrate that in-context learning and\nreinforcement learning with human feedback (RLHF) training significantly\nenhance the model's ability to avoid hallucination. We show that utilizing MWP\nis a reliable and effective approach to assess hallucination. Our code and data\nare available at https://github.com/Yuki-Asuuna/UMWP.", "journal": ""}
{"doi": "10.48550/arXiv.2403.03750", "date": "2024-03-06", "title": "German also Hallucinates! Inconsistency Detection in News Summaries with the Absinth Dataset", "authors": "Laura Mascarell, Ribin Chalumattu, Annette Rios", "abstract": "The advent of Large Language Models (LLMs) has led to remarkable progress on\na wide range of natural language processing tasks. Despite the advances, these\nlarge-sized models still suffer from hallucinating information in their output,\nwhich poses a major issue in automatic text summarization, as we must guarantee\nthat the generated summary is consistent with the content of the source\ndocument. Previous research addresses the challenging task of detecting\nhallucinations in the output (i.e. inconsistency detection) in order to\nevaluate the faithfulness of the generated summaries. However, these works\nprimarily focus on English and recent multilingual approaches lack German data.\nThis work presents absinth, a manually annotated dataset for hallucination\ndetection in German news summarization and explores the capabilities of novel\nopen-source LLMs on this task in both fine-tuning and in-context learning\nsettings. We open-source and release the absinth dataset to foster further\nresearch on hallucination detection in German.", "journal": ""}
{"doi": "10.48550/arXiv.2403.10492", "date": "2024-03-15", "title": "Mitigating Dialogue Hallucination for Large Vision Language Models via Adversarial Instruction Tuning", "authors": "Dongmin Park, Zhaofang Qian, Guangxing Han, Ser-Nam Lim", "abstract": "Mitigating hallucinations of Large Vision Language Models,(LVLMs) is crucial\nto enhance their reliability for general-purpose assistants. This paper shows\nthat such hallucinations of LVLMs can be significantly exacerbated by preceding\nuser-system dialogues. To precisely measure this, we first present an\nevaluation benchmark by extending popular multi-modal benchmark datasets with\nprepended hallucinatory dialogues powered by our novel Adversarial Question\nGenerator (AQG), which can automatically generate image-related yet adversarial\ndialogues by adopting adversarial attacks on LVLMs. On our benchmark, the\nzero-shot performance of state-of-the-art LVLMs drops significantly for both\nthe VQA and Captioning tasks. Next, we further reveal this hallucination is\nmainly due to the prediction bias toward preceding dialogues rather than visual\ncontent. To reduce this bias, we propose Adversarial Instruction Tuning (AIT)\nthat robustly fine-tunes LVLMs against hallucinatory dialogues. Extensive\nexperiments show our proposed approach successfully reduces dialogue\nhallucination while maintaining performance.", "journal": ""}
{"doi": "10.48550/arXiv.2404.04845", "date": "2024-04-07", "title": "SLPL SHROOM at SemEval2024 Task 06: A comprehensive study on models ability to detect hallucination", "authors": "Pouya Fallah, Soroush Gooran, Mohammad Jafarinasab, Pouya Sadeghi, Reza Farnia, Amirreza Tarabkhah, Zainab Sadat Taghavi, Hossein Sameti", "abstract": "Language models, particularly generative models, are susceptible to\nhallucinations, generating outputs that contradict factual knowledge or the\nsource text. This study explores methods for detecting hallucinations in three\nSemEval-2024 Task 6 tasks: Machine Translation, Definition Modeling, and\nParaphrase Generation. We evaluate two methods: semantic similarity between the\ngenerated text and factual references, and an ensemble of language models that\njudge each other's outputs. Our results show that semantic similarity achieves\nmoderate accuracy and correlation scores in trial data, while the ensemble\nmethod offers insights into the complexities of hallucination detection but\nfalls short of expectations. This work highlights the challenges of\nhallucination detection and underscores the need for further research in this\ncritical area.", "journal": ""}
{"doi": "10.48550/arXiv.2405.00611", "date": "2024-05-01", "title": "Addressing Topic Granularity and Hallucination in Large Language Models for Topic Modelling", "authors": "Yida Mu, Peizhen Bai, Kalina Bontcheva, Xingyi Song", "abstract": "Large language models (LLMs) with their strong zero-shot topic extraction\ncapabilities offer an alternative to probabilistic topic modelling and\nclosed-set topic classification approaches. As zero-shot topic extractors, LLMs\nare expected to understand human instructions to generate relevant and\nnon-hallucinated topics based on the given documents. However, LLM-based topic\nmodelling approaches often face difficulties in generating topics with\nadherence to granularity as specified in human instructions, often resulting in\nmany near-duplicate topics. Furthermore, methods for addressing hallucinated\ntopics generated by LLMs have not yet been investigated. In this paper, we\nfocus on addressing the issues of topic granularity and hallucinations for\nbetter LLM-based topic modelling. To this end, we introduce a novel approach\nthat leverages Direct Preference Optimisation (DPO) to fine-tune open-source\nLLMs, such as Mistral-7B. Our approach does not rely on traditional human\nannotation to rank preferred answers but employs a reconstruction pipeline to\nmodify raw topics generated by LLMs, thus enabling a fast and efficient\ntraining and inference framework. Comparative experiments show that our\nfine-tuning approach not only significantly improves the LLM's capability to\nproduce more coherent, relevant, and precise topics, but also reduces the\nnumber of hallucinated topics.", "journal": ""}
{"doi": "10.48550/arXiv.2406.00975", "date": "2024-06-03", "title": "Luna: An Evaluation Foundation Model to Catch Language Model Hallucinations with High Accuracy and Low Cost", "authors": "Masha Belyi, Robert Friel, Shuai Shao, Atindriyo Sanyal", "abstract": "Retriever Augmented Generation (RAG) systems have become pivotal in enhancing\nthe capabilities of language models by incorporating external knowledge\nretrieval mechanisms. However, a significant challenge in deploying these\nsystems in industry applications is the detection and mitigation of\nhallucinations: instances where the model generates information that is not\ngrounded in the retrieved context. Addressing this issue is crucial for\nensuring the reliability and accuracy of responses generated by large language\nmodels (LLMs) in diverse industry settings. Current hallucination detection\ntechniques fail to deliver accuracy, low latency, and low cost simultaneously.\nWe introduce Luna: a DeBERTA-large (440M) encoder, finetuned for hallucination\ndetection in RAG settings. We demonstrate that Luna outperforms GPT-3.5 and\ncommercial evaluation frameworks on the hallucination detection task, with 97%\nand 91% reduction in cost and latency, respectively. Luna is lightweight and\ngeneralizes across multiple industry verticals and out-of-domain data, making\nit an ideal candidate for industry LLM applications.", "journal": ""}
{"doi": "10.48550/arXiv.2406.04306", "date": "2024-06-06", "title": "Semantically Diverse Language Generation for Uncertainty Estimation in Language Models", "authors": "Lukas Aichberger, Kajetan Schweighofer, Mykyta Ielanskyi, Sepp Hochreiter", "abstract": "Large language models (LLMs) can suffer from hallucinations when generating\ntext. These hallucinations impede various applications in society and industry\nby making LLMs untrustworthy. Current LLMs generate text in an autoregressive\nfashion by predicting and appending text tokens. When an LLM is uncertain about\nthe semantic meaning of the next tokens to generate, it is likely to start\nhallucinating. Thus, it has been suggested that hallucinations stem from\npredictive uncertainty. We introduce Semantically Diverse Language Generation\n(SDLG) to quantify predictive uncertainty in LLMs. SDLG steers the LLM to\ngenerate semantically diverse yet likely alternatives for an initially\ngenerated text. This approach provides a precise measure of aleatoric semantic\nuncertainty, detecting whether the initial text is likely to be hallucinated.\nExperiments on question-answering tasks demonstrate that SDLG consistently\noutperforms existing methods while being the most computationally efficient,\nsetting a new standard for uncertainty estimation in LLMs.", "journal": ""}
{"doi": "10.48550/arXiv.2406.05494", "date": "2024-06-08", "title": "Investigating and Addressing Hallucinations of LLMs in Tasks Involving Negation", "authors": "Neeraj Varshney, Satyam Raj, Venkatesh Mishra, Agneet Chatterjee, Ritika Sarkar, Amir Saeidi, Chitta Baral", "abstract": "Large Language Models (LLMs) have achieved remarkable performance across a\nwide variety of natural language tasks. However, they have been shown to suffer\nfrom a critical limitation pertinent to 'hallucination' in their output. Recent\nresearch has focused on investigating and addressing this problem for a variety\nof tasks such as biography generation, question answering, abstractive\nsummarization, and dialogue generation. However, the crucial aspect pertaining\nto 'negation' has remained considerably underexplored. Negation is important\nbecause it adds depth and nuance to the understanding of language and is also\ncrucial for logical reasoning and inference. In this work, we address the above\nlimitation and particularly focus on studying the impact of negation in LLM\nhallucinations. Specifically, we study four tasks with negation: 'false premise\ncompletion', 'constrained fact generation', 'multiple choice question\nanswering', and 'fact generation'. We show that open-source state-of-the-art\nLLMs such as LLaMA-2-chat, Vicuna, and Orca-2 hallucinate considerably on all\nthese tasks involving negation which underlines a critical shortcoming of these\nmodels. Addressing this problem, we further study numerous strategies to\nmitigate these hallucinations and demonstrate their impact.", "journal": ""}
{"doi": "10.48550/arXiv.2406.10809", "date": "2024-06-16", "title": "Post-hoc Utterance Refining Method by Entity Mining for Faithful Knowledge Grounded Conversations", "authors": "Yoonna Jang, Suhyune Son, Jeongwoo Lee, Junyoung Son, Yuna Hur, Jungwoo Lim, Hyeonseok Moon, Kisu Yang, Heuiseok Lim", "abstract": "Despite the striking advances in recent language generation performance,\nmodel-generated responses have suffered from the chronic problem of\nhallucinations that are either untrue or unfaithful to a given source.\nEspecially in the task of knowledge grounded conversation, the models are\nrequired to generate informative responses, but hallucinated utterances lead to\nmiscommunication. In particular, entity-level hallucination that causes\ncritical misinformation and undesirable conversation is one of the major\nconcerns. To address this issue, we propose a post-hoc refinement method called\nREM. It aims to enhance the quality and faithfulness of hallucinated utterances\nby refining them based on the source knowledge. If the generated utterance has\na low source-faithfulness score with the given knowledge, REM mines the key\nentities in the knowledge and implicitly uses them for refining the utterances.\nWe verify that our method reduces entity hallucination in the utterance. Also,\nwe show the adaptability and efficacy of REM with extensive experiments and\ngenerative results. Our code is available at https://github.com/YOONNAJANG/REM.", "journal": ""}
{"doi": "10.48550/arXiv.2406.11277", "date": "2024-06-17", "title": "Small Agent Can Also Rock! Empowering Small Language Models as Hallucination Detector", "authors": "Xiaoxue Cheng, Junyi Li, Wayne Xin Zhao, Hongzhi Zhang, Fuzheng Zhang, Di Zhang, Kun Gai, Ji-Rong Wen", "abstract": "Hallucination detection is a challenging task for large language models\n(LLMs), and existing studies heavily rely on powerful closed-source LLMs such\nas GPT-4. In this paper, we propose an autonomous LLM-based agent framework,\ncalled HaluAgent, which enables relatively smaller LLMs (e.g. Baichuan2-Chat\n7B) to actively select suitable tools for detecting multiple hallucination\ntypes such as text, code, and mathematical expression. In HaluAgent, we\nintegrate the LLM, multi-functional toolbox, and design a fine-grained\nthree-stage detection framework along with memory mechanism. To facilitate the\neffectiveness of HaluAgent, we leverage existing Chinese and English datasets\nto synthesize detection trajectories for fine-tuning, which endows HaluAgent\nwith the capability for bilingual hallucination detection. Extensive\nexperiments demonstrate that only using 2K samples for tuning LLMs, HaluAgent\ncan perform hallucination detection on various types of tasks and datasets,\nachieving performance comparable to or even higher than GPT-4 without tool\nenhancements on both in-domain and out-of-domain datasets. We release our\ndataset and code at https://github.com/RUCAIBox/HaluAgent.", "journal": ""}
{"doi": "10.48550/arXiv.2406.17260", "date": "2024-06-25", "title": "Mitigating Hallucination in Fictional Character Role-Play", "authors": "Nafis Sadeq, Zhouhang Xie, Byungkyu Kang, Prarit Lamba, Xiang Gao, Julian McAuley", "abstract": "Role-playing has wide-ranging applications in customer support, embodied\nagents, and computational social science. The influence of parametric world\nknowledge of large language models (LLMs) often causes role-playing characters\nto act out of character and to hallucinate about things outside the scope of\ntheir knowledge. In this work, we focus on the evaluation and mitigation of\nhallucination in fictional character role-play. We introduce a dataset with\nover 2,000 characters and 72,000 interviews, including 18,000 adversarial\nquestions. We propose RoleFact, a role-playing method that mitigates\nhallucination by modulating the influence of parametric knowledge using a\npre-calibrated confidence threshold. Experiments show that the proposed method\nimproves the factual precision of generated responses by 18% for adversarial\nquestions with a 44% reduction in temporal hallucination for time-sensitive\ninterviews. The code and the dataset are available at\nhttps://github.com/NafisSadeq/rolefact.git.", "journal": ""}
{"doi": "10.48550/arXiv.2407.02352", "date": "2024-07-02", "title": "Pelican: Correcting Hallucination in Vision-LLMs via Claim Decomposition and Program of Thought Verification", "authors": "Pritish Sahu, Karan Sikka, Ajay Divakaran", "abstract": "Large Visual Language Models (LVLMs) struggle with hallucinations in visual\ninstruction following task(s), limiting their trustworthiness and real-world\napplicability. We propose Pelican -- a novel framework designed to detect and\nmitigate hallucinations through claim verification. Pelican first decomposes\nthe visual claim into a chain of sub-claims based on first-order predicates.\nThese sub-claims consist of (predicate, question) pairs and can be\nconceptualized as nodes of a computational graph. We then use\nProgram-of-Thought prompting to generate Python code for answering these\nquestions through flexible composition of external tools. Pelican improves over\nprior work by introducing (1) intermediate variables for precise grounding of\nobject instances, and (2) shared computation for answering the sub-question to\nenable adaptive corrections and inconsistency identification. We finally use\nreasoning abilities of LLMs to verify the correctness of the claim by\nconsidering the consistency and confidence of the (question, answer) pairs from\neach sub-claim. Our experiments reveal a drop in hallucination rate by ~ 8% -\n32% across various baseline LVLMs and a 27% drop compared to approaches\nproposed for hallucination mitigation on MMHal-Bench. Results on two other\nbenchmarks further corroborate our results.", "journal": ""}
{"doi": "10.48550/arXiv.2407.06071", "date": "2024-07-08", "title": "From Loops to Oops: Fallback Behaviors of Language Models Under Uncertainty", "authors": "Maor Ivgi, Ori Yoran, Jonathan Berant, Mor Geva", "abstract": "Large language models (LLMs) often exhibit undesirable behaviors, such as\nhallucinations and sequence repetitions. We propose to view these behaviors as\nfallbacks that models exhibit under epistemic uncertainty, and investigate the\nconnection between them. We categorize fallback behaviors - sequence\nrepetitions, degenerate text, and hallucinations - and extensively analyze them\nin models from the same family that differ by the amount of pretraining tokens,\nparameter count, or the inclusion of instruction-following training. Our\nexperiments reveal a clear and consistent ordering of fallback behaviors,\nacross all these axes: the more advanced an LLM is (i.e., trained on more\ntokens, has more parameters, or instruction-tuned), its fallback behavior\nshifts from sequence repetitions, to degenerate text, and then to\nhallucinations. Moreover, the same ordering is observed during the generation\nof a single sequence, even for the best-performing models; as uncertainty\nincreases, models shift from generating hallucinations to producing degenerate\ntext and finally sequence repetitions. Lastly, we demonstrate that while common\ndecoding techniques, such as random sampling, alleviate unwanted behaviors like\nsequence repetitions, they increase harder-to-detect hallucinations.", "journal": ""}
{"doi": "10.48550/arXiv.2407.08488", "date": "2024-07-11", "title": "Lynx: An Open Source Hallucination Evaluation Model", "authors": "Selvan Sunitha Ravi, Bartosz Mielczarek, Anand Kannappan, Douwe Kiela, Rebecca Qian", "abstract": "Retrieval Augmented Generation (RAG) techniques aim to mitigate\nhallucinations in Large Language Models (LLMs). However, LLMs can still produce\ninformation that is unsupported or contradictory to the retrieved contexts. We\nintroduce LYNX, a SOTA hallucination detection LLM that is capable of advanced\nreasoning on challenging real-world hallucination scenarios. To evaluate LYNX,\nwe present HaluBench, a comprehensive hallucination evaluation benchmark,\nconsisting of 15k samples sourced from various real-world domains. Our\nexperiment results show that LYNX outperforms GPT-4o, Claude-3-Sonnet, and\nclosed and open-source LLM-as-a-judge models on HaluBench. We release LYNX,\nHaluBench and our evaluation code for public access.", "journal": ""}
{"doi": "10.48550/arXiv.2407.17468", "date": "2024-07-24", "title": "WildHallucinations: Evaluating Long-form Factuality in LLMs with Real-World Entity Queries", "authors": "Wenting Zhao, Tanya Goyal, Yu Ying Chiu, Liwei Jiang, Benjamin Newman, Abhilasha Ravichander, Khyathi Chandu, Ronan Le Bras, Claire Cardie, Yuntian Deng, Yejin Choi", "abstract": "While hallucinations of large language models (LLMs) prevail as a major\nchallenge, existing evaluation benchmarks on factuality do not cover the\ndiverse domains of knowledge that the real-world users of LLMs seek information\nabout. To bridge this gap, we introduce WildHallucinations, a benchmark that\nevaluates factuality. It does so by prompting LLMs to generate information\nabout entities mined from user-chatbot conversations in the wild. These\ngenerations are then automatically fact-checked against a systematically\ncurated knowledge source collected from web search. Notably, half of these\nreal-world entities do not have associated Wikipedia pages. We evaluate 118,785\ngenerations from 15 LLMs on 7,919 entities. We find that LLMs consistently\nhallucinate more on entities without Wikipedia pages and exhibit varying\nhallucination rates across different domains. Finally, given the same base\nmodels, adding a retrieval component only slightly reduces hallucinations but\ndoes not eliminate hallucinations.", "journal": ""}
{"doi": "10.48550/arXiv.2408.05767", "date": "2024-08-11", "title": "Reference-free Hallucination Detection for Large Vision-Language Models", "authors": "Qing Li, Jiahui Geng, Chenyang Lyu, Derui Zhu, Maxim Panov, Fakhri Karray", "abstract": "Large vision-language models (LVLMs) have made significant progress in recent\nyears. While LVLMs exhibit excellent ability in language understanding,\nquestion answering, and conversations of visual inputs, they are prone to\nproducing hallucinations. While several methods are proposed to evaluate the\nhallucinations in LVLMs, most are reference-based and depend on external tools,\nwhich complicates their practical application. To assess the viability of\nalternative methods, it is critical to understand whether the reference-free\napproaches, which do not rely on any external tools, can efficiently detect\nhallucinations. Therefore, we initiate an exploratory study to demonstrate the\neffectiveness of different reference-free solutions in detecting hallucinations\nin LVLMs. In particular, we conduct an extensive study on three kinds of\ntechniques: uncertainty-based, consistency-based, and supervised uncertainty\nquantification methods on four representative LVLMs across two different tasks.\nThe empirical results show that the reference-free approaches are capable of\neffectively detecting non-factual responses in LVLMs, with the supervised\nuncertainty quantification method outperforming the others, achieving the best\nperformance across different settings.", "journal": ""}
{"doi": "10.48550/arXiv.2409.19255", "date": "2024-09-28", "title": "DENEB: A Hallucination-Robust Automatic Evaluation Metric for Image Captioning", "authors": "Kazuki Matsuda, Yuiga Wada, Komei Sugiura", "abstract": "In this work, we address the challenge of developing automatic evaluation\nmetrics for image captioning, with a particular focus on robustness against\nhallucinations. Existing metrics are often inadequate for handling\nhallucinations, primarily due to their limited ability to compare candidate\ncaptions with multifaceted reference captions. To address this shortcoming, we\npropose DENEB, a novel supervised automatic evaluation metric specifically\nrobust against hallucinations. DENEB incorporates the Sim-Vec Transformer, a\nmechanism that processes multiple references simultaneously, thereby\nefficiently capturing the similarity between an image, a candidate caption, and\nreference captions. To train DENEB, we construct the diverse and balanced\nNebula dataset comprising 32,978 images, paired with human judgments provided\nby 805 annotators. We demonstrated that DENEB achieves state-of-the-art\nperformance among existing LLM-free metrics on the FOIL, Composite,\nFlickr8K-Expert, Flickr8K-CF, Nebula, and PASCAL-50S datasets, validating its\neffectiveness and robustness against hallucinations.", "journal": ""}
{"doi": "10.48550/arXiv.2410.11779", "date": "2024-10-15", "title": "MLLM can see? Dynamic Correction Decoding for Hallucination Mitigation", "authors": "Chenxi Wang, Xiang Chen, Ningyu Zhang, Bozhong Tian, Haoming Xu, Shumin Deng, Huajun Chen", "abstract": "Multimodal Large Language Models (MLLMs) frequently exhibit hallucination\nphenomena, but the underlying reasons remain poorly understood. In this paper,\nwe present an empirical analysis and find that, although MLLMs incorrectly\ngenerate the objects in the final output, they are actually able to recognize\nvisual objects in the preceding layers. We speculate that this may be due to\nthe strong knowledge priors of the language model suppressing the visual\ninformation, leading to hallucinations. Motivated by this, we propose a novel\ndynamic correction decoding method for MLLMs DeCo, which adaptively selects the\nappropriate preceding layers and proportionally integrates knowledge into the\nfinal layer to adjust the output logits. Note that DeCo is model agnostic and\ncan be seamlessly incorporated with various classic decoding strategies and\napplied to different MLLMs. We evaluate DeCo on widely-used benchmarks,\ndemonstrating that it can reduce hallucination rates by a large margin compared\nto baselines, highlighting its potential to mitigate hallucinations. Code is\navailable at https://github.com/zjunlp/DeCo.", "journal": ""}
{"doi": "10.48550/arXiv.2410.14748", "date": "2024-10-17", "title": "ETF: An Entity Tracing Framework for Hallucination Detection in Code Summaries", "authors": "Kishan Maharaj, Vitobha Munigala, Srikanth G. Tamilselvam, Prince Kumar, Sayandeep Sen, Palani Kodeswaran, Abhijit Mishra, Pushpak Bhattacharyya", "abstract": "Recent advancements in large language models (LLMs) have significantly\nenhanced their ability to understand both natural language and code, driving\ntheir use in tasks like natural language-to-code (NL2Code) and code\nsummarization. However, LLMs are prone to hallucination-outputs that stray from\nintended meanings. Detecting hallucinations in code summarization is especially\ndifficult due to the complex interplay between programming and natural\nlanguages. We introduce a first-of-its-kind dataset with $\\sim$10K samples,\ncurated specifically for hallucination detection in code summarization. We\nfurther propose a novel Entity Tracing Framework (ETF) that a) utilizes static\nprogram analysis to identify code entities from the program and b) uses LLMs to\nmap and verify these entities and their intents within generated code\nsummaries. Our experimental analysis demonstrates the effectiveness of the\nframework, leading to a 0.73 F1 score. This approach provides an interpretable\nmethod for detecting hallucinations by grounding entities, allowing us to\nevaluate summary accuracy.", "journal": ""}
{"doi": "10.48550/arXiv.2410.15116", "date": "2024-10-19", "title": "Coarse-to-Fine Highlighting: Reducing Knowledge Hallucination in Large Language Models", "authors": "Qitan Lv, Jie Wang, Hanzhu Chen, Bin Li, Yongdong Zhang, Feng Wu", "abstract": "Generation of plausible but incorrect factual information, often termed\nhallucination, has attracted significant research interest. Retrieval-augmented\nlanguage model (RALM) -- which enhances models with up-to-date knowledge --\nemerges as a promising method to reduce hallucination. However, existing RALMs\nmay instead exacerbate hallucination when retrieving lengthy contexts. To\naddress this challenge, we propose COFT, a novel\n\\textbf{CO}arse-to-\\textbf{F}ine highligh\\textbf{T}ing method to focus on\ndifferent granularity-level key texts, thereby avoiding getting lost in lengthy\ncontexts. Specifically, COFT consists of three components: \\textit{recaller},\n\\textit{scorer}, and \\textit{selector}. First, \\textit{recaller} applies a\nknowledge graph to extract potential key entities in a given context. Second,\n\\textit{scorer} measures the importance of each entity by calculating its\ncontextual weight. Finally, \\textit{selector} selects high contextual weight\nentities with a dynamic threshold algorithm and highlights the corresponding\nparagraphs, sentences, or words in a coarse-to-fine manner. Extensive\nexperiments on the knowledge hallucination benchmark demonstrate the\neffectiveness of COFT, leading to a superior performance over $30\\%$ in the F1\nscore metric. Moreover, COFT also exhibits remarkable versatility across\nvarious long-form tasks, such as reading comprehension and question answering.", "journal": ""}
{"doi": "10.48550/arXiv.2410.18860", "date": "2024-10-24", "title": "DeCoRe: Decoding by Contrasting Retrieval Heads to Mitigate Hallucinations", "authors": "Aryo Pradipta Gema, Chen Jin, Ahmed Abdulaal, Tom Diethe, Philip Teare, Beatrice Alex, Pasquale Minervini, Amrutha Saseendran", "abstract": "Large Language Models (LLMs) often hallucinate, producing unfaithful or\nfactually incorrect outputs by misrepresenting the provided context or\nincorrectly recalling internal knowledge. Recent studies have identified\nspecific attention heads within the Transformer architecture, known as\nretrieval heads, responsible for extracting relevant contextual information. We\nhypothesise that masking these retrieval heads can induce hallucinations and\nthat contrasting the outputs of the base LLM and the masked LLM can reduce\nhallucinations. To this end, we propose Decoding by Contrasting Retrieval Heads\n(DeCoRe), a novel training-free decoding strategy that amplifies information\nfound in the context and model parameters. DeCoRe mitigates potentially\nhallucinated responses by dynamically contrasting the outputs of the base LLM\nand the masked LLM, using conditional entropy as a guide. Our extensive\nexperiments confirm that DeCoRe significantly improves performance on tasks\nrequiring high contextual faithfulness, such as summarisation (XSum by 18.6%),\ninstruction following (MemoTrap by 10.9%), and open-book question answering\n(NQ-Open by 2.4% and NQ-Swap by 5.5%).", "journal": ""}
{"doi": "10.48550/arXiv.2410.19217", "date": "2024-10-24", "title": "No Free Lunch: Fundamental Limits of Learning Non-Hallucinating Generative Models", "authors": "Changlong Wu, Ananth Grama, Wojciech Szpankowski", "abstract": "Generative models have shown impressive capabilities in synthesizing\nhigh-quality outputs across various domains. However, a persistent challenge is\nthe occurrence of \"hallucinations\", where the model produces outputs that are\nplausible but invalid. While empirical strategies have been explored to\nmitigate this issue, a rigorous theoretical understanding remains elusive. In\nthis paper, we develop a theoretical framework to analyze the learnability of\nnon-hallucinating generative models from a learning-theoretic perspective. Our\nresults reveal that non-hallucinating learning is statistically impossible when\nrelying solely on the training dataset, even for a hypothesis class of size two\nand when the entire training set is truthful. To overcome these limitations, we\nshow that incorporating inductive biases aligned with the actual facts into the\nlearning process is essential. We provide a systematic approach to achieve this\nby restricting the facts set to a concept class of finite VC-dimension and\ndemonstrate its effectiveness under various learning paradigms. Although our\nfindings are primarily conceptual, they represent a first step towards a\nprincipled approach to addressing hallucinations in learning generative models.", "journal": "International Conference on Learning Representations (ICLR 2025).\n  URL: https://openreview.net/pdf?id=OwNoTs2r8e"}
{"doi": "10.48550/arXiv.2410.20024", "date": "2024-10-26", "title": "Beyond Fine-Tuning: Effective Strategies for Mitigating Hallucinations in Large Language Models for Data Analytics", "authors": "Mikhail Rumiantsau, Aliaksei Vertsel, Ilya Hrytsuk, Isaiah Ballah", "abstract": "Large Language Models (LLMs) have become increasingly important in natural\nlanguage processing, enabling advanced data analytics through natural language\nqueries. However, these models often generate \"hallucinations\"-inaccurate or\nfabricated information-that can undermine their reliability in critical\ndata-driven decision-making. Addressing the challenge of hallucinations is\nessential to improve the accuracy and trustworthiness of LLMs in processing\nnatural language queries. This research focuses on mitigating hallucinations in\nLLMs, specifically within the context of data analytics. We introduce and\nevaluate four targeted strategies: Structured Output Generation, Strict Rules\nEnforcement, System Prompt Enhancements, and Semantic Layer Integration. Our\nfindings show that these methods are more effective than traditional\nfine-tuning approaches in reducing hallucinations, offering a more reliable\nframework for deploying LLMs in natural language queries for data analytics.\nThis research demonstrates the potential of these strategies to enhance the\naccuracy of LLM-driven data queries, ensuring more dependable results in\ndata-driven environments.", "journal": ""}
{"doi": "10.48550/arXiv.2411.05270", "date": "2024-11-08", "title": "Seeing Through the Fog: A Cost-Effectiveness Analysis of Hallucination Detection Systems", "authors": "Alexander Thomas, Seth Rosen, Vishnu Vettrivel", "abstract": "This paper presents a comparative analysis of hallucination detection systems\nfor AI, focusing on automatic summarization and question answering tasks for\nLarge Language Models (LLMs). We evaluate different hallucination detection\nsystems using the diagnostic odds ratio (DOR) and cost-effectiveness metrics.\nOur results indicate that although advanced models can perform better they come\nat a much higher cost. We also demonstrate how an ideal hallucination detection\nsystem needs to maintain performance across different model sizes. Our findings\nhighlight the importance of choosing a detection system aligned with specific\napplication needs and resource constraints. Future research will explore hybrid\nsystems and automated identification of underperforming components to enhance\nAI reliability and efficiency in detecting and mitigating hallucinations.", "journal": ""}
{"doi": "10.48550/arXiv.2411.07031", "date": "2024-11-11", "title": "Evaluating the Accuracy of Chatbots in Financial Literature", "authors": "Orhan Erdem, Kristi Hassett, Feyzullah Egriboyun", "abstract": "We evaluate the reliability of two chatbots, ChatGPT (4o and o1-preview\nversions), and Gemini Advanced, in providing references on financial literature\nand employing novel methodologies. Alongside the conventional binary approach\ncommonly used in the literature, we developed a nonbinary approach and a\nrecency measure to assess how hallucination rates vary with how recent a topic\nis. After analyzing 150 citations, ChatGPT-4o had a hallucination rate of 20.0%\n(95% CI, 13.6%-26.4%), while the o1-preview had a hallucination rate of 21.3%\n(95% CI, 14.8%-27.9%). In contrast, Gemini Advanced exhibited higher\nhallucination rates: 76.7% (95% CI, 69.9%-83.4%). While hallucination rates\nincreased for more recent topics, this trend was not statistically significant\nfor Gemini Advanced. These findings emphasize the importance of verifying\nchatbot-provided references, particularly in rapidly evolving fields.", "journal": ""}
{"doi": "10.48550/arXiv.2411.09255", "date": "2024-11-14", "title": "DAHL: Domain-specific Automated Hallucination Evaluation of Long-Form Text through a Benchmark Dataset in Biomedicine", "authors": "Jean Seo, Jongwon Lim, Dongjun Jang, Hyopil Shin", "abstract": "We introduce DAHL, a benchmark dataset and automated evaluation system\ndesigned to assess hallucination in long-form text generation, specifically\nwithin the biomedical domain. Our benchmark dataset, meticulously curated from\nbiomedical research papers, consists of 8,573 questions across 29 categories.\nDAHL evaluates fact-conflicting hallucinations in Large Language Models (LLMs)\nby deconstructing responses into atomic units, each representing a single piece\nof information. The accuracy of these responses is averaged to produce the DAHL\nScore, offering a more in-depth evaluation of hallucinations compared to\nprevious methods that rely on multiple-choice tasks. We conduct experiments\nwith 8 different models, finding that larger models tend to hallucinate less;\nhowever, beyond a model size of 7 to 8 billion parameters, further scaling does\nnot significantly improve factual accuracy. The DAHL Score holds potential as\nan efficient alternative to human-annotated preference labels, being able to be\nexpanded to other specialized domains. We release the dataset and code in\npublic.", "journal": ""}
{"doi": "10.48550/arXiv.2411.11919", "date": "2024-11-18", "title": "VL-Uncertainty: Detecting Hallucination in Large Vision-Language Model via Uncertainty Estimation", "authors": "Ruiyang Zhang, Hu Zhang, Zhedong Zheng", "abstract": "Given the higher information load processed by large vision-language models\n(LVLMs) compared to single-modal LLMs, detecting LVLM hallucinations requires\nmore human and time expense, and thus rise a wider safety concerns. In this\npaper, we introduce VL-Uncertainty, the first uncertainty-based framework for\ndetecting hallucinations in LVLMs. Different from most existing methods that\nrequire ground-truth or pseudo annotations, VL-Uncertainty utilizes uncertainty\nas an intrinsic metric. We measure uncertainty by analyzing the prediction\nvariance across semantically equivalent but perturbed prompts, including visual\nand textual data. When LVLMs are highly confident, they provide consistent\nresponses to semantically equivalent queries. However, when uncertain, the\nresponses of the target LVLM become more random. Considering semantically\nsimilar answers with different wordings, we cluster LVLM responses based on\ntheir semantic content and then calculate the cluster distribution entropy as\nthe uncertainty measure to detect hallucination. Our extensive experiments on\n10 LVLMs across four benchmarks, covering both free-form and multi-choice\ntasks, show that VL-Uncertainty significantly outperforms strong baseline\nmethods in hallucination detection.", "journal": ""}
{"doi": "10.48550/arXiv.2411.18672", "date": "2024-11-27", "title": "FactCheXcker: Mitigating Measurement Hallucinations in Chest X-ray Report Generation Models", "authors": "Alice Heiman, Xiaoman Zhang, Emma Chen, Sung Eun Kim, Pranav Rajpurkar", "abstract": "Medical vision-language models often struggle with generating accurate\nquantitative measurements in radiology reports, leading to hallucinations that\nundermine clinical reliability. We introduce FactCheXcker, a modular framework\nthat de-hallucinates radiology report measurements by leveraging an improved\nquery-code-update paradigm. Specifically, FactCheXcker employs specialized\nmodules and the code generation capabilities of large language models to solve\nmeasurement queries generated based on the original report. After extracting\nmeasurable findings, the results are incorporated into an updated report. We\nevaluate FactCheXcker on endotracheal tube placement, which accounts for an\naverage of 78% of report measurements, using the MIMIC-CXR dataset and 11\nmedical report-generation models. Our results show that FactCheXcker\nsignificantly reduces hallucinations, improves measurement precision, and\nmaintains the quality of the original reports. Specifically, FactCheXcker\nimproves the performance of 10/11 models and achieves an average improvement of\n135.0% in reducing measurement hallucinations measured by mean absolute error.\nCode is available at https://github.com/rajpurkarlab/FactCheXcker.", "journal": ""}
{"doi": "10.48550/arXiv.2411.19187", "date": "2024-11-28", "title": "Beyond Logit Lens: Contextual Embeddings for Robust Hallucination Detection & Grounding in VLMs", "authors": "Anirudh Phukan, Divyansh, Harshit Kumar Morj, Vaishnavi, Apoorv Saxena, Koustava Goswami", "abstract": "The rapid development of Large Multimodal Models (LMMs) has significantly\nadvanced multimodal understanding by harnessing the language abilities of Large\nLanguage Models (LLMs) and integrating modality-specific encoders. However,\nLMMs are plagued by hallucinations that limit their reliability and adoption.\nWhile traditional methods to detect and mitigate these hallucinations often\ninvolve costly training or rely heavily on external models, recent approaches\nutilizing internal model features present a promising alternative. In this\npaper, we critically assess the limitations of the state-of-the-art\ntraining-free technique, the logit lens, in handling generalized visual\nhallucinations. We introduce ContextualLens, a refined method that leverages\ncontextual token embeddings from middle layers of LMMs. This approach\nsignificantly improves hallucination detection and grounding across diverse\ncategories, including actions and OCR, while also excelling in tasks requiring\ncontextual understanding, such as spatial relations and attribute comparison.\nOur novel grounding technique yields highly precise bounding boxes,\nfacilitating a transition from Zero-Shot Object Segmentation to Grounded Visual\nQuestion Answering. Our contributions pave the way for more reliable and\ninterpretable multimodal models.", "journal": ""}
{"doi": "10.48550/arXiv.2412.04235", "date": "2024-12-05", "title": "Addressing Hallucinations with RAG and NMISS in Italian Healthcare LLM Chatbots", "authors": "Maria Paola Priola", "abstract": "I combine detection and mitigation techniques to addresses hallucinations in\nLarge Language Models (LLMs). Mitigation is achieved in a question-answering\nRetrieval-Augmented Generation (RAG) framework while detection is obtained by\nintroducing the Negative Missing Information Scoring System (NMISS), which\naccounts for contextual relevance in responses. While RAG mitigates\nhallucinations by grounding answers in external data, NMISS refines the\nevaluation by identifying cases where traditional metrics incorrectly flag\ncontextually accurate responses as hallucinations. I use Italian health news\narticles as context to evaluate LLM performance. Results show that Gemma2 and\nGPT-4 outperform the other models, with GPT-4 producing answers closely aligned\nwith reference responses. Mid-tier models, such as Llama2, Llama3, and Mistral\nbenefit significantly from NMISS, highlighting their ability to provide richer\ncontextual information. This combined approach offers new insights into the\nreduction and more accurate assessment of hallucinations in LLMs, with\napplications in real-world healthcare tasks and other domains.", "journal": ""}
{"doi": "10.48550/arXiv.2412.14905", "date": "2024-12-19", "title": "Dehallucinating Parallel Context Extension for Retrieval-Augmented Generation", "authors": "Zexiong Ma, Shengnan An, Zeqi Lin, Yanzhen Zou, Jian-Guang Lou, Bing Xie", "abstract": "Large language models (LLMs) are susceptible to generating hallucinated\ninformation, despite the integration of retrieval-augmented generation (RAG).\nParallel context extension (PCE) is a line of research attempting to\neffectively integrating parallel (unordered) contexts, while it still suffers\nfrom hallucinations when adapted to RAG scenarios. In this paper, we propose\nDePaC (Dehallucinating Parallel Context Extension), which alleviates the\nhallucination problem with context-aware negative training and\ninformation-calibrated aggregation. DePaC is designed to alleviate two types of\nin-context hallucination: fact fabrication (i.e., LLMs present claims that are\nnot supported by the contexts) and fact omission (i.e., LLMs fail to present\nclaims that can be supported by the contexts). Specifically, (1) for fact\nfabrication, we apply the context-aware negative training that fine-tunes the\nLLMs with negative supervisions, thus explicitly guiding the LLMs to refuse to\nanswer when contexts are not related to questions; (2) for fact omission, we\npropose the information-calibrated aggregation which prioritizes context\nwindows with higher information increment from their contexts. The experimental\nresults on nine RAG tasks demonstrate that DePaC significantly alleviates the\ntwo types of hallucination and consistently achieves better performances on\nthese tasks.", "journal": ""}
{"doi": "10.48550/arXiv.2412.15264", "date": "2024-12-17", "title": "ReXTrust: A Model for Fine-Grained Hallucination Detection in AI-Generated Radiology Reports", "authors": "Romain Hardy, Sung Eun Kim, Du Hyun Ro, Pranav Rajpurkar", "abstract": "The increasing adoption of AI-generated radiology reports necessitates robust\nmethods for detecting hallucinations--false or unfounded statements that could\nimpact patient care. We present ReXTrust, a novel framework for fine-grained\nhallucination detection in AI-generated radiology reports. Our approach\nleverages sequences of hidden states from large vision-language models to\nproduce finding-level hallucination risk scores. We evaluate ReXTrust on a\nsubset of the MIMIC-CXR dataset and demonstrate superior performance compared\nto existing approaches, achieving an AUROC of 0.8751 across all findings and\n0.8963 on clinically significant findings. Our results show that white-box\napproaches leveraging model hidden states can provide reliable hallucination\ndetection for medical AI systems, potentially improving the safety and\nreliability of automated radiology reporting.", "journal": ""}
{"doi": "10.48550/arXiv.2501.06521", "date": "2025-01-11", "title": "Fine-tuning Large Language Models for Improving Factuality in Legal Question Answering", "authors": "Yinghao Hu, Leilei Gan, Wenyi Xiao, Kun Kuang, Fei Wu", "abstract": "Hallucination, or the generation of incorrect or fabricated information,\nremains a critical challenge in large language models (LLMs), particularly in\nhigh-stake domains such as legal question answering (QA). In order to mitigate\nthe hallucination rate in legal QA, we first introduce a benchmark called\nLegalHalBench and three automatic metrics to evaluate the common hallucinations\nwhen LLMs answer legal questions. We then propose a hallucination mitigation\nmethod that integrates behavior cloning and a novel Hard Sample-aware Iterative\nDirect Preference Optimization (HIPO). We conduct extensive real-data\nexperiments to validate the effectiveness of our approach. Our results\ndemonstrate remarkable improvements in various metrics, including the newly\nproposed Non-Hallucinated Statute Rate, Statute Relevance Rate, Legal Claim\nTruthfulness, as well as traditional metrics such as METEOR, BERTScore,\nROUGE-L, and win rates.", "journal": ""}
{"doi": "10.48550/arXiv.2502.05825", "date": "2025-02-09", "title": "Delta -- Contrastive Decoding Mitigates Text Hallucinations in Large Language Models", "authors": "Cheng Peng Huang, Hao-Yuan Chen", "abstract": "Large language models (LLMs) demonstrate strong capabilities in natural\nlanguage processing but remain prone to hallucinations, generating factually\nincorrect or fabricated content. This issue undermines their reliability,\nparticularly in high-stakes domains such as healthcare and legal advisory. To\naddress this challenge, we propose Delta, an inference-time method that reduces\nhallucinations without requiring model retraining or additional data. Delta\nworks by randomly masking parts of the input prompt and contrasting the output\ndistributions for the original and masked inputs, effectively suppressing\nhallucinations through inference-only computations. We evaluate Delta on\ncontext-rich question-answering benchmarks, achieving absolute improvements of\napproximately 3 and 6 percentage points on SQuAD v1.1 and v2, respectively, and\n7 and 2 percentage points on TriviaQA and Natural Questions under-sampling\ndecoding. Delta also improves the no-answer exact match score on SQuAD v2 by\nover ten percentage points, demonstrating its effectiveness in mitigating\nhallucinations arising from contextual ambiguity. These results highlight Delta\nas a computationally efficient and scalable approach for improving the\nreliability of LLMs in real-world applications.", "journal": ""}
{"doi": "10.48550/arXiv.2502.07905", "date": "2025-02-11", "title": "DeepSeek on a Trip: Inducing Targeted Visual Hallucinations via Representation Vulnerabilities", "authors": "Chashi Mahiul Islam, Samuel Jacob Chacko, Preston Horne, Xiuwen Liu", "abstract": "Multimodal Large Language Models (MLLMs) represent the cutting edge of AI\ntechnology, with DeepSeek models emerging as a leading open-source alternative\noffering competitive performance to closed-source systems. While these models\ndemonstrate remarkable capabilities, their vision-language integration\nmechanisms introduce specific vulnerabilities. We implement an adapted\nembedding manipulation attack on DeepSeek Janus that induces targeted visual\nhallucinations through systematic optimization of image embeddings. Through\nextensive experimentation across COCO, DALL-E 3, and SVIT datasets, we achieve\nhallucination rates of up to 98.0% while maintaining high visual fidelity (SSIM\n> 0.88) of the manipulated images on open-ended questions. Our analysis\ndemonstrates that both 1B and 7B variants of DeepSeek Janus are susceptible to\nthese attacks, with closed-form evaluation showing consistently higher\nhallucination rates compared to open-ended questioning. We introduce a novel\nmulti-prompt hallucination detection framework using LLaMA-3.1 8B Instruct for\nrobust evaluation. The implications of these findings are particularly\nconcerning given DeepSeek's open-source nature and widespread deployment\npotential. This research emphasizes the critical need for embedding-level\nsecurity measures in MLLM deployment pipelines and contributes to the broader\ndiscussion of responsible AI implementation.", "journal": ""}
{"doi": "10.48550/arXiv.2502.08904", "date": "2025-02-13", "title": "MIH-TCCT: Mitigating Inconsistent Hallucinations in LLMs via Event-Driven Text-Code Cyclic Training", "authors": "Xinxin You, Xien Liu, Qixin Sun, Huan Zhang, Kaiyin Zhou, Shaohui Liu, GuoPing Hu, ShiJin Wang, Si Liu, Ji Wu", "abstract": "Recent methodologies utilizing synthetic datasets have aimed to address\ninconsistent hallucinations in large language models (LLMs); however,these\napproaches are primarily tailored to specific tasks, limiting their\ngeneralizability. Inspired by the strong performance of code-trained models in\nlogic-intensive domains, we propose a novel framework that leverages\nevent-based text to generate corresponding code and employs cyclic training to\ntransfer the logical consistency of code to natural language effectively. Our\nmethod significantly reduces inconsistent hallucinations across three leading\nLLMs and two categories of natural language tasks while maintaining overall\nperformance. This framework effectively alleviates hallucinations without\nnecessitating adaptation to downstream tasks, demonstrating generality and\nproviding new perspectives to tackle the challenge of inconsistent\nhallucinations.", "journal": ""}
{"doi": "10.48550/arXiv.2502.12591", "date": "2025-02-18", "title": "CutPaste&Find: Efficient Multimodal Hallucination Detector with Visual-aid Knowledge Base", "authors": "Cong-Duy Nguyen, Xiaobao Wu, Duc Anh Vu, Shuai Zhao, Thong Nguyen, Anh Tuan Luu", "abstract": "Large Vision-Language Models (LVLMs) have demonstrated impressive multimodal\nreasoning capabilities, but they remain susceptible to hallucination,\nparticularly object hallucination where non-existent objects or incorrect\nattributes are fabricated in generated descriptions. Existing detection methods\nachieve strong performance but rely heavily on expensive API calls and\niterative LVLM-based validation, making them impractical for large-scale or\noffline use. To address these limitations, we propose CutPaste\\&Find, a\nlightweight and training-free framework for detecting hallucinations in\nLVLM-generated outputs. Our approach leverages off-the-shelf visual and\nlinguistic modules to perform multi-step verification efficiently without\nrequiring LVLM inference. At the core of our framework is a Visual-aid\nKnowledge Base that encodes rich entity-attribute relationships and associated\nimage representations. We introduce a scaling factor to refine similarity\nscores, mitigating the issue of suboptimal alignment values even for\nground-truth image-text pairs. Comprehensive evaluations on benchmark datasets,\nincluding POPE and R-Bench, demonstrate that CutPaste\\&Find achieves\ncompetitive hallucination detection performance while being significantly more\nefficient and cost-effective than previous methods.", "journal": ""}
{"doi": "10.48550/arXiv.2502.14908", "date": "2025-02-19", "title": "SegSub: Evaluating Robustness to Knowledge Conflicts and Hallucinations in Vision-Language Models", "authors": "Peter Carragher, Nikitha Rao, Abhinand Jha, R Raghav, Kathleen M. Carley", "abstract": "Vision language models (VLM) demonstrate sophisticated multimodal reasoning\nyet are prone to hallucination when confronted with knowledge conflicts,\nimpeding their deployment in information-sensitive contexts. While existing\nresearch addresses robustness in unimodal models, the multimodal domain lacks\nsystematic investigation of cross-modal knowledge conflicts. This research\nintroduces \\segsub, a framework for applying targeted image perturbations to\ninvestigate VLM resilience against knowledge conflicts. Our analysis reveals\ndistinct vulnerability patterns: while VLMs are robust to parametric conflicts\n(20% adherence rates), they exhibit significant weaknesses in identifying\ncounterfactual conditions (<30% accuracy) and resolving source conflicts (<1%\naccuracy). Correlations between contextual richness and hallucination rate (r =\n-0.368, p = 0.003) reveal the kinds of images that are likely to cause\nhallucinations. Through targeted fine-tuning on our benchmark dataset, we\ndemonstrate improvements in VLM knowledge conflict detection, establishing a\nfoundation for developing hallucination-resilient multimodal systems in\ninformation-sensitive environments.", "journal": ""}
{"doi": "10.48550/arXiv.2502.20034", "date": "2025-02-27", "title": "Vision-Encoders (Already) Know What They See: Mitigating Object Hallucination via Simple Fine-Grained CLIPScore", "authors": "Hongseok Oh, Wonseok Hwang", "abstract": "Recently, Large Vision-Language Models (LVLMs) show remarkable performance\nacross various domains. However, these models suffer from object hallucination.\nThis study revisits the previous claim that the primary cause of such\nhallucination lies in the limited representational capacity of the vision\nencoder. Our analysis reveals that the capacity of the vision encoder itself is\nalready adequate for detecting object hallucination. Based on this insight, we\npropose a Fine-grained CLIPScore (F-CLIPScore), a simple yet effective\nevaluation metric that enhances object-level granularity by incorporating text\nembeddings at the noun level. Evaluations on the OHD-Caps benchmark show that\nF-CLIPScore significantly outperforms conventional CLIPScore in accuracy by a\nlarge margin of 39.6\\% without additional training. We further demonstrate that\nF-CLIPScore-based data filtering reduces object hallucination in LVLMs (4.9\\%\nin POPE).", "journal": ""}
{"doi": "10.48550/arXiv.2503.01917", "date": "2025-03-01", "title": "Steer LLM Latents for Hallucination Detection", "authors": "Seongheon Park, Xuefeng Du, Min-Hsuan Yeh, Haobo Wang, Yixuan Li", "abstract": "Hallucinations in LLMs pose a significant concern to their safe deployment in\nreal-world applications. Recent approaches have leveraged the latent space of\nLLMs for hallucination detection, but their embeddings, optimized for\nlinguistic coherence rather than factual accuracy, often fail to clearly\nseparate truthful and hallucinated content. To this end, we propose the\nTruthfulness Separator Vector (TSV), a lightweight and flexible steering vector\nthat reshapes the LLM's representation space during inference to enhance the\nseparation between truthful and hallucinated outputs, without altering model\nparameters. Our two-stage framework first trains TSV on a small set of labeled\nexemplars to form compact and well-separated clusters. It then augments the\nexemplar set with unlabeled LLM generations, employing an optimal\ntransport-based algorithm for pseudo-labeling combined with a confidence-based\nfiltering process. Extensive experiments demonstrate that TSV achieves\nstate-of-the-art performance with minimal labeled data, exhibiting strong\ngeneralization across datasets and providing a practical solution for\nreal-world LLM applications.", "journal": ""}
{"doi": "10.48550/arXiv.2503.01921", "date": "2025-03-02", "title": "NCL-UoR at SemEval-2025 Task 3: Detecting Multilingual Hallucination and Related Observable Overgeneration Text Spans with Modified RefChecker and Modified SeflCheckGPT", "authors": "Jiaying Hong, Thanet Markchom, Jianfei Xu, Tong Wu, Huizhi Liang", "abstract": "SemEval-2025 Task 3 (Mu-SHROOM) focuses on detecting hallucinations in\ncontent generated by various large language models (LLMs) across multiple\nlanguages. This task involves not only identifying the presence of\nhallucinations but also pinpointing their specific occurrences. To tackle this\nchallenge, this study introduces two methods: modified RefChecker and modified\nSelfCheckGPT. The modified RefChecker integrates prompt-based factual\nverification into References, structuring them as claim-based tests rather than\nsingle external knowledge sources. The modified SelfCheckGPT incorporates\nexternal knowledge to overcome its reliance on internal knowledge. In addition,\nboth methods' original prompt designs are enhanced to identify hallucinated\nwords within LLM-generated texts. Experimental results demonstrate the\neffectiveness of the approach, achieving a high ranking on the test dataset in\ndetecting hallucinations across various languages, with an average IoU of\n0.5310 and an average COR of 0.5669.", "journal": ""}
{"doi": "10.48550/arXiv.2503.03106", "date": "2025-03-05", "title": "Monitoring Decoding: Mitigating Hallucination via Evaluating the Factuality of Partial Response during Generation", "authors": "Yurui Chang, Bochuan Cao, Lu Lin", "abstract": "While large language models have demonstrated exceptional performance across\na wide range of tasks, they remain susceptible to hallucinations -- generating\nplausible yet factually incorrect contents. Existing methods to mitigating such\nrisk often rely on sampling multiple full-length generations, which introduces\nsignificant response latency and becomes ineffective when the model\nconsistently produces hallucinated outputs with high confidence. To address\nthese limitations, we introduce Monitoring Decoding (MD), a novel framework\nthat dynamically monitors the generation process and selectively applies\nin-process interventions, focusing on revising crucial tokens responsible for\nhallucinations. Instead of waiting until completion of multiple full-length\ngenerations, we identify hallucination-prone tokens during generation using a\nmonitor function, and further refine these tokens through a tree-based decoding\nstrategy. This approach ensures an enhanced factual accuracy and coherence in\nthe generated output while maintaining efficiency. Experimental results\ndemonstrate that MD consistently outperforms self-consistency-based approaches\nin both effectiveness and efficiency, achieving higher factual accuracy while\nsignificantly reducing computational overhead.", "journal": ""}
{"doi": "10.48550/arXiv.2503.04615", "date": "2025-03-06", "title": "HalluCounter: Reference-free LLM Hallucination Detection in the Wild!", "authors": "Ashok Urlana, Gopichand Kanumolu, Charaka Vinayak Kumar, Bala Mallikarjunarao Garlapati, Rahul Mishra", "abstract": "Response consistency-based, reference-free hallucination detection (RFHD)\nmethods do not depend on internal model states, such as generation\nprobabilities or gradients, which Grey-box models typically rely on but are\ninaccessible in closed-source LLMs. However, their inability to capture\nquery-response alignment patterns often results in lower detection accuracy.\nAdditionally, the lack of large-scale benchmark datasets spanning diverse\ndomains remains a challenge, as most existing datasets are limited in size and\nscope. To this end, we propose HalluCounter, a novel reference-free\nhallucination detection method that utilizes both response-response and\nquery-response consistency and alignment patterns. This enables the training of\na classifier that detects hallucinations and provides a confidence score and an\noptimal response for user queries. Furthermore, we introduce HalluCounterEval,\na benchmark dataset comprising both synthetically generated and human-curated\nsamples across multiple domains. Our method outperforms state-of-the-art\napproaches by a significant margin, achieving over 90\\% average confidence in\nhallucination detection across datasets.", "journal": ""}
{"doi": "10.48550/arXiv.2503.06486", "date": "2025-03-09", "title": "PerturboLLaVA: Reducing Multimodal Hallucinations with Perturbative Visual Training", "authors": "Cong Chen, Mingyu Liu, Chenchen Jing, Yizhou Zhou, Fengyun Rao, Hao Chen, Bo Zhang, Chunhua Shen", "abstract": "This paper aims to address the challenge of hallucinations in Multimodal\nLarge Language Models (MLLMs) particularly for dense image captioning tasks. To\ntackle the challenge, we identify the current lack of a metric that finely\nmeasures the caption quality in concept level. We hereby introduce HalFscore, a\nnovel metric built upon the language graph and is designed to evaluate both the\naccuracy and completeness of dense captions at a granular level. Additionally,\nwe identify the root cause of hallucination as the model's over-reliance on its\nlanguage prior. To address this, we propose PerturboLLaVA, which reduces the\nmodel's reliance on the language prior by incorporating adversarially perturbed\ntext during training. This method enhances the model's focus on visual inputs,\neffectively reducing hallucinations and producing accurate, image-grounded\ndescriptions without incurring additional computational overhead. PerturboLLaVA\nsignificantly improves the fidelity of generated captions, outperforming\nexisting approaches in handling multimodal hallucinations and achieving\nimproved performance across general multimodal benchmarks.", "journal": ""}
{"doi": "10.48550/arXiv.2503.14895", "date": "2025-03-19", "title": "Mitigating Object Hallucinations in MLLMs via Multi-Frequency Perturbations", "authors": "Shuo Li, Jiajun Sun, Guodong Zheng, Xiaoran Fan, Yujiong Shen, Yi Lu, Zhiheng Xi, Yuming Yang, Wenming Tan, Tao Ji, Tao Gui, Qi Zhang, Xuanjing Huang", "abstract": "Recently, multimodal large language models (MLLMs) have demonstrated\nremarkable performance in visual-language tasks. However, the authenticity of\nthe responses generated by MLLMs is often compromised by object hallucinations.\nWe identify that a key cause of these hallucinations is the model's\nover-susceptibility to specific image frequency features in detecting objects.\nIn this paper, we introduce Multi-Frequency Perturbations (MFP), a simple,\ncost-effective, and pluggable method that leverages both low-frequency and\nhigh-frequency features of images to perturb visual feature representations and\nexplicitly suppress redundant frequency-domain features during inference,\nthereby mitigating hallucinations. Experimental results demonstrate that our\nmethod significantly mitigates object hallucinations across various model\narchitectures. Furthermore, as a training-time method, MFP can be combined with\ninference-time methods to achieve state-of-the-art performance on the CHAIR\nbenchmark.", "journal": ""}
{"doi": "10.48550/arXiv.2503.21813", "date": "2025-03-25", "title": "OAEI-LLM-T: A TBox Benchmark Dataset for Understanding Large Language Model Hallucinations in Ontology Matching", "authors": "Zhangcheng Qiang, Kerry Taylor, Weiqing Wang, Jing Jiang", "abstract": "Hallucinations are often inevitable in downstream tasks using large language\nmodels (LLMs). To tackle the substantial challenge of addressing hallucinations\nfor LLM-based ontology matching (OM) systems, we introduce a new benchmark\ndataset OAEI-LLM-T. The dataset evolves from seven TBox datasets in the\nOntology Alignment Evaluation Initiative (OAEI), capturing hallucinations of\nten different LLMs performing OM tasks. These OM-specific hallucinations are\norganised into two primary categories and six sub-categories. We showcase the\nusefulness of the dataset in constructing an LLM leaderboard for OM tasks and\nfor fine-tuning LLMs used in OM tasks.", "journal": ""}
{"doi": "10.48550/arXiv.2504.04335", "date": "2025-04-06", "title": "Hallucination Detection using Multi-View Attention Features", "authors": "Yuya Ogasa, Yuki Arase", "abstract": "This study tackles token-level hallucination detection in outputs of large\nlanguage models. Previous studies revealed that attention exhibits irregular\npatterns when hallucination occurs. Inspired by this, we extract features from\nthe attention matrix that provide complementary views of (a) the average\nattention each token receives, which helps identify whether certain tokens are\noverly influential or ignored, (b) the diversity of attention each token\nreceives, which reveals whether attention is biased toward specific subsets,\nand (c) the diversity of tokens a token attends to during generation, which\nindicates whether the model references a narrow or broad range of information.\nThese features are input to a Transformer-based classifier to conduct\ntoken-level classification to identify hallucinated spans. Experimental results\nindicate that the proposed method outperforms strong baselines on hallucination\ndetection with longer input contexts, i.e., data-to-text and summarization\ntasks.", "journal": ""}
{"doi": "10.48550/arXiv.2504.06438", "date": "2025-04-08", "title": "Don't Let It Hallucinate: Premise Verification via Retrieval-Augmented Logical Reasoning", "authors": "Yuehan Qin, Shawn Li, Yi Nian, Xinyan Velocity Yu, Yue Zhao, Xuezhe Ma", "abstract": "Large language models (LLMs) have shown substantial capacity for generating\nfluent, contextually appropriate responses. However, they can produce\nhallucinated outputs, especially when a user query includes one or more false\npremises-claims that contradict established facts. Such premises can mislead\nLLMs into offering fabricated or misleading details. Existing approaches\ninclude pretraining, fine-tuning, and inference-time techniques that often rely\non access to logits or address hallucinations after they occur. These methods\ntend to be computationally expensive, require extensive training data, or lack\nproactive mechanisms to prevent hallucination before generation, limiting their\nefficiency in real-time applications. We propose a retrieval-based framework\nthat identifies and addresses false premises before generation. Our method\nfirst transforms a user's query into a logical representation, then applies\nretrieval-augmented generation (RAG) to assess the validity of each premise\nusing factual sources. Finally, we incorporate the verification results into\nthe LLM's prompt to maintain factual consistency in the final output.\nExperiments show that this approach effectively reduces hallucinations,\nimproves factual accuracy, and does not require access to model logits or\nlarge-scale fine-tuning.", "journal": ""}
{"doi": "10.48550/arXiv.2504.08020", "date": "2025-04-10", "title": "Learning Fine-grained Domain Generalization via Hyperbolic State Space Hallucination", "authors": "Qi Bi, Jingjun Yi, Haolan Zhan, Wei Ji, Gui-Song Xia", "abstract": "Fine-grained domain generalization (FGDG) aims to learn a fine-grained\nrepresentation that can be well generalized to unseen target domains when only\ntrained on the source domain data. Compared with generic domain generalization,\nFGDG is particularly challenging in that the fine-grained category can be only\ndiscerned by some subtle and tiny patterns. Such patterns are particularly\nfragile under the cross-domain style shifts caused by illumination, color and\netc. To push this frontier, this paper presents a novel Hyperbolic State Space\nHallucination (HSSH) method. It consists of two key components, namely, state\nspace hallucination (SSH) and hyperbolic manifold consistency (HMC). SSH\nenriches the style diversity for the state embeddings by firstly extrapolating\nand then hallucinating the source images. Then, the pre- and post- style\nhallucinate state embeddings are projected into the hyperbolic manifold. The\nhyperbolic state space models the high-order statistics, and allows a better\ndiscernment of the fine-grained patterns. Finally, the hyperbolic distance is\nminimized, so that the impact of style variation on fine-grained patterns can\nbe eliminated. Experiments on three FGDG benchmarks demonstrate its\nstate-of-the-art performance.", "journal": ""}
{"doi": "10.48550/arXiv.2504.12012", "date": "2025-04-16", "title": "Purposefully Induced Psychosis (PIP): Embracing Hallucination as Imagination in Large Language Models", "authors": "Kris Pilcher, Esen K. T\u00fct\u00fcnc\u00fc", "abstract": "Hallucinations in Large Language Models (LLMs) are widely regarded as errors\n- outputs that deviate from factual accuracy. However, in creative or\nexploratory contexts, these \"mistakes\" may represent unexpected avenues for\ninnovation. We introduce Purposefully Induced Psychosis (PIP), a novel approach\nthat amplifies LLM hallucinations for imaginative tasks such as speculative\nfiction, interactive storytelling, and mixed-reality simulations. Drawing on\nHerman Melville's Moby-Dick, where Pip's \"madness\" reveals profound insight, we\nreframe hallucinations as a source of computational imagination rather than a\nflaw. Our method fine-tunes LLMs to encourage speculative, metaphorical, and\nsurreal outputs - hallucinations that are useful when factual accuracy is not\nthe chief objective. Inspired by the consensual illusions of theater and stage\nmagic, PIP situates these creative missteps in contexts where users willingly\nsuspend disbelief, thereby transforming \"errors\" into catalysts for new ways of\nthinking. We discuss potential applications, design principles for ensuring\nuser consent, preliminary observations, and implications for broader AI ethics\nand human-AI collaboration.", "journal": ""}
{"doi": "10.48550/arXiv.2504.13777", "date": "2025-04-18", "title": "Beyond Misinformation: A Conceptual Framework for Studying AI Hallucinations in (Science) Communication", "authors": "Anqi Shao", "abstract": "This paper proposes a conceptual framework for understanding AI\nhallucinations as a distinct form of misinformation. While misinformation\nscholarship has traditionally focused on human intent, generative AI systems\nnow produce false yet plausible outputs absent of such intent. I argue that\nthese AI hallucinations should not be treated merely as technical failures but\nas communication phenomena with social consequences. Drawing on a\nsupply-and-demand model and the concept of distributed agency, the framework\noutlines how hallucinations differ from human-generated misinformation in\nproduction, perception, and institutional response. I conclude by outlining a\nresearch agenda for communication scholars to investigate the emergence,\ndissemination, and audience reception of hallucinated content, with attention\nto macro (institutional), meso (group), and micro (individual) levels. This\nwork urges communication researchers to rethink the boundaries of\nmisinformation theory in light of probabilistic, non-human actors increasingly\nembedded in knowledge production.", "journal": ""}
{"doi": "10.48550/arXiv.2504.19457", "date": "2025-04-28", "title": "Towards Long Context Hallucination Detection", "authors": "Siyi Liu, Kishaloy Halder, Zheng Qi, Wei Xiao, Nikolaos Pappas, Phu Mon Htut, Neha Anna John, Yassine Benajiba, Dan Roth", "abstract": "Large Language Models (LLMs) have demonstrated remarkable performance across\nvarious tasks. However, they are prone to contextual hallucination, generating\ninformation that is either unsubstantiated or contradictory to the given\ncontext. Although many studies have investigated contextual hallucinations in\nLLMs, addressing them in long-context inputs remains an open problem. In this\nwork, we take an initial step toward solving this problem by constructing a\ndataset specifically designed for long-context hallucination detection.\nFurthermore, we propose a novel architecture that enables pre-trained encoder\nmodels, such as BERT, to process long contexts and effectively detect\ncontextual hallucinations through a decomposition and aggregation mechanism.\nOur experimental results show that the proposed architecture significantly\noutperforms previous models of similar size as well as LLM-based models across\nvarious metrics, while providing substantially faster inference.", "journal": ""}
{"doi": "10.48550/arXiv.2505.14101", "date": "2025-05-20", "title": "MultiHal: Multilingual Dataset for Knowledge-Graph Grounded Evaluation of LLM Hallucinations", "authors": "Ernests Lavrinovics, Russa Biswas, Katja Hose, Johannes Bjerva", "abstract": "Large Language Models (LLMs) have inherent limitations of faithfulness and\nfactuality, commonly referred to as hallucinations. Several benchmarks have\nbeen developed that provide a test bed for factuality evaluation within the\ncontext of English-centric datasets, while relying on supplementary informative\ncontext like web links or text passages but ignoring the available structured\nfactual resources. To this end, Knowledge Graphs (KGs) have been identified as\na useful aid for hallucination mitigation, as they provide a structured way to\nrepresent the facts about entities and their relations with minimal linguistic\noverhead. We bridge the lack of KG paths and multilinguality for factual\nlanguage modeling within the existing hallucination evaluation benchmarks and\npropose a KG-based multilingual, multihop benchmark called \\textbf{MultiHal}\nframed for generative text evaluation. As part of our data collection pipeline,\nwe mined 140k KG-paths from open-domain KGs, from which we pruned noisy\nKG-paths, curating a high-quality subset of 25.9k. Our baseline evaluation\nshows an absolute scale increase by approximately 0.12 to 0.36 points for the\nsemantic similarity score in KG-RAG over vanilla QA across multiple languages\nand multiple models, demonstrating the potential of KG integration. We\nanticipate MultiHal will foster future research towards several graph-based\nhallucination mitigation and fact-checking tasks.", "journal": ""}
{"doi": "10.48550/arXiv.2505.17558", "date": "2025-05-23", "title": "Teaching with Lies: Curriculum DPO on Synthetic Negatives for Hallucination Detection", "authors": "Shrey Pandit, Ashwin Vinod, Liu Leqi, Ying Ding", "abstract": "Aligning large language models (LLMs) to accurately detect hallucinations\nremains a significant challenge due to the sophisticated nature of hallucinated\ntext. Recognizing that hallucinated samples typically exhibit higher deceptive\nquality than traditional negative samples, we use these carefully engineered\nhallucinations as negative examples in the DPO alignment procedure. Our method\nincorporates a curriculum learning strategy, gradually transitioning the\ntraining from easier samples, identified based on the greatest reduction in\nprobability scores from independent fact checking models, to progressively\nharder ones. This structured difficulty scaling ensures stable and incremental\nlearning. Experimental evaluation demonstrates that our HaluCheck models,\ntrained with curriculum DPO approach and high quality negative samples,\nsignificantly improves model performance across various metrics, achieving\nimprovements of upto 24% on difficult benchmarks like MedHallu and HaluEval.\nAdditionally, HaluCheck models demonstrate robustness in zero-shot settings,\nsignificantly outperforming larger state-of-the-art models across various\nbenchmarks.", "journal": ""}
{"doi": "10.48550/arXiv.2505.19108", "date": "2025-05-25", "title": "CCHall: A Novel Benchmark for Joint Cross-Lingual and Cross-Modal Hallucinations Detection in Large Language Models", "authors": "Yongheng Zhang, Xu Liu, Ruoxi Zhou, Qiguang Chen, Hao Fei, Wenpeng Lu, Libo Qin", "abstract": "Investigating hallucination issues in large language models (LLMs) within\ncross-lingual and cross-modal scenarios can greatly advance the large-scale\ndeployment in real-world applications. Nevertheless, the current studies are\nlimited to a single scenario, either cross-lingual or cross-modal, leaving a\ngap in the exploration of hallucinations in the joint cross-lingual and\ncross-modal scenarios. Motivated by this, we introduce a novel joint\nCross-lingual and Cross-modal Hallucinations benchmark (CCHall) to fill this\ngap. Specifically, CCHall simultaneously incorporates both cross-lingual and\ncross-modal hallucination scenarios, which can be used to assess the\ncross-lingual and cross-modal capabilities of LLMs. Furthermore, we conduct a\ncomprehensive evaluation on CCHall, exploring both mainstream open-source and\nclosed-source LLMs. The experimental results highlight that current LLMs still\nstruggle with CCHall. We hope CCHall can serve as a valuable resource to assess\nLLMs in joint cross-lingual and cross-modal scenarios.", "journal": ""}
{"doi": "10.48550/arXiv.2505.19678", "date": "2025-05-26", "title": "Grounding Language with Vision: A Conditional Mutual Information Calibrated Decoding Strategy for Reducing Hallucinations in LVLMs", "authors": "Hao Fang, Changle Zhou, Jiawei Kong, Kuofeng Gao, Bin Chen, Tao Liang, Guojun Ma, Shu-Tao Xia", "abstract": "Large Vision-Language Models (LVLMs) are susceptible to hallucinations, where\ngenerated responses seem semantically plausible yet exhibit little or no\nrelevance to the input image. Previous studies reveal that this issue primarily\nstems from LVLMs' over-reliance on language priors while disregarding the\nvisual information during decoding. To alleviate this issue, we introduce a\nnovel Conditional Pointwise Mutual Information (C-PMI) calibrated decoding\nstrategy, which adaptively strengthens the mutual dependency between generated\ntexts and input images to mitigate hallucinations. Unlike existing methods\nsolely focusing on text token sampling, we propose to jointly model the\ncontributions of visual and textual tokens to C-PMI, formulating hallucination\nmitigation as a bi-level optimization problem aimed at maximizing mutual\ninformation. To solve it, we design a token purification mechanism that\ndynamically regulates the decoding process by sampling text tokens remaining\nmaximally relevant to the given image, while simultaneously refining image\ntokens most pertinent to the generated response. Extensive experiments across\nvarious benchmarks reveal that the proposed method significantly reduces\nhallucinations in LVLMs while preserving decoding efficiency.", "journal": ""}
{"doi": "10.48550/arXiv.2505.21786", "date": "2025-05-27", "title": "VeriTrail: Closed-Domain Hallucination Detection with Traceability", "authors": "Dasha Metropolitansky, Jonathan Larson", "abstract": "Even when instructed to adhere to source material, Language Models often\ngenerate unsubstantiated content - a phenomenon known as \"closed-domain\nhallucination.\" This risk is amplified in processes with multiple generative\nsteps (MGS), compared to processes with a single generative step (SGS).\nHowever, due to the greater complexity of MGS processes, we argue that\ndetecting hallucinations in their final outputs is necessary but not\nsufficient: it is equally important to trace where hallucinated content was\nlikely introduced and how faithful content may have been derived from the\nsource through intermediate outputs. To address this need, we present\nVeriTrail, the first closed-domain hallucination detection method designed to\nprovide traceability for both MGS and SGS processes. We also introduce the\nfirst datasets to include all intermediate outputs as well as human annotations\nof final outputs' faithfulness for their respective MGS processes. We\ndemonstrate that VeriTrail outperforms baseline methods on both datasets.", "journal": ""}
{"doi": "10.48550/arXiv.2505.24630", "date": "2025-05-30", "title": "The Hallucination Dilemma: Factuality-Aware Reinforcement Learning for Large Reasoning Models", "authors": "Junyi Li, Hwee Tou Ng", "abstract": "Large language models (LLMs) have significantly advanced in reasoning tasks\nthrough reinforcement learning (RL) optimization, achieving impressive\ncapabilities across various challenging benchmarks. However, our empirical\nanalysis reveals a critical drawback: reasoning-oriented RL fine-tuning\nsignificantly increases the prevalence of hallucinations. We theoretically\nanalyze the RL training dynamics, identifying high-variance gradient,\nentropy-induced randomness, and susceptibility to spurious local optima as key\nfactors leading to hallucinations. To address this drawback, we propose\nFactuality-aware Step-wise Policy Optimization (FSPO), an innovative RL\nfine-tuning algorithm incorporating explicit factuality verification at each\nreasoning step. FSPO leverages automated verification against given evidence to\ndynamically adjust token-level advantage values, incentivizing factual\ncorrectness throughout the reasoning process. Experiments across mathematical\nreasoning and hallucination benchmarks using Qwen2.5 and Llama models\ndemonstrate that FSPO effectively reduces hallucinations while enhancing\nreasoning accuracy, substantially improving both reliability and performance.", "journal": ""}
{"doi": "10.48550/arXiv.2505.24649", "date": "2025-05-30", "title": "BIMA: Bijective Maximum Likelihood Learning Approach to Hallucination Prediction and Mitigation in Large Vision-Language Models", "authors": "Huu-Thien Tran, Thanh-Dat Truong, Khoa Luu", "abstract": "Large vision-language models have become widely adopted to advance in various\ndomains. However, developing a trustworthy system with minimal interpretable\ncharacteristics of large-scale models presents a significant challenge. One of\nthe most prevalent terms associated with the fallacy functions caused by these\nsystems is hallucination, where the language model generates a response that\ndoes not correspond to the visual content. To mitigate this problem, several\napproaches have been developed, and one prominent direction is to ameliorate\nthe decoding process. In this paper, we propose a new Bijective Maximum\nLikelihood Learning (BIMA) approach to hallucination mitigation using\nnormalizing flow theories. The proposed BIMA method can efficiently mitigate\nthe hallucination problem in prevailing vision-language models, resulting in\nsignificant improvements. Notably, BIMA achieves the average F1 score of 85.06%\non POPE benchmark and remarkably reduce CHAIRS and CHAIRI by 7.6% and 2.6%,\nrespectively. To the best of our knowledge, this is one of the first studies\nthat contemplates the bijection means to reduce hallucination induced by large\nvision-language models.", "journal": ""}
{"doi": "10.48550/arXiv.2506.03357", "date": "2025-06-03", "title": "Ask a Local: Detecting Hallucinations With Specialized Model Divergence", "authors": "Aldan Creo, H\u00e9ctor Cerezo-Costas, Pedro Alonso-Doval, Maximiliano Hormaz\u00e1bal-Lagos", "abstract": "Hallucinations in large language models (LLMs) - instances where models\ngenerate plausible but factually incorrect information - present a significant\nchallenge for AI.\n  We introduce \"Ask a Local\", a novel hallucination detection method exploiting\nthe intuition that specialized models exhibit greater surprise when\nencountering domain-specific inaccuracies. Our approach computes divergence\nbetween perplexity distributions of language-specialized models to identify\npotentially hallucinated spans. Our method is particularly well-suited for a\nmultilingual context, as it naturally scales to multiple languages without the\nneed for adaptation, relying on external data sources, or performing training.\nMoreover, we select computationally efficient models, providing a scalable\nsolution that can be applied to a wide range of languages and domains.\n  Our results on a human-annotated question-answer dataset spanning 14\nlanguages demonstrate consistent performance across languages, with\nIntersection-over-Union (IoU) scores around 0.3 and comparable Spearman\ncorrelation values. Our model shows particularly strong performance on Italian\nand Catalan, with IoU scores of 0.42 and 0.38, respectively, while maintaining\ncross-lingual effectiveness without language-specific adaptations. We release\nour code and architecture to facilitate further research in multilingual\nhallucination detection.", "journal": ""}
{"doi": "10.48550/arXiv.2506.06729", "date": "2025-06-07", "title": "Mitigating Object Hallucination via Robust Local Perception Search", "authors": "Zixian Gao, Chao Yang, Zhanhui Zhou, Xing Xu, Chaochao Lu", "abstract": "Recent advancements in Multimodal Large Language Models (MLLMs) have enabled\nthem to effectively integrate vision and language, addressing a variety of\ndownstream tasks. However, despite their significant success, these models\nstill exhibit hallucination phenomena, where the outputs appear plausible but\ndo not align with the content of the images. To mitigate this issue, we\nintroduce Local Perception Search (LPS), a decoding method during inference\nthat is both simple and training-free, yet effectively suppresses\nhallucinations. This method leverages local visual prior information as a value\nfunction to correct the decoding process. Additionally, we observe that the\nimpact of the local visual prior on model performance is more pronounced in\nscenarios with high levels of image noise. Notably, LPS is a plug-and-play\napproach that is compatible with various models. Extensive experiments on\nwidely used hallucination benchmarks and noisy data demonstrate that LPS\nsignificantly reduces the incidence of hallucinations compared to the baseline,\nshowing exceptional performance, particularly in noisy settings.", "journal": ""}
{"doi": "10.48550/arXiv.2506.14766", "date": "2025-06-17", "title": "ASCD: Attention-Steerable Contrastive Decoding for Reducing Hallucination in MLLM", "authors": "Yujun Wang, Jinhe Bi, Yunpu Ma, Soeren Pirk", "abstract": "Multimodal Large Language Model (MLLM) often suffer from hallucinations. They\nover-rely on partial cues and generate incorrect responses. Recently, methods\nlike Visual Contrastive Decoding (VCD) and Instruction Contrastive Decoding\n(ICD) have been proposed to mitigate hallucinations by contrasting predictions\nfrom perturbed or negatively prefixed inputs against original outputs. In this\nwork, we uncover that methods like VCD and ICD fundamentally influence internal\nattention dynamics of the model. This observation suggests that their\neffectiveness may not stem merely from surface-level modifications to logits\nbut from deeper shifts in attention distribution. Inspired by this insight, we\npropose an attention-steerable contrastive decoding framework that directly\nintervenes in attention mechanisms of the model to offer a more principled\napproach to mitigating hallucinations. Our experiments across multiple MLLM\narchitectures and diverse decoding methods demonstrate that our approach\nsignificantly reduces hallucinations and improves the performance on benchmarks\nsuch as POPE, CHAIR, and MMHal-Bench, while simultaneously enhancing\nperformance on standard VQA benchmarks.", "journal": ""}
{"doi": "10.48550/arXiv.2207.03901", "date": "2022-07-08", "title": "Reproducing sensory induced hallucinations via neural fields", "authors": "Cyprien Tamekue, Dario Prandi, Yacine Chitour", "abstract": "Understanding sensory-induced cortical patterns in the primary visual cortex\nV1 is an important challenge both for physiological motivations and for\nimproving our understanding of human perception and visual organisation. In\nthis work, we focus on pattern formation in the visual cortex when the cortical\nactivity is driven by a geometric visual hallucination-like stimulus. In\nparticular, we present a theoretical framework for sensory-induced\nhallucinations which allows one to reproduce novel psychophysical results such\nas the MacKay effect (Nature, 1957) and the Billock and Tsou experiences (PNAS,\n2007).", "journal": ""}
{"doi": "10.48550/arXiv.2311.17080", "date": "2023-11-28", "title": "Combating the \"Sameness\" in AI Art: Reflections on the Interactive AI Installation Fencing Hallucination", "authors": "Weihao Qiu, George Legrady", "abstract": "The article summarizes three types of \"sameness\" issues in Artificial\nIntelligence(AI) art, each occurring at different stages of development in AI\nimage creation tools. Through the Fencing Hallucination project, the article\nreflects on the design of AI art production in alleviating the sense of\nuniformity, maintaining the uniqueness of images from an AI image synthesizer,\nand enhancing the connection between the artworks and the audience. This paper\nendeavors to stimulate the creation of distinctive AI art by recounting the\nefforts and insights derived from the Fencing Hallucination project, all\ndedicated to addressing the issue of \"sameness\".", "journal": ""}
{"doi": "10.48550/arXiv.2401.05827", "date": "2024-01-11", "title": "Hallucination Benchmark in Medical Visual Question Answering", "authors": "Jinge Wu, Yunsoo Kim, Honghan Wu", "abstract": "The recent success of large language and vision models (LLVMs) on vision\nquestion answering (VQA), particularly their applications in medicine\n(Med-VQA), has shown a great potential of realizing effective visual assistants\nfor healthcare. However, these models are not extensively tested on the\nhallucination phenomenon in clinical settings. Here, we created a hallucination\nbenchmark of medical images paired with question-answer sets and conducted a\ncomprehensive evaluation of the state-of-the-art models. The study provides an\nin-depth analysis of current models' limitations and reveals the effectiveness\nof various prompting strategies.", "journal": ""}
{"doi": "10.48550/arXiv.2401.07897", "date": "2024-01-15", "title": "The Pitfalls of Defining Hallucination", "authors": "Kees van Deemter", "abstract": "Despite impressive advances in Natural Language Generation (NLG) and Large\nLanguage Models (LLMs), researchers are still unclear about important aspects\nof NLG evaluation. To substantiate this claim, I examine current\nclassifications of hallucination and omission in Data-text NLG, and I propose a\nlogic-based synthesis of these classfications. I conclude by highlighting some\nremaining limitations of all current thinking about hallucination and by\ndiscussing implications for LLMs.", "journal": ""}
{"doi": "10.48550/arXiv.2402.13331", "date": "2024-02-20", "title": "Enhanced Hallucination Detection in Neural Machine Translation through Simple Detector Aggregation", "authors": "Anas Himmi, Guillaume Staerman, Marine Picot, Pierre Colombo, Nuno M. Guerreiro", "abstract": "Hallucinated translations pose significant threats and safety concerns when\nit comes to the practical deployment of machine translation systems. Previous\nresearch works have identified that detectors exhibit complementary performance\ndifferent detectors excel at detecting different types of hallucinations. In\nthis paper, we propose to address the limitations of individual detectors by\ncombining them and introducing a straightforward method for aggregating\nmultiple detectors. Our results demonstrate the efficacy of our aggregated\ndetector, providing a promising step towards evermore reliable machine\ntranslation systems.", "journal": ""}
{"doi": "10.48550/arXiv.2403.00964", "date": "2024-03-01", "title": "MALTO at SemEval-2024 Task 6: Leveraging Synthetic Data for LLM Hallucination Detection", "authors": "Federico Borra, Claudio Savelli, Giacomo Rosso, Alkis Koudounas, Flavio Giobergia", "abstract": "In Natural Language Generation (NLG), contemporary Large Language Models\n(LLMs) face several challenges, such as generating fluent yet inaccurate\noutputs and reliance on fluency-centric metrics. This often leads to neural\nnetworks exhibiting \"hallucinations\". The SHROOM challenge focuses on\nautomatically identifying these hallucinations in the generated text. To tackle\nthese issues, we introduce two key components, a data augmentation pipeline\nincorporating LLM-assisted pseudo-labelling and sentence rephrasing, and a\nvoting ensemble from three models pre-trained on Natural Language Inference\n(NLI) tasks and fine-tuned on diverse datasets.", "journal": ""}
{"doi": "10.48550/arXiv.2407.05474", "date": "2024-07-07", "title": "Enhancing Hallucination Detection through Perturbation-Based Synthetic Data Generation in System Responses", "authors": "Dongxu Zhang, Varun Gangal, Barrett Martin Lattimer, Yi Yang", "abstract": "Detecting hallucinations in large language model (LLM) outputs is pivotal,\nyet traditional fine-tuning for this classification task is impeded by the\nexpensive and quickly outdated annotation process, especially across numerous\nvertical domains and in the face of rapid LLM advancements. In this study, we\nintroduce an approach that automatically generates both faithful and\nhallucinated outputs by rewriting system responses. Experimental findings\ndemonstrate that a T5-base model, fine-tuned on our generated dataset,\nsurpasses state-of-the-art zero-shot detectors and existing synthetic\ngeneration methods in both accuracy and latency, indicating efficacy of our\napproach.", "journal": ""}
{"doi": "10.48550/arXiv.2409.16658", "date": "2024-09-25", "title": "Pre-trained Language Models Return Distinguishable Probability Distributions to Unfaithfully Hallucinated Texts", "authors": "Taehun Cha, Donghun Lee", "abstract": "In this work, we show the pre-trained language models return distinguishable\ngeneration probability and uncertainty distribution to unfaithfully\nhallucinated texts, regardless of their size and structure. By examining 24\nmodels on 6 data sets, we find out that 88-98% of cases return statistically\nsignificantly distinguishable generation probability and uncertainty\ndistributions. Using this general phenomenon, we showcase a\nhallucination-reducing training algorithm. Our algorithm outperforms other\nbaselines by achieving higher faithfulness metrics while maintaining sound\ngeneral text quality measures.", "journal": ""}
{"doi": "10.48550/arXiv.2412.07965", "date": "2024-12-10", "title": "HalluCana: Fixing LLM Hallucination with A Canary Lookahead", "authors": "Tianyi Li, Erenay Dayanik, Shubhi Tyagi, Andrea Pierleoni", "abstract": "In this paper, we present HalluCana, a canary lookahead to detect and correct\nfactuality hallucinations of Large Language Models (LLMs) in long-form\ngeneration. HalluCana detects and intervenes as soon as traces of hallucination\nemerge, during and even before generation. To support timely detection, we\nexploit the internal factuality representation in the LLM hidden space, where\nwe investigate various proxies to the LLMs' factuality self-assessment, and\ndiscuss its relation to the models' context familiarity from their\npre-training. On biography generation, our method improves generation quality\nby up to 2.5x, while consuming over 6 times less compute.", "journal": ""}
{"doi": "10.48550/arXiv.2412.10246", "date": "2024-12-13", "title": "Detecting LLM Hallucination Through Layer-wise Information Deficiency: Analysis of Unanswerable Questions and Ambiguous Prompts", "authors": "Hazel Kim, Adel Bibi, Philip Torr, Yarin Gal", "abstract": "Large language models (LLMs) frequently generate confident yet inaccurate\nresponses, introducing significant risks for deployment in safety-critical\ndomains. We present a novel approach to detecting model hallucination through\nsystematic analysis of information flow across model layers when processing\ninputs with insufficient or ambiguous context. Our investigation reveals that\nhallucination manifests as usable information deficiencies in inter-layer\ntransmissions. While existing approaches primarily focus on final-layer output\nanalysis, we demonstrate that tracking cross-layer information dynamics\n($\\mathcal{L}$I) provides robust indicators of model reliability, accounting\nfor both information gain and loss during computation. $\\mathcal{L}$I improves\nmodel reliability by immediately integrating with universal LLMs without\nadditional training or architectural modifications.", "journal": ""}
{"doi": "10.48550/arXiv.2501.01059", "date": "2025-01-02", "title": "Dynamic Attention-Guided Context Decoding for Mitigating Context Faithfulness Hallucinations in Large Language Models", "authors": "Yanwen Huang, Yong Zhang, Ning Cheng, Zhitao Li, Shaojun Wang, Jing Xiao", "abstract": "Large language models (LLMs) often exhibit Context Faithfulness\nHallucinations, where outputs deviate from retrieved information due to\nincomplete context integration. Our analysis reveals a strong correlation\nbetween token-level uncertainty and hallucinations. We hypothesize that\nattention mechanisms inherently encode context utilization signals, supported\nby probing analysis. Based on these insights, we propose Dynamic\nAttention-Guided Context Decoding (DAGCD), a lightweight framework that\nleverages attention distributions and uncertainty signals in a single-pass\ndecoding. Experiments on open-book QA datasets demonstrate DAGCD's\neffectiveness, yielding significant improvements in faithfulness and robustness\nwhile preserving computational efficiency.", "journal": ""}
{"doi": "10.48550/arXiv.2502.18536", "date": "2025-02-25", "title": "FilterRAG: Zero-Shot Informed Retrieval-Augmented Generation to Mitigate Hallucinations in VQA", "authors": "S M Sarwar", "abstract": "Visual Question Answering requires models to generate accurate answers by\nintegrating visual and textual understanding. However, VQA models still\nstruggle with hallucinations, producing convincing but incorrect answers,\nparticularly in knowledge-driven and Out-of-Distribution scenarios. We\nintroduce FilterRAG, a retrieval-augmented framework that combines BLIP-VQA\nwith Retrieval-Augmented Generation to ground answers in external knowledge\nsources like Wikipedia and DBpedia. FilterRAG achieves 36.5% accuracy on the\nOK-VQA dataset, demonstrating its effectiveness in reducing hallucinations and\nimproving robustness in both in-domain and Out-of-Distribution settings. These\nfindings highlight the potential of FilterRAG to improve Visual Question\nAnswering systems for real-world deployment.", "journal": ""}
{"doi": "10.48550/arXiv.2503.02442", "date": "2025-03-04", "title": "AILS-NTUA at SemEval-2025 Task 3: Leveraging Large Language Models and Translation Strategies for Multilingual Hallucination Detection", "authors": "Dimitra Karkani, Maria Lymperaiou, Giorgos Filandrianos, Nikolaos Spanos, Athanasios Voulodimos, Giorgos Stamou", "abstract": "Multilingual hallucination detection stands as an underexplored challenge,\nwhich the Mu-SHROOM shared task seeks to address. In this work, we propose an\nefficient, training-free LLM prompting strategy that enhances detection by\ntranslating multilingual text spans into English. Our approach achieves\ncompetitive rankings across multiple languages, securing two first positions in\nlow-resource languages. The consistency of our results highlights the\neffectiveness of our translation strategy for hallucination detection,\ndemonstrating its applicability regardless of the source language.", "journal": ""}
{"doi": "10.48550/arXiv.2503.21157", "date": "2025-03-27", "title": "Real-Time Evaluation Models for RAG: Who Detects Hallucinations Best?", "authors": "Ashish Sardana", "abstract": "This article surveys Evaluation models to automatically detect hallucinations\nin Retrieval-Augmented Generation (RAG), and presents a comprehensive benchmark\nof their performance across six RAG applications. Methods included in our study\ninclude: LLM-as-a-Judge, Prometheus, Lynx, the Hughes Hallucination Evaluation\nModel (HHEM), and the Trustworthy Language Model (TLM). These approaches are\nall reference-free, requiring no ground-truth answers/labels to catch incorrect\nLLM responses. Our study reveals that, across diverse RAG applications, some of\nthese approaches consistently detect incorrect RAG responses with high\nprecision/recall.", "journal": ""}
{"doi": "10.48550/arXiv.1811.07104", "date": "2018-11-17", "title": "On Hallucinating Context and Background Pixels from a Face Mask using Multi-scale GANs", "authors": "Sandipan Banerjee, Walter J. Scheirer, Kevin W. Bowyer, Patrick J. Flynn", "abstract": "We propose a multi-scale GAN model to hallucinate realistic context\n(forehead, hair, neck, clothes) and background pixels automatically from a\nsingle input face mask. Instead of swapping a face on to an existing picture,\nour model directly generates realistic context and background pixels based on\nthe features of the provided face mask. Unlike face inpainting algorithms, it\ncan generate realistic hallucinations even for a large number of missing\npixels. Our model is composed of a cascaded network of GAN blocks, each tasked\nwith hallucination of missing pixels at a particular resolution while guiding\nthe synthesis process of the next GAN block. The hallucinated full face image\nis made photo-realistic by using a combination of reconstruction, perceptual,\nadversarial and identity preserving losses at each block of the network. With a\nset of extensive experiments, we demonstrate the effectiveness of our model in\nhallucinating context and background pixels from face masks varying in facial\npose, expression and lighting, collected from multiple datasets subject\ndisjoint with our training data. We also compare our method with two popular\nface swapping and face completion methods in terms of visual quality and\nrecognition performance. Additionally, we analyze our cascaded pipeline and\ncompare it with the recently proposed progressive growing of GANs.", "journal": ""}
{"doi": "10.48550/arXiv.2007.06166", "date": "2020-07-13", "title": "Low to High Dimensional Modality Hallucination using Aggregated Fields of View", "authors": "Kausic Gunasekar, Qiang Qiu, Yezhou Yang", "abstract": "Real-world robotics systems deal with data from a multitude of modalities,\nespecially for tasks such as navigation and recognition. The performance of\nthose systems can drastically degrade when one or more modalities become\ninaccessible, due to factors such as sensors' malfunctions or adverse\nenvironments. Here, we argue modality hallucination as one effective way to\nensure consistent modality availability and thereby reduce unfavorable\nconsequences. While hallucinating data from a modality with richer information,\ne.g., RGB to depth, has been researched extensively, we investigate the more\nchallenging low-to-high modality hallucination with interesting use cases in\nrobotics and autonomous systems. We present a novel hallucination architecture\nthat aggregates information from multiple fields of view of the local\nneighborhood to recover the lost information from the extant modality. The\nprocess is implemented by capturing a non-linear mapping between the data\nmodalities and the learned mapping is used to aid the extant modality to\nmitigate the risk posed to the system in the adverse scenarios which involve\nmodality loss. We also conduct extensive classification and segmentation\nexperiments on UWRGBD and NYUD datasets and demonstrate that hallucination\nallays the negative effects of the modality loss. Implementation and models:\nhttps://github.com/kausic94/Hallucination", "journal": "IEEE Robotics and Automation Letters, vol. 5, no. 2, pp.\n  1983-1990, April 2020"}
{"doi": "10.48550/arXiv.2106.04144", "date": "2021-06-08", "title": "Adversarial Semantic Hallucination for Domain Generalized Semantic Segmentation", "authors": "Gabriel Tjio, Ping Liu, Joey Tianyi Zhou, Rick Siow Mong Goh", "abstract": "Convolutional neural networks typically perform poorly when the test (target\ndomain) and training (source domain) data have significantly different\ndistributions. While this problem can be mitigated by using the target domain\ndata to align the source and target domain feature representations, the target\ndomain data may be unavailable due to privacy concerns. Consequently, there is\na need for methods that generalize well despite restricted access to target\ndomain data during training. In this work, we propose an adversarial semantic\nhallucination approach (ASH), which combines a class-conditioned hallucination\nmodule and a semantic segmentation module. Since the segmentation performance\nvaries across different classes, we design a semantic-conditioned style\nhallucination module to generate affine transformation parameters from semantic\ninformation in the segmentation probability maps of the source domain image.\nUnlike previous adaptation approaches, which treat all classes equally, ASH\nconsiders the class-wise differences. The segmentation module and the\nhallucination module compete adversarially, with the hallucination module\ngenerating increasingly \"difficult\" stylized images to challenge the\nsegmentation module. In response, the segmentation module improves as it is\ntrained with generated samples at an appropriate class-wise difficulty level.\nOur results on the Cityscapes and Mapillary benchmark datasets show that our\nmethod is competitive with state of the art work. Code is made available at\nhttps://github.com/gabriel-tjio/ASH.", "journal": ""}
{"doi": "10.48550/arXiv.2208.05309", "date": "2022-08-10", "title": "Looking for a Needle in a Haystack: A Comprehensive Study of Hallucinations in Neural Machine Translation", "authors": "Nuno M. Guerreiro, Elena Voita, Andr\u00e9 F. T. Martins", "abstract": "Although the problem of hallucinations in neural machine translation (NMT)\nhas received some attention, research on this highly pathological phenomenon\nlacks solid ground. Previous work has been limited in several ways: it often\nresorts to artificial settings where the problem is amplified, it disregards\nsome (common) types of hallucinations, and it does not validate adequacy of\ndetection heuristics. In this paper, we set foundations for the study of NMT\nhallucinations. First, we work in a natural setting, i.e., in-domain data\nwithout artificial noise neither in training nor in inference. Next, we\nannotate a dataset of over 3.4k sentences indicating different kinds of\ncritical errors and hallucinations. Then, we turn to detection methods and both\nrevisit methods used previously and propose using glass-box uncertainty-based\ndetectors. Overall, we show that for preventive settings, (i) previously used\nmethods are largely inadequate, (ii) sequence log-probability works best and\nperforms on par with reference-based methods. Finally, we propose\nDeHallucinator, a simple method for alleviating hallucinations at test time\nthat significantly reduces the hallucinatory rate. To ease future research, we\nrelease our annotated dataset for WMT18 German-English data, along with the\nmodel, training data, and code.", "journal": ""}
{"doi": "10.48550/arXiv.2212.08597", "date": "2022-12-16", "title": "Detecting and Mitigating Hallucinations in Machine Translation: Model Internal Workings Alone Do Well, Sentence Similarity Even Better", "authors": "David Dale, Elena Voita, Lo\u00efc Barrault, Marta R. Costa-juss\u00e0", "abstract": "While the problem of hallucinations in neural machine translation has long\nbeen recognized, so far the progress on its alleviation is very little. Indeed,\nrecently it turned out that without artificially encouraging models to\nhallucinate, previously existing methods fall short and even the standard\nsequence log-probability is more informative. It means that characteristics\ninternal to the model can give much more information than we expect, and before\nusing external models and measures, we first need to ask: how far can we go if\nwe use nothing but the translation model itself ? We propose to use a method\nthat evaluates the percentage of the source contribution to a generated\ntranslation. Intuitively, hallucinations are translations \"detached\" from the\nsource, hence they can be identified by low source contribution. This method\nimproves detection accuracy for the most severe hallucinations by a factor of 2\nand is able to alleviate hallucinations at test time on par with the previous\nbest approach that relies on external models. Next, if we move away from\ninternal model characteristics and allow external tools, we show that using\nsentence similarity from cross-lingual embeddings further improves these\nresults.", "journal": ""}
{"doi": "10.48550/arXiv.2304.09347", "date": "2023-04-18", "title": "Dual Stage Stylization Modulation for Domain Generalized Semantic Segmentation", "authors": "Gabriel Tjio, Ping Liu, Chee-Keong Kwoh, Joey Tianyi Zhou", "abstract": "Obtaining sufficient labeled data for training deep models is often\nchallenging in real-life applications. To address this issue, we propose a\nnovel solution for single-source domain generalized semantic segmentation.\nRecent approaches have explored data diversity enhancement using hallucination\ntechniques. However, excessive hallucination can degrade performance,\nparticularly for imbalanced datasets. As shown in our experiments, minority\nclasses are more susceptible to performance reduction due to hallucination\ncompared to majority classes. To tackle this challenge, we introduce a\ndual-stage Feature Transform (dFT) layer within the Adversarial Semantic\nHallucination+ (ASH+) framework. The ASH+ framework performs a dual-stage\nmanipulation of hallucination strength. By leveraging semantic information for\neach pixel, our approach adaptively adjusts the pixel-wise hallucination\nstrength, thus providing fine-grained control over hallucination. We validate\nthe effectiveness of our proposed method through comprehensive experiments on\npublicly available semantic segmentation benchmark datasets (Cityscapes and\nSYNTHIA). Quantitative and qualitative comparisons demonstrate that our\napproach is competitive with state-of-the-art methods for the Cityscapes\ndataset and surpasses existing solutions for the SYNTHIA dataset. Code for our\nframework will be made readily available to the research community.", "journal": ""}
{"doi": "10.48550/arXiv.2310.05338", "date": "2023-10-09", "title": "Negative Object Presence Evaluation (NOPE) to Measure Object Hallucination in Vision-Language Models", "authors": "Holy Lovenia, Wenliang Dai, Samuel Cahyawijaya, Ziwei Ji, Pascale Fung", "abstract": "Object hallucination poses a significant challenge in vision-language (VL)\nmodels, often leading to the generation of nonsensical or unfaithful responses\nwith non-existent objects. However, the absence of a general measurement for\nevaluating object hallucination in VL models has hindered our understanding and\nability to mitigate this issue. In this work, we present NOPE (Negative Object\nPresence Evaluation), a novel benchmark designed to assess object hallucination\nin VL models through visual question answering (VQA). We propose a\ncost-effective and scalable approach utilizing large language models to\ngenerate 29.5k synthetic negative pronoun (NegP) data of high quality for NOPE.\nWe extensively investigate the performance of 10 state-of-the-art VL models in\ndiscerning the non-existence of objects in visual questions, where the ground\ntruth answers are denoted as NegP (e.g., \"none\"). Additionally, we evaluate\ntheir standard performance on visual questions on 9 other VQA datasets. Through\nour experiments, we demonstrate that no VL model is immune to the vulnerability\nof object hallucination, as all models achieve accuracy below 10\\% on NegP.\nFurthermore, we uncover that lexically diverse visual questions, question types\nwith large scopes, and scene-relevant objects capitalize the risk of object\nhallucination in VL models.", "journal": ""}
{"doi": "10.48550/arXiv.2310.18794", "date": "2023-10-28", "title": "Sequence-Level Certainty Reduces Hallucination In Knowledge-Grounded Dialogue Generation", "authors": "Yixin Wan, Fanyou Wu, Weijie Xu, Srinivasan H. Sengamedu", "abstract": "In this work, we propose sequence-level certainty as a common theme over\nhallucination in Knowledge Grounded Dialogue Generation (KGDG). We explore the\ncorrelation between the level of hallucination in model responses and two types\nof sequence-level certainty: probabilistic certainty and semantic certainty.\nEmpirical results reveal that higher levels of both types of certainty in model\nresponses are correlated with lower levels of hallucination. We further propose\nCertainty-based Response Ranking (CRR), a decoding-time hallucination\nmitigation method that samples several response candidates, ranks them based on\nsequence-level certainty, and outputs the response with the highest certainty\nlevel. Aligning with our definitions of sequence-level certainty, we design 2\ntypes of CRR approaches: Probabilistic CRR (P-CRR) and Semantic CRR (S-CRR).\nP-CRR ranks individually sampled model responses using the arithmetic mean\nlog-probability of the entire sequence. S-CRR approaches certainty estimation\nfrom meaning-space, and ranks model response candidates based on their semantic\ncertainty level as measured by an entailment-based Agreement Score (AS).\nThrough extensive experiments across 3 KGDG datasets, 3 decoding methods, and 4\nKGDG models, we validate the effectiveness of CRR for reducing hallucination in\nKGDG task.", "journal": ""}
{"doi": "10.48550/arXiv.2311.09335", "date": "2023-11-15", "title": "Investigating Hallucinations in Pruned Large Language Models for Abstractive Summarization", "authors": "George Chrysostomou, Zhixue Zhao, Miles Williams, Nikolaos Aletras", "abstract": "Despite the remarkable performance of generative large language models (LLMs)\non abstractive summarization, they face two significant challenges: their\nconsiderable size and tendency to hallucinate. Hallucinations are concerning\nbecause they erode reliability and raise safety issues. Pruning is a technique\nthat reduces model size by removing redundant weights, enabling more efficient\nsparse inference. Pruned models yield downstream task performance comparable to\nthe original, making them ideal alternatives when operating on a limited\nbudget. However, the effect that pruning has upon hallucinations in abstractive\nsummarization with LLMs has yet to be explored. In this paper, we provide an\nextensive empirical study across five summarization datasets, two\nstate-of-the-art pruning methods, and five instruction-tuned LLMs.\nSurprisingly, we find that hallucinations are less prevalent from pruned LLMs\nthan the original models. Our analysis suggests that pruned models tend to\ndepend more on the source document for summary generation. This leads to a\nhigher lexical overlap between the generated summary and the source document,\nwhich could be a reason for the reduction in hallucination risk.", "journal": "Transactions of the Association for Computational Linguistics\n  (2024) 12: 1163-1181"}
{"doi": "10.48550/arXiv.2311.16479", "date": "2023-11-27", "title": "Mitigating Hallucination in Visual Language Models with Visual Supervision", "authors": "Zhiyang Chen, Yousong Zhu, Yufei Zhan, Zhaowen Li, Chaoyang Zhao, Jinqiao Wang, Ming Tang", "abstract": "Large vision-language models (LVLMs) suffer from hallucination a lot,\ngenerating responses that apparently contradict to the image content\noccasionally. The key problem lies in its weak ability to comprehend detailed\ncontent in a multi-modal context, which can be mainly attributed to two factors\nin training data and loss function. The vision instruction dataset primarily\nfocuses on global description, and the auto-regressive loss function favors\ntext modeling rather than image understanding. In this paper, we bring more\ndetailed vision annotations and more discriminative vision models to facilitate\nthe training of LVLMs, so that they can generate more precise responses without\nencounter hallucination. On one hand, we generate image-text pairs with\ndetailed relationship annotations in panoptic scene graph dataset (PSG). These\nconversations pay more attention on detailed facts in the image, encouraging\nthe model to answer questions based on multi-modal contexts. On the other hand,\nwe integrate SAM and mask prediction loss as auxiliary supervision, forcing the\nLVLMs to have the capacity to identify context-related objects, so that they\ncan generate more accurate responses, mitigating hallucination. Moreover, to\nprovide a deeper evaluation on the hallucination in LVLMs, we propose a new\nbenchmark, RAH-Bench. It divides vision hallucination into three different\ntypes that contradicts the image with wrong categories, attributes or\nrelations, and introduces False Positive Rate as detailed sub-metric for each\ntype. In this benchmark, our approach demonstrates an +8.4% enhancement\ncompared to original LLaVA and achieves widespread performance improvements\nacross other models.", "journal": ""}
{"doi": "10.48550/arXiv.2311.17911", "date": "2023-11-29", "title": "OPERA: Alleviating Hallucination in Multi-Modal Large Language Models via Over-Trust Penalty and Retrospection-Allocation", "authors": "Qidong Huang, Xiaoyi Dong, Pan Zhang, Bin Wang, Conghui He, Jiaqi Wang, Dahua Lin, Weiming Zhang, Nenghai Yu", "abstract": "Hallucination, posed as a pervasive challenge of multi-modal large language\nmodels (MLLMs), has significantly impeded their real-world usage that demands\nprecise judgment. Existing methods mitigate this issue with either training\nwith specific designed data or inferencing with external knowledge from other\nsources, incurring inevitable additional costs. In this paper, we present\nOPERA, a novel MLLM decoding method grounded in an Over-trust Penalty and a\nRetrospection-Allocation strategy, serving as a nearly free lunch to alleviate\nthe hallucination issue without additional data, knowledge, or training. Our\napproach begins with an interesting observation that, most hallucinations are\nclosely tied to the knowledge aggregation patterns manifested in the\nself-attention matrix, i.e., MLLMs tend to generate new tokens by focusing on a\nfew summary tokens, but not all the previous tokens. Such partial over-trust\ninclination results in the neglecting of image tokens and describes the image\ncontent with hallucination. Based on the observation, OPERA introduces a\npenalty term on the model logits during the beam-search decoding to mitigate\nthe over-trust issue, along with a rollback strategy that retrospects the\npresence of summary tokens in the previously generated tokens, and re-allocate\nthe token selection if necessary. With extensive experiments, OPERA shows\nsignificant hallucination-mitigating performance on different MLLMs and\nmetrics, proving its effectiveness and generality. Our code is available at:\nhttps://github.com/shikiw/OPERA.", "journal": ""}
{"doi": "10.48550/arXiv.2401.01701", "date": "2024-01-03", "title": "De-Hallucinator: Mitigating LLM Hallucinations in Code Generation Tasks via Iterative Grounding", "authors": "Aryaz Eghbali, Michael Pradel", "abstract": "Large language models (LLMs) trained on datasets of publicly available source\ncode have established a new state of the art in code generation tasks. However,\nthese models are mostly unaware of the code that exists within a specific\nproject, preventing the models from making good use of existing APIs. Instead,\nLLMs often invent, or \"hallucinate\", non-existent APIs or produce variants of\nalready existing code. This paper presents De-Hallucinator, a technique that\ngrounds the predictions of an LLM through a novel combination of retrieving\nsuitable API references and iteratively querying the model with increasingly\nsuitable context information in the prompt. The approach exploits the\nobservation that predictions by LLMs often resemble the desired code, but they\nfail to correctly refer to already existing APIs. De-Hallucinator automatically\nidentifies project-specific API references related to the model's initial\npredictions and adds these references into the prompt. Unlike\nretrieval-augmented generation (RAG), our approach uses the initial\nprediction(s) by the model to iteratively retrieve increasingly suitable API\nreferences. Our evaluation applies the approach to two tasks: predicting API\nusages in Python and generating tests in JavaScript. We show that\nDe-Hallucinator consistently improves the generated code across five LLMs. In\nparticular, the approach improves the edit distance by 23.3-50.6% and the\nrecall of correctly predicted API usages by 23.9-61.0% for code completion, and\nimproves the number of fixed tests that initially failed because of\nhallucinations by 63.2%, resulting in a 15.5% increase in statement coverage\nfor test generation.", "journal": ""}
{"doi": "10.48550/arXiv.2401.08273", "date": "2024-01-16", "title": "Large Language Models are Null-Shot Learners", "authors": "Pittawat Taveekitworachai, Febri Abdullah, Ruck Thawonmas", "abstract": "This paper presents null-shot prompting. Null-shot prompting exploits\nhallucination in large language models (LLMs) by instructing LLMs to utilize\ninformation from the \"Examples\" section that never exists within the provided\ncontext to perform a task. While reducing hallucination is crucial and\nnon-negligible for daily and critical uses of LLMs, we propose that in the\ncurrent landscape in which these LLMs still hallucinate, it is possible, in\nfact, to exploit hallucination to increase performance in performing tasks\ncompared to standard zero-shot prompting. Experiments with eight LLMs show\nimprovements in performance across the majority of eight datasets, including\nreading comprehension, arithmetic reasoning, and closed-book question\nanswering. The observed inconsistency in increased relative performance across\nthe LLMs also potentially indicates a different degree of inherent\nhallucination in each model. These differences show that it is possible to\nutilize null-shot prompting as a way to detect degrees of hallucination in LLMs\nusing existing benchmarking datasets. We also perform ablation studies,\nincluding experimenting with a modified version of null-shot prompting that\nincorporates ideas from zero-shot chain-of-thought prompting, which shows\ndifferent trends of results.", "journal": ""}
{"doi": "10.48550/arXiv.2402.01345", "date": "2024-02-02", "title": "Skip \\n: A Simple Method to Reduce Hallucination in Large Vision-Language Models", "authors": "Zongbo Han, Zechen Bai, Haiyang Mei, Qianli Xu, Changqing Zhang, Mike Zheng Shou", "abstract": "Recent advancements in large vision-language models (LVLMs) have demonstrated\nimpressive capability in visual information understanding with human language.\nDespite these advances, LVLMs still face challenges with multimodal\nhallucination, such as generating text descriptions of objects that are not\npresent in the visual information. However, the underlying fundamental reasons\nof multimodal hallucinations remain poorly explored. In this paper, we propose\na new perspective, suggesting that the inherent biases in LVLMs might be a key\nfactor in hallucinations. Specifically, we systematically identify a semantic\nshift bias related to paragraph breaks (\\n\\n), where the content before and\nafter '\\n\\n' in the training data frequently exhibit significant semantic\nchanges. This pattern leads the model to infer that the contents following\n'\\n\\n' should be obviously different from the preceding contents with less\nhallucinatory descriptions, thereby increasing the probability of hallucinatory\ndescriptions subsequent to the '\\n\\n'. We have validated this hypothesis on\nmultiple publicly available LVLMs. Besides, we find that deliberately inserting\n'\\n\\n' at the generated description can induce more hallucinations. A simple\nmethod is proposed to effectively mitigate the hallucination of LVLMs by\nskipping the output of '\\n'.", "journal": ""}
{"doi": "10.48550/arXiv.2403.11116", "date": "2024-03-17", "title": "PhD: A ChatGPT-Prompted Visual hallucination Evaluation Dataset", "authors": "Jiazhen Liu, Yuhan Fu, Ruobing Xie, Runquan Xie, Xingwu Sun, Fengzong Lian, Zhanhui Kang, Xirong Li", "abstract": "Multimodal Large Language Models (MLLMs) hallucinate, resulting in an\nemerging topic of visual hallucination evaluation (VHE). This paper contributes\na ChatGPT-Prompted visual hallucination evaluation Dataset (PhD) for objective\nVHE at a large scale. The essence of VHE is to ask an MLLM questions about\nspecific images to assess its susceptibility to hallucination. Depending on\nwhat to ask (objects, attributes, sentiment, etc.) and how the questions are\nasked, we structure PhD along two dimensions, i.e. task and mode. Five visual\nrecognition tasks, ranging from low-level (object / attribute recognition) to\nmiddle-level (sentiment / position recognition and counting), are considered.\nBesides a normal visual QA mode, which we term PhD-base, PhD also asks\nquestions with specious context (PhD-sec) or with incorrect context ({PhD-icc),\nor with AI-generated counter common sense images (PhD-ccs). We construct PhD by\na ChatGPT-assisted semi-automated pipeline, encompassing four pivotal modules:\ntask-specific hallucinatory item (hitem) selection, hitem-embedded question\ngeneration, specious / incorrect context generation, and counter-common-sense\n(CCS) image generation. With over 14k daily images, 750 CCS images and 102k VQA\ntriplets in total, PhD reveals considerable variability in MLLMs' performance\nacross various modes and tasks, offering valuable insights into the nature of\nhallucination. As such, PhD stands as a potent tool not only for VHE but may\nalso play a significant role in the refinement of MLLMs.", "journal": ""}
{"doi": "10.48550/arXiv.2403.14003", "date": "2024-03-20", "title": "Multi-Modal Hallucination Control by Visual Information Grounding", "authors": "Alessandro Favero, Luca Zancato, Matthew Trager, Siddharth Choudhary, Pramuditha Perera, Alessandro Achille, Ashwin Swaminathan, Stefano Soatto", "abstract": "Generative Vision-Language Models (VLMs) are prone to generate\nplausible-sounding textual answers that, however, are not always grounded in\nthe input image. We investigate this phenomenon, usually referred to as\n\"hallucination\" and show that it stems from an excessive reliance on the\nlanguage prior. In particular, we show that as more tokens are generated, the\nreliance on the visual prompt decreases, and this behavior strongly correlates\nwith the emergence of hallucinations. To reduce hallucinations, we introduce\nMulti-Modal Mutual-Information Decoding (M3ID), a new sampling method for\nprompt amplification. M3ID amplifies the influence of the reference image over\nthe language prior, hence favoring the generation of tokens with higher mutual\ninformation with the visual prompt. M3ID can be applied to any pre-trained\nautoregressive VLM at inference time without necessitating further training and\nwith minimal computational overhead. If training is an option, we show that\nM3ID can be paired with Direct Preference Optimization (DPO) to improve the\nmodel's reliance on the prompt image without requiring any labels. Our\nempirical findings show that our algorithms maintain the fluency and linguistic\ncapabilities of pre-trained VLMs while reducing hallucinations by mitigating\nvisually ungrounded answers. Specifically, for the LLaVA 13B model, M3ID and\nM3ID+DPO reduce the percentage of hallucinated objects in captioning tasks by\n25% and 28%, respectively, and improve the accuracy on VQA benchmarks such as\nPOPE by 21% and 24%.", "journal": "IEEE/CVF Conference on Computer Vision and Pattern Recognition\n  (CVPR) 2024"}
{"doi": "10.48550/arXiv.2404.03491", "date": "2024-04-04", "title": "A Cause-Effect Look at Alleviating Hallucination of Knowledge-grounded Dialogue Generation", "authors": "Jifan Yu, Xiaohan Zhang, Yifan Xu, Xuanyu Lei, Zijun Yao, Jing Zhang, Lei Hou, Juanzi Li", "abstract": "Empowered by the large-scale pretrained language models, existing dialogue\nsystems have demonstrated impressive performance conducting fluent and\nnatural-sounding conversations. However, they are still plagued by the\nhallucination problem, causing unpredictable factual errors in the generated\nresponses. Recently, knowledge-grounded dialogue generation models, that\nintentionally invoke external knowledge resources to more informative\nresponses, are also proven to be effective in reducing hallucination. Following\nthe idea of getting high-quality knowledge, a few efforts have achieved pretty\ngood performance on this issue. As some inevitable knowledge noises may also\nlead to hallucinations, it is emergent to investigate the reason and future\ndirections for building noise-tolerant methods in KGD tasks. In this paper, we\nanalyze the causal story behind this problem with counterfactual reasoning\nmethods. Based on the causal effect analysis, we propose a possible solution\nfor alleviating the hallucination in KGD by exploiting the dialogue-knowledge\ninteraction. Experimental results of our example implementation show that this\nmethod can reduce hallucination without disrupting other dialogue performance,\nwhile keeping adaptive to different generation models. We hope our efforts can\nsupport and call for more attention to developing lightweight techniques\ntowards robust and trusty dialogue systems.", "journal": ""}
{"doi": "10.48550/arXiv.2404.05980", "date": "2024-04-09", "title": "Tackling Structural Hallucination in Image Translation with Local Diffusion", "authors": "Seunghoi Kim, Chen Jin, Tom Diethe, Matteo Figini, Henry F. J. Tregidgo, Asher Mullokandov, Philip Teare, Daniel C. Alexander", "abstract": "Recent developments in diffusion models have advanced conditioned image\ngeneration, yet they struggle with reconstructing out-of-distribution (OOD)\nimages, such as unseen tumors in medical images, causing \"image hallucination\"\nand risking misdiagnosis. We hypothesize such hallucinations result from local\nOOD regions in the conditional images. We verify that partitioning the OOD\nregion and conducting separate image generations alleviates hallucinations in\nseveral applications. From this, we propose a training-free diffusion framework\nthat reduces hallucination with multiple Local Diffusion processes. Our\napproach involves OOD estimation followed by two modules: a \"branching\" module\ngenerates locally both within and outside OOD regions, and a \"fusion\" module\nintegrates these predictions into one. Our evaluation shows our method\nmitigates hallucination over baseline models quantitatively and qualitatively,\nreducing misdiagnosis by 40% and 25% in the real-world medical and natural\nimage datasets, respectively. It also demonstrates compatibility with various\npre-trained diffusion models.", "journal": ""}
{"doi": "10.48550/arXiv.2405.09589", "date": "2024-05-15", "title": "A Comprehensive Survey of Hallucination in Large Language, Image, Video and Audio Foundation Models", "authors": "Pranab Sahoo, Prabhash Meharia, Akash Ghosh, Sriparna Saha, Vinija Jain, Aman Chadha", "abstract": "The rapid advancement of foundation models (FMs) across language, image,\naudio, and video domains has shown remarkable capabilities in diverse tasks.\nHowever, the proliferation of FMs brings forth a critical challenge: the\npotential to generate hallucinated outputs, particularly in high-stakes\napplications. The tendency of foundation models to produce hallucinated content\narguably represents the biggest hindrance to their widespread adoption in\nreal-world scenarios, especially in domains where reliability and accuracy are\nparamount. This survey paper presents a comprehensive overview of recent\ndevelopments that aim to identify and mitigate the problem of hallucination in\nFMs, spanning text, image, video, and audio modalities. By synthesizing recent\nadvancements in detecting and mitigating hallucination across various\nmodalities, the paper aims to provide valuable insights for researchers,\ndevelopers, and practitioners. Essentially, it establishes a clear framework\nencompassing definition, taxonomy, and detection strategies for addressing\nhallucination in multimodal foundation models, laying the foundation for future\nresearch in this pivotal area.", "journal": ""}
{"doi": "10.48550/arXiv.2406.07735", "date": "2024-06-11", "title": "REAL Sampling: Boosting Factuality and Diversity of Open-Ended Generation via Asymptotic Entropy", "authors": "Haw-Shiuan Chang, Nanyun Peng, Mohit Bansal, Anil Ramakrishna, Tagyoung Chung", "abstract": "Decoding methods for large language models (LLMs) usually struggle with the\ntradeoff between ensuring factuality and maintaining diversity. For example, a\nhigher p threshold in the nucleus (top-p) sampling increases the diversity but\ndecreases the factuality, and vice versa. In this paper, we propose REAL\n(Residual Entropy from Asymptotic Line) sampling, a decoding method that\nachieves improved factuality and diversity over nucleus sampling by predicting\nan adaptive threshold of $p$. Specifically, REAL sampling predicts the\nstep-wise likelihood of an LLM to hallucinate, and lowers the p threshold when\nan LLM is likely to hallucinate. Otherwise, REAL sampling increases the p\nthreshold to boost the diversity. To predict the step-wise hallucination\nlikelihood without supervision, we construct a Token-level Hallucination\nForecasting (THF) model to predict the asymptotic entropy (i.e., inherent\nuncertainty) of the next token by extrapolating the next-token entropies from a\nseries of LLMs with different sizes. If a LLM's entropy is higher than the\nasymptotic entropy (i.e., the LLM is more uncertain than it should be), the THF\nmodel predicts a high hallucination hazard, which leads to a lower p threshold\nin REAL sampling. In the FactualityPrompts benchmark, we demonstrate that REAL\nsampling based on a 70M THF model can substantially improve the factuality and\ndiversity of 7B LLMs simultaneously, judged by both retrieval-based metrics and\nhuman evaluation. After combined with contrastive decoding, REAL sampling\noutperforms 9 sampling methods, and generates texts that are more factual than\nthe greedy sampling and more diverse than the nucleus sampling with $p=0.5$.\nFurthermore, the predicted asymptotic entropy is also a useful unsupervised\nsignal for hallucination detection tasks.", "journal": ""}
