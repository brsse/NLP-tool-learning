{"doi": "10.48550/arXiv.2502.01083", "date": "2025-02-03", "title": "Tool Unlearning for Tool-Augmented LLMs", "authors": "Jiali Cheng, Hadi Amiri", "abstract": "Tool-augmented large language models (LLMs) are often trained on datasets of\nquery-response pairs, which embed the ability to use tools or APIs directly\ninto the parametric knowledge of LLMs. Tool-augmented LLMs need the ability to\nforget learned tools due to security vulnerabilities, privacy regulations, or\ntool deprecations. However, ``tool unlearning'' has not been investigated in\nunlearning literature. We introduce this novel task, which requires addressing\ndistinct challenges compared to traditional unlearning: knowledge removal\nrather than forgetting individual samples, the high cost of optimizing LLMs,\nand the need for principled evaluation metrics. To bridge these gaps, we\npropose ToolDelete, the first approach for unlearning tools from tool-augmented\nLLMs. It implements three key properties to address the above challenges for\neffective tool unlearning and introduces a new membership inference attack\n(MIA) model for effective evaluation. Extensive experiments on multiple tool\nlearning datasets and tool-augmented LLMs show that ToolDelete effectively\nunlearns randomly selected tools, while preserving the LLM's knowledge on\nnon-deleted tools and maintaining performance on general tasks.", "journal": ""}
{"doi": "10.48550/arXiv.1610.00984", "date": "2016-10-01", "title": "On the State of Computing in Statistics Education: Tools for Learning and for Doing", "authors": "Amelia McNamara", "abstract": "This paper lays out the current landscape of tools used in statistics\neducation. In particular, it considers graphing calculators, spreadsheets,\napplets and microworlds, standalone educational software, statistical\nprogramming tools, tools for reproducible research and bespoke tools. The\nstrengths and weaknesses of the tools are considered, particularly in the\ncontext of McNamara (2016)'s list of attributes for a statistical computing\ntool. Best practices for computing in introductory statistics are suggested.", "journal": ""}
{"doi": "10.48550/arXiv.2406.17465", "date": "2024-06-25", "title": "Enhancing Tool Retrieval with Iterative Feedback from Large Language Models", "authors": "Qiancheng Xu, Yongqi Li, Heming Xia, Wenjie Li", "abstract": "Tool learning aims to enhance and expand large language models' (LLMs)\ncapabilities with external tools, which has gained significant attention\nrecently. Current methods have shown that LLMs can effectively handle a certain\namount of tools through in-context learning or fine-tuning. However, in\nreal-world scenarios, the number of tools is typically extensive and\nirregularly updated, emphasizing the necessity for a dedicated tool retrieval\ncomponent. Tool retrieval is nontrivial due to the following challenges: 1)\ncomplex user instructions and tool descriptions; 2) misalignment between tool\nretrieval and tool usage models. To address the above issues, we propose to\nenhance tool retrieval with iterative feedback from the large language model.\nSpecifically, we prompt the tool usage model, i.e., the LLM, to provide\nfeedback for the tool retriever model in multi-round, which could progressively\nimprove the tool retriever's understanding of instructions and tools and reduce\nthe gap between the two standalone components. We build a unified and\ncomprehensive benchmark to evaluate tool retrieval models. The extensive\nexperiments indicate that our proposed approach achieves advanced performance\nin both in-domain evaluation and out-of-domain evaluation.", "journal": ""}
{"doi": "10.48550/arXiv.2410.07745", "date": "2024-10-10", "title": "StepTool: Enhancing Multi-Step Tool Usage in LLMs through Step-Grained Reinforcement Learning", "authors": "Yuanqing Yu, Zhefan Wang, Weizhi Ma, Shuai Wang, Chuhan Wu, Zhiqiang Guo, Min Zhang", "abstract": "Despite powerful text generation capabilities, large language models (LLMs)\nstill need to learn how to utilize external tools to solve complex tasks, a\nprocess known as tool learning. Existing methods primarily rely on supervised\nfine-tuning to enhance tool-use capabilities, treating tool learning as a\ntext-generation task while overlooking the decision-making complexities\ninherent in multi-step contexts. In this work, we propose modeling tool\nlearning as a dynamic decision-making task and introduce StepTool, a novel\nstep-grained reinforcement learning framework that enhances the multi-step tool\nuse capabilities of LLMs. StepTool consists of two main components:\nStep-grained Reward Shaping, which assigns rewards at each tool interaction\nbased on the success of tool invocation and its contribution to the task; and\nStep-grained Optimization, which uses policy gradient methods to optimize the\nmodel in a multi-step manner. Experimental results demonstrate that StepTool\nsignificantly outperforms existing methods in multi-step, tool-based tasks,\noffering a robust solution for tool learning.", "journal": ""}
{"doi": "10.48550/arXiv.2405.08355", "date": "2024-05-14", "title": "Seal-Tools: Self-Instruct Tool Learning Dataset for Agent Tuning and Detailed Benchmark", "authors": "Mengsong Wu, Tong Zhu, Han Han, Chuanyuan Tan, Xiang Zhang, Wenliang Chen", "abstract": "This paper presents a new tool learning dataset Seal-Tools, which contains\nself-instruct API-like tools. Seal-Tools not only offers a large number of\ntools, but also includes instances which demonstrate the practical application\nof tools. Seeking to generate data on a large scale while ensuring reliability,\nwe propose a self-instruct method to generate tools and instances, allowing\nprecise control over the process. Moreover, our Seal-Tools contains hard\ninstances that call multiple tools to complete the job, among which some are\nnested tool callings. For precise and comprehensive evaluation, we use strict\nformat control and design three metrics from different dimensions. Therefore,\nSeal-Tools can serve as a new benchmark to evaluate the tool-calling ability of\nLLMs. Finally, we evaluate several prevalent LLMs and our finetuned model on\nSeal-Tools. The results show that current systems are far from perfect. The\ncode, data and experiment results are available at\nhttps://github.com/fairyshine/Seal-Tools .", "journal": ""}
{"doi": "10.48550/arXiv.2503.23383", "date": "2025-03-30", "title": "ToRL: Scaling Tool-Integrated RL", "authors": "Xuefeng Li, Haoyang Zou, Pengfei Liu", "abstract": "We introduce ToRL (Tool-Integrated Reinforcement Learning), a framework for\ntraining large language models (LLMs) to autonomously use computational tools\nvia reinforcement learning. Unlike supervised fine-tuning, ToRL allows models\nto explore and discover optimal strategies for tool use. Experiments with\nQwen2.5-Math models show significant improvements: ToRL-7B reaches 43.3\\%\naccuracy on AIME~24, surpassing reinforcement learning without tool integration\nby 14\\% and the best existing Tool-Integrated Reasoning (TIR) model by 17\\%.\nFurther analysis reveals emergent behaviors such as strategic tool invocation,\nself-regulation of ineffective code, and dynamic adaptation between\ncomputational and analytical reasoning, all arising purely through\nreward-driven learning.", "journal": ""}
{"doi": "10.48550/arXiv.1602.06926", "date": "2016-02-22", "title": "Improving Students' Understanding of Quantum Measurement Part 2: Development of Research-based Learning Tools", "authors": "Guangtian Zhu, Chandralekha Singh", "abstract": "We describe the development and implementation of research-based learning\ntools such as the Quantum Interactive Learning Tutorials (QuILTs) and peer\ninstruction tools to reduce students' common difficulties with issues related\nto measurement in quantum mechanics. A preliminary evaluation shows that these\nlearning tools are effective in improving students' understanding of concepts\nrelated to quantum measurement.", "journal": ""}
{"doi": "10.48550/arXiv.2410.11805", "date": "2024-10-15", "title": "NesTools: A Dataset for Evaluating Nested Tool Learning Abilities of Large Language Models", "authors": "Han Han, Tong Zhu, Xiang Zhang, Mengsong Wu, Hao Xiong, Wenliang Chen", "abstract": "Large language models (LLMs) combined with tool learning have gained\nimpressive results in real-world applications. During tool learning, LLMs may\ncall multiple tools in nested orders, where the latter tool call may take the\nformer response as its input parameters. However, current research on the\nnested tool learning capabilities is still under-explored, since the existing\nbenchmarks lack relevant data instances. To address this problem, we introduce\nNesTools to bridge the current gap in comprehensive nested tool learning\nevaluations. NesTools comprises a novel automatic data generation method to\nconstruct large-scale nested tool calls with different nesting structures. With\nmanual review and refinement, the dataset is in high quality and closely\naligned with real-world scenarios. Therefore, NesTools can serve as a new\nbenchmark to evaluate the nested tool learning abilities of LLMs. We conduct\nextensive experiments on 22 LLMs, and provide in-depth analyses with NesTools,\nwhich shows that current LLMs still suffer from the complex nested tool\nlearning task.", "journal": ""}
{"doi": "10.48550/arXiv.2403.06551", "date": "2024-03-11", "title": "ToolRerank: Adaptive and Hierarchy-Aware Reranking for Tool Retrieval", "authors": "Yuanhang Zheng, Peng Li, Wei Liu, Yang Liu, Jian Luan, Bin Wang", "abstract": "Tool learning aims to extend the capabilities of large language models (LLMs)\nwith external tools. A major challenge in tool learning is how to support a\nlarge number of tools, including unseen tools. To address this challenge,\nprevious studies have proposed retrieving suitable tools for the LLM based on\nthe user query. However, previously proposed methods do not consider the\ndifferences between seen and unseen tools, nor do they take the hierarchy of\nthe tool library into account, which may lead to suboptimal performance for\ntool retrieval. Therefore, to address the aforementioned issues, we propose\nToolRerank, an adaptive and hierarchy-aware reranking method for tool retrieval\nto further refine the retrieval results. Specifically, our proposed ToolRerank\nincludes Adaptive Truncation, which truncates the retrieval results related to\nseen and unseen tools at different positions, and Hierarchy-Aware Reranking,\nwhich makes retrieval results more concentrated for single-tool queries and\nmore diverse for multi-tool queries. Experimental results show that ToolRerank\ncan improve the quality of the retrieval results, leading to better execution\nresults generated by the LLM.", "journal": "In Proceedings of LREC-COLING 2024, pages 16263-16273"}
{"doi": "10.48550/arXiv.2505.17106", "date": "2025-05-21", "title": "RRTL: Red Teaming Reasoning Large Language Models in Tool Learning", "authors": "Yifei Liu, Yu Cui, Haibin Zhang", "abstract": "While tool learning significantly enhances the capabilities of large language\nmodels (LLMs), it also introduces substantial security risks. Prior research\nhas revealed various vulnerabilities in traditional LLMs during tool learning.\nHowever, the safety of newly emerging reasoning LLMs (RLLMs), such as\nDeepSeek-R1, in the context of tool learning remains underexplored. To bridge\nthis gap, we propose RRTL, a red teaming approach specifically designed to\nevaluate RLLMs in tool learning. It integrates two novel strategies: (1) the\nidentification of deceptive threats, which evaluates the model's behavior in\nconcealing the usage of unsafe tools and their potential risks; and (2) the use\nof Chain-of-Thought (CoT) prompting to force tool invocation. Our approach also\nincludes a benchmark for traditional LLMs. We conduct a comprehensive\nevaluation on seven mainstream RLLMs and uncover three key findings: (1) RLLMs\ngenerally achieve stronger safety performance than traditional LLMs, yet\nsubstantial safety disparities persist across models; (2) RLLMs can pose\nserious deceptive risks by frequently failing to disclose tool usage and to\nwarn users of potential tool output risks; (3) CoT prompting reveals\nmulti-lingual safety vulnerabilities in RLLMs. Our work provides important\ninsights into enhancing the security of RLLMs in tool learning.", "journal": ""}
{"doi": "10.48550/arXiv.2306.05301", "date": "2023-06-08", "title": "ToolAlpaca: Generalized Tool Learning for Language Models with 3000 Simulated Cases", "authors": "Qiaoyu Tang, Ziliang Deng, Hongyu Lin, Xianpei Han, Qiao Liang, Boxi Cao, Le Sun", "abstract": "Enabling large language models to utilize real-world tools effectively is\ncrucial for achieving embodied intelligence. Existing approaches to tool\nlearning have either primarily relied on extremely large language models, such\nas GPT-4, to attain generalized tool-use abilities in a zero-shot manner, or\nutilized supervised learning to train limited scopes of tools on compact\nmodels. However, it remains uncertain whether smaller language models can\nachieve generalized tool-use abilities without tool-specific training. To\naddress this question, this paper introduces ToolAlpaca, a novel framework\ndesigned to automatically generate a diverse tool-use corpus and learn\ngeneralized tool-use abilities on compact language models with minimal human\nintervention. Specifically, ToolAlpaca first automatically creates a highly\ndiversified tool-use corpus by building a multi-agent simulation environment.\nThe corpus contains 3938 tool-use instances from more than 400 real-world tool\nAPIs spanning 50 distinct categories. Subsequently, the constructed corpus is\nemployed to fine-tune compact language models, resulting in two models, namely\nToolAlpaca-7B and ToolAlpaca-13B, respectively. Finally, we evaluate the\nability of these models to utilize previously unseen tools without specific\ntraining. Experimental results demonstrate that ToolAlpaca achieves effective\ngeneralized tool-use capabilities comparable to those of extremely large\nlanguage models like GPT-3.5, demonstrating that learning generalized tool-use\nability is feasible for compact language models.", "journal": ""}
{"doi": "10.48550/arXiv.2301.06661", "date": "2023-01-17", "title": "An Empirical Study of Deep Learning Sentiment Detection Tools for Software Engineering in Cross-Platform Settings", "authors": "Gias Uddin, Md Abdullah Al Alamin, Ajoy Das", "abstract": "Sentiment detection in software engineering (SE) has shown promise to support\ndiverse development activities. However, given the diversity of SE platforms,\nSE-specific sentiment detection tools may suffer in performance in\ncross-platform settings. Recently deep learning (DL)-based SE-specific\nsentiment detection tools are found to offer superior performance than shallow\nmachine learning (ML) based/rule-based tools. However, it is not known how the\nDL tools perform in cross-platform settings. In this paper, we study whether\nSE-specific DL sentiment detectors are more effective than shallow\nML-based/rule-based sentiment detectors in cross-platform settings. In three\ndatasets, we study three DL tools (SEntiMoji, BERT4SEntiSE, RNN4SentiSE) and\ncompare those against three baselines: two shallow learning tools (Senti4SD,\nSentiCR) and one rule-based tool (SentistrengthSE). We find that (1) The deep\nlearning SD tools for SE, BERT4SentiSE outperform other supervised tools in\ncross-platform settings in most cases, but then the tool is outperformed by the\nrule-based tool SentistrengthSE in most cases. (2) BERT4SentiSE outperforms\nSentistrengthSE by large margin in within-platform settings across the three\ndatasets and is only narrowly outperformed by SentiStrengthSE in four out of\nthe six cross-platform settings. This finding offers hope for the feasibility\nto further improve a pre-trained transformer model like BERT4SentiSE in\ncross-platform settings. (3) The two best-performing deep learning tools\n(BERT4SentiSE and SentiMoji) show varying level performance drop across the\nthree datasets. We find that this inconsistency is mainly due to the\n\"subjectivity in annotation\" and performance improvement for the studied\nsupervised tools in cross-platform settings may require the fixing of the\ndatasets.", "journal": ""}
{"doi": "10.48550/arXiv.2305.13068", "date": "2023-05-22", "title": "Making Language Models Better Tool Learners with Execution Feedback", "authors": "Shuofei Qiao, Honghao Gui, Chengfei Lv, Qianghuai Jia, Huajun Chen, Ningyu Zhang", "abstract": "Tools serve as pivotal interfaces that enable humans to understand and\nreshape the environment. With the advent of foundation models, AI systems can\nutilize tools to expand their capabilities and interact with the real world.\nExisting tool learning methodologies, encompassing supervised fine-tuning and\nprompt engineering approaches, often induce large language models to utilize\ntools indiscriminately, as complex tasks often exceed their own competencies.\nHowever, introducing tools for simple tasks, which the models themselves can\nreadily resolve, can inadvertently propagate errors rather than enhance\nperformance. This leads to the research question: can we teach language models\nwhen and how to use tools? To meet this need, we propose Tool leaRning wIth\nexeCution fEedback (TRICE), a two-stage end-to-end framework that enables the\nmodel to continually learn through feedback derived from tool execution,\nthereby learning when and how to use tools effectively. Experimental results,\nbacked by further analysis, show that TRICE can make the large language model\nselectively use tools by improving the accuracy of tool usage while enhancing\ninsufficient tool learning and mitigating excessive reliance on tools. Code is\navailable at https://github.com/zjunlp/TRICE.", "journal": ""}
{"doi": "10.48550/arXiv.1912.09336", "date": "2019-12-19", "title": "VizWiz Dataset Browser: A Tool for Visualizing Machine Learning Datasets", "authors": "Nilavra Bhattacharya, Danna Gurari", "abstract": "We present a visualization tool to exhaustively search and browse through a\nset of large-scale machine learning datasets. Built on the top of the VizWiz\ndataset, our dataset browser tool has the potential to support and enable a\nvariety of qualitative and quantitative research, and open new directions for\nvisualizing and researching with multimodal information. The tool is publicly\navailable at https://vizwiz.org/browse.", "journal": ""}
{"doi": "10.48550/arXiv.2502.18980", "date": "2025-02-26", "title": "PEToolLLM: Towards Personalized Tool Learning in Large Language Models", "authors": "Qiancheng Xu, Yongqi Li, Heming Xia, Fan Liu, Min Yang, Wenjie Li", "abstract": "Tool learning has emerged as a promising direction by extending Large\nLanguage Models' (LLMs) capabilities with external tools. Existing tool\nlearning studies primarily focus on the general-purpose tool-use capability,\nwhich addresses explicit user requirements in instructions. However, they\noverlook the importance of personalized tool-use capability, leading to an\ninability to handle implicit user preferences. To address the limitation, we\nfirst formulate the task of personalized tool learning, which integrates user's\ninteraction history towards personalized tool usage. To fill the gap of missing\nbenchmarks, we construct PEToolBench, featuring diverse user preferences\nreflected in interaction history under three distinct personalized settings,\nand encompassing a wide range of tool-use scenarios. Moreover, we propose a\nframework PEToolLLaMA to adapt LLMs to the personalized tool learning task,\nwhich is trained through supervised fine-tuning and direct preference\noptimization. Extensive experiments on PEToolBench demonstrate the superiority\nof PEToolLLaMA over existing LLMs.", "journal": ""}
{"doi": "10.48550/arXiv.2405.17935", "date": "2024-05-28", "title": "Tool Learning with Large Language Models: A Survey", "authors": "Changle Qu, Sunhao Dai, Xiaochi Wei, Hengyi Cai, Shuaiqiang Wang, Dawei Yin, Jun Xu, Ji-Rong Wen", "abstract": "Recently, tool learning with large language models (LLMs) has emerged as a\npromising paradigm for augmenting the capabilities of LLMs to tackle highly\ncomplex problems. Despite growing attention and rapid advancements in this\nfield, the existing literature remains fragmented and lacks systematic\norganization, posing barriers to entry for newcomers. This gap motivates us to\nconduct a comprehensive survey of existing works on tool learning with LLMs. In\nthis survey, we focus on reviewing existing literature from the two primary\naspects (1) why tool learning is beneficial and (2) how tool learning is\nimplemented, enabling a comprehensive understanding of tool learning with LLMs.\nWe first explore the \"why\" by reviewing both the benefits of tool integration\nand the inherent benefits of the tool learning paradigm from six specific\naspects. In terms of \"how\", we systematically review the literature according\nto a taxonomy of four key stages in the tool learning workflow: task planning,\ntool selection, tool calling, and response generation. Additionally, we provide\na detailed summary of existing benchmarks and evaluation methods, categorizing\nthem according to their relevance to different stages. Finally, we discuss\ncurrent challenges and outline potential future directions, aiming to inspire\nboth researchers and industrial developers to further explore this emerging and\npromising area. We also maintain a GitHub repository to continually keep track\nof the relevant papers and resources in this rising area at\nhttps://github.com/quchangle1/LLM-Tool-Survey.", "journal": ""}
{"doi": "10.48550/arXiv.2506.04625", "date": "2025-06-05", "title": "Advancing Tool-Augmented Large Language Models via Meta-Verification and Reflection Learning", "authors": "Zhiyuan Ma, Jiayu Liu, Xianzhen Luo, Zhenya Huang, Qingfu Zhu, Wanxiang Che", "abstract": "Empowering large language models (LLMs) with effective tool utilization\ncapabilities is crucial for enabling AI agents to solve complex problems.\nHowever, current models face two major limitations: (1) unreliable tool\nplanning and invocation due to low-quality instruction datasets (e.g.,\nwidespread hallucinated API calls), and (2) weak tool reflection abilities\n(over 90% of errors cannot be corrected) resulting from static imitation\nlearning. To address these critical limitations, we propose Tool-MVR, a novel\nTool-Augmented LLM that achieves comprehensive System 2 reasoning through two\nkey innovations. Specifically, we first introduce Multi-Agent Meta-Verification\n(MAMV), a systematic pipeline that rigorously validates APIs, queries, and\nreasoning trajectories to construct ToolBench-V, a new high-quality instruction\ndataset that addresses the limitation of unreliable tool planning and\ninvocation. Second, we propose Exploration-based Reflection Learning (EXPLORE),\nwhich enhances tool reflection capabilities by leveraging tool feedback through\na dynamic \"Error -> Reflection -> Correction\" learning paradigm, resulting in\nour reflection dataset ToolBench-R and addressing the critical weakness in tool\nreflection. Finally, we obtain Tool-MVR by finetuning open-source LLMs (e.g.,\nQwen-7B) on both ToolBench-V and ToolBench-R. Our experiments demonstrate that\nTool-MVR achieves state-of-the-art performance on StableToolBench, surpassing\nboth ToolLLM (by 23.9%) and GPT-4 (by 15.3%) while reducing API calls by 31.4%,\nwith strong generalization capabilities across unseen tools and scenarios.\nAdditionally, on our proposed RefineToolBench, the first benchmark specifically\ndesigned to evaluate tool reflection capabilities, Tool-MVR achieves a 58.9%\nerror correction rate, significantly outperforming ToolLLM's 9.1%.", "journal": ""}
{"doi": "10.48550/arXiv.2505.16410", "date": "2025-05-22", "title": "Tool-Star: Empowering LLM-Brained Multi-Tool Reasoner via Reinforcement Learning", "authors": "Guanting Dong, Yifei Chen, Xiaoxi Li, Jiajie Jin, Hongjin Qian, Yutao Zhu, Hangyu Mao, Guorui Zhou, Zhicheng Dou, Ji-Rong Wen", "abstract": "Recently, large language models (LLMs) have shown remarkable reasoning\ncapabilities via large-scale reinforcement learning (RL). However, leveraging\nthe RL algorithm to empower effective multi-tool collaborative reasoning in\nLLMs remains an open challenge. In this paper, we introduce Tool-Star, an\nRL-based framework designed to empower LLMs to autonomously invoke multiple\nexternal tools during stepwise reasoning. Tool-Star integrates six types of\ntools and incorporates systematic designs in both data synthesis and training.\nTo address the scarcity of tool-use data, we propose a general tool-integrated\nreasoning data synthesis pipeline, which combines tool-integrated prompting\nwith hint-based sampling to automatically and scalably generate tool-use\ntrajectories. A subsequent quality normalization and difficulty-aware\nclassification process filters out low-quality samples and organizes the\ndataset from easy to hard. Furthermore, we propose a two-stage training\nframework to enhance multi-tool collaborative reasoning by: (1) cold-start\nfine-tuning, which guides LLMs to explore reasoning patterns via\ntool-invocation feedback; and (2) a multi-tool self-critic RL algorithm with\nhierarchical reward design, which reinforces reward understanding and promotes\neffective tool collaboration. Experimental analyses on over 10 challenging\nreasoning benchmarks highlight the effectiveness and efficiency of Tool-Star.\nThe code is available at https://github.com/dongguanting/Tool-Star.", "journal": ""}
{"doi": "10.48550/arXiv.2305.11554", "date": "2023-05-19", "title": "ToolkenGPT: Augmenting Frozen Language Models with Massive Tools via Tool Embeddings", "authors": "Shibo Hao, Tianyang Liu, Zhen Wang, Zhiting Hu", "abstract": "Augmenting large language models (LLMs) with external tools has emerged as a\npromising approach to solving complex problems. However, traditional methods,\nwhich finetune LLMs with tool demonstration data, can be both costly and\nrestricted to a predefined set of tools. Recent in-context learning paradigm\nalleviates these issues, but the limited context length only allows for a few\nshots of demonstrations, leading to suboptimal understandings of the tools.\nMoreover, when there are numerous tools to choose from, in-context learning\ncould completely fail to work. In this paper, we propose an alternative\napproach, $\\textbf{ToolkenGPT}$, which combines the benefits of both sides. Our\napproach represents each $\\underline{tool}$ as a to$\\underline{ken}$\n($\\textit{toolken}$) and learns an embedding for it, enabling tool calls in the\nsame way as generating a regular word token. Once a toolken is triggered, the\nLLM is prompted to complete arguments for the tool to execute. ToolkenGPT\noffers the flexibility to plug in an arbitrary number of tools by expanding the\nset of toolkens on the fly. In addition, it improves tool use by allowing\nextensive demonstration data for learning the toolken embeddings. In diverse\ndomains, including numerical reasoning, knowledge-based question answering, and\nembodied plan generation, our approach effectively augments LLMs with tools and\nsubstantially outperforms various latest baselines. ToolkenGPT demonstrates the\npromising ability to use relevant tools from a large tool set in complex\nscenarios.", "journal": ""}
{"doi": "10.48550/arXiv.2403.04746", "date": "2024-03-07", "title": "LLMs in the Imaginarium: Tool Learning through Simulated Trial and Error", "authors": "Boshi Wang, Hao Fang, Jason Eisner, Benjamin Van Durme, Yu Su", "abstract": "Tools are essential for large language models (LLMs) to acquire up-to-date\ninformation and take consequential actions in external environments. Existing\nwork on tool-augmented LLMs primarily focuses on the broad coverage of tools\nand the flexibility of adding new tools. However, a critical aspect that has\nsurprisingly been understudied is simply how accurately an LLM uses tools for\nwhich it has been trained. We find that existing LLMs, including GPT-4 and\nopen-source LLMs specifically fine-tuned for tool use, only reach a correctness\nrate in the range of 30% to 60%, far from reliable use in practice. We propose\na biologically inspired method for tool-augmented LLMs, simulated trial and\nerror (STE), that orchestrates three key mechanisms for successful tool use\nbehaviors in the biological system: trial and error, imagination, and memory.\nSpecifically, STE leverages an LLM's 'imagination' to simulate plausible\nscenarios for using a tool, after which the LLM interacts with the tool to\nlearn from its execution feedback. Both short-term and long-term memory are\nemployed to improve the depth and breadth of the exploration, respectively.\nComprehensive experiments on ToolBench show that STE substantially improves\ntool learning for LLMs under both in-context learning and fine-tuning settings,\nbringing a boost of 46.7% to Mistral-Instruct-7B and enabling it to outperform\nGPT-4. We also show effective continual learning of tools via a simple\nexperience replay strategy.", "journal": ""}
{"doi": "10.48550/arXiv.2402.15960", "date": "2024-02-25", "title": "Budget-Constrained Tool Learning with Planning", "authors": "Yuanhang Zheng, Peng Li, Ming Yan, Ji Zhang, Fei Huang, Yang Liu", "abstract": "Despite intensive efforts devoted to tool learning, the problem of\nbudget-constrained tool learning, which focuses on resolving user queries\nwithin a specific budget constraint, has been widely overlooked. This paper\nproposes a novel method for budget-constrained tool learning. Our approach\ninvolves creating a preferable plan under the budget constraint before\nutilizing the tools. This plan outlines the feasible tools and the maximum\nnumber of times they can be employed, offering a comprehensive overview of the\ntool learning process for large language models. This allows them to allocate\nthe budget from a broader perspective. To devise the plan without incurring\nsignificant extra costs, we suggest initially estimating the usefulness of the\ncandidate tools based on past experience. Subsequently, we employ dynamic\nprogramming to formulate the plan. Experimental results demonstrate that our\nmethod can be integrated with various tool learning methods, significantly\nenhancing their effectiveness under strict budget constraints.", "journal": ""}
{"doi": "10.48550/arXiv.2310.19124", "date": "2023-10-29", "title": "Good Tools are Half the Work: Tool Usage in Deep Learning Projects", "authors": "Evangelia Panourgia, Theodoros Plessas, Ilias Balampanis, Diomidis Spinellis", "abstract": "The rising popularity of deep learning (DL) methods and techniques has\ninvigorated interest in the topic of SE4DL (Software Engineering for Deep\nLearning), the application of software engineering (SE) practices on deep\nlearning software. Despite the novel engineering challenges brought on by the\ndata-driven and non-deterministic paradigm of DL software, little work has been\ninvested into developing DL-targeted SE tools. On the other hand, tools\ntackling non-SE issues specific to DL are actively used and referred to under\nthe umbrella term \"MLOps (Machine Learning Operations) tools\". Nevertheless,\nthe available literature supports the utility of conventional SE tooling in DL\nsoftware development. Building upon previous mining software repositories (MSR)\nresearch on tool usage in open-source software works, we identify conventional\nand MLOps tools adopted in popular applied DL projects that use Python as the\nmain programming language. About 63\\% of the GitHub repositories we examined\ncontained at least one conventional SE tool. Software construction tools are\nthe most widely adopted, while the opposite applies to management and\nmaintenance tools. Relatively few MLOps tools were found to be use, with only\n20 tools out of a sample of 74 used in at least one repository. The majority of\nthem were open-source rather than proprietary. One of these tools, TensorBoard,\nwas found to be adopted in about half of the repositories in our study.\nConsequently, the widespread use of conventional SE tooling demonstrates its\nrelevance to DL software. Further research is recommended on the adoption of\nMLOps tooling, focusing on the relevance of particular tool types, the\ndevelopment of required tools, as well as ways to promote the use of already\navailable tools.", "journal": ""}
{"doi": "10.48550/arXiv.2308.14034", "date": "2023-08-27", "title": "Confucius: Iterative Tool Learning from Introspection Feedback by Easy-to-Difficult Curriculum", "authors": "Shen Gao, Zhengliang Shi, Minghang Zhu, Bowen Fang, Xin Xin, Pengjie Ren, Zhumin Chen, Jun Ma, Zhaochun Ren", "abstract": "Augmenting large language models (LLMs) with external tools has emerged as a\npromising approach to extending the capability of LLMs. Although some works\nemploy open-source LLMs for the tool learning task, most of them are trained in\na controlled environment in which LLMs only learn to execute the human-provided\ntools. However, selecting proper tools from the large toolset is also a crucial\nability for the tool learning model to be applied in real-world applications.\nExisting methods usually directly employ self-instruction methods to train the\nmodel, which ignores differences in tool complexity. In this paper, we propose\nthe Confucius, a novel tool learning framework to train LLM to use complicated\ntools in real-world scenarios, which contains two main phases: (1) We first\npropose a multi-stage learning method to teach the LLM to use various tools\nfrom an easy-to-difficult curriculum; (2) thenceforth, we propose the Iterative\nSelf-instruct from Introspective Feedback (ISIF) to dynamically construct the\ndataset to improve the ability to use the complicated tool. Extensive\nexperiments conducted on both controlled and real-world settings demonstrate\nthe superiority of our tool learning framework in the real-world application\nscenarios compared to both tuning-free (e.g. ChatGPT, Claude) and tuning-based\nbaselines (e.g. GPT4Tools).", "journal": ""}
{"doi": "10.48550/arXiv.1207.2280", "date": "2012-07-10", "title": "Understanding the Learners' Actions when using Mathematics Learning Tools", "authors": "Paul Libbrecht, Sandra Rebholz, Daniel Herding, Wolfgang M\u00fcller, Felix Tscheulin", "abstract": "The use of computer-based mathematics tools is widespread in learning.\nDepending on the way that these tools assess the learner's solution paths, one\ncan distinguish between automatic assessment tools and semi-automatic\nassessment tools. Automatic assessment tools directly provide all feedback\nnecessary to the learners, while semi-automatic assessment tools involve the\nteachers as part the assessment process. They are provided with as much\ninformation as possible on the learners' interactions with the tool.\n  How can the teachers know how the learning tools were used and which\nintermediate steps led to a solution? How can the teachers respond to a\nlearner's question that arises while using a computer tool? Little is available\nto answer this beyond interacting directly with the computer and performing a\nfew manipulations to understand the tools' state.\n  This paper presents SMALA, a web-based logging architecture that addresses\nthese problems by recording, analyzing and representing user actions. While\nrespecting the learner's privacy, the SMALA architecture supports the teachers\nby offering fine-grained representations of the learners' activities as well as\noverviews of the progress of a classroom.", "journal": ""}
{"doi": "10.48550/arXiv.2503.16779", "date": "2025-03-21", "title": "Chain-of-Tools: Utilizing Massive Unseen Tools in the CoT Reasoning of Frozen Language Models", "authors": "Mengsong Wu, Tong Zhu, Han Han, Xiang Zhang, Wenbiao Shao, Wenliang Chen", "abstract": "Tool learning can further broaden the usage scenarios of large language\nmodels (LLMs). However most of the existing methods either need to finetune\nthat the model can only use tools seen in the training data, or add tool\ndemonstrations into the prompt with lower efficiency. In this paper, we present\na new Tool Learning method Chain-of-Tools. It makes full use of the powerful\nsemantic representation capability of frozen LLMs to finish tool calling in CoT\nreasoning with a huge and flexible tool pool which may contain unseen tools.\nEspecially, to validate the effectiveness of our approach in the massive unseen\ntool scenario, we construct a new dataset SimpleToolQuestions. We conduct\nexperiments on two numerical reasoning benchmarks (GSM8K-XL and FuncQA) and two\nknowledge-based question answering benchmarks (KAMEL and SimpleToolQuestions).\nExperimental results show that our approach performs better than the baseline.\nWe also identify dimensions of the model output that are critical in tool\nselection, enhancing the model interpretability. Our code and data are\navailable at: https://github.com/fairyshine/Chain-of-Tools .", "journal": ""}
{"doi": "10.48550/arXiv.2406.03807", "date": "2024-06-06", "title": "Tool-Planner: Task Planning with Clusters across Multiple Tools", "authors": "Yanming Liu, Xinyue Peng, Jiannan Cao, Shi Bo, Yuwei Zhang, Xuhong Zhang, Sheng Cheng, Xun Wang, Jianwei Yin, Tianyu Du", "abstract": "Large language models (LLMs) have demonstrated exceptional reasoning\ncapabilities, enabling them to solve various complex problems. Recently, this\nability has been applied to the paradigm of tool learning. Tool learning\ninvolves providing examples of tool usage and their corresponding functions,\nallowing LLMs to formulate plans and demonstrate the process of invoking and\nexecuting each tool. LLMs can address tasks that they cannot complete\nindependently, thereby enhancing their potential across different tasks.\nHowever, this approach faces two key challenges. First, redundant error\ncorrection leads to unstable planning and long execution time. Additionally,\ndesigning a correct plan among multiple tools is also a challenge in tool\nlearning. To address these issues, we propose Tool-Planner, a task-processing\nframework based on toolkits. Tool-Planner groups tools based on the API\nfunctions with the same function into a toolkit and allows LLMs to implement\nplanning across the various toolkits. When a tool error occurs, the language\nmodel can reselect and adjust tools based on the toolkit. Experiments show that\nour approach demonstrates a high pass and win rate across different datasets\nand optimizes the planning scheme for tool learning in models such as GPT-4 and\nClaude 3, showcasing the potential of our method. Our code is public at\nhttps://github.com/OceannTwT/Tool-Planner", "journal": ""}
{"doi": "10.48550/arXiv.2111.04455", "date": "2021-10-29", "title": "Systematic Review for AI-based Language Learning Tools", "authors": "Jin Ha Woo, Heeyoul Choi", "abstract": "The Second Language Acquisition field has been significantly impacted by a\ngreater emphasis on individualized learning and rapid developments in\nartificial intelligence (AI). Although increasingly adaptive language learning\ntools are being developed with the application of AI to the Computer Assisted\nLanguage Learning field, there have been concerns regarding insufficient\ninformation and teacher preparation. To effectively utilize these tools,\nteachers need an in-depth overview on recently developed AI-based language\nlearning tools. Therefore, this review synthesized information on AI tools that\nwere developed between 2017 and 2020. A majority of these tools utilized\nmachine learning and natural language processing, and were used to identify\nerrors, provide feedback, and assess language abilities. After using these\ntools, learners demonstrated gains in their language abilities and knowledge.\nThis review concludes by presenting pedagogical implications and emerging\nthemes in the future research of AI-based language learning tools.", "journal": ""}
{"doi": "10.48550/arXiv.2101.05840", "date": "2021-01-14", "title": "A Neophyte With AutoML: Evaluating the Promises of Automatic Machine Learning Tools", "authors": "Oleg Bezrukavnikov, Rhema Linder", "abstract": "This paper discusses modern Auto Machine Learning (AutoML) tools from the\nperspective of a person with little prior experience in Machine Learning (ML).\nThere are many AutoML tools both ready-to-use and under development, which are\ncreated to simplify and democratize usage of ML technologies in everyday life.\nOur position is that ML should be easy to use and available to a greater number\nof people. Prior research has identified the need for intuitive AutoML tools.\nThis work seeks to understand how well AutoML tools have achieved that goal in\npractice. We evaluate three AutoML Tools to evaluate the end-user experience\nand system performance. We evaluate the tools by having them create models from\na competition dataset on banking data. We report on their performance and the\ndetails of our experience. This process provides a unique understanding of the\nstate of the art of AutoML tools. Finally, we use these experiences to inform a\ndiscussion on how future AutoML tools can improve the user experience for\nneophytes of Machine Learning.", "journal": ""}
{"doi": "10.48550/arXiv.1809.08613", "date": "2018-09-23", "title": "Detecting Features of Tools, Objects, and Actions from Effects in a Robot using Deep Learning", "authors": "Namiko Saito, Kitae Kim, Shingo Murata, Tetsuya Ogata, Shigeki Sugano", "abstract": "We propose a tool-use model that can detect the features of tools, target\nobjects, and actions from the provided effects of object manipulation. We\nconstruct a model that enables robots to manipulate objects with tools, using\ninfant learning as a concept. To realize this, we train sensory-motor data\nrecorded during a tool-use task performed by a robot with deep learning.\nExperiments include four factors: (1) tools, (2) objects, (3) actions, and (4)\neffects, which the model considers simultaneously. For evaluation, the robot\ngenerates predicted images and motions given information of the effects of\nusing unknown tools and objects. We confirm that the robot is capable of\ndetecting features of tools, objects, and actions by learning the effects and\nexecuting the task.", "journal": ""}
{"doi": "10.48550/arXiv.2206.13074", "date": "2022-06-27", "title": "Leveraging Language for Accelerated Learning of Tool Manipulation", "authors": "Allen Z. Ren, Bharat Govil, Tsung-Yen Yang, Karthik Narasimhan, Anirudha Majumdar", "abstract": "Robust and generalized tool manipulation requires an understanding of the\nproperties and affordances of different tools. We investigate whether\nlinguistic information about a tool (e.g., its geometry, common uses) can help\ncontrol policies adapt faster to new tools for a given task. We obtain diverse\ndescriptions of various tools in natural language and use pre-trained language\nmodels to generate their feature representations. We then perform\nlanguage-conditioned meta-learning to learn policies that can efficiently adapt\nto new tools given their corresponding text descriptions. Our results\ndemonstrate that combining linguistic information and meta-learning\nsignificantly accelerates tool learning in several manipulation tasks including\npushing, lifting, sweeping, and hammering.", "journal": ""}
{"doi": "10.48550/arXiv.2407.03007", "date": "2024-07-03", "title": "What Affects the Stability of Tool Learning? An Empirical Study on the Robustness of Tool Learning Frameworks", "authors": "Chengrui Huang, Zhengliang Shi, Yuntao Wen, Xiuying Chen, Peng Han, Shen Gao, Shuo Shang", "abstract": "Tool learning methods have enhanced the ability of large language models\n(LLMs) to interact with real-world applications. Many existing works fine-tune\nLLMs or design prompts to enable LLMs to select appropriate tools and correctly\ninvoke them to meet user requirements. However, it is observed in previous\nworks that the performance of tool learning varies from tasks, datasets,\ntraining settings, and algorithms. Without understanding the impact of these\nfactors, it can lead to inconsistent results, inefficient model deployment, and\nsuboptimal tool utilization, ultimately hindering the practical integration and\nscalability of LLMs in real-world scenarios. Therefore, in this paper, we\nexplore the impact of both internal and external factors on the performance of\ntool learning frameworks. Through extensive experiments on two benchmark\ndatasets, we find several insightful conclusions for future work, including the\nobservation that LLMs can benefit significantly from increased trial and\nexploration. We believe our empirical study provides a new perspective for\nfuture tool learning research.", "journal": ""}
{"doi": "10.48550/arXiv.1204.4092", "date": "2012-04-17", "title": "Conception of a management tool of Technology Enhanced Learning Environments", "authors": "Sergio Andre Ferreira, Antonio Andrade", "abstract": "This paper describes the process of the conception of a software tool of TELE\nmanagement. The proposed management tool combines information from two sources:\ni) the automatic reports produced by the Learning Content Management System\n(LCMS) Blackboard and ii) the views of students and teachers on the use of the\nLCMS in the process of teaching and learning. The results show that the\narchitecture of the proposed management tool has the features of a management\ntool, since its potential to control, to reset and to enhance the use of an\nLCMS in the process of teaching and learning and teacher training, is shown.", "journal": "International Journal of Advanced Computer Science and\n  Applications,Vol. 3, No.2, 2012, pp.42-47"}
{"doi": "10.48550/arXiv.1602.00203", "date": "2016-01-31", "title": "Greedy Deep Dictionary Learning", "authors": "Snigdha Tariyal, Angshul Majumdar, Richa Singh, Mayank Vatsa", "abstract": "In this work we propose a new deep learning tool called deep dictionary\nlearning. Multi-level dictionaries are learnt in a greedy fashion, one layer at\na time. This requires solving a simple (shallow) dictionary learning problem,\nthe solution to this is well known. We apply the proposed technique on some\nbenchmark deep learning datasets. We compare our results with other deep\nlearning tools like stacked autoencoder and deep belief network; and state of\nthe art supervised dictionary learning tools like discriminative KSVD and label\nconsistent KSVD. Our method yields better results than all.", "journal": ""}
{"doi": "10.48550/arXiv.2208.13116", "date": "2022-08-28", "title": "An Empirical Study on the Usage of Automated Machine Learning Tools", "authors": "Forough Majidi, Moses Openja, Foutse Khomh, Heng Li", "abstract": "The popularity of automated machine learning (AutoML) tools in different\ndomains has increased over the past few years. Machine learning (ML)\npractitioners use AutoML tools to automate and optimize the process of feature\nengineering, model training, and hyperparameter optimization and so on. Recent\nwork performed qualitative studies on practitioners' experiences of using\nAutoML tools and compared different AutoML tools based on their performance and\nprovided features, but none of the existing work studied the practices of using\nAutoML tools in real-world projects at a large scale. Therefore, we conducted\nan empirical study to understand how ML practitioners use AutoML tools in their\nprojects. To this end, we examined the top 10 most used AutoML tools and their\nrespective usages in a large number of open-source project repositories hosted\non GitHub. The results of our study show 1) which AutoML tools are mostly used\nby ML practitioners and 2) the characteristics of the repositories that use\nthese AutoML tools. Also, we identified the purpose of using AutoML tools (e.g.\nmodel parameter sampling, search space management, model\nevaluation/error-analysis, Data/ feature transformation, and data labeling) and\nthe stages of the ML pipeline (e.g. feature engineering) where AutoML tools are\nused. Finally, we report how often AutoML tools are used together in the same\nsource code files. We hope our results can help ML practitioners learn about\ndifferent AutoML tools and their usages, so that they can pick the right tool\nfor their purposes. Besides, AutoML tool developers can benefit from our\nfindings to gain insight into the usages of their tools and improve their tools\nto better fit the users' usages and needs.", "journal": ""}
{"doi": "10.48550/arXiv.2403.00839", "date": "2024-02-29", "title": "ToolNet: Connecting Large Language Models with Massive Tools via Tool Graph", "authors": "Xukun Liu, Zhiyuan Peng, Xiaoyuan Yi, Xing Xie, Lirong Xiang, Yuchen Liu, Dongkuan Xu", "abstract": "While achieving remarkable progress in a broad range of tasks, large language\nmodels (LLMs) remain significantly limited in properly using massive external\ntools. Existing in-context learning approaches simply format tools into a list\nof plain text descriptions and input them to LLMs, from which, LLMs generate a\nsequence of tool calls to solve problems step by step. Such a paradigm ignores\nthe intrinsic dependency between tools and offloads all reasoning loads to\nLLMs, making them restricted to a limited number of specifically designed\ntools. It thus remains challenging for LLMs to operate on a library of massive\ntools, casting a great limitation when confronted with real-world scenarios.\nThis paper proposes ToolNet, a plug-and-play framework that scales up the\nnumber of tools to thousands with a moderate increase in token consumption.\nToolNet organizes tools into a directed graph. Each node represents a tool, and\nweighted edges denote tool transition. Starting from an initial tool node, an\nLLM navigates in the graph by iteratively choosing the next one from its\nsuccessors until the task is resolved. Extensive experiments show that ToolNet\ncan achieve impressive results in challenging multi-hop tool learning datasets\nand is resilient to tool failures.", "journal": ""}
{"doi": "10.48550/arXiv.2304.12602", "date": "2023-04-25", "title": "Is deep learning a useful tool for the pure mathematician?", "authors": "Geordie Williamson", "abstract": "A personal and informal account of what a pure mathematician might expect\nwhen using tools from deep learning in their research.", "journal": ""}
{"doi": "10.48550/arXiv.2401.00741", "date": "2024-01-01", "title": "ToolEyes: Fine-Grained Evaluation for Tool Learning Capabilities of Large Language Models in Real-world Scenarios", "authors": "Junjie Ye, Guanyu Li, Songyang Gao, Caishuang Huang, Yilong Wu, Sixian Li, Xiaoran Fan, Shihan Dou, Tao Ji, Qi Zhang, Tao Gui, Xuanjing Huang", "abstract": "Existing evaluations of tool learning primarily focus on validating the\nalignment of selected tools for large language models (LLMs) with expected\noutcomes. However, these approaches rely on a limited set of scenarios where\nanswers can be pre-determined, diverging from genuine needs. Furthermore, a\nsole emphasis on outcomes disregards the complex capabilities required for LLMs\nto effectively use tools. To tackle this issue, we propose ToolEyes, a\nfine-grained system tailored for the evaluation of the LLMs' tool learning\ncapabilities in authentic scenarios. The system meticulously examines seven\nreal-world scenarios, analyzing five dimensions crucial to LLMs in tool\nlearning: format alignment, intent comprehension, behavior planning, tool\nselection, and answer organization. Additionally, ToolEyes incorporates a tool\nlibrary boasting approximately 600 tools, serving as an intermediary between\nLLMs and the physical world. Evaluations involving ten LLMs across three\ncategories reveal a preference for specific scenarios and limited cognitive\nabilities in tool learning. Intriguingly, expanding the model size even\nexacerbates the hindrance to tool learning. The code and data are available at\nhttps://github.com/Junjie-Ye/ToolEyes.", "journal": ""}
{"doi": "10.48550/arXiv.2505.08617", "date": "2025-05-13", "title": "OpenThinkIMG: Learning to Think with Images via Visual Tool Reinforcement Learning", "authors": "Zhaochen Su, Linjie Li, Mingyang Song, Yunzhuo Hao, Zhengyuan Yang, Jun Zhang, Guanjie Chen, Jiawei Gu, Juntao Li, Xiaoye Qu, Yu Cheng", "abstract": "While humans can flexibly leverage interactive visual cognition for complex\nproblem-solving, enabling Large Vision-Language Models (LVLMs) to learn\nsimilarly adaptive behaviors with visual tools remains challenging. A\nsignificant hurdle is the current lack of standardized infrastructure, which\nhinders integrating diverse tools, generating rich interaction data, and\ntraining robust agents effectively. To address these gaps, we introduce\nOpenThinkIMG, the first open-source, comprehensive end-to-end framework for\ntool-augmented LVLMs. It features standardized vision tool interfaces, scalable\ntrajectory generation for policy initialization, and a flexible training\nenvironment. Furthermore, considering supervised fine-tuning (SFT) on static\ndemonstrations offers limited policy generalization for dynamic tool\ninvocation, we propose a novel reinforcement learning (RL) framework V-ToolRL\nto train LVLMs to learn adaptive policies for invoking external vision tools.\nV-ToolRL enables LVLMs to autonomously discover optimal tool-usage strategies\nby directly optimizing for task success using feedback from tool interactions.\nWe empirically validate V-ToolRL on challenging chart reasoning tasks. Our\nRL-trained agent, built upon a Qwen2-VL-2B, significantly outperforms its\nSFT-initialized counterpart (+28.83 points) and surpasses established\nsupervised tool-learning baselines like Taco and CogCom by an average of +12.7\npoints. Notably, it also surpasses prominent closed-source models like GPT-4.1\nby +8.68 accuracy points. We hope OpenThinkIMG can serve as a foundational\nframework for advancing dynamic, tool-augmented visual reasoning, helping the\ncommunity develop AI agents that can genuinely \"think with images\".", "journal": ""}
{"doi": "10.48550/arXiv.2305.17126", "date": "2023-05-26", "title": "Large Language Models as Tool Makers", "authors": "Tianle Cai, Xuezhi Wang, Tengyu Ma, Xinyun Chen, Denny Zhou", "abstract": "Recent research has highlighted the potential of large language models (LLMs)\nto improve their problem-solving capabilities with the aid of suitable external\ntools. In our work, we further advance this concept by introducing a\nclosed-loop framework, referred to as LLMs A s Tool Makers (LATM), where LLMs\ncreate their own reusable tools for problem-solving. Our approach consists of\ntwo phases: 1) tool making: an LLM acts as the tool maker that crafts tools for\na set of tasks. 2) tool using: another LLM acts as the tool user, which applies\nthe tool built by the tool maker for problem-solving. On the problem-solving\nserver side, tool-making enables continual tool generation and caching as new\nrequests emerge. This framework enables subsequent requests to access cached\ntools via their corresponding APIs, enhancing the efficiency of task\nresolution. Recognizing that tool-making requires more sophisticated\ncapabilities, we assign this task to a powerful, albeit resource-intensive,\nmodel. Conversely, the simpler tool-using phase is delegated to a lightweight\nmodel. This strategic division of labor allows the once-off cost of tool-making\nto be spread over multiple instances of tool-using, significantly reducing\naverage costs while maintaining strong performance. Furthermore, our method\noffers a functional cache through the caching and reuse of tools, which stores\nthe functionality of a class of requests instead of the natural language\nresponses from LLMs, thus extending the applicability of the conventional cache\nmechanism. We evaluate our approach across various complex reasoning tasks,\nincluding Big-Bench tasks. With GPT-4 as the tool maker and GPT-3.5 as the tool\nuser, LATM demonstrates performance equivalent to using GPT-4 for both roles,\nbut with a significantly reduced inference cost.", "journal": ""}
{"doi": "10.48550/arXiv.2211.16846", "date": "2022-11-30", "title": "Universal Feature Selection Tool (UniFeat): An Open-Source Tool for Dimensionality Reduction", "authors": "Sina Tabakhi, Parham Moradi", "abstract": "The Universal Feature Selection Tool (UniFeat) is an open-source tool\ndeveloped entirely in Java for performing feature selection processes in\nvarious research areas. It provides a set of well-known and advanced feature\nselection methods within its significant auxiliary tools. This allows users to\ncompare the performance of feature selection methods. Moreover, due to the\nopen-source nature of UniFeat, researchers can use and modify it in their\nresearch, which facilitates the rapid development of new feature selection\nalgorithms.", "journal": ""}
{"doi": "10.48550/arXiv.2212.14133", "date": "2022-12-29", "title": "Investigating Sindy As a Tool For Causal Discovery In Time Series Signals", "authors": "Andrew O'Brien, Rosina Weber, Edward Kim", "abstract": "The SINDy algorithm has been successfully used to identify the governing\nequations of dynamical systems from time series data. In this paper, we argue\nthat this makes SINDy a potentially useful tool for causal discovery and that\nexisting tools for causal discovery can be used to dramatically improve the\nperformance of SINDy as tool for robust sparse modeling and system\nidentification. We then demonstrate empirically that augmenting the SINDy\nalgorithm with tools from causal discovery can provides engineers with a tool\nfor learning causally robust governing equations.", "journal": ""}
{"doi": "10.48550/arXiv.2404.09339", "date": "2024-04-14", "title": "Towards Practical Tool Usage for Continually Learning LLMs", "authors": "Jerry Huang, Prasanna Parthasarathi, Mehdi Rezagholizadeh, Sarath Chandar", "abstract": "Large language models (LLMs) show an innate skill for solving language based\ntasks. But insights have suggested an inability to adjust for information or\ntask-solving skills becoming outdated, as their knowledge, stored directly\nwithin their parameters, remains static in time. Tool use helps by offloading\nwork to systems that the LLM can access through an interface, but LLMs that use\nthem still must adapt to nonstationary environments for prolonged use, as new\ntools can emerge and existing tools can change. Nevertheless, tools require\nless specialized knowledge, therefore we hypothesize they are better suited for\ncontinual learning (CL) as they rely less on parametric memory for solving\ntasks and instead focus on learning when to apply pre-defined tools. To verify\nthis, we develop a synthetic benchmark and follow this by aggregating existing\nNLP tasks to form a more realistic testing scenario. While we demonstrate\nscaling model size is not a solution, regardless of tool usage, continual\nlearning techniques can enable tool LLMs to both adapt faster while forgetting\nless, highlighting their potential as continual learners.", "journal": ""}
{"doi": "10.48550/arXiv.2304.03254", "date": "2023-04-06", "title": "Toward End-to-End MLOps Tools Map: A Preliminary Study based on a Multivocal Literature Review", "authors": "Sergio Moreschi, Gilberto Recupito, Valentina Lenarduzzi, Fabio Palomba, David Hastbacka, Davide Taibi", "abstract": "MLOps tools enable continuous development of machine learning, following the\nDevOps process. Different MLOps tools have been presented on the market,\nhowever, such a number of tools often create confusion on the most appropriate\ntool to be used in each DevOps phase. To overcome this issue, we conducted a\nmultivocal literature review mapping 84 MLOps tools identified from 254 Primary\nStudies, on the DevOps phases, highlighting their purpose, and possible\nincompatibilities. The result of this work will be helpful to both\npractitioners and researchers, as a starting point for future investigations on\nMLOps tools, pipelines, and processes.", "journal": ""}
{"doi": "10.48550/arXiv.1705.02477", "date": "2017-05-06", "title": "Metacognitive Learning Approach for Online Tool Condition Monitoring", "authors": "Mahardhika Pratama, Eric Dimla, Chow Yin Lai, Edwin Lughofer", "abstract": "As manufacturing processes become increasingly automated, so should tool\ncondition monitoring (TCM) as it is impractical to have human workers monitor\nthe state of the tools continuously. Tool condition is crucial to ensure the\ngood quality of products: Worn tools affect not only the surface quality but\nalso the dimensional accuracy, which means higher reject rate of the products.\nTherefore, there is an urgent need to identify tool failures before it occurs\non the fly. While various versions of intelligent tool condition monitoring\nhave been proposed, most of them suffer from a cognitive nature of traditional\nmachine learning algorithms. They focus on the how to learn process without\npaying attention to other two crucial issues: what to learn, and when to learn.\nThe what to learn and the when to learn provide self regulating mechanisms to\nselect the training samples and to determine time instants to train a model. A\nnovel tool condition monitoring approach based on a psychologically plausible\nconcept, namely the metacognitive scaffolding theory, is proposed and built\nupon a recently published algorithm, recurrent classifier (rClass). The\nlearning process consists of three phases: what to learn, how to learn, when to\nlearn and makes use of a generalized recurrent network structure as a cognitive\ncomponent. Experimental studies with real-world manufacturing data streams were\nconducted where rClass demonstrated the highest accuracy while retaining the\nlowest complexity over its counterparts.", "journal": ""}
{"doi": "10.48550/arXiv.2310.00156", "date": "2023-09-29", "title": "Learning Generalizable Tool-use Skills through Trajectory Generation", "authors": "Carl Qi, Yilin Wu, Lifan Yu, Haoyue Liu, Bowen Jiang, Xingyu Lin, David Held", "abstract": "Autonomous systems that efficiently utilize tools can assist humans in\ncompleting many common tasks such as cooking and cleaning. However, current\nsystems fall short of matching human-level of intelligence in terms of adapting\nto novel tools. Prior works based on affordance often make strong assumptions\nabout the environments and cannot scale to more complex, contact-rich tasks. In\nthis work, we tackle this challenge and explore how agents can learn to use\npreviously unseen tools to manipulate deformable objects. We propose to learn a\ngenerative model of the tool-use trajectories as a sequence of tool point\nclouds, which generalizes to different tool shapes. Given any novel tool, we\nfirst generate a tool-use trajectory and then optimize the sequence of tool\nposes to align with the generated trajectory. We train a single model on four\ndifferent challenging deformable object manipulation tasks, using demonstration\ndata from only one tool per task. The model generalizes to various novel tools,\nsignificantly outperforming baselines. We further test our trained policy in\nthe real world with unseen tools, where it achieves the performance comparable\nto human. Additional materials can be found on our project website:\nhttps://sites.google.com/view/toolgen.", "journal": ""}
{"doi": "10.48550/arXiv.2410.08197", "date": "2024-10-10", "title": "From Exploration to Mastery: Enabling LLMs to Master Tools via Self-Driven Interactions", "authors": "Changle Qu, Sunhao Dai, Xiaochi Wei, Hengyi Cai, Shuaiqiang Wang, Dawei Yin, Jun Xu, Ji-Rong Wen", "abstract": "Tool learning enables Large Language Models (LLMs) to interact with external\nenvironments by invoking tools, serving as an effective strategy to mitigate\nthe limitations inherent in their pre-training data. In this process, tool\ndocumentation plays a crucial role by providing usage instructions for LLMs,\nthereby facilitating effective tool utilization. This paper concentrates on the\ncritical challenge of bridging the comprehension gap between LLMs and external\ntools due to the inadequacies and inaccuracies inherent in existing\nhuman-centric tool documentation. We propose a novel framework, DRAFT, aimed at\nDynamically Refining tool documentation through the Analysis of Feedback and\nTrials emanating from LLMs' interactions with external tools. This methodology\npivots on an innovative trial-and-error approach, consisting of three distinct\nlearning phases: experience gathering, learning from experience, and\ndocumentation rewriting, to iteratively enhance the tool documentation. This\nprocess is further optimized by implementing a diversity-promoting exploration\nstrategy to ensure explorative diversity and a tool-adaptive termination\nmechanism to prevent overfitting while enhancing efficiency. Extensive\nexperiments on multiple datasets demonstrate that DRAFT's iterative,\nfeedback-based refinement significantly ameliorates documentation quality,\nfostering a deeper comprehension and more effective utilization of tools by\nLLMs. Notably, our analysis reveals that the tool documentation refined via our\napproach demonstrates robust cross-model generalization capabilities.", "journal": ""}
{"doi": "10.48550/arXiv.1906.11608", "date": "2019-06-27", "title": "Simple Natural Language Processing Tools for Danish", "authors": "Leon Derczynski", "abstract": "This technical note describes a set of baseline tools for automatic\nprocessing of Danish text. The tools are machine-learning based, using natural\nlanguage processing models trained over previously annotated documents. They\nare maintained at ITU Copenhagen and will always be freely available.", "journal": ""}
{"doi": "10.48550/arXiv.1602.05720", "date": "2016-02-18", "title": "Improving Students' Understanding of Quantum Measurement", "authors": "Guangtian Zhu, Chandralekha Singh", "abstract": "We describe the difficulties advanced undergraduate and graduate students\nhave with quantum measurement. To reduce these difficulties, we have developed\nresearch-based learning tools such as the Quantum Interactive Learning Tutorial\n(QuILT) and peer instruction tools. A preliminary evaluation shows that these\nlearning tools are effective in improving students' understanding of concepts\nrelated to quantum measurement.", "journal": ""}
{"doi": "10.48550/arXiv.2407.01200", "date": "2024-07-01", "title": "Deep Learning Approach for Enhanced Transferability and Learning Capacity in Tool Wear Estimation", "authors": "Zongshuo Li, Markus Meurer, Thomas Bergs", "abstract": "As an integral part of contemporary manufacturing, monitoring systems obtain\nvaluable information during machining to oversee the condition of both the\nprocess and the machine. Recently, diverse algorithms have been employed to\ndetect tool wear using single or multiple sources of measurements. In this\nstudy, a deep learning approach is proposed for estimating tool wear,\nconsidering cutting parameters. The model's accuracy and transferability in\ntool wear estimation were assessed with milling experiments conducted under\nvarying cutting parameters. The results indicate that the proposed method\noutperforms conventional methods in terms of both transferability and rapid\nlearning capabilities.", "journal": ""}
{"doi": "10.48550/arXiv.2304.08354", "date": "2023-04-17", "title": "Tool Learning with Foundation Models", "authors": "Yujia Qin, Shengding Hu, Yankai Lin, Weize Chen, Ning Ding, Ganqu Cui, Zheni Zeng, Yufei Huang, Chaojun Xiao, Chi Han, Yi Ren Fung, Yusheng Su, Huadong Wang, Cheng Qian, Runchu Tian, Kunlun Zhu, Shihao Liang, Xingyu Shen, Bokai Xu, Zhen Zhang, Yining Ye, Bowen Li, Ziwei Tang, Jing Yi, Yuzhang Zhu, Zhenning Dai, Lan Yan, Xin Cong, Yaxi Lu, Weilin Zhao, Yuxiang Huang, Junxi Yan, Xu Han, Xian Sun, Dahai Li, Jason Phang, Cheng Yang, Tongshuang Wu, Heng Ji, Zhiyuan Liu, Maosong Sun", "abstract": "Humans possess an extraordinary ability to create and utilize tools, allowing\nthem to overcome physical limitations and explore new frontiers. With the\nadvent of foundation models, AI systems have the potential to be equally adept\nin tool use as humans. This paradigm, i.e., tool learning with foundation\nmodels, combines the strengths of specialized tools and foundation models to\nachieve enhanced accuracy, efficiency, and automation in problem-solving.\nDespite its immense potential, there is still a lack of a comprehensive\nunderstanding of key challenges, opportunities, and future endeavors in this\nfield. To this end, we present a systematic investigation of tool learning in\nthis paper. We first introduce the background of tool learning, including its\ncognitive origins, the paradigm shift of foundation models, and the\ncomplementary roles of tools and models. Then we recapitulate existing tool\nlearning research into tool-augmented and tool-oriented learning. We formulate\na general tool learning framework: starting from understanding the user\ninstruction, models should learn to decompose a complex task into several\nsubtasks, dynamically adjust their plan through reasoning, and effectively\nconquer each sub-task by selecting appropriate tools. We also discuss how to\ntrain models for improved tool-use capabilities and facilitate the\ngeneralization in tool learning. Considering the lack of a systematic tool\nlearning evaluation in prior works, we experiment with 18 representative tools\nand show the potential of current foundation models in skillfully utilizing\ntools. Finally, we discuss several open problems that require further\ninvestigation for tool learning. In general, we hope this paper could inspire\nfuture research in integrating tools with foundation models.", "journal": ""}
{"doi": "10.48550/arXiv.2404.00450", "date": "2024-03-30", "title": "Planning and Editing What You Retrieve for Enhanced Tool Learning", "authors": "Tenghao Huang, Dongwon Jung, Muhao Chen", "abstract": "Recent advancements in integrating external tools with Large Language Models\n(LLMs) have opened new frontiers, with applications in mathematical reasoning,\ncode generators, and smart assistants. However, existing methods, relying on\nsimple one-time retrieval strategies, fall short on effectively and accurately\nshortlisting relevant tools. This paper introduces a novel PLUTO (Planning,\nLearning, and Understanding for TOols) approach, encompassing\n`Plan-and-Retrieve (P&R)` and `Edit-and-Ground (E&G)` paradigms. The P&R\nparadigm consists of a neural retrieval module for shortlisting relevant tools\nand an LLM-based query planner that decomposes complex queries into actionable\ntasks, enhancing the effectiveness of tool utilization. The E&G paradigm\nutilizes LLMs to enrich tool descriptions based on user scenarios, bridging the\ngap between user queries and tool functionalities. Experiment results\ndemonstrate that these paradigms significantly improve the recall and NDCG in\ntool retrieval tasks, significantly surpassing current state-of-the-art models.", "journal": ""}
{"doi": "10.48550/arXiv.2506.13977", "date": "2025-06-11", "title": "CRITICTOOL: Evaluating Self-Critique Capabilities of Large Language Models in Tool-Calling Error Scenarios", "authors": "Shiting Huang, Zhen Fang, Zehui Chen, Siyu Yuan, Junjie Ye, Yu Zeng, Lin Chen, Qi Mao, Feng Zhao", "abstract": "The ability of large language models (LLMs) to utilize external tools has\nenabled them to tackle an increasingly diverse range of tasks. However, as the\ntasks become more complex and long-horizon, the intricate tool utilization\nprocess may trigger various unexpected errors. Therefore, how to effectively\nhandle such errors, including identifying, diagnosing, and recovering from\nthem, has emerged as a key research direction for advancing tool learning. In\nthis work, we first extensively analyze the types of errors encountered during\nthe function-calling process on several competitive tool evaluation benchmarks.\nBased on it, we introduce CRITICTOOL, a comprehensive critique evaluation\nbenchmark specialized for tool learning. Building upon a novel evolutionary\nstrategy for dataset construction, CRITICTOOL holds diverse tool-use errors\nwith varying complexities, which better reflects real-world scenarios. We\nconduct extensive experiments on CRITICTOOL, and validate the generalization\nand effectiveness of our constructed benchmark strategy. We also provide an\nin-depth analysis of the tool reflection ability on various LLMs, offering a\nnew perspective on the field of tool learning in LLMs. The code is available at\n\\href{https://github.com/Shellorley0513/CriticTool}{https://github.com/Shellorley0513/CriticTool}.", "journal": ""}
{"doi": "10.48550/arXiv.2505.11833", "date": "2025-05-17", "title": "ToLeaP: Rethinking Development of Tool Learning with Large Language Models", "authors": "Haotian Chen, Zijun Song, Boye Niu, Ke Zhang, Litu Ou, Yaxi Lu, Zhong Zhang, Xin Cong, Yankai Lin, Zhiyuan Liu, Maosong Sun", "abstract": "Tool learning, which enables large language models (LLMs) to utilize external\ntools effectively, has garnered increasing attention for its potential to\nrevolutionize productivity across industries. Despite rapid development in tool\nlearning, key challenges and opportunities remain understudied, limiting deeper\ninsights and future advancements. In this paper, we investigate the tool\nlearning ability of 41 prevalent LLMs by reproducing 33 benchmarks and enabling\none-click evaluation for seven of them, forming a Tool Learning Platform named\nToLeaP. We also collect 21 out of 33 potential training datasets to facilitate\nfuture exploration. After analyzing over 3,000 bad cases of 41 LLMs based on\nToLeaP, we identify four main critical challenges: (1) benchmark limitations\ninduce both the neglect and lack of (2) autonomous learning, (3)\ngeneralization, and (4) long-horizon task-solving capabilities of LLMs. To aid\nfuture advancements, we take a step further toward exploring potential\ndirections, namely (1) real-world benchmark construction, (2)\ncompatibility-aware autonomous learning, (3) rationale learning by thinking,\nand (4) identifying and recalling key clues. The preliminary experiments\ndemonstrate their effectiveness, highlighting the need for further research and\nexploration.", "journal": ""}
{"doi": "10.48550/arXiv.2206.10613", "date": "2022-06-20", "title": "The Right Tool for the Job: Open-Source Auditing Tools in Machine Learning", "authors": "Cherie M Poland", "abstract": "In recent years, discussions about fairness in machine learning, AI ethics\nand algorithm audits have increased. Many entities have developed framework\nguidance to establish a baseline rubric for fairness and accountability.\nHowever, in spite of increased discussions and multiple frameworks, algorithm\nand data auditing still remain difficult to execute in practice. Many\nopen-source auditing tools are available, but users aren't always aware of the\ntools, what they are useful for, or how to access them. Model auditing and\nevaluation are not frequently emphasized skills in machine learning. There are\nalso legal reasons for the proactive adoption of these tools that extend beyond\nthe desire for greater fairness in machine learning. There are positive social\nissues of public perception and goodwill that matter in our highly connected\nglobal society. Greater awareness of these tools and the reasons for actively\nutilizing them may be helpful to the entire continuum of programmers, data\nscientists, engineers, researchers, users and consumers of AI and machine\nlearning products. It is important for everyone to better understand the input\nand output differentials, how they are occurring, and what can be done to\npromote FATE (fairness, accountability, transparency, and ethics) in machine-\nand deep learning. The ability to freely access open-source auditing tools\nremoves barriers to fairness assessment at the most basic levels of machine\nlearning. This paper aims to reinforce the urgent need to actually use these\ntools and provides motivations for doing so. The exemplary tools highlighted\nherein are open-source with software or code-base repositories available that\ncan be used immediately by anyone worldwide.", "journal": ""}
{"doi": "10.48550/arXiv.2405.16089", "date": "2024-05-25", "title": "Towards Completeness-Oriented Tool Retrieval for Large Language Models", "authors": "Changle Qu, Sunhao Dai, Xiaochi Wei, Hengyi Cai, Shuaiqiang Wang, Dawei Yin, Jun Xu, Ji-Rong Wen", "abstract": "Recently, integrating external tools with Large Language Models (LLMs) has\ngained significant attention as an effective strategy to mitigate the\nlimitations inherent in their pre-training data. However, real-world systems\noften incorporate a wide array of tools, making it impractical to input all\ntools into LLMs due to length limitations and latency constraints. Therefore,\nto fully exploit the potential of tool-augmented LLMs, it is crucial to develop\nan effective tool retrieval system. Existing tool retrieval methods primarily\nfocus on semantic matching between user queries and tool descriptions,\nfrequently leading to the retrieval of redundant, similar tools. Consequently,\nthese methods fail to provide a complete set of diverse tools necessary for\naddressing the multifaceted problems encountered by LLMs. In this paper, we\npropose a novel modelagnostic COllaborative Learning-based Tool Retrieval\napproach, COLT, which captures not only the semantic similarities between user\nqueries and tool descriptions but also takes into account the collaborative\ninformation of tools. Specifically, we first fine-tune the PLM-based retrieval\nmodels to capture the semantic relationships between queries and tools in the\nsemantic learning stage. Subsequently, we construct three bipartite graphs\namong queries, scenes, and tools and introduce a dual-view graph collaborative\nlearning framework to capture the intricate collaborative relationships among\ntools during the collaborative learning stage. Extensive experiments on both\nthe open benchmark and the newly introduced ToolLens dataset show that COLT\nachieves superior performance. Notably, the performance of BERT-mini (11M) with\nour proposed model framework outperforms BERT-large (340M), which has 30 times\nmore parameters. Furthermore, we will release ToolLens publicly to facilitate\nfuture research on tool retrieval.", "journal": ""}
{"doi": "10.48550/arXiv.2407.12823", "date": "2024-07-02", "title": "WTU-EVAL: A Whether-or-Not Tool Usage Evaluation Benchmark for Large Language Models", "authors": "Kangyun Ning, Yisong Su, Xueqiang Lv, Yuanzhe Zhang, Jian Liu, Kang Liu, Jinan Xu", "abstract": "Although Large Language Models (LLMs) excel in NLP tasks, they still need\nexternal tools to extend their ability. Current research on tool learning with\nLLMs often assumes mandatory tool use, which does not always align with\nreal-world situations, where the necessity for tools is uncertain, and\nincorrect or unnecessary use of tools can damage the general abilities of LLMs.\nTherefore, we propose to explore whether LLMs can discern their ability\nboundaries and use tools flexibly. We then introduce the Whether-or-not tool\nusage Evaluation benchmark (WTU-Eval) to assess LLMs with eleven datasets,\nwhere six of them are tool-usage datasets, and five are general datasets. LLMs\nare prompted to use tools according to their needs. The results of eight LLMs\non WTU-Eval reveal that LLMs frequently struggle to determine tool use in\ngeneral datasets, and LLMs' performance in tool-usage datasets improves when\ntheir ability is similar to ChatGPT. In both datasets, incorrect tool usage\nsignificantly impairs LLMs' performance. To mitigate this, we also develop the\nfinetuning dataset to enhance tool decision-making. Fine-tuning Llama2-7B\nresults in a 14\\% average performance improvement and a 16.8\\% decrease in\nincorrect tool usage. We will release the WTU-Eval benchmark.", "journal": ""}
{"doi": "10.48550/arXiv.2101.04834", "date": "2021-01-13", "title": "Whither AutoML? Understanding the Role of Automation in Machine Learning Workflows", "authors": "Doris Xin, Eva Yiwei Wu, Doris Jung-Lin Lee, Niloufar Salehi, Aditya Parameswaran", "abstract": "Efforts to make machine learning more widely accessible have led to a rapid\nincrease in Auto-ML tools that aim to automate the process of training and\ndeploying machine learning. To understand how Auto-ML tools are used in\npractice today, we performed a qualitative study with participants ranging from\nnovice hobbyists to industry researchers who use Auto-ML tools. We present\ninsights into the benefits and deficiencies of existing tools, as well as the\nrespective roles of the human and automation in ML workflows. Finally, we\ndiscuss design implications for the future of Auto-ML tool development. We\nargue that instead of full automation being the ultimate goal of Auto-ML,\ndesigners of these tools should focus on supporting a partnership between the\nuser and the Auto-ML tool. This means that a range of Auto-ML tools will need\nto be developed to support varying user goals such as simplicity,\nreproducibility, and reliability.", "journal": ""}
{"doi": "10.48550/arXiv.2505.18135", "date": "2025-05-23", "title": "Gaming Tool Preferences in Agentic LLMs", "authors": "Kazem Faghih, Wenxiao Wang, Yize Cheng, Siddhant Bharti, Gaurang Sriramanan, Sriram Balasubramanian, Parsa Hosseini, Soheil Feizi", "abstract": "Large language models (LLMs) can now access a wide range of external tools,\nthanks to the Model Context Protocol (MCP). This greatly expands their\nabilities as various agents. However, LLMs rely entirely on the text\ndescriptions of tools to decide which ones to use--a process that is\nsurprisingly fragile. In this work, we expose a vulnerability in prevalent\ntool/function-calling protocols by investigating a series of edits to tool\ndescriptions, some of which can drastically increase a tool's usage from LLMs\nwhen competing with alternatives. Through controlled experiments, we show that\ntools with properly edited descriptions receive over 10 times more usage from\nGPT-4.1 and Qwen2.5-7B than tools with original descriptions. We further\nevaluate how various edits to tool descriptions perform when competing directly\nwith one another and how these trends generalize or differ across a broader set\nof 10 different models. These phenomenons, while giving developers a powerful\nway to promote their tools, underscore the need for a more reliable foundation\nfor agentic LLMs to select and utilize tools and resources.", "journal": ""}
{"doi": "10.48550/arXiv.2402.16696", "date": "2024-02-26", "title": "Look Before You Leap: Towards Decision-Aware and Generalizable Tool-Usage for Large Language Models", "authors": "Anchun Gui, Jian Li, Yong Dai, Nan Du, Han Xiao", "abstract": "Tool-augmented large language models (LLMs) are attracting widespread\nattention when accessing up-to-date knowledge and alleviating hallucination\nissues. Nowadays, advanced closed-source LLMs (e.g., ChatGPT) have demonstrated\nsurprising tool-usage capabilities through prompting and in-context learning\ntechniques. To empower the capabilities of open-source LLMs (e.g., LLaMA) in\nmanipulating tools, current efforts focus on either template-driven or\ntoken-triggered tool-usage. However, the former hampers LLMs' flexibility to\naddress diverse user's queries due to constrained tool interactions, while the\nlatter limits the generalizability when engaging with new tools, since\ntool-usage learning is based on task- and tool-specific datasets. To alleviate\nthese concerns, in this paper, we propose a decision-aware and generalizable\ntool-usage framework (DEER). Specifically, we first construct the tool-usage\nsamples with multiple decision branches via an automatic generation pipeline,\nthereby inspiring the decision-making awareness of LLMs under diverse\nscenarios. Meanwhile, we propose a novel tool sampling strategy to enhance the\ngeneralizability of LLMs over unseen tools. Extensive experiments demonstrate\nthat our proposed DEER is effective and significantly outperforms baselines\nacross various datasets.", "journal": ""}
{"doi": "10.48550/arXiv.2410.03439", "date": "2024-10-04", "title": "ToolGen: Unified Tool Retrieval and Calling via Generation", "authors": "Renxi Wang, Xudong Han, Lei Ji, Shu Wang, Timothy Baldwin, Haonan Li", "abstract": "As large language models (LLMs) advance, their inability to autonomously\nexecute tasks by directly interacting with external tools remains a critical\nlimitation. Traditional methods rely on inputting tool descriptions as context,\nwhich is constrained by context length and requires separate, often\ninefficient, retrieval mechanisms. We introduce ToolGen, a paradigm shift that\nintegrates tool knowledge directly into the LLM's parameters by representing\neach tool as a unique token. This enables the LLM to generate tool calls and\narguments as part of its next token prediction capabilities, seamlessly\nblending tool invocation with language generation. Our framework allows the LLM\nto access and utilize a vast amount of tools with no additional retrieval step,\nsignificantly enhancing both performance and scalability. Experimental results\nwith over 47,000 tools show that ToolGen not only achieves superior results in\nboth tool retrieval and autonomous task completion but also sets the stage for\na new era of AI agents that can adapt to tools across diverse domains. By\nfundamentally transforming tool retrieval into a generative process, ToolGen\npaves the way for more versatile, efficient, and autonomous AI systems. ToolGen\nenables end-to-end tool learning and opens opportunities for integration with\nother advanced techniques such as chain-of-thought and reinforcement learning,\nthereby expanding the practical capabilities of LLMs.", "journal": ""}
{"doi": "10.48550/arXiv.2404.01036", "date": "2024-04-01", "title": "Higher education assessment practice in the era of generative AI tools", "authors": "Bayode Ogunleye, Kudirat Ibilola Zakariyyah, Oluwaseun Ajao, Olakunle Olayinka, Hemlata Sharma", "abstract": "The higher education (HE) sector benefits every nation's economy and society\nat large. However, their contributions are challenged by advanced technologies\nlike generative artificial intelligence (GenAI) tools. In this paper, we\nprovide a comprehensive assessment of GenAI tools towards assessment and\npedagogic practice and, subsequently, discuss the potential impacts. This study\nexperimented using three assessment instruments from data science, data\nanalytics, and construction management disciplines. Our findings are two-fold:\nfirst, the findings revealed that GenAI tools exhibit subject knowledge,\nproblem-solving, analytical, critical thinking, and presentation skills and\nthus can limit learning when used unethically. Secondly, the design of the\nassessment of certain disciplines revealed the limitations of the GenAI tools.\nBased on our findings, we made recommendations on how AI tools can be utilised\nfor teaching and learning in HE.", "journal": "Higher education assessment practice in the era of generative AI\n  tools. (2024). Journal of applied learning and teaching, 7(1)"}
{"doi": "10.48550/arXiv.2503.10071", "date": "2025-03-13", "title": "Advanced Tool Learning and Selection System (ATLASS): A Closed-Loop Framework Using LLM", "authors": "Mohd Ariful Haque, Justin Williams, Sunzida Siddique, Md. Hujaifa Islam, Hasmot Ali, Kishor Datta Gupta, Roy George", "abstract": "The combination of LLM agents with external tools enables models to solve\ncomplex tasks beyond their knowledge base. Human-designed tools are inflexible\nand restricted to solutions within the scope of pre-existing tools created by\nexperts. To address this problem, we propose ATLASS, an advanced tool learning\nand selection system designed as a closed-loop framework. It enables the LLM to\nsolve problems by dynamically generating external tools on demand. In this\nframework, agents play a crucial role in orchestrating tool selection,\nexecution, and refinement, ensuring adaptive problem-solving capabilities. The\noperation of ATLASS follows three phases: The first phase, Understanding Tool\nRequirements, involves the Agents determining whether tools are required and\nspecifying their functionality; the second phase, Tool Retrieval/Generation,\ninvolves the Agents retrieving or generating tools based on their availability;\nand the third phase, Task Solving, involves combining all the component tools\nnecessary to complete the initial task. The Tool Dataset stores the generated\ntools, ensuring reusability and minimizing inference cost. Current LLM-based\ntool generation systems have difficulty creating complex tools that need APIs\nor external packages. In ATLASS, we solve the problem by automatically setting\nup the environment, fetching relevant API documentation online, and using a\nPython interpreter to create a reliable, versatile tool that works in a wider\nrange of situations. OpenAI GPT-4.0 is used as the LLM agent, and safety and\nethical concerns are handled through human feedback before executing generated\ncode. By addressing the limitations of predefined toolsets and enhancing\nadaptability, ATLASS serves as a real-world solution that empowers users with\ndynamically generated tools for complex problem-solving.", "journal": ""}
{"doi": "10.48550/arXiv.2203.14227", "date": "2022-03-27", "title": "OneLabeler: A Flexible System for Building Data Labeling Tools", "authors": "Yu Zhang, Yun Wang, Haidong Zhang, Bin Zhu, Siming Chen, Dongmei Zhang", "abstract": "Labeled datasets are essential for supervised machine learning. Various data\nlabeling tools have been built to collect labels in different usage scenarios.\nHowever, developing labeling tools is time-consuming, costly, and\nexpertise-demanding on software development. In this paper, we propose a\nconceptual framework for data labeling and OneLabeler based on the conceptual\nframework to support easy building of labeling tools for diverse usage\nscenarios. The framework consists of common modules and states in labeling\ntools summarized through coding of existing tools. OneLabeler supports\nconfiguration and composition of common software modules through visual\nprogramming to build data labeling tools. A module can be a human, machine, or\nmixed computation procedure in data labeling. We demonstrate the expressiveness\nand utility of the system through ten example labeling tools built with\nOneLabeler. A user study with developers provides evidence that OneLabeler\nsupports efficient building of diverse data labeling tools.", "journal": ""}
{"doi": "10.48550/arXiv.2502.11358", "date": "2025-02-17", "title": "Mimicking the Familiar: Dynamic Command Generation for Information Theft Attacks in LLM Tool-Learning System", "authors": "Ziyou Jiang, Mingyang Li, Guowei Yang, Junjie Wang, Yuekai Huang, Zhiyuan Chang, Qing Wang", "abstract": "Information theft attacks pose a significant risk to Large Language Model\n(LLM) tool-learning systems. Adversaries can inject malicious commands through\ncompromised tools, manipulating LLMs to send sensitive information to these\ntools, which leads to potential privacy breaches. However, existing attack\napproaches are black-box oriented and rely on static commands that cannot adapt\nflexibly to the changes in user queries and the invocation chain of tools. It\nmakes malicious commands more likely to be detected by LLM and leads to attack\nfailure. In this paper, we propose AutoCMD, a dynamic attack comment generation\napproach for information theft attacks in LLM tool-learning systems. Inspired\nby the concept of mimicking the familiar, AutoCMD is capable of inferring the\ninformation utilized by upstream tools in the toolchain through learning on\nopen-source systems and reinforcement with target system examples, thereby\ngenerating more targeted commands for information theft. The evaluation results\nshow that AutoCMD outperforms the baselines with +13.2% $ASR_{Theft}$, and can\nbe generalized to new tool-learning systems to expose their information leakage\nrisks. We also design four defense methods to effectively protect tool-learning\nsystems from the attack.", "journal": ""}
{"doi": "10.48550/arXiv.2505.01441", "date": "2025-04-28", "title": "Agentic Reasoning and Tool Integration for LLMs via Reinforcement Learning", "authors": "Joykirat Singh, Raghav Magazine, Yash Pandya, Akshay Nambi", "abstract": "Large language models (LLMs) have achieved remarkable progress in complex\nreasoning tasks, yet they remain fundamentally limited by their reliance on\nstatic internal knowledge and text-only reasoning. Real-world problem solving\noften demands dynamic, multi-step reasoning, adaptive decision making, and the\nability to interact with external tools and environments. In this work, we\nintroduce ARTIST (Agentic Reasoning and Tool Integration in Self-improving\nTransformers), a unified framework that tightly couples agentic reasoning,\nreinforcement learning, and tool integration for LLMs. ARTIST enables models to\nautonomously decide when, how, and which tools to invoke within multi-turn\nreasoning chains, leveraging outcome-based RL to learn robust strategies for\ntool use and environment interaction without requiring step-level supervision.\nExtensive experiments on mathematical reasoning and multi-turn function calling\nbenchmarks show that ARTIST consistently outperforms state-of-the-art\nbaselines, with up to 22% absolute improvement over base models and strong\ngains on the most challenging tasks. Detailed studies and metric analyses\nreveal that agentic RL training leads to deeper reasoning, more effective tool\nuse, and higher-quality solutions. Our results establish agentic RL with tool\nintegration as a powerful new frontier for robust, interpretable, and\ngeneralizable problem-solving in LLMs.", "journal": ""}
{"doi": "10.48550/arXiv.2505.20289", "date": "2025-05-26", "title": "VisualToolAgent (VisTA): A Reinforcement Learning Framework for Visual Tool Selection", "authors": "Zeyi Huang, Yuyang Ji, Anirudh Sundara Rajan, Zefan Cai, Wen Xiao, Junjie Hu, Yong Jae Lee", "abstract": "We introduce VisTA, a new reinforcement learning framework that empowers\nvisual agents to dynamically explore, select, and combine tools from a diverse\nlibrary based on empirical performance. Existing methods for tool-augmented\nreasoning either rely on training-free prompting or large-scale fine-tuning;\nboth lack active tool exploration and typically assume limited tool diversity,\nand fine-tuning methods additionally demand extensive human supervision. In\ncontrast, VisTA leverages end-to-end reinforcement learning to iteratively\nrefine sophisticated, query-specific tool selection strategies, using task\noutcomes as feedback signals. Through Group Relative Policy Optimization\n(GRPO), our framework enables an agent to autonomously discover effective\ntool-selection pathways without requiring explicit reasoning supervision.\nExperiments on the ChartQA, Geometry3K, and BlindTest benchmarks demonstrate\nthat VisTA achieves substantial performance gains over training-free baselines,\nespecially on out-of-distribution examples. These results highlight VisTA's\nability to enhance generalization, adaptively utilize diverse tools, and pave\nthe way for flexible, experience-driven visual reasoning systems.", "journal": ""}
{"doi": "10.48550/arXiv.2001.07317", "date": "2020-01-21", "title": "Sampling and Learning for Boolean Function", "authors": "Chuyu Xiong", "abstract": "In this article, we continue our study on universal learning machine by\nintroducing new tools. We first discuss boolean function and boolean circuit,\nand we establish one set of tools, namely, fitting extremum and proper sampling\nset. We proved the fundamental relationship between proper sampling set and\ncomplexity of boolean circuit. Armed with this set of tools, we then introduce\nmuch more effective learning strategies. We show that with such learning\nstrategies and learning dynamics, universal learning can be achieved, and\nrequires much less data.", "journal": ""}
{"doi": "10.48550/arXiv.2406.08686", "date": "2024-06-12", "title": "Opportunities in deep learning methods development for computational biology", "authors": "Alex Jihun Lee, Reza Abbasi-Asl", "abstract": "Advances in molecular technologies underlie an enormous growth in the size of\ndata sets pertaining to biology and biomedicine. These advances parallel those\nin the deep learning subfield of machine learning. Components in the\ndifferentiable programming toolbox that makes deep learning possible are\nallowing computer scientists to address an increasingly large array of problems\nwith flexible and effective tools. However many of these tools have not fully\nproliferated into the computational biology and bioinformatics fields. In this\nperspective we survey some of these advances and highlight exemplary examples\nof their utilization in the biosciences, with the goal of increasing awareness\namong practitioners of emerging opportunities to blend expert knowledge with\nnewly emerging deep learning architectural tools.", "journal": ""}
{"doi": "10.48550/arXiv.2010.06251", "date": "2020-10-13", "title": "Annotationsaurus: A Searchable Directory of Annotation Tools", "authors": "Mariana Neves, Jurica Seva", "abstract": "Manual annotation of textual documents is a necessary task when constructing\nbenchmark corpora for training and evaluating machine learning algorithms. We\ncreated a comprehensive directory of annotation tools that currently includes\n93 tools. We analyzed the tools over a set of 31 features and implemented\nsimple scripts and a Web application that filters the tools based on chosen\ncriteria. We present two use cases using the directory and propose ideas for\nits maintenance. The directory, source codes for scripts, and link to the Web\napplication are available at: https://github.com/mariananeves/annotation-tools", "journal": ""}
{"doi": "10.48550/arXiv.2410.12004", "date": "2024-10-15", "title": "Toolken+: Improving LLM Tool Usage with Reranking and a Reject Option", "authors": "Konstantin Yakovlev, Sergey Nikolenko, Andrey Bout", "abstract": "The recently proposed ToolkenGPT tool learning paradigm demonstrates\npromising performance but suffers from two major issues: first, it cannot\nbenefit from tool documentation, and second, it often makes mistakes in whether\nto use a tool at all. We introduce Toolken+ that mitigates the first problem by\nreranking top $k$ tools selected by ToolkenGPT and the second problem with a\nspecial \"Reject\" option such that the model will generate a vocabulary token if\n\"Reject\" is ranked first. We demonstrate the effectiveness of Toolken+ on\nmultistep numerical reasoning and tool selection tasks.", "journal": ""}
{"doi": "10.48550/arXiv.2410.12115", "date": "2024-10-15", "title": "A Web App for Teaching Finite State Automata", "authors": "Christopher William Schankula, Lucas Dutton", "abstract": "We present the open-source tool finsm.io, a tool for creating, simulating and\nexporting deterministic and non-deterministic finite state automata (DFA/NFA).\nWe first describe the conceptual background on which the tool is based,\nfollowed by a description of features and preliminary evaluation of the tool\nbased on use spanning multiple years and hundreds of student users. Preliminary\nevaluation found that instructors and students overwhelmingly recommend the\ntool to others and agree that it has improved their learning and teaching. The\nauthors invite interested educators to use the tool in their finite automata\ncourses.", "journal": ""}
{"doi": "10.48550/arXiv.1808.06853", "date": "2018-08-21", "title": "Demonstrating PAR4SEM - A Semantic Writing Aid with Adaptive Paraphrasing", "authors": "Seid Muhie Yimam, Chris Biemann", "abstract": "In this paper, we present Par4Sem, a semantic writing aid tool based on\nadaptive paraphrasing. Unlike many annotation tools that are primarily used to\ncollect training examples, Par4Sem is integrated into a real word application,\nin this case a writing aid tool, in order to collect training examples from\nusage data. Par4Sem is a tool, which supports an adaptive, iterative, and\ninteractive process where the underlying machine learning models are updated\nfor each iteration using new training examples from usage data. After\nmotivating the use of ever-learning tools in NLP applications, we evaluate\nPar4Sem by adopting it to a text simplification task through mere usage.", "journal": ""}
{"doi": "10.48550/arXiv.1903.01889", "date": "2019-03-05", "title": "Practical Knowledge Management Tool Use in a Software Consulting Company", "authors": "Torgeir Dings\u00f8yr, Emil R\u00f8yrvik, Hans Karim Djarraya", "abstract": "Tools for managing technical skills are used in many companies, but there has\nbeen little discussion about how such tools are used in practice. We report\nhere on different types of actual usage in a medium-size software consulting\ncompany. We expected such tools to be used for allocating resources to new\nprojects and for searching for competence to solve problems, but also observed\ntwo other types of usage: identifying new project opportunities, and skills\nupgrading. This multitude of uses support learning practices and motivates tool\nuse both at individual and company levels, which is crucial to support\norganizational learning.", "journal": "Communications of the ACM, vol. 48, pp. 96 - 100, 2005"}
{"doi": "10.48550/arXiv.2106.12798", "date": "2021-06-24", "title": "Evaluation of Representation Models for Text Classification with AutoML Tools", "authors": "Sebastian Br\u00e4ndle, Marc Hanussek, Matthias Blohm, Maximilien Kintz", "abstract": "Automated Machine Learning (AutoML) has gained increasing success on tabular\ndata in recent years. However, processing unstructured data like text is a\nchallenge and not widely supported by open-source AutoML tools. This work\ncompares three manually created text representations and text embeddings\nautomatically created by AutoML tools. Our benchmark includes four popular\nopen-source AutoML tools and eight datasets for text classification purposes.\nThe results show that straightforward text representations perform better than\nAutoML tools with automatically created text embeddings.", "journal": ""}
{"doi": "10.48550/arXiv.2008.07331", "date": "2020-08-14", "title": "Interactive Visualization for Debugging RL", "authors": "Shuby Deshpande, Benjamin Eysenbach, Jeff Schneider", "abstract": "Visualization tools for supervised learning allow users to interpret,\nintrospect, and gain an intuition for the successes and failures of their\nmodels. While reinforcement learning practitioners ask many of the same\nquestions, existing tools are not applicable to the RL setting as these tools\naddress challenges typically found in the supervised learning regime. In this\nwork, we design and implement an interactive visualization tool for debugging\nand interpreting RL algorithms. Our system addresses many features missing from\nprevious tools such as (1) tools for supervised learning often are not\ninteractive; (2) while debugging RL policies researchers use state\nrepresentations that are different from those seen by the agent; (3) a\nframework designed to make the debugging RL policies more conducive. We provide\nan example workflow of how this system could be used, along with ideas for\nfuture extensions.", "journal": ""}
{"doi": "10.48550/arXiv.2403.09566", "date": "2024-03-14", "title": "PaperBot: Learning to Design Real-World Tools Using Paper", "authors": "Ruoshi Liu, Junbang Liang, Sruthi Sudhakar, Huy Ha, Cheng Chi, Shuran Song, Carl Vondrick", "abstract": "Paper is a cheap, recyclable, and clean material that is often used to make\npractical tools. Traditional tool design either relies on simulation or\nphysical analysis, which is often inaccurate and time-consuming. In this paper,\nwe propose PaperBot, an approach that directly learns to design and use a tool\nin the real world using paper without human intervention. We demonstrated the\neffectiveness and efficiency of PaperBot on two tool design tasks: 1. learning\nto fold and throw paper airplanes for maximum travel distance 2. learning to\ncut paper into grippers that exert maximum gripping force. We present a\nself-supervised learning framework that learns to perform a sequence of\nfolding, cutting, and dynamic manipulation actions in order to optimize the\ndesign and use of a tool. We deploy our system to a real-world two-arm robotic\nsystem to solve challenging design tasks that involve aerodynamics (paper\nairplane) and friction (paper gripper) that are impossible to simulate\naccurately.", "journal": ""}
{"doi": "10.48550/arXiv.2503.06708", "date": "2025-03-09", "title": "Alignment for Efficient Tool Calling of Large Language Models", "authors": "Hongshen Xu, Zihan Wang, Zichen Zhu, Lei Pan, Xingyu Chen, Lu Chen, Kai Yu", "abstract": "Recent advancements in tool learning have enabled large language models\n(LLMs) to integrate external tools, enhancing their task performance by\nexpanding their knowledge boundaries. However, relying on tools often\nintroduces tradeoffs between performance, speed, and cost, with LLMs sometimes\nexhibiting overreliance and overconfidence in tool usage. This paper addresses\nthe challenge of aligning LLMs with their knowledge boundaries to make more\nintelligent decisions about tool invocation. We propose a multi objective\nalignment framework that combines probabilistic knowledge boundary estimation\nwith dynamic decision making, allowing LLMs to better assess when to invoke\ntools based on their confidence. Our framework includes two methods for\nknowledge boundary estimation, consistency based and absolute estimation, and\ntwo training strategies for integrating these estimates into the model decision\nmaking process. Experimental results on various tool invocation scenarios\ndemonstrate the effectiveness of our framework, showing significant\nimprovements in tool efficiency by reducing unnecessary tool usage.", "journal": ""}
{"doi": "10.48550/arXiv.2504.04809", "date": "2025-04-07", "title": "Select Me! When You Need a Tool: A Black-box Text Attack on Tool Selection", "authors": "Liuji Chen, Hao Gao, Jinghao Zhang, Qiang Liu, Shu Wu, Liang Wang", "abstract": "Tool learning serves as a powerful auxiliary mechanism that extends the\ncapabilities of large language models (LLMs), enabling them to tackle complex\ntasks requiring real-time relevance or high precision operations. Behind its\npowerful capabilities lie some potential security issues. However, previous\nwork has primarily focused on how to make the output of the invoked tools\nincorrect or malicious, with little attention given to the manipulation of tool\nselection. To fill this gap, we introduce, for the first time, a black-box\ntext-based attack that can significantly increase the probability of the target\ntool being selected in this paper. We propose a two-level text perturbation\nattack witha coarse-to-fine granularity, attacking the text at both the word\nlevel and the character level. We conduct comprehensive experiments that\ndemonstrate the attacker only needs to make some perturbations to the tool's\ntextual information to significantly increase the possibility of the target\ntool being selected and ranked higher among the candidate tools. Our research\nreveals the vulnerability of the tool selection process and paves the way for\nfuture research on protecting this process.", "journal": ""}
{"doi": "10.48550/arXiv.2006.05478", "date": "2020-06-09", "title": "ToolNet: Using Commonsense Generalization for Predicting Tool Use for Robot Plan Synthesis", "authors": "Rajas Bansal, Shreshth Tuli, Rohan Paul, Mausam", "abstract": "A robot working in a physical environment (like home or factory) needs to\nlearn to use various available tools for accomplishing different tasks, for\ninstance, a mop for cleaning and a tray for carrying objects. The number of\npossible tools is large and it may not be feasible to demonstrate usage of each\nindividual tool during training. Can a robot learn commonsense knowledge and\nadapt to novel settings where some known tools are missing, but alternative\nunseen tools are present? We present a neural model that predicts the best tool\nfrom the available objects for achieving a given declarative goal. This model\nis trained by user demonstrations, which we crowd-source through humans\ninstructing a robot in a physics simulator. This dataset maintains user plans\ninvolving multi-step object interactions along with symbolic state changes. Our\nneural model, ToolNet, combines a graph neural network to encode the current\nenvironment state, and goal-conditioned spatial attention to predict the\nappropriate tool. We find that providing metric and semantic properties of\nobjects, and pre-trained object embeddings derived from a commonsense knowledge\nrepository such as ConceptNet, significantly improves the model's ability to\ngeneralize to unseen tools. The model makes accurate and generalizable tool\npredictions. When compared to a graph neural network baseline, it achieves\n14-27% accuracy improvement for predicting known tools from new world scenes,\nand 44-67% improvement in generalization for novel objects not encountered\nduring training.", "journal": ""}
{"doi": "10.48550/arXiv.2502.11744", "date": "2025-02-17", "title": "FUNCTO: Function-Centric One-Shot Imitation Learning for Tool Manipulation", "authors": "Chao Tang, Anxing Xiao, Yuhong Deng, Tianrun Hu, Wenlong Dong, Hanbo Zhang, David Hsu, Hong Zhang", "abstract": "Learning tool use from a single human demonstration video offers a highly\nintuitive and efficient approach to robot teaching. While humans can\neffortlessly generalize a demonstrated tool manipulation skill to diverse tools\nthat support the same function (e.g., pouring with a mug versus a teapot),\ncurrent one-shot imitation learning (OSIL) methods struggle to achieve this. A\nkey challenge lies in establishing functional correspondences between\ndemonstration and test tools, considering significant geometric variations\namong tools with the same function (i.e., intra-function variations). To\naddress this challenge, we propose FUNCTO (Function-Centric OSIL for Tool\nManipulation), an OSIL method that establishes function-centric correspondences\nwith a 3D functional keypoint representation, enabling robots to generalize\ntool manipulation skills from a single human demonstration video to novel tools\nwith the same function despite significant intra-function variations. With this\nformulation, we factorize FUNCTO into three stages: (1) functional keypoint\nextraction, (2) function-centric correspondence establishment, and (3)\nfunctional keypoint-based action planning. We evaluate FUNCTO against exiting\nmodular OSIL methods and end-to-end behavioral cloning methods through\nreal-robot experiments on diverse tool manipulation tasks. The results\ndemonstrate the superiority of FUNCTO when generalizing to novel tools with\nintra-function geometric variations. More details are available at\nhttps://sites.google.com/view/functo.", "journal": ""}
{"doi": "10.48550/arXiv.2505.16113", "date": "2025-05-22", "title": "Tools in the Loop: Quantifying Uncertainty of LLM Question Answering Systems That Use Tools", "authors": "Panagiotis Lymperopoulos, Vasanth Sarathy", "abstract": "Modern Large Language Models (LLMs) often require external tools, such as\nmachine learning classifiers or knowledge retrieval systems, to provide\naccurate answers in domains where their pre-trained knowledge is insufficient.\nThis integration of LLMs with external tools expands their utility but also\nintroduces a critical challenge: determining the trustworthiness of responses\ngenerated by the combined system. In high-stakes applications, such as medical\ndecision-making, it is essential to assess the uncertainty of both the LLM's\ngenerated text and the tool's output to ensure the reliability of the final\nresponse. However, existing uncertainty quantification methods do not account\nfor the tool-calling scenario, where both the LLM and external tool contribute\nto the overall system's uncertainty. In this work, we present a novel framework\nfor modeling tool-calling LLMs that quantifies uncertainty by jointly\nconsidering the predictive uncertainty of the LLM and the external tool. We\nextend previous methods for uncertainty quantification over token sequences to\nthis setting and propose efficient approximations that make uncertainty\ncomputation practical for real-world applications. We evaluate our framework on\ntwo new synthetic QA datasets, derived from well-known machine learning\ndatasets, which require tool-calling for accurate answers. Additionally, we\napply our method to retrieval-augmented generation (RAG) systems and conduct a\nproof-of-concept experiment demonstrating the effectiveness of our uncertainty\nmetrics in scenarios where external information retrieval is needed. Our\nresults show that the framework is effective in enhancing trust in LLM-based\nsystems, especially in cases where the LLM's internal knowledge is insufficient\nand external tools are required.", "journal": ""}
{"doi": "10.48550/arXiv.2506.14248", "date": "2025-06-17", "title": "Re-Initialization Token Learning for Tool-Augmented Large Language Models", "authors": "Chenghao Li, Liu Liu, Baosheng Yu, Jiayan Qiu, Yibing Zhan", "abstract": "Large language models have demonstrated exceptional performance, yet struggle\nwith complex tasks such as numerical reasoning, plan generation. Integrating\nexternal tools, such as calculators and databases, into large language models\n(LLMs) is crucial for enhancing problem-solving capabilities. Current methods\nassign a unique token to each tool, enabling LLMs to call tools through token\nprediction-similar to word generation. However, this approach fails to account\nfor the relationship between tool and word tokens, limiting adaptability within\npre-trained LLMs. To address this issue, we propose a novel token learning\nmethod that aligns tool tokens with the existing word embedding space from the\nperspective of initialization, thereby enhancing model performance. We begin by\nconstructing prior token embeddings for each tool based on the tool's name or\ndescription, which are used to initialize and regularize the learnable tool\ntoken embeddings. This ensures the learned embeddings are well-aligned with the\nword token space, improving tool call accuracy. We evaluate the method on tasks\nsuch as numerical reasoning, knowledge-based question answering, and embodied\nplan generation using GSM8K-XL, FuncQA, KAMEL, and VirtualHome datasets. The\nresults demonstrate clear improvements over recent baselines, including CoT,\nREACT, ICL, and ToolkenGPT, indicating that our approach effectively augments\nLLMs with tools through relevant tokens across diverse domains.", "journal": ""}
{"doi": "10.48550/arXiv.2410.03212", "date": "2024-10-04", "title": "Data-Efficient Massive Tool Retrieval: A Reinforcement Learning Approach for Query-Tool Alignment with Language Models", "authors": "Yuxiang Zhang, Xin Fan, Junjie Wang, Chongxian Chen, Fan Mo, Tetsuya Sakai, Hayato Yamana", "abstract": "Recent advancements in large language models (LLMs) integrated with external\ntools and APIs have successfully addressed complex tasks by using in-context\nlearning or fine-tuning. Despite this progress, the vast scale of tool\nretrieval remains challenging due to stringent input length constraints. In\nresponse, we propose a pre-retrieval strategy from an extensive repository,\neffectively framing the problem as the massive tool retrieval (MTR) task. We\nintroduce the MTRB (massive tool retrieval benchmark) to evaluate real-world\ntool-augmented LLM scenarios with a large number of tools. This benchmark is\ndesigned for low-resource scenarios and includes a diverse collection of tools\nwith descriptions refined for consistency and clarity. It consists of three\nsubsets, each containing 90 test samples and 10 training samples. To handle the\nlow-resource MTR task, we raise a new query-tool alignment (QTA) framework\nleverages LLMs to enhance query-tool alignment by rewriting user queries\nthrough ranking functions and the direct preference optimization (DPO) method.\nThis approach consistently outperforms existing state-of-the-art models in\ntop-5 and top-10 retrieval tasks across the MTRB benchmark, with improvements\nup to 93.28% based on the metric Sufficiency@k, which measures the adequacy of\ntool retrieval within the first k results. Furthermore, ablation studies\nvalidate the efficacy of our framework, highlighting its capacity to optimize\nperformance even with limited annotated samples. Specifically, our framework\nachieves up to 78.53% performance improvement in Sufficiency@k with just a\nsingle annotated sample. Additionally, QTA exhibits strong cross-dataset\ngeneralizability, emphasizing its potential for real-world applications.", "journal": ""}
{"doi": "10.48550/arXiv.2106.08671", "date": "2021-06-16", "title": "Comparison of Automated Machine Learning Tools for SMS Spam Message Filtering", "authors": "Waddah Saeed", "abstract": "Short Message Service (SMS) is a very popular service used for communication\nby mobile users. However, this popular service can be abused by executing\nillegal activities and influencing security risks. Nowadays, many automatic\nmachine learning (AutoML) tools exist which can help domain experts and lay\nusers to build high-quality ML models with little or no machine learning\nknowledge. In this work, a classification performance comparison was conducted\nbetween three automatic ML tools for SMS spam message filtering. These tools\nare mljar-supervised AutoML, H2O AutoML, and Tree-based Pipeline Optimization\nTool (TPOT) AutoML. Experimental results showed that ensemble models achieved\nthe best classification performance. The Stacked Ensemble model, which was\nbuilt using H2O AutoML, achieved the best performance in terms of Log Loss\n(0.8370), true positive (1088/1116), and true negative (281/287) metrics. There\nis a 19.05\\% improvement in Log Loss with respect to TPOT AutoML and 5.56\\%\nimprovement with respect to mljar-supervised AutoML. The satisfactory filtering\nperformance achieved with AutoML tools provides a potential application for\nAutoML tools to automatically determine the best ML model that can perform best\nfor SMS spam message filtering.", "journal": ""}
{"doi": "10.48550/arXiv.2210.03836", "date": "2022-10-07", "title": "Learning the Dynamics of Compliant Tool-Environment Interaction for Visuo-Tactile Contact Servoing", "authors": "Mark Van der Merwe, Dmitry Berenson, Nima Fazeli", "abstract": "Many manipulation tasks require the robot to control the contact between a\ngrasped compliant tool and the environment, e.g. scraping a frying pan with a\nspatula. However, modeling tool-environment interaction is difficult,\nespecially when the tool is compliant, and the robot cannot be expected to have\nthe full geometry and physical properties (e.g., mass, stiffness, and friction)\nof all the tools it must use. We propose a framework that learns to predict the\neffects of a robot's actions on the contact between the tool and the\nenvironment given visuo-tactile perception. Key to our framework is a novel\ncontact feature representation that consists of a binary contact value, the\nline of contact, and an end-effector wrench. We propose a method to learn the\ndynamics of these contact features from real world data that does not require\npredicting the geometry of the compliant tool. We then propose a controller\nthat uses this dynamics model for visuo-tactile contact servoing and show that\nit is effective at performing scraping tasks with a spatula, even in scenarios\nwhere precise contact needs to be made to avoid obstacles.", "journal": ""}
{"doi": "10.48550/arXiv.2303.00192", "date": "2023-03-01", "title": "Exploring Challenges and Opportunities to Support Designers in Learning to Co-create with AI-based Manufacturing Design Tools", "authors": "Frederic Gmeiner, Humphrey Yang, Lining Yao, Kenneth Holstein, Nikolas Martelaro", "abstract": "AI-based design tools are proliferating in professional software to assist\nengineering and industrial designers in complex manufacturing and design tasks.\nThese tools take on more agentic roles than traditional computer-aided design\ntools and are often portrayed as \"co-creators.\" Yet, working effectively with\nsuch systems requires different skills than working with complex CAD tools\nalone. To date, we know little about how engineering designers learn to work\nwith AI-based design tools. In this study, we observed trained designers as\nthey learned to work with two AI-based tools on a realistic design task. We\nfind that designers face many challenges in learning to effectively co-create\nwith current systems, including challenges in understanding and adjusting AI\noutputs and in communicating their design goals. Based on our findings, we\nhighlight several design opportunities to better support designer-AI\nco-creation.", "journal": ""}
{"doi": "10.48550/arXiv.2106.02445", "date": "2021-06-04", "title": "How to select and use tools? : Active Perception of Target Objects Using Multimodal Deep Learning", "authors": "Namiko Saito, Tetsuya Ogata, Satoshi Funabashi, Hiroki Mori, Shigeki Sugano", "abstract": "Selection of appropriate tools and use of them when performing daily tasks is\na critical function for introducing robots for domestic applications. In\nprevious studies, however, adaptability to target objects was limited, making\nit difficult to accordingly change tools and adjust actions. To manipulate\nvarious objects with tools, robots must both understand tool functions and\nrecognize object characteristics to discern a tool-object-action relation. We\nfocus on active perception using multimodal sensorimotor data while a robot\ninteracts with objects, and allow the robot to recognize their extrinsic and\nintrinsic characteristics. We construct a deep neural networks (DNN) model that\nlearns to recognize object characteristics, acquires tool-object-action\nrelations, and generates motions for tool selection and handling. As an example\ntool-use situation, the robot performs an ingredients transfer task, using a\nturner or ladle to transfer an ingredient from a pot to a bowl. The results\nconfirm that the robot recognizes object characteristics and servings even when\nthe target ingredients are unknown. We also examine the contributions of\nimages, force, and tactile data and show that learning a variety of multimodal\ninformation results in rich perception for tool use.", "journal": "IEEE Robotics and Automation Letters 2021"}
{"doi": "10.48550/arXiv.2406.08335", "date": "2024-06-12", "title": "A Survey of Pipeline Tools for Data Engineering", "authors": "Anthony Mbata, Yaji Sripada, Mingjun Zhong", "abstract": "Currently, a variety of pipeline tools are available for use in data\nengineering. Data scientists can use these tools to resolve data wrangling\nissues associated with data and accomplish some data engineering tasks from\ndata ingestion through data preparation to utilization as input for machine\nlearning (ML). Some of these tools have essential built-in components or can be\ncombined with other tools to perform desired data engineering operations. While\nsome tools are wholly or partly commercial, several open-source tools are\navailable to perform expert-level data engineering tasks. This survey examines\nthe broad categories and examples of pipeline tools based on their design and\ndata engineering intentions. These categories are Extract Transform\nLoad/Extract Load Transform (ETL/ELT), pipelines for Data Integration,\nIngestion, and Transformation, Data Pipeline Orchestration and Workflow\nManagement, and Machine Learning Pipelines. The survey also provides a broad\noutline of the utilization with examples within these broad groups and finally,\na discussion is presented with case studies indicating the usage of pipeline\ntools for data engineering. The studies present some first-user application\nexperiences with sample data, some complexities of the applied pipeline, and a\nsummary note of approaches to using these tools to prepare data for machine\nlearning.", "journal": ""}
{"doi": "10.48550/arXiv.2007.04083", "date": "2020-07-06", "title": "Holistic framework to help students learn effectively from research-validated self-paced learning tools", "authors": "Emily Marshman, Seth DeVore, Chandralekha Singh", "abstract": "With limited time available in the classroom, e-learning tools can supplement\nin-class learning by providing opportunities for students to study and learn\noutside of class. Such tools can be especially helpful for students who lack\nadequate prior preparation. However, one critical issue is ensuring that\nstudents, especially those in need of additional help, engage with the tools as\nintended. Here we first discuss an empirical investigation in which students in\na large algebra-based physics course were given opportunities to work through\nresearch-validated tutorials outside of class as self-study tools. Students\nwere provided these optional tutorials after traditional instruction in\nrelevant topics and were then given quizzes that included problems that were\nidentical to the tutorial problems with regard to the physics principles\ninvolved but had different contexts. We find that students who worked through\nthe tutorials as self-study tools struggled to transfer their learning to solve\nproblems that used the same physics principles. On the other hand, students who\nworked on the tutorials in supervised, one-on-one situations performed\nsignificantly better than them. These empirical findings suggest that many\nintroductory physics students may not engage effectively with self-paced\nlearning tools unless they are provided additional incentives and support,\ne.g., to aid with self-regulation. Inspired by the empirical findings, we\npropose a holistic theoretical framework to help create learning environments\nin which students with diverse backgrounds are provided support to engage\neffectively with self-study tools.", "journal": "Phys. Rev. Phys. Educ. Res. 16, 020108 (2020)"}
{"doi": "10.48550/arXiv.2207.01484", "date": "2022-06-04", "title": "Understanding EFL Student Idea Generation Strategies for Creative Writing with NLG Tools", "authors": "David James Woo, Yanzhi Wang, Hengky Susanto, Kai Guo", "abstract": "Natural language generation (NLG) is a process within artificial intelligence\nwhere computer systems produce human-comprehensible language texts from\ninformation. English as a foreign language (EFL) students' use of NLG tools\nmight facilitate their idea generation, which is fundamental to creative\nwriting. However, little is known about how EFL students interact with NLG\ntools to generate ideas. This study explores strategies adopted by EFL students\nwhen searching for ideas using NLG tools, evaluating ideas generated by NLG\ntools and selecting NLG tools for ideas generation. Four Hong Kong secondary\nschool students attended workshops where they learned to write stories\ncomprising their own words and words generated by NLG tools. After the\nworkshops, they answered questions to reflect on their writing experience with\nNLG tools. In a thematic analysis of the written reflections, we found students\nmay have existing ideas when searching for ideas and evaluating ideas with NLG\ntools. Students showed some aversion to ideas generated by NLG tools and\nselected NLG tools that generated a greater quantity of ideas. The findings\ninform our understanding of EFL students' concerns when using NLG tools for\nidea generation and can inform educators' instruction to implement NLG tools\nfor classroom creative writing.", "journal": ""}
{"doi": "10.48550/arXiv.2406.03095", "date": "2024-06-05", "title": "EgoSurgery-Tool: A Dataset of Surgical Tool and Hand Detection from Egocentric Open Surgery Videos", "authors": "Ryo Fujii, Hideo Saito, Hiroki Kajita", "abstract": "Surgical tool detection is a fundamental task for understanding egocentric\nopen surgery videos. However, detecting surgical tools presents significant\nchallenges due to their highly imbalanced class distribution, similar shapes\nand similar textures, and heavy occlusion. The lack of a comprehensive\nlarge-scale dataset compounds these challenges. In this paper, we introduce\nEgoSurgery-Tool, an extension of the existing EgoSurgery-Phase dataset, which\ncontains real open surgery videos captured using an egocentric camera attached\nto the surgeon's head, along with phase annotations. EgoSurgery-Tool has been\ndensely annotated with surgical tools and comprises over 49K surgical tool\nbounding boxes across 15 categories, constituting a large-scale surgical tool\ndetection dataset. EgoSurgery-Tool also provides annotations for hand detection\nwith over 46K hand-bounding boxes, capturing hand-object interactions that are\ncrucial for understanding activities in egocentric open surgery.\nEgoSurgery-Tool is superior to existing datasets due to its larger scale,\ngreater variety of surgical tools, more annotations, and denser scenes. We\nconduct a comprehensive analysis of EgoSurgery-Tool using nine popular object\ndetectors to assess their effectiveness in both surgical tool and hand\ndetection. The dataset will be released at\nhttps://github.com/Fujiry0/EgoSurgery.", "journal": ""}
{"doi": "10.48550/arXiv.2503.01763", "date": "2025-03-03", "title": "Retrieval Models Aren't Tool-Savvy: Benchmarking Tool Retrieval for Large Language Models", "authors": "Zhengliang Shi, Yuhan Wang, Lingyong Yan, Pengjie Ren, Shuaiqiang Wang, Dawei Yin, Zhaochun Ren", "abstract": "Tool learning aims to augment large language models (LLMs) with diverse\ntools, enabling them to act as agents for solving practical tasks. Due to the\nlimited context length of tool-using LLMs, adopting information retrieval (IR)\nmodels to select useful tools from large toolsets is a critical initial step.\nHowever, the performance of IR models in tool retrieval tasks remains\nunderexplored and unclear. Most tool-use benchmarks simplify this step by\nmanually pre-annotating a small set of relevant tools for each task, which is\nfar from the real-world scenarios. In this paper, we propose ToolRet, a\nheterogeneous tool retrieval benchmark comprising 7.6k diverse retrieval tasks,\nand a corpus of 43k tools, collected from existing datasets. We benchmark six\ntypes of models on ToolRet. Surprisingly, even the models with strong\nperformance in conventional IR benchmarks, exhibit poor performance on ToolRet.\nThis low retrieval quality degrades the task pass rate of tool-use LLMs. As a\nfurther step, we contribute a large-scale training dataset with over 200k\ninstances, which substantially optimizes the tool retrieval ability of IR\nmodels.", "journal": ""}
{"doi": "10.48550/arXiv.1907.02050", "date": "2019-07-03", "title": "Reasoning and Generalization in RL: A Tool Use Perspective", "authors": "Sam Wenke, Dan Saunders, Mike Qiu, Jim Fleming", "abstract": "Learning to use tools to solve a variety of tasks is an innate ability of\nhumans and has been observed of animals in the wild. However, the underlying\nmechanisms that are required to learn to use tools are abstract and widely\ncontested in the literature. In this paper, we study tool use in the context of\nreinforcement learning and propose a framework for analyzing generalization\ninspired by a classic study of tool using behavior, the trap-tube task.\nRecently, it has become common in reinforcement learning to measure\ngeneralization performance on a single test set of environments. We instead\npropose transfers that produce multiple test sets that are used to measure\nspecified types of generalization, inspired by abilities demonstrated by animal\nand human tool users. The source code to reproduce our experiments is publicly\navailable at https://github.com/fomorians/gym_tool_use.", "journal": ""}
{"doi": "10.48550/arXiv.2106.10910", "date": "2021-06-21", "title": "Fostering Student Engagement in a Mobile Formative Assessment System for High-School Economics", "authors": "Fotis Lazarinis, Dimitris Kanellopoulos", "abstract": "In a mobile learning environment, students can learn via mobile devices\nwithout being limited by time and space. Therefore, it is vital to develop\ntools to assist students to learn and assess their knowledge in such\nenvironments. This paper presents a tool/application for formative\nself-assessment. The tool supports the selection of questions based on\nuser-defined criteria concerning (1) the difficulty level; (2) the associated\nconcepts; and (3) the purposes of the test taker. The main purpose of the\npresented tool is to better support the learning aims of the participants and\nto increase their engagement in the learning process. The focus of this study\nis to evaluate the tool using quizzes in Microeconomics to realize its\npotential in this specific domain. Teachers and students were involved in the\nexperiments conducted. The experiments demonstrated that the presented tool is\nusable; it motivates the students and improves their understanding", "journal": "Social Education Research, 2021"}
{"doi": "10.48550/arXiv.2211.02201", "date": "2022-11-04", "title": "Learning Tool Morphology for Contact-Rich Manipulation Tasks with Differentiable Simulation", "authors": "Mengxi Li, Rika Antonova, Dorsa Sadigh, Jeannette Bohg", "abstract": "When humans perform contact-rich manipulation tasks, customized tools are\noften necessary to simplify the task. For instance, we use various utensils for\nhandling food, such as knives, forks and spoons. Similarly, robots may benefit\nfrom specialized tools that enable them to more easily complete a variety of\ntasks. We present an end-to-end framework to automatically learn tool\nmorphology for contact-rich manipulation tasks by leveraging differentiable\nphysics simulators. Previous work relied on manually constructed priors\nrequiring detailed specification of a 3D object model, grasp pose and task\ndescription to facilitate the search or optimization process. Our approach only\nrequires defining the objective with respect to task performance and enables\nlearning a robust morphology through randomizing variations of the task. We\nmake this optimization tractable by casting it as a continual learning problem.\nWe demonstrate the effectiveness of our method for designing new tools in\nseveral scenarios, such as winding ropes, flipping a box and pushing peas onto\na scoop in simulation. Additionally, experiments with real robots show that the\ntool shapes discovered by our method help them succeed in these scenarios.", "journal": ""}
{"doi": "10.48550/arXiv.2307.16499", "date": "2023-07-31", "title": "Learning Generalizable Tool Use with Non-rigid Grasp-pose Registration", "authors": "Malte Mosbach, Sven Behnke", "abstract": "Tool use, a hallmark feature of human intelligence, remains a challenging\nproblem in robotics due the complex contacts and high-dimensional action space.\nIn this work, we present a novel method to enable reinforcement learning of\ntool use behaviors. Our approach provides a scalable way to learn the operation\nof tools in a new category using only a single demonstration. To this end, we\npropose a new method for generalizing grasping configurations of multi-fingered\nrobotic hands to novel objects. This is used to guide the policy search via\nfavorable initializations and a shaped reward signal. The learned policies\nsolve complex tool use tasks and generalize to unseen tools at test time.\nVisualizations and videos of the trained policies are available at\nhttps://maltemosbach.github.io/generalizable_tool_use.", "journal": ""}
{"doi": "10.48550/arXiv.2412.09001", "date": "2024-12-12", "title": "MindScratch: A Visual Programming Support Tool for Classroom Learning Based on Multimodal Generative AI", "authors": "Yunnong Chen, Shuhong Xiao, Yaxuan Song, Zejian Li, Lingyun Sun, Liuqing Chen", "abstract": "Programming has become an essential component of K-12 education and serves as\na pathway for developing computational thinking skills. Given the complexity of\nprogramming and the advanced skills it requires, previous research has\nintroduced user-friendly tools to support young learners. However, our\ninterviews with six programming educators revealed that current tools often\nfail to reflect classroom learning objectives, offer flexible, high-quality\nguidance, and foster student creativity. This highlights the need for more\nadaptive and reflective tools. Therefore, we introduced MindScratch, a\nmultimodal generative AI (GAI) powered visual programming support tool.\nMindScratch aims to balance structured classroom activities with free\nprogramming creation, supporting students in completing creative programming\nprojects based on teacher-set learning objectives while also providing\nprogramming scaffolding. Our user study results indicate that, compared to the\nbaseline, MindScratch more effectively helps students achieve high-quality\nprojects aligned with learning objectives. It also enhances students'\ncomputational thinking skills and creative thinking. Overall, we believe that\nGAI-driven educational tools like MindScratch offer students a focused and\nengaging learning experience.", "journal": ""}
{"doi": "10.48550/arXiv.2008.03273", "date": "2020-08-07", "title": "SafePILCO: a software tool for safe and data-efficient policy synthesis", "authors": "Kyriakos Polymenakos, Nikitas Rontsis, Alessandro Abate, Stephen Roberts", "abstract": "SafePILCO is a software tool for safe and data-efficient policy search with\nreinforcement learning. It extends the known PILCO algorithm, originally\nwritten in MATLAB, to support safe learning. We provide a Python implementation\nand leverage existing libraries that allow the codebase to remain short and\nmodular, which is appropriate for wider use by the verification, reinforcement\nlearning, and control communities.", "journal": ""}
{"doi": "10.48550/arXiv.2505.02402", "date": "2025-05-05", "title": "A probabilistic view on Riemannian machine learning models for SPD matrices", "authors": "Thibault de Surrel, Florian Yger, Fabien Lotte, Sylvain Chevallier", "abstract": "The goal of this paper is to show how different machine learning tools on the\nRiemannian manifold $\\mathcal{P}_d$ of Symmetric Positive Definite (SPD)\nmatrices can be united under a probabilistic framework. For this, we will need\nseveral Gaussian distributions defined on $\\mathcal{P}_d$. We will show how\npopular classifiers on $\\mathcal{P}_d$ can be reinterpreted as Bayes\nClassifiers using these Gaussian distributions. These distributions will also\nbe used for outlier detection and dimension reduction. By showing that those\ndistributions are pervasive in the tools used on $\\mathcal{P}_d$, we allow for\nother machine learning tools to be extended to $\\mathcal{P}_d$.", "journal": ""}
{"doi": "10.48550/arXiv.2111.03088", "date": "2021-11-04", "title": "Learning to Manipulate Tools by Aligning Simulation to Video Demonstration", "authors": "Kateryna Zorina, Justin Carpentier, Josef Sivic, Vladim\u00edr Petr\u00edk", "abstract": "A seamless integration of robots into human environments requires robots to\nlearn how to use existing human tools. Current approaches for learning tool\nmanipulation skills mostly rely on expert demonstrations provided in the target\nrobot environment, for example, by manually guiding the robot manipulator or by\nteleoperation. In this work, we introduce an automated approach that replaces\nan expert demonstration with a Youtube video for learning a tool manipulation\nstrategy. The main contributions are twofold. First, we design an alignment\nprocedure that aligns the simulated environment with the real-world scene\nobserved in the video. This is formulated as an optimization problem that finds\na spatial alignment of the tool trajectory to maximize the sparse goal reward\ngiven by the environment. Second, we describe an imitation learning approach\nthat focuses on the trajectory of the tool rather than the motion of the human.\nFor this we combine reinforcement learning with an optimization procedure to\nfind a control policy and the placement of the robot based on the tool motion\nin the aligned environment. We demonstrate the proposed approach on spade,\nscythe and hammer tools in simulation, and show the effectiveness of the\ntrained policy for the spade on a real Franka Emika Panda robot demonstration.", "journal": ""}
{"doi": "10.48550/arXiv.1904.05538", "date": "2019-04-11", "title": "Improvisation through Physical Understanding: Using Novel Objects as Tools with Visual Foresight", "authors": "Annie Xie, Frederik Ebert, Sergey Levine, Chelsea Finn", "abstract": "Machine learning techniques have enabled robots to learn narrow, yet complex\ntasks and also perform broad, yet simple skills with a wide variety of objects.\nHowever, learning a model that can both perform complex tasks and generalize to\npreviously unseen objects and goals remains a significant challenge. We study\nthis challenge in the context of \"improvisational\" tool use: a robot is\npresented with novel objects and a user-specified goal (e.g., sweep some\nclutter into the dustpan), and must figure out, using only raw image\nobservations, how to accomplish the goal using the available objects as tools.\nWe approach this problem by training a model with both a visual and physical\nunderstanding of multi-object interactions, and develop a sampling-based\noptimizer that can leverage these interactions to accomplish tasks. We do so by\ncombining diverse demonstration data with self-supervised interaction data,\naiming to leverage the interaction data to build generalizable models and the\ndemonstration data to guide the model-based RL planner to solve complex tasks.\nOur experiments show that our approach can solve a variety of complex tool use\ntasks from raw pixel inputs, outperforming both imitation learning and\nself-supervised learning individually. Furthermore, we show that the robot can\nperceive and use novel objects as tools, including objects that are not\nconventional tools, while also choosing dynamically to use or not use tools\ndepending on whether or not they are required.", "journal": ""}
{"doi": "10.48550/arXiv.2402.10753", "date": "2024-02-16", "title": "ToolSword: Unveiling Safety Issues of Large Language Models in Tool Learning Across Three Stages", "authors": "Junjie Ye, Sixian Li, Guanyu Li, Caishuang Huang, Songyang Gao, Yilong Wu, Qi Zhang, Tao Gui, Xuanjing Huang", "abstract": "Tool learning is widely acknowledged as a foundational approach or deploying\nlarge language models (LLMs) in real-world scenarios. While current research\nprimarily emphasizes leveraging tools to augment LLMs, it frequently neglects\nemerging safety considerations tied to their application. To fill this gap, we\npresent *ToolSword*, a comprehensive framework dedicated to meticulously\ninvestigating safety issues linked to LLMs in tool learning. Specifically,\nToolSword delineates six safety scenarios for LLMs in tool learning,\nencompassing **malicious queries** and **jailbreak attacks** in the input\nstage, **noisy misdirection** and **risky cues** in the execution stage, and\n**harmful feedback** and **error conflicts** in the output stage. Experiments\nconducted on 11 open-source and closed-source LLMs reveal enduring safety\nchallenges in tool learning, such as handling harmful queries, employing risky\ntools, and delivering detrimental feedback, which even GPT-4 is susceptible to.\nMoreover, we conduct further studies with the aim of fostering research on tool\nlearning safety. The data is released in\nhttps://github.com/Junjie-Ye/ToolSword.", "journal": "Proceedings of the 62nd Annual Meeting of the Association for\n  Computational Linguistics 2024 (Volume 1: Long Papers)"}
{"doi": "10.48550/arXiv.2306.14447", "date": "2023-06-26", "title": "RoboCook: Long-Horizon Elasto-Plastic Object Manipulation with Diverse Tools", "authors": "Haochen Shi, Huazhe Xu, Samuel Clarke, Yunzhu Li, Jiajun Wu", "abstract": "Humans excel in complex long-horizon soft body manipulation tasks via\nflexible tool use: bread baking requires a knife to slice the dough and a\nrolling pin to flatten it. Often regarded as a hallmark of human cognition,\ntool use in autonomous robots remains limited due to challenges in\nunderstanding tool-object interactions. Here we develop an intelligent robotic\nsystem, RoboCook, which perceives, models, and manipulates elasto-plastic\nobjects with various tools. RoboCook uses point cloud scene representations,\nmodels tool-object interactions with Graph Neural Networks (GNNs), and combines\ntool classification with self-supervised policy learning to devise manipulation\nplans. We demonstrate that from just 20 minutes of real-world interaction data\nper tool, a general-purpose robot arm can learn complex long-horizon soft\nobject manipulation tasks, such as making dumplings and alphabet letter\ncookies. Extensive evaluations show that RoboCook substantially outperforms\nstate-of-the-art approaches, exhibits robustness against severe external\ndisturbances, and demonstrates adaptability to different materials.", "journal": ""}
{"doi": "10.48550/arXiv.2405.04826", "date": "2024-05-08", "title": "Adaptive Whole-body Robotic Tool-use Learning on Low-rigidity Plastic-made Humanoids Using Vision and Tactile Sensors", "authors": "Kento Kawaharazuka, Kei Okada, Masayuki Inaba", "abstract": "Various robots have been developed so far; however, we face challenges in\nmodeling the low-rigidity bodies of some robots. In particular, the deflection\nof the body changes during tool-use due to object grasping, resulting in\nsignificant shifts in the tool-tip position and the body's center of gravity.\nMoreover, this deflection varies depending on the weight and length of the\ntool, making these models exceptionally complex. However, there is currently no\ncontrol or learning method that takes all of these effects into account. In\nthis study, we propose a method for constructing a neural network that\ndescribes the mutual relationship among joint angle, visual information, and\ntactile information from the feet. We aim to train this network using the\nactual robot data and utilize it for tool-tip control. Additionally, we employ\nParametric Bias to capture changes in this mutual relationship caused by\nvariations in the weight and length of tools, enabling us to understand the\ncharacteristics of the grasped tool from the current sensor information. We\napply this approach to the whole-body tool-use on KXR, a low-rigidity\nplastic-made humanoid robot, to validate its effectiveness.", "journal": ""}
{"doi": "10.48550/arXiv.2410.09871", "date": "2024-10-13", "title": "A Comparative Study of PDF Parsing Tools Across Diverse Document Categories", "authors": "Narayan S. Adhikari, Shradha Agarwal", "abstract": "PDF is one of the most prominent data formats, making PDF parsing crucial for\ninformation extraction and retrieval, particularly with the rise of RAG\nsystems. While various PDF parsing tools exist, their effectiveness across\ndifferent document types remains understudied, especially beyond academic\npapers. Our research aims to address this gap by comparing 10 popular PDF\nparsing tools across 6 document categories using the DocLayNet dataset. These\ntools include PyPDF, pdfminer-six, PyMuPDF, pdfplumber, pypdfium2,\nUnstructured, Tabula, Camelot, as well as the deep learning-based tools Nougat\nand Table Transformer(TATR). We evaluated both text extraction and table\ndetection capabilities. For text extraction, PyMuPDF and pypdfium generally\noutperformed others, but all parsers struggled with Scientific and Patent\ndocuments. For these challenging categories, learning-based tools like Nougat\ndemonstrated superior performance. In table detection, TATR excelled in the\nFinancial, Patent, Law & Regulations, and Scientific categories. Table\ndetection tool Camelot performed best for tender documents, while PyMuPDF\nperformed superior in the Manual category. Our findings highlight the\nimportance of selecting appropriate parsing tools based on document type and\nspecific tasks, providing valuable insights for researchers and practitioners\nworking with diverse document sources.", "journal": ""}
{"doi": "10.48550/arXiv.2506.00042", "date": "2025-05-28", "title": "Enhancing Tool Learning in Large Language Models with Hierarchical Error Checklists", "authors": "Yue Cui, Liuyi Yao, Shuchang Tao, Weijie Shi, Yaliang Li, Bolin Ding, Xiaofang Zhou", "abstract": "Large language models (LLMs) have significantly advanced natural language\nprocessing, particularly through the integration of external tools and APIs.\nHowever, their effectiveness is frequently hampered by parameter mis-filling\nduring tool calling. In this paper, we propose the Hierarchical Tool Error\nChecklist (HiTEC) framework to systematically diagnose and mitigate\ntool-calling errors without relying on extensive real-world interactions. HiTEC\nintroduces a two-tiered approach: a global error checklist that identifies\ncommon, cross-tool issues, and a local error checklist that targets\ntool-specific and contextual failures. Building on this structure, we propose\ntwo deployments: HiTEC-In Context Learning (HiTEC-ICL) and\nHiTEC-Kahneman-Tversky Optimization (HiTEC-KTO). HiTEC-ICL embeds the global\nchecklist in the initial prompts and leverages a two-round conversational\ninteraction to dynamically refine parameter handling, while HiTEC-KTO generates\nhigh-quality negative examples to drive fine-tuning via preference-based\noptimization. Extensive experiments across five public datasets demonstrate\nthat our framework significantly improves parameter-filling accuracy and\ntool-calling success rates compared to baseline methods.", "journal": ""}
{"doi": "10.48550/arXiv.2410.06617", "date": "2024-10-09", "title": "Learning Evolving Tools for Large Language Models", "authors": "Guoxin Chen, Zhong Zhang, Xin Cong, Fangda Guo, Yesai Wu, Yankai Lin, Wenzheng Feng, Yasheng Wang", "abstract": "Tool learning enables large language models (LLMs) to interact with external\ntools and APIs, greatly expanding the application scope of LLMs. However, due\nto the dynamic nature of external environments, these tools and APIs may become\noutdated over time, preventing LLMs from correctly invoking tools. Existing\nresearch primarily focuses on static environments and overlooks this issue,\nlimiting the adaptability of LLMs in real-world applications. In this paper, we\npropose ToolEVO, a novel framework designed to enhance the adaptive and\nreflective capabilities of LLMs against tool variability. By leveraging Monte\nCarlo Tree Search, ToolEVO facilitates active exploration and interaction of\nLLMs within dynamic environments, allowing for autonomous self-reflection and\nself-updating of tool usage based on environmental feedback. Additionally, we\nintroduce ToolQA-D, a benchmark specifically designed to evaluate the impact of\ntool variability. Extensive experiments demonstrate the effectiveness and\nstability of our approach, highlighting the importance of adaptability to tool\nvariability for effective tool learning. Code:\nhttps://github.com/Chen-GX/ToolEVO", "journal": ""}
{"doi": "10.48550/arXiv.2411.16313", "date": "2024-11-25", "title": "CATP-LLM: Empowering Large Language Models for Cost-Aware Tool Planning", "authors": "Duo Wu, Jinghe Wang, Yuan Meng, Yanning Zhang, Le Sun, Zhi Wang", "abstract": "Utilizing large language models (LLMs) for tool planning has emerged as a\npromising avenue for developing general AI systems, where LLMs automatically\nschedule external tools (e.g. vision models) to tackle complex tasks based on\ntask descriptions. To push this paradigm toward practical applications, it is\ncrucial for LLMs to consider tool execution costs (e.g. execution time) for\ntool planning. Unfortunately, prior studies overlook the tool execution costs,\nleading to the generation of expensive plans of which the costs outweigh task\nperformance. To fill this gap, we propose the Cost-Aware Tool Planning with\nLLMs (CATP-LLM) framework, which for the first time provides a coherent design\nto empower LLMs for cost-aware tool planning. Specifically, CATP-LLM\nincorporates a tool planning language to enhance the LLM to generate\nnon-sequential plans of multiple branches for efficient concurrent tool\nexecution and cost reduction. Moreover, it further designs a cost-aware offline\nreinforcement learning algorithm to fine-tune the LLM to optimize the\nperformance-cost trade-off in tool planning. In lack of public cost-related\ndatasets, we further present OpenCATP, the first platform for cost-aware\nplanning evaluation. Experiments on OpenCATP show that CATP-LLM outperforms\nGPT-4 even when using Llama2-7B as its backbone, with the average improvement\nof 28.2%-30.2% higher plan performance and 24.7%-45.8% lower costs even on the\nchallenging planning tasks. The codes and dataset will be available at:\nhttps://github.com/duowuyms/OpenCATP-LLM.", "journal": ""}
{"doi": "10.48550/arXiv.2412.03573", "date": "2024-11-17", "title": "Improving Tool Retrieval by Leveraging Large Language Models for Query Generation", "authors": "Mohammad Kachuee, Sarthak Ahuja, Vaibhav Kumar, Puyang Xu, Xiaohu Liu", "abstract": "Using tools by Large Language Models (LLMs) is a promising avenue to extend\ntheir reach beyond language or conversational settings. The number of tools can\nscale to thousands as they enable accessing sensory information, fetching\nupdated factual knowledge, or taking actions in the real world. In such\nsettings, in-context learning by providing a short list of relevant tools in\nthe prompt is a viable approach. To retrieve relevant tools, various approaches\nhave been suggested, ranging from simple frequency-based matching to dense\nembedding-based semantic retrieval. However, such approaches lack the\ncontextual and common-sense understanding required to retrieve the right tools\nfor complex user requests. Rather than increasing the complexity of the\nretrieval component itself, we propose leveraging LLM understanding to generate\na retrieval query. Then, the generated query is embedded and used to find the\nmost relevant tools via a nearest-neighbor search. We investigate three\napproaches for query generation: zero-shot prompting, supervised fine-tuning on\ntool descriptions, and alignment learning by iteratively optimizing a reward\nmetric measuring retrieval performance. By conducting extensive experiments on\na dataset covering complex and multi-tool scenarios, we show that leveraging\nLLMs for query generation improves the retrieval for in-domain (seen tools) and\nout-of-domain (unseen tools) settings.", "journal": "COLING 2025"}
{"doi": "10.48550/arXiv.1704.04846", "date": "2017-04-17", "title": "PerspectivesX: A Proposed Tool to Scaffold Collaborative Learning Activities within MOOCs", "authors": "Aneesha Bakharia", "abstract": "In this work-in-progress paper, we introduce the PerspectivesX tool which\naims to scaffold collaborative learning activities within MOOCs. The\nPerspectivesX tool has been designed to promote learner knowledge construction\nand curation for a range of multi-perspective elaboration techniques (e.g.,\nSWOT analysis and Six Thinking Hats). The PerspectivesX tool is designed to\nstore learner submissions in a searchable knowledge base which is able to be\npersisted across course re-runs and promotes the use of natural language\nprocessing techniques to allow course moderators to provide scalable feedback.\nIn this paper we outline the design principles that structured collaborative\nlearning tools need to adhere to, design a prototype tool (PerspectivesX) and\nevaluate whether MOOC platform extension frameworks are able to support the\nimplementation of the tool.", "journal": ""}
{"doi": "10.48550/arXiv.1806.09266", "date": "2018-06-25", "title": "Learning Task-Oriented Grasping for Tool Manipulation from Simulated Self-Supervision", "authors": "Kuan Fang, Yuke Zhu, Animesh Garg, Andrey Kurenkov, Viraj Mehta, Li Fei-Fei, Silvio Savarese", "abstract": "Tool manipulation is vital for facilitating robots to complete challenging\ntask goals. It requires reasoning about the desired effect of the task and thus\nproperly grasping and manipulating the tool to achieve the task. Task-agnostic\ngrasping optimizes for grasp robustness while ignoring crucial task-specific\nconstraints. In this paper, we propose the Task-Oriented Grasping Network\n(TOG-Net) to jointly optimize both task-oriented grasping of a tool and the\nmanipulation policy for that tool. The training process of the model is based\non large-scale simulated self-supervision with procedurally generated tool\nobjects. We perform both simulated and real-world experiments on two tool-based\nmanipulation tasks: sweeping and hammering. Our model achieves overall 71.1%\ntask success rate for sweeping and 80.0% task success rate for hammering.\nSupplementary material is available at: bit.ly/task-oriented-grasp", "journal": ""}
{"doi": "10.48550/arXiv.2008.12321", "date": "2020-08-27", "title": "Learning Representations of Endoscopic Videos to Detect Tool Presence Without Supervision", "authors": "David Z. Li, Masaru Ishii, Russell H. Taylor, Gregory D. Hager, Ayushi Sinha", "abstract": "In this work, we explore whether it is possible to learn representations of\nendoscopic video frames to perform tasks such as identifying surgical tool\npresence without supervision. We use a maximum mean discrepancy (MMD)\nvariational autoencoder (VAE) to learn low-dimensional latent representations\nof endoscopic videos and manipulate these representations to distinguish frames\ncontaining tools from those without tools. We use three different methods to\nmanipulate these latent representations in order to predict tool presence in\neach frame. Our fully unsupervised methods can identify whether endoscopic\nvideo frames contain tools with average precision of 71.56, 73.93, and 76.18,\nrespectively, comparable to supervised methods. Our code is available at\nhttps://github.com/zdavidli/tool-presence/", "journal": ""}
{"doi": "10.48550/arXiv.2406.00059", "date": "2024-05-29", "title": "Conveyor: Efficient Tool-aware LLM Serving with Tool Partial Execution", "authors": "Yechen Xu, Xinhao Kong, Tingjun Chen, Danyang Zhuo", "abstract": "The complexity of large language model (LLM) serving workloads has\nsubstantially increased due to the integration with external tool invocations,\nsuch as ChatGPT plugins. In this paper, we identify a new opportunity for\nefficient LLM serving for requests that trigger tools: tool partial execution\nalongside LLM decoding. To this end, we design Conveyor, an efficient LLM\nserving system optimized for handling requests involving external tools. We\nintroduce a novel interface for tool developers to expose partial execution\nopportunities to the LLM serving system and a request scheduler that\nfacilitates partial tool execution. Our results demonstrate that tool partial\nexecution can improve request completion latency by up to 38.8%.", "journal": ""}
{"doi": "10.48550/arXiv.2407.08052", "date": "2024-07-10", "title": "Adaptive Robotic Tool-Tip Control Learning Considering Online Changes in Grasping State", "authors": "Kento Kawaharazuka, Kei Okada, Masayuki Inaba", "abstract": "Various robotic tool manipulation methods have been developed so far.\nHowever, to our knowledge, none of them have taken into account the fact that\nthe grasping state such as grasping position and tool angle can change at any\ntime during the tool manipulation. In addition, there are few studies that can\nhandle deformable tools. In this study, we develop a method for estimating the\nposition of a tool-tip, controlling the tool-tip, and handling online\nadaptation to changes in the relationship between the body and the tool, using\na neural network including parametric bias. We demonstrate the effectiveness of\nour method for online change in grasping state and for deformable tools, in\nexperiments using two different types of robots: axis-driven robot PR2 and\ntendon-driven robot MusashiLarm.", "journal": ""}
{"doi": "10.48550/arXiv.2409.18807", "date": "2024-09-24", "title": "LLM With Tools: A Survey", "authors": "Zhuocheng Shen", "abstract": "The integration of tools in augmenting large language models presents a novel\napproach toward enhancing the efficiency and accuracy of these models in\nhandling specific, complex tasks. This paper delves into the\nmethodology,challenges, and developments in the realm of teaching LLMs to use\nexternal tools, thereby pushing the boundaries of their capabilities beyond\npre-existing knowledge bases. We introduce a standardized paradigm for tool\nintegration guided by a series of functions that map user instructions to\nactionable plans and their execution, emphasizing the significance of\nunderstanding user intent, tool selection, and dynamic plan adjustment. Our\nexploration reveals the various challenges encountered, such as tool invocation\ntiming, selection accuracy, and the need for robust reasoning processes. In\naddressing these challenges, we investigate techniques within the context of\nfine-tuning and incontext learning paradigms, highlighting innovative\napproaches to ensure diversity, augment datasets, and improve\ngeneralization.Furthermore, we investigate a perspective on enabling LLMs to\nnot only utilize but also autonomously create tools, which may redefine their\nrole from mere tool users to tool creators. Finally,we reproduced Chameleon's\nresults on ScienceQA and analyzed the code structure.", "journal": ""}
{"doi": "10.48550/arXiv.2503.14432", "date": "2025-03-18", "title": "PLAY2PROMPT: Zero-shot Tool Instruction Optimization for LLM Agents via Tool Play", "authors": "Wei Fang, Yang Zhang, Kaizhi Qian, James Glass, Yada Zhu", "abstract": "Large language models (LLMs) are increasingly integrated with specialized\nexternal tools, yet many tasks demand zero-shot tool usage with minimal or\nnoisy documentation. Existing solutions rely on manual rewriting or labeled\ndata for validation, making them inapplicable in true zero-shot settings. To\naddress these challenges, we propose PLAY2PROMPT, an automated framework that\nsystematically \"plays\" with each tool to explore its input-output behaviors.\nThrough this iterative trial-and-error process, PLAY2PROMPT refines tool\ndocumentation and generates usage examples without any labeled data. These\nexamples not only guide LLM inference but also serve as validation to further\nenhance tool utilization. Extensive experiments on real-world tasks demonstrate\nthat PLAY2PROMPT significantly improves zero-shot tool performance across both\nopen and closed models, offering a scalable and effective solution for\ndomain-specific tool integration.", "journal": ""}
{"doi": "10.48550/arXiv.2503.18755", "date": "2025-03-24", "title": "EgoSurgery-HTS: A Dataset for Egocentric Hand-Tool Segmentation in Open Surgery Videos", "authors": "Nathan Darjana, Ryo Fujii, Hideo Saito, Hiroki Kajita", "abstract": "Egocentric open-surgery videos capture rich, fine-grained details essential\nfor accurately modeling surgical procedures and human behavior in the operating\nroom. A detailed, pixel-level understanding of hands and surgical tools is\ncrucial for interpreting a surgeon's actions and intentions. We introduce\nEgoSurgery-HTS, a new dataset with pixel-wise annotations and a benchmark suite\nfor segmenting surgical tools, hands, and interacting tools in egocentric\nopen-surgery videos. Specifically, we provide a labeled dataset for (1) tool\ninstance segmentation of 14 distinct surgical tools, (2) hand instance\nsegmentation, and (3) hand-tool segmentation to label hands and the tools they\nmanipulate. Using EgoSurgery-HTS, we conduct extensive evaluations of\nstate-of-the-art segmentation methods and demonstrate significant improvements\nin the accuracy of hand and hand-tool segmentation in egocentric open-surgery\nvideos compared to existing datasets. The dataset will be released at\nhttps://github.com/Fujiry0/EgoSurgery.", "journal": ""}
{"doi": "10.48550/arXiv.1901.02144", "date": "2019-01-08", "title": "Guidelines and Benchmarks for Deployment of Deep Learning Models on Smartphones as Real-Time Apps", "authors": "Abhishek Sehgal, Nasser Kehtarnavaz", "abstract": "Deep learning solutions are being increasingly used in mobile applications.\nAlthough there are many open-source software tools for the development of deep\nlearning solutions, there are no guidelines in one place in a unified manner\nfor using these tools towards real-time deployment of these solutions on\nsmartphones. From the variety of available deep learning tools, the most suited\nones are used in this paper to enable real-time deployment of deep learning\ninference networks on smartphones. A uniform flow of implementation is devised\nfor both Android and iOS smartphones. The advantage of using multi-threading to\nachieve or improve real-time throughputs is also showcased. A benchmarking\nframework consisting of accuracy, CPU/GPU consumption and real-time throughput\nis considered for validation purposes. The developed deployment approach allows\ndeep learning models to be turned into real-time smartphone apps with ease\nbased on publicly available deep learning and smartphone software tools. This\napproach is applied to six popular or representative convolutional neural\nnetwork models and the validation results based on the benchmarking metrics are\nreported.", "journal": ""}
{"doi": "10.48550/arXiv.2411.00412", "date": "2024-11-01", "title": "Adapting While Learning: Grounding LLMs for Scientific Problems with Intelligent Tool Usage Adaptation", "authors": "Bohan Lyu, Yadi Cao, Duncan Watson-Parris, Leon Bergen, Taylor Berg-Kirkpatrick, Rose Yu", "abstract": "Large Language Models (LLMs) demonstrate promising capabilities in solving\nscientific problems but often suffer from the issue of hallucination. While\nintegrating LLMs with tools can mitigate this issue, models fine-tuned on tool\nusage become overreliant on them and incur unnecessary costs. Inspired by how\nhuman experts assess problem complexity before selecting solutions, we propose\na novel two-component fine-tuning method, Adapting While Learning (AWL). In the\nfirst component, World Knowledge Learning (WKL), LLMs internalize scientific\nknowledge by learning from tool-generated solutions. In the second component,\nTool Usage Adaptation (TUA), we categorize problems as easy or hard based on\nthe model's accuracy, and train it to maintain direct reasoning for easy\nproblems while switching to tools for hard ones. We validate our method on six\nscientific benchmark datasets across climate science, epidemiology, physics,\nand other domains. Compared to the original instruct model (8B), models\npost-trained with AWL achieve 29.11% higher answer accuracy and 12.72% better\ntool usage accuracy, even surpassing state-of-the-art models including GPT-4o\nand Claude-3.5 on four custom-created datasets. Our code is open-source at\nhttps://github.com/Rose-STL-Lab/Adapting-While-Learning.", "journal": "In Proceedings of the Forty-second International Conference on\n  Machine Learning (ICML 2025)"}
{"doi": "10.48550/arXiv.1807.00279", "date": "2018-07-01", "title": "Using Blippar Augmented Reality Browser in the Practical Training of Mechanical Engineers", "authors": "Andrii Striuk, Maryna Rassovytska, Svitlana Shokaliuk", "abstract": "The purpose of the study is to justify the expediency of using the Blippar\naugmented reality browser for professional and practical training of future\nmechanical engineers. Tasks of the research: to analyze the expediency of using\naugmented reality tools in the professional training of bachelors of applied\nmechanics; to carry out the selection of augmented reality tools, which is\nexpedient to use in the training of future engineer mechanics; to develop\neducational materials using the chosen augmented reality tools. The object of\nthe study is the professional training of future mechanical engineers. The\nsubject of the study is the use of the augmented reality tools in the\nprofessional training of bachelors of applied mechanics. The paper analyzes the\nrelevance and expediency of the use of the augmented reality tools in the\nprofessional training of future mechanical engineers. It is determined that the\naugmented reality tools will promote the development of ICT competence and\ngraphic competence of bachelors of applied mechanics The model of the use of\nthe augmented reality tools in the training of future mechanical engineers is\nproposed. As the main tool, the Blippar browser and Blippbuilder's cloud-based\nscript development tool are chosen. An example of the creation of markers and\nscenes of augmented reality using the selected tools is given. The advantages\nand disadvantages of used tools are indicated. The proposed learning tools and\nmethods can be applied to vocational and practical training of mechanical\nengineers.", "journal": "CEUR Workshop Proceedings Vol. 2104 (2018) 412-419"}
{"doi": "10.48550/arXiv.2401.17464", "date": "2024-01-30", "title": "Efficient Tool Use with Chain-of-Abstraction Reasoning", "authors": "Silin Gao, Jane Dwivedi-Yu, Ping Yu, Xiaoqing Ellen Tan, Ramakanth Pasunuru, Olga Golovneva, Koustuv Sinha, Asli Celikyilmaz, Antoine Bosselut, Tianlu Wang", "abstract": "To achieve faithful reasoning that aligns with human expectations, large\nlanguage models (LLMs) need to ground their reasoning to real-world knowledge\n(e.g., web facts, math and physical rules). Tools help LLMs access this\nexternal knowledge, but there remains challenges for fine-tuning LLM agents\n(e.g., Toolformer) to invoke tools in multi-step reasoning problems, where\ninter-connected tool calls require holistic and efficient tool usage planning.\n  In this work, we propose a new method for LLMs to better leverage tools in\nmulti-step reasoning. Our method, Chain-of-Abstraction (CoA), trains LLMs to\nfirst decode reasoning chains with abstract placeholders, and then call domain\ntools to reify each reasoning chain by filling in specific knowledge. This\nplanning with abstract chains enables LLMs to learn more general reasoning\nstrategies, which are robust to shifts of domain knowledge (e.g., math results)\nrelevant to different reasoning questions. It also allows LLMs to perform\ndecoding and calling of external tools in parallel, which avoids the inference\ndelay caused by waiting for tool responses. In mathematical reasoning and Wiki\nQA domains, we show that our method consistently outperforms previous\nchain-of-thought and tool-augmented baselines on both in-distribution and\nout-of-distribution test sets, with an average ~6% absolute QA accuracy\nimprovement. LLM agents trained with our method also show more efficient tool\nuse, with inference speed being on average ~1.4x faster than baseline\ntool-augmented LLMs.", "journal": ""}
{"doi": "10.48550/arXiv.2409.02141", "date": "2024-09-02", "title": "Efficient and Scalable Estimation of Tool Representations in Vector Space", "authors": "Suhong Moon, Siddharth Jha, Lutfi Eren Erdogan, Sehoon Kim, Woosang Lim, Kurt Keutzer, Amir Gholami", "abstract": "Recent advancements in function calling and tool use have significantly\nenhanced the capabilities of large language models (LLMs) by enabling them to\ninteract with external information sources and execute complex tasks. However,\nthe limited context window of LLMs presents challenges when a large number of\ntools are available, necessitating efficient methods to manage prompt length\nand maintain accuracy. Existing approaches, such as fine-tuning LLMs or\nleveraging their reasoning capabilities, either require frequent retraining or\nincur significant latency overhead. A more efficient solution involves training\nsmaller models to retrieve the most relevant tools for a given query, although\nthis requires high quality, domain-specific data. To address those challenges,\nwe present a novel framework for generating synthetic data for tool retrieval\napplications and an efficient data-driven tool retrieval strategy using small\nencoder models. Empowered by LLMs, we create ToolBank, a new tool retrieval\ndataset that reflects real human user usages. For tool retrieval methodologies,\nwe propose novel approaches: (1) Tool2Vec: usage-driven tool embedding\ngeneration for tool retrieval, (2) ToolRefiner: a staged retrieval method that\niteratively improves the quality of retrieved tools, and (3) MLC: framing tool\nretrieval as a multi-label classification problem. With these new methods, we\nachieve improvements of up to 27.28 in Recall@K on the ToolBench dataset and\n30.5 in Recall@K on ToolBank. Additionally, we present further experimental\nresults to rigorously validate our methods. Our code is available at\n\\url{https://github.com/SqueezeAILab/Tool2Vec}", "journal": ""}
{"doi": "10.48550/arXiv.2502.11435", "date": "2025-02-17", "title": "SMART: Self-Aware Agent for Tool Overuse Mitigation", "authors": "Cheng Qian, Emre Can Acikgoz, Hongru Wang, Xiusi Chen, Avirup Sil, Dilek Hakkani-T\u00fcr, Gokhan Tur, Heng Ji", "abstract": "Current Large Language Model (LLM) agents demonstrate strong reasoning and\ntool use capabilities, but often lack self-awareness, failing to balance these\napproaches effectively. This imbalance leads to Tool Overuse, where models\nunnecessarily rely on external tools for tasks solvable with parametric\nknowledge, increasing computational overhead. Inspired by human metacognition,\nwe introduce SMART (Strategic Model-Aware Reasoning with Tools), a paradigm\nthat enhances an agent's self-awareness to optimize task handling and reduce\ntool overuse. To support this paradigm, we introduce SMART-ER, a dataset\nspanning three domains, where reasoning alternates between parametric knowledge\nand tool-dependent steps, with each step enriched by rationales explaining when\ntools are necessary. Through supervised training, we develop SMARTAgent, a\nfamily of models that dynamically balance parametric knowledge and tool use.\nEvaluations show that SMARTAgent reduces tool use by 24% while improving\nperformance by over 37%, enabling 7B-scale models to match its 70B counterpart\nand GPT-4o. Additionally, SMARTAgent generalizes to out-of-distribution test\ndata like GSM8K and MINTQA, maintaining accuracy with just one-fifth the tool\ncalls. These highlight the potential of strategic tool use to enhance\nreasoning, mitigate overuse, and bridge the gap between model size and\nperformance, advancing intelligent and resource-efficient agent designs.", "journal": ""}
{"doi": "10.48550/arXiv.2202.10169", "date": "2022-02-21", "title": "Machine Learning Operations: A Survey on MLOps Tool Support", "authors": "Nipuni Hewage, Dulani Meedeniya", "abstract": "Machine Learning (ML) has become a fast-growing, trending approach in\nsolution development in practice. Deep Learning (DL) which is a subset of ML,\nlearns using deep neural networks to simulate the human brain. It trains\nmachines to learn techniques and processes individually using computer\nalgorithms, which is also considered to be a role of Artificial Intelligence\n(AI). In this paper, we study current technical issues related to software\ndevelopment and delivery in organizations that work on ML projects. Therefore,\nthe importance of the Machine Learning Operations (MLOps) concept, which can\ndeliver appropriate solutions for such concerns, is discussed. We investigate\ncommercially available MLOps tool support in software development. The\ncomparison between MLOps tools analyzes the performance of each system and its\nuse cases. Moreover, we examine the features and usability of MLOps tools to\nidentify the most appropriate tool support for given scenarios. Finally, we\nrecognize that there is a shortage in the availability of a fully functional\nMLOps platform on which processes can be automated by reducing human\nintervention.", "journal": ""}
{"doi": "10.48550/arXiv.2211.09006", "date": "2022-11-16", "title": "ToolFlowNet: Robotic Manipulation with Tools via Predicting Tool Flow from Point Clouds", "authors": "Daniel Seita, Yufei Wang, Sarthak J. Shetty, Edward Yao Li, Zackory Erickson, David Held", "abstract": "Point clouds are a widely available and canonical data modality which convey\nthe 3D geometry of a scene. Despite significant progress in classification and\nsegmentation from point clouds, policy learning from such a modality remains\nchallenging, and most prior works in imitation learning focus on learning\npolicies from images or state information. In this paper, we propose a novel\nframework for learning policies from point clouds for robotic manipulation with\ntools. We use a novel neural network, ToolFlowNet, which predicts dense\nper-point flow on the tool that the robot controls, and then uses the flow to\nderive the transformation that the robot should execute. We apply this\nframework to imitation learning of challenging deformable object manipulation\ntasks with continuous movement of tools, including scooping and pouring, and\ndemonstrate significantly improved performance over baselines which do not use\nflow. We perform 50 physical scooping experiments with ToolFlowNet and attain\n82% scooping success. See https://tinyurl.com/toolflownet for supplementary\nmaterial.", "journal": ""}
{"doi": "10.48550/arXiv.2305.12013", "date": "2023-05-19", "title": "Constructing Dreams using Generative AI", "authors": "Safinah Ali, Daniella DiPaola, Randi Williams, Prerna Ravi, Cynthia Breazeal", "abstract": "Generative AI tools introduce new and accessible forms of media creation for\nyouth. They also raise ethical concerns about the generation of fake media,\ndata protection, privacy and ownership of AI-generated art. Since generative AI\nis already being used in products used by youth, it is critical that they\nunderstand how these tools work and how they can be used or misused. In this\nwork, we facilitated students' generative AI learning through expression of\ntheir imagined future identities. We designed a learning workshop - Dreaming\nwith AI - where students learned about the inner workings of generative AI\ntools, used text-to-image generation algorithms to create their imaged future\ndreams, reflected on the potential benefits and harms of generative AI tools\nand voiced their opinions about policies for the use of these tools in\nclassrooms. In this paper, we present the learning activities and experiences\nof 34 high school students who engaged in our workshops. Students reached\ncreative learning objectives by using prompt engineering to create their future\ndreams, gained technical knowledge by learning the abilities, limitations,\ntext-visual mappings and applications of generative AI, and identified most\npotential societal benefits and harms of generative AI.", "journal": ""}
{"doi": "10.48550/arXiv.2505.12147", "date": "2025-05-17", "title": "Causal Machine Learning in IoT-based Engineering Problems: A Tool Comparison in the Case of Household Energy Consumption", "authors": "Nikolaos-Lysias Kosioris, Sotirios Nikoletseas, Gavrilis Filios, Stefanos Panagiotou", "abstract": "The rapid increase in computing power and the ability to store Big Data in\nthe infrastructure has enabled predictions in a large variety of domains by\nMachine Learning. However, in many cases, existing Machine Learning tools are\nconsidered insufficient or incorrect since they exploit only probabilistic\ndependencies rather than inference logic. Causal Machine Learning methods seem\nto close this gap. In this paper, two prevalent tools based on Causal Machine\nLearning methods are compared, as well as their mathematical underpinning\nbackground. The operation of the tools is demonstrated by examining their\nresponse to 18 queries, based on the IDEAL Household Energy Dataset, published\nby the University of Edinburgh. First, it was important to evaluate the causal\nrelations assumption that allowed the use of this approach; this was based on\nthe preexisting scientific knowledge of the domain and was implemented by use\nof the in-built validation tools. Results were encouraging and may easily be\nextended to other domains.", "journal": ""}
{"doi": "10.48550/arXiv.2410.07403", "date": "2024-10-09", "title": "On the Feasibility of A Mixed-Method Approach for Solving Long Horizon Task-Oriented Dexterous Manipulation", "authors": "Shaunak A. Mehta, Rana Soltani Zarrin", "abstract": "In-hand manipulation of tools using dexterous hands in real-world is an\nunderexplored problem in the literature. In addition to more complex geometry\nand larger size of the tools compared to more commonly used objects like cubes\nor cylinders, task oriented in-hand tool manipulation involves many sub-tasks\nto be performed sequentially. This may involve reaching to the tool, picking it\nup, reorienting it in hand with or without regrasping to reach to a desired\nfinal grasp appropriate for the tool usage, and carrying the tool to the\ndesired pose. Research on long-horizon manipulation using dexterous hands is\nrather limited and the existing work focus on learning the individual sub-tasks\nusing a method like reinforcement learning (RL) and combine the policies for\ndifferent subtasks to perform a long horizon task. However, in general a single\nmethod may not be the best for all the sub-tasks, and this can be more\npronounced when dealing with multi-fingered hands manipulating objects with\ncomplex geometry like tools. In this paper, we investigate the use of a\nmixed-method approach to solve for the long-horizon task of tool usage and we\nuse imitation learning, reinforcement learning and model based control. We also\ndiscuss a new RL-based teacher-student framework that combines real world data\ninto offline training. We show that our proposed approach for each subtask\noutperforms the commonly adopted reinforcement learning approach across\ndifferent subtasks and in performing the long horizon task in simulation.\nFinally we show the successful transferability to real world.", "journal": ""}
{"doi": "10.48550/arXiv.1910.11977", "date": "2019-10-26", "title": "KETO: Learning Keypoint Representations for Tool Manipulation", "authors": "Zengyi Qin, Kuan Fang, Yuke Zhu, Li Fei-Fei, Silvio Savarese", "abstract": "We aim to develop an algorithm for robots to manipulate novel objects as\ntools for completing different task goals. An efficient and informative\nrepresentation would facilitate the effectiveness and generalization of such\nalgorithms. For this purpose, we present KETO, a framework of learning keypoint\nrepresentations of tool-based manipulation. For each task, a set of\ntask-specific keypoints is jointly predicted from 3D point clouds of the tool\nobject by a deep neural network. These keypoints offer a concise and\ninformative description of the object to determine grasps and subsequent\nmanipulation actions. The model is learned from self-supervised robot\ninteractions in the task environment without the need for explicit human\nannotations. We evaluate our framework in three manipulation tasks with tool\nuse. Our model consistently outperforms state-of-the-art methods in terms of\ntask success rates. Qualitative results of keypoint prediction and tool\ngeneration are shown to visualize the learned representations.", "journal": ""}
{"doi": "10.48550/arXiv.2407.00065", "date": "2024-06-17", "title": "A Personalised Learning Tool for Physics Undergraduate Students Built On a Large Language Model for Symbolic Regression", "authors": "Yufan Zhu, Zi-Yu Khoo, Jonathan Sze Choong Low, Stephane Bressan", "abstract": "Interleaved practice enhances the memory and problem-solving ability of\nstudents in undergraduate courses. We introduce a personalized learning tool\nbuilt on a Large Language Model (LLM) that can provide immediate and\npersonalized attention to students as they complete homework containing\nproblems interleaved from undergraduate physics courses. Our tool leverages the\ndimensional analysis method, enhancing students' qualitative thinking and\nproblem-solving skills for complex phenomena. Our approach combines LLMs for\nsymbolic regression with dimensional analysis via prompt engineering and offers\nstudents a unique perspective to comprehend relationships between physics\nvariables. This fosters a broader and more versatile understanding of physics\nand mathematical principles and complements a conventional undergraduate\nphysics education that relies on interpreting and applying established\nequations within specific contexts. We test our personalized learning tool on\nthe equations from Feynman's lectures on physics. Our tool can correctly\nidentify relationships between physics variables for most equations,\nunderscoring its value as a complementary personalized learning tool for\nundergraduate physics students.", "journal": ""}
{"doi": "10.48550/arXiv.2102.03670", "date": "2021-02-06", "title": "Recommending More Efficient Workflows to Software Developers", "authors": "Dylan Bates", "abstract": "Existing recommendation systems can help developers improve their software\ndevelopment abilities by recommending new programming tools, such as a\nrefactoring tool or a program navigation tool. However, simply recommending\ntools in isolation may not, in and of itself, allow developers to successfully\ncomplete their tasks. In this paper, I introduce a new recommendation system\nthat recommends workflows, or sequences of tools, to developers. By learning\nmore efficient workflows, the system could make software developers more\nefficient.", "journal": ""}
{"doi": "10.48550/arXiv.2406.12429", "date": "2024-06-18", "title": "Query Routing for Homogeneous Tools: An Instantiation in the RAG Scenario", "authors": "Feiteng Mu, Yong Jiang, Liwen Zhang, Chu Liu, Wenjie Li, Pengjun Xie, Fei Huang", "abstract": "Current research on tool learning primarily focuses on selecting the most\neffective tool from a wide array of options, often overlooking\ncost-effectiveness, a crucial factor in human problem-solving. In this paper,\nwe address the selection of homogeneous tools by predicting both their\nperformance and the associated cost required to accomplish a given task. We\nthen assign queries to the optimal tools in a cost-effective manner. Our\nexperimental results demonstrate that our method achieves higher performance at\na lower cost compared to strong baseline approaches.", "journal": ""}
{"doi": "10.48550/arXiv.1004.0314", "date": "2010-04-02", "title": "Visualization of Manifold-Valued Elements by Multidimensional Scaling", "authors": "Simone Fiori", "abstract": "The present contribution suggests the use of a multidimensional scaling (MDS)\nalgorithm as a visualization tool for manifold-valued elements. A visualization\ntool of this kind is useful in signal processing and machine learning whenever\nlearning/adaptation algorithms insist on high-dimensional parameter manifolds.", "journal": ""}
{"doi": "10.48550/arXiv.1712.05016", "date": "2017-12-13", "title": "Deep Prior", "authors": "Alexandre Lacoste, Thomas Boquet, Negar Rostamzadeh, Boris Oreshkin, Wonchang Chung, David Krueger", "abstract": "The recent literature on deep learning offers new tools to learn a rich\nprobability distribution over high dimensional data such as images or sounds.\nIn this work we investigate the possibility of learning the prior distribution\nover neural network parameters using such tools. Our resulting variational\nBayes algorithm generalizes well to new tasks, even when very few training\nexamples are provided. Furthermore, this learned prior allows the model to\nextrapolate correctly far from a given task's training data on a meta-dataset\nof periodic signals.", "journal": ""}
{"doi": "10.48550/arXiv.2007.01977", "date": "2020-07-04", "title": "Lale: Consistent Automated Machine Learning", "authors": "Guillaume Baudart, Martin Hirzel, Kiran Kate, Parikshit Ram, Avraham Shinnar", "abstract": "Automated machine learning makes it easier for data scientists to develop\npipelines by searching over possible choices for hyperparameters, algorithms,\nand even pipeline topologies. Unfortunately, the syntax for automated machine\nlearning tools is inconsistent with manual machine learning, with each other,\nand with error checks. Furthermore, few tools support advanced features such as\ntopology search or higher-order operators. This paper introduces Lale, a\nlibrary of high-level Python interfaces that simplifies and unifies automated\nmachine learning in a consistent way.", "journal": ""}
{"doi": "10.48550/arXiv.1507.03685", "date": "2015-07-14", "title": "TryLogic tutorial: an approach to Learning Logic by proving and refuting", "authors": "Patrick Terrematte, Jo\u00e3o Marcos", "abstract": "Aiming to offer a framework for blended learning to the teaching of proof\ntheory, the present paper describes an interactive tutorial, called\n\\textsc{TryLogic}, teaching how to solve logical conjectures either by proofs\nor refutations. The paper also describes the integration of our infrastructure\nwith the Virtual Learning Environment \\texttt{Moodle} through the IMS Learning\nTools Interoperability specification, and evaluates the tool we have developed.", "journal": ""}
{"doi": "10.48550/arXiv.1402.1188", "date": "2014-02-05", "title": "On Designing Better Tools for Learning APIs", "authors": "Adrian Kuhn, Robert DeLine", "abstract": "Modern software development requires a large investment in learning\napplication programming interfaces (APIs). Recent research found that the\nlearning materials themselves are often inadequate: developers struggle to find\nanswers beyond simple usage scenarios. Solving these problems requires a large\ninvestment in tool and search engine development. To understand where further\ninvestment would be most useful, we ran a study with 19 professional developers\nto understand what a solution might look like, free of technical constraints.\nIn this paper, we report on design implications of tools for API learning,\ngrounded in the reality of the professional developers themselves. The\nreoccurring themes in the participants' feedback were trustworthiness,\nconfidentiality, information overload and the need for code examples as\nfirst-class documentation artifacts.", "journal": ""}
{"doi": "10.48550/arXiv.2012.03575", "date": "2020-12-07", "title": "Leveraging Automated Machine Learning for Text Classification: Evaluation of AutoML Tools and Comparison with Human Performance", "authors": "Matthias Blohm, Marc Hanussek, Maximilien Kintz", "abstract": "Recently, Automated Machine Learning (AutoML) has registered increasing\nsuccess with respect to tabular data. However, the question arises whether\nAutoML can also be applied effectively to text classification tasks. This work\ncompares four AutoML tools on 13 different popular datasets, including Kaggle\ncompetitions, and opposes human performance. The results show that the AutoML\ntools perform better than the machine learning community in 4 out of 13 tasks\nand that two stand out.", "journal": ""}
{"doi": "10.48550/arXiv.2111.04936", "date": "2021-11-09", "title": "An Interactive Visualization Tool for Understanding Active Learning", "authors": "Zihan Wang, Jialin Lu, Oliver Snow, Martin Ester", "abstract": "Despite recent progress in artificial intelligence and machine learning, many\nstate-of-the-art methods suffer from a lack of explainability and transparency.\nThe ability to interpret the predictions made by machine learning models and\naccurately evaluate these models is crucially important. In this paper, we\npresent an interactive visualization tool to elucidate the training process of\nactive learning. This tool enables one to select a sample of interesting data\npoints, view how their prediction values change at different querying stages,\nand thus better understand when and how active learning works. Additionally,\nusers can utilize this tool to compare different active learning strategies\nsimultaneously and inspect why some strategies outperform others in certain\ncontexts. With some preliminary experiments, we demonstrate that our\nvisualization panel has a great potential to be used in various active learning\nexperiments and help users evaluate their models appropriately.", "journal": ""}
{"doi": "10.48550/arXiv.2305.04347", "date": "2023-05-07", "title": "Interpreting Training Aspects of Deep-Learned Error-Correcting Codes", "authors": "N. Devroye, A. Mulgund, R. Shekhar, Gy. Tur\u00e1n, M. \u017defran, Y. Zhou", "abstract": "As new deep-learned error-correcting codes continue to be introduced, it is\nimportant to develop tools to interpret the designed codes and understand the\ntraining process. Prior work focusing on the deep-learned TurboAE has both\ninterpreted the learned encoders post-hoc by mapping these onto nearby\n``interpretable'' encoders, and experimentally evaluated the performance of\nthese interpretable encoders with various decoders. Here we look at developing\ntools for interpreting the training process for deep-learned error-correcting\ncodes, focusing on: 1) using the Goldreich-Levin algorithm to quickly interpret\nthe learned encoder; 2) using Fourier coefficients as a tool for understanding\nthe training dynamics and the loss landscape; 3) reformulating the training\nloss, the binary cross entropy, by relating it to encoder and decoder\nparameters, and the bit error rate (BER); 4) using these insights to formulate\nand study a new training procedure. All tools are demonstrated on TurboAE, but\nare applicable to other deep-learned forward error correcting codes (without\nfeedback).", "journal": ""}
{"doi": "10.48550/arXiv.2407.11922", "date": "2024-07-16", "title": "Learning secondary tool affordances of human partners using iCub robot's egocentric data", "authors": "Bosong Ding, Erhan Oztop, Giacomo Spigler, Murat Kirtay", "abstract": "Objects, in particular tools, provide several action possibilities to the\nagents that can act on them, which are generally associated with the term of\naffordances. A tool is typically designed for a specific purpose, such as\ndriving a nail in the case of a hammer, which we call as the primary\naffordance. A tool can also be used beyond its primary purpose, in which case\nwe can associate this auxiliary use with the term secondary affordance.\nPrevious work on affordance perception and learning has been mostly focused on\nprimary affordances. Here, we address the less explored problem of learning the\nsecondary tool affordances of human partners. To do this, we use the iCub robot\nto observe human partners with three cameras while they perform actions on\ntwenty objects using four different tools. In our experiments, human partners\nutilize tools to perform actions that do not correspond to their primary\naffordances. For example, the iCub robot observes a human partner using a ruler\nfor pushing, pulling, and moving objects instead of measuring their lengths. In\nthis setting, we constructed a dataset by taking images of objects before and\nafter each action is executed. We then model learning secondary affordances by\ntraining three neural networks (ResNet-18, ResNet-50, and ResNet-101) each on\nthree tasks, using raw images showing the `initial' and `final' position of\nobjects as input: (1) predicting the tool used to move an object, (2)\npredicting the tool used with an additional categorical input that encoded the\naction performed, and (3) joint prediction of both tool used and action\nperformed. Our results indicate that deep learning architectures enable the\niCub robot to predict secondary tool affordances, thereby paving the road for\nhuman-robot collaborative object manipulation involving complex affordances.", "journal": ""}
{"doi": "10.48550/arXiv.2303.04023", "date": "2023-03-07", "title": "Cross-Tool and Cross-Behavior Perceptual Knowledge Transfer for Grounded Object Recognition", "authors": "Gyan Tatiya, Jonathan Francis, Jivko Sinapov", "abstract": "Humans learn about objects via interaction and using multiple perceptions,\nsuch as vision, sound, and touch. While vision can provide information about an\nobject's appearance, non-visual sensors, such as audio and haptics, can provide\ninformation about its intrinsic properties, such as weight, temperature,\nhardness, and the object's sound. Using tools to interact with objects can\nreveal additional object properties that are otherwise hidden (e.g., knives and\nspoons can be used to examine the properties of food, including its texture and\nconsistency). Robots can use tools to interact with objects and gather\ninformation about their implicit properties via non-visual sensors. However, a\nrobot's model for recognizing objects using a tool-mediated behavior does not\ngeneralize to a new tool or behavior due to differing observed data\ndistributions. To address this challenge, we propose a framework to enable\nrobots to transfer implicit knowledge about granular objects across different\ntools and behaviors. The proposed approach learns a shared latent space from\nmultiple robots' contexts produced by respective sensory data while interacting\nwith objects using tools. We collected a dataset using a UR5 robot that\nperformed 5,400 interactions using 6 tools and 6 behaviors on 15 granular\nobjects and tested our method on cross-tool and cross-behavioral transfer\ntasks. Our results show the less experienced target robot can benefit from the\nexperience gained from the source robot and perform recognition on a set of\nnovel objects. We have released the code, datasets, and additional results:\nhttps://github.com/gtatiya/Tool-Knowledge-Transfer.", "journal": ""}
{"doi": "10.48550/arXiv.2401.10727", "date": "2024-01-19", "title": "MLLM-Tool: A Multimodal Large Language Model For Tool Agent Learning", "authors": "Chenyu Wang, Weixin Luo, Sixun Dong, Xiaohua Xuan, Zhengxin Li, Lin Ma, Shenghua Gao", "abstract": "Recently, the astonishing performance of large language models (LLMs) in\nnatural language comprehension and generation tasks triggered lots of\nexploration of using them as central controllers to build agent systems.\nMultiple studies focus on bridging the LLMs to external tools to extend the\napplication scenarios. However, the current LLMs' ability to perceive tool use\nis limited to a single text query, which may result in ambiguity in\nunderstanding the users' real intentions. LLMs are expected to eliminate that\nby perceiving the information in the visual- or auditory-grounded instructions.\nTherefore, in this paper, we propose MLLM-Tool, a system incorporating\nopen-source LLMs and multi-modal encoders so that the learned LLMs can be\nconscious of multi-modal input instruction and then select the function-matched\ntool correctly. To facilitate the evaluation of the model's capability, we\ncollect a dataset featuring multi-modal input tools from HuggingFace. Another\nessential feature of our dataset is that it also contains multiple potential\nchoices for the same instruction due to the existence of identical functions\nand synonymous functions, which provides more potential solutions for the same\nquery. The experiments reveal that our MLLM-Tool is capable of recommending\nappropriate tools for multi-modal instructions. Codes and data are available at\nhttps://github.com/MLLM-Tool/MLLM-Tool.", "journal": ""}
{"doi": "10.48550/arXiv.2407.12871", "date": "2024-07-15", "title": "MetaTool: Facilitating Large Language Models to Master Tools with Meta-task Augmentation", "authors": "Xiaohan Wang, Dian Li, Yilin Zhao, Sinbadliu, Hui Wang", "abstract": "Utilizing tools with Large Language Models (LLMs) is essential for grounding\nAI agents in real-world applications. The prevailing approach involves few-shot\nprompting with demonstrations or fine-tuning with expert annotations. However,\nmere in-context demonstrations may fail to cover sufficient knowledge for\ncomplex tools and tasks. Training on solution paths is also hindered by the\nhigh cost of expert annotations and generalizing to new tools. A core challenge\nof generalizable tool use lies in understanding the \"meta\", or fundamental\nnatures of tools that are transferable across tasks, such as causality and\nconstraints. In this paper, we present MetaTool, a novel tool learning\nmethodology designed to generalize across any reusable toolset. Our approach\nincorporates a self-supervised augmentation technique derived from a series of\nmeta-tasks. This involves predicting masked elements in the tool execution\nprocess. The self-supervised procedure enables scalable generation of\nhigh-quality QA data, which is handy for supervising tool understanding. By\nincorporating meta-task data into task-oriented training, our method\nsignificantly enhances the performance of open-source LLMs, achieving results\ncomparable to ChatGPT in both tool-based planning and chatting scenarios.\nThrough large-scale instruction tuning, the MetaTool model demonstrates\nimpressive zero-shot generalizability on new tasks.", "journal": ""}
{"doi": "10.48550/arXiv.2505.00024", "date": "2025-04-25", "title": "Nemotron-Research-Tool-N1: Exploring Tool-Using Language Models with Reinforced Reasoning", "authors": "Shaokun Zhang, Yi Dong, Jieyu Zhang, Jan Kautz, Bryan Catanzaro, Andrew Tao, Qingyun Wu, Zhiding Yu, Guilin Liu", "abstract": "Enabling large language models with external tools has become a pivotal\nstrategy for extending their functionality beyond text space. To enhance LLMs'\ntool-calling abilities, previous approaches primarily rely on supervised\nfine-tuning (SFT) with trajectories distilled from stronger models, often\nresulting in imitative reasoning that limits generalization. In this work, we\nexplore rule-based reinforcement learning to enhance tool-calling in LLMs,\nresulting in Nemotron-Research-Tool-N1, a series of tool-calling reasoning\nmodels. Rather than enforcing supervision over intermediate distilled reasoning\ntraces, Tool-N1 is trained with a binary RL reward that assesses only the\nformat validity and functional correctness of tool invocations. This\nlightweight supervision allows the model to develop reasoning strategies\nindependently, without relying on annotated trajectories. Experiments on\nseveral major benchmarks show that Tool-N1-7B/14B clearly outperform GPT-4o. We\nconduct a systematic study on the design of rule-based reinforcement learning\nstrategies for training tool-calling models. Using 5,518 distilled reasoning\ntrajectories, we compare SFT, RL, and the SFT-then-RL pipeline, finding that\nthe widely adopted SFT-then-RL paradigm does not necessarily outperform pure\nRL.", "journal": ""}
{"doi": "10.48550/arXiv.1611.00379", "date": "2016-11-01", "title": "The Machine Learning Algorithm as Creative Musical Tool", "authors": "Rebecca Fiebrink, Baptiste Caramiaux", "abstract": "Machine learning is the capacity of a computational system to learn\nstructures from datasets in order to make predictions on newly seen data. Such\nan approach offers a significant advantage in music scenarios in which\nmusicians can teach the system to learn an idiosyncratic style, or can break\nthe rules to explore the system's capacity in unexpected ways. In this chapter\nwe draw on music, machine learning, and human-computer interaction to elucidate\nan understanding of machine learning algorithms as creative tools for music and\nthe sonic arts. We motivate a new understanding of learning algorithms as\nhuman-computer interfaces. We show that, like other interfaces, learning\nalgorithms can be characterised by the ways their affordances intersect with\ngoals of human users. We also argue that the nature of interaction between\nusers and algorithms impacts the usability and usefulness of those algorithms\nin profound ways. This human-centred view of machine learning motivates our\nconcluding discussion of what it means to employ machine learning as a creative\ntool.", "journal": ""}
{"doi": "10.48550/arXiv.1807.06722", "date": "2018-07-18", "title": "Machine Learning Interpretability: A Science rather than a tool", "authors": "Abdul Karim, Avinash Mishra, MA Hakim Newton, Abdul Sattar", "abstract": "The term \"interpretability\" is oftenly used by machine learning researchers\neach with their own intuitive understanding of it. There is no universal well\nagreed upon definition of interpretability in machine learning. As any type of\nscience discipline is mainly driven by the set of formulated questions rather\nthan by different tools in that discipline, e.g. astrophysics is the discipline\nthat learns the composition of stars, not as the discipline that use the\nspectroscopes. Similarly, we propose that machine learning interpretability\nshould be a discipline that answers specific questions related to\ninterpretability. These questions can be of statistical, causal and\ncounterfactual nature. Therefore, there is a need to look into the\ninterpretability problem of machine learning in the context of questions that\nneed to be addressed rather than different tools. We discuss about a\nhypothetical interpretability framework driven by a question based scientific\napproach rather than some specific machine learning model. Using a question\nbased notion of interpretability, we can step towards understanding the science\nof machine learning rather than its engineering. This notion will also help us\nunderstanding any specific problem more in depth rather than relying solely on\nmachine learning methods.", "journal": ""}
{"doi": "10.48550/arXiv.2012.07369", "date": "2020-12-14", "title": "Learning for MPC with Stability & Safety Guarantees", "authors": "S\u00e9bastien Gros, Mario Zanon", "abstract": "The combination of learning methods with Model Predictive Control (MPC) has\nattracted a significant amount of attention in the recent literature. The hope\nof this combination is to reduce the reliance of MPC schemes on accurate\nmodels, and to tap into the fast developing machine learning and reinforcement\nlearning tools to exploit the growing amount of data available for many\nsystems. In particular, the combination of reinforcement learning and MPC has\nbeen proposed as a viable and theoretically justified approach to introduce\nexplainable, safe and stable policies in reinforcement learning. However, a\nformal theory detailing how the safety and stability of an MPC-based policy can\nbe maintained through the parameter updates delivered by the learning tools is\nstill lacking. This paper addresses this gap. The theory is developed for the\ngeneric Robust MPC case, and applied in simulation in the robust tube-based\nlinear MPC case, where the theory is fairly easy to deploy in practice. The\npaper focuses on Reinforcement Learning as a learning tool, but it applies to\nany learning method that updates the MPC parameters online.", "journal": ""}
{"doi": "10.48550/arXiv.1804.03022", "date": "2018-04-09", "title": "Learning at the Ends: From Hand to Tool Affordances in Humanoid Robots", "authors": "Giovanni Saponaro, Pedro Vicente, Atabak Dehban, Lorenzo Jamone, Alexandre Bernardino, Jos\u00e9 Santos-Victor", "abstract": "One of the open challenges in designing robots that operate successfully in\nthe unpredictable human environment is how to make them able to predict what\nactions they can perform on objects, and what their effects will be, i.e., the\nability to perceive object affordances. Since modeling all the possible world\ninteractions is unfeasible, learning from experience is required, posing the\nchallenge of collecting a large amount of experiences (i.e., training data).\nTypically, a manipulative robot operates on external objects by using its own\nhands (or similar end-effectors), but in some cases the use of tools may be\ndesirable, nevertheless, it is reasonable to assume that while a robot can\ncollect many sensorimotor experiences using its own hands, this cannot happen\nfor all possible human-made tools.\n  Therefore, in this paper we investigate the developmental transition from\nhand to tool affordances: what sensorimotor skills that a robot has acquired\nwith its bare hands can be employed for tool use? By employing a visual and\nmotor imagination mechanism to represent different hand postures compactly, we\npropose a probabilistic model to learn hand affordances, and we show how this\nmodel can generalize to estimate the affordances of previously unseen tools,\nultimately supporting planning, decision-making and tool selection tasks in\nhumanoid robots. We present experimental results with the iCub humanoid robot,\nand we publicly release the collected sensorimotor data in the form of a hand\nposture affordances dataset.", "journal": ""}
{"doi": "10.48550/arXiv.2209.12652", "date": "2022-09-13", "title": "AI-powered Language Assessment Tools for Dementia", "authors": "Mahboobeh Parsapoor, Muhammad Raisul Alam, Alex Mihailidis", "abstract": "The main objective of this paper is to propose an approach for developing an\nArtificial Intelligence (AI)-powered Language Assessment (LA) tool. Such tools\ncan be used to assess language impairments associated with dementia in older\nadults. The Machine Learning (ML) classifiers are the main parts of our\nproposed approach, therefore to develop an accurate tool with high sensitivity\nand specificity, we consider different binary classifiers and evaluate their\nperformances. We also assess the reliability and validity of our approach by\ncomparing the impact of different types of language tasks, features, and\nrecording media on the performance of ML classifiers.", "journal": ""}
{"doi": "10.48550/arXiv.2504.04612", "date": "2025-04-06", "title": "Tool-as-Interface: Learning Robot Policies from Human Tool Usage through Imitation Learning", "authors": "Haonan Chen, Cheng Zhu, Yunzhu Li, Katherine Driggs-Campbell", "abstract": "Tool use is critical for enabling robots to perform complex real-world tasks,\nand leveraging human tool-use data can be instrumental for teaching robots.\nHowever, existing data collection methods like teleoperation are slow, prone to\ncontrol delays, and unsuitable for dynamic tasks. In contrast, human natural\ndata, where humans directly perform tasks with tools, offers natural,\nunstructured interactions that are both efficient and easy to collect. Building\non the insight that humans and robots can share the same tools, we propose a\nframework to transfer tool-use knowledge from human data to robots. Using two\nRGB cameras, our method generates 3D reconstruction, applies Gaussian splatting\nfor novel view augmentation, employs segmentation models to extract\nembodiment-agnostic observations, and leverages task-space tool-action\nrepresentations to train visuomotor policies. We validate our approach on\ndiverse real-world tasks, including meatball scooping, pan flipping, wine\nbottle balancing, and other complex tasks. Our method achieves a 71\\% higher\naverage success rate compared to diffusion policies trained with teleoperation\ndata and reduces data collection time by 77\\%, with some tasks solvable only by\nour framework. Compared to hand-held gripper, our method cuts data collection\ntime by 41\\%. Additionally, our method bridges the embodiment gap, improves\nrobustness to variations in camera viewpoints and robot configurations, and\ngeneralizes effectively across objects and spatial setups.", "journal": ""}
{"doi": "10.48550/arXiv.2405.20333", "date": "2024-05-30", "title": "SurgiTrack: Fine-Grained Multi-Class Multi-Tool Tracking in Surgical Videos", "authors": "Chinedu Innocent Nwoye, Nicolas Padoy", "abstract": "Accurate tool tracking is essential for the success of computer-assisted\nintervention. Previous efforts often modeled tool trajectories rigidly,\noverlooking the dynamic nature of surgical procedures, especially tracking\nscenarios like out-of-body and out-of-camera views. Addressing this limitation,\nthe new CholecTrack20 dataset provides detailed labels that account for\nmultiple tool trajectories in three perspectives: (1) intraoperative, (2)\nintracorporeal, and (3) visibility, representing the different types of\ntemporal duration of tool tracks. These fine-grained labels enhance tracking\nflexibility but also increase the task complexity. Re-identifying tools after\nocclusion or re-insertion into the body remains challenging due to high visual\nsimilarity, especially among tools of the same category. This work recognizes\nthe critical role of the tool operators in distinguishing tool track instances,\nespecially those belonging to the same tool category. The operators'\ninformation are however not explicitly captured in surgical videos. We\ntherefore propose SurgiTrack, a novel deep learning method that leverages\nYOLOv7 for precise tool detection and employs an attention mechanism to model\nthe originating direction of the tools, as a proxy to their operators, for tool\nre-identification. To handle diverse tool trajectory perspectives, SurgiTrack\nemploys a harmonizing bipartite matching graph, minimizing conflicts and\nensuring accurate tool identity association. Experimental results on\nCholecTrack20 demonstrate SurgiTrack's effectiveness, outperforming baselines\nand state-of-the-art methods with real-time inference capability. This work\nsets a new standard in surgical tool tracking, providing dynamic trajectories\nfor more adaptable and precise assistance in minimally invasive surgeries.", "journal": "Medical Image Analysis, Volume 101, Article 103438 (April 2025)"}
{"doi": "10.48550/arXiv.1608.07249", "date": "2016-08-25", "title": "Benchmarking State-of-the-Art Deep Learning Software Tools", "authors": "Shaohuai Shi, Qiang Wang, Pengfei Xu, Xiaowen Chu", "abstract": "Deep learning has been shown as a successful machine learning method for a\nvariety of tasks, and its popularity results in numerous open-source deep\nlearning software tools. Training a deep network is usually a very\ntime-consuming process. To address the computational challenge in deep\nlearning, many tools exploit hardware features such as multi-core CPUs and\nmany-core GPUs to shorten the training time. However, different tools exhibit\ndifferent features and running performance when training different types of\ndeep networks on different hardware platforms, which makes it difficult for end\nusers to select an appropriate pair of software and hardware. In this paper, we\naim to make a comparative study of the state-of-the-art GPU-accelerated deep\nlearning software tools, including Caffe, CNTK, MXNet, TensorFlow, and Torch.\nWe first benchmark the running performance of these tools with three popular\ntypes of neural networks on two CPU platforms and three GPU platforms. We then\nbenchmark some distributed versions on multiple GPUs. Our contribution is\ntwo-fold. First, for end users of deep learning tools, our benchmarking results\ncan serve as a guide to selecting appropriate hardware platforms and software\ntools. Second, for software developers of deep learning tools, our in-depth\nanalysis points out possible future directions to further optimize the running\nperformance.", "journal": ""}
{"doi": "10.48550/arXiv.1204.4145", "date": "2012-04-18", "title": "Learning From An Optimization Viewpoint", "authors": "Karthik Sridharan", "abstract": "In this dissertation we study statistical and online learning problems from\nan optimization viewpoint.The dissertation is divided into two parts :\n  I. We first consider the question of learnability for statistical learning\nproblems in the general learning setting. The question of learnability is well\nstudied and fully characterized for binary classification and for real valued\nsupervised learning problems using the theory of uniform convergence. However\nwe show that for the general learning setting uniform convergence theory fails\nto characterize learnability. To fill this void we use stability of learning\nalgorithms to fully characterize statistical learnability in the general\nsetting. Next we consider the problem of online learning. Unlike the\nstatistical learning framework there is a dearth of generic tools that can be\nused to establish learnability and rates for online learning problems in\ngeneral. We provide online analogs to classical tools from statistical learning\ntheory like Rademacher complexity, covering numbers, etc. We further use these\ntools to fully characterize learnability for online supervised learning\nproblems.\n  II. In the second part, for general classes of convex learning problems, we\nprovide appropriate mirror descent (MD) updates for online and statistical\nlearning of these problems. Further, we show that the the MD is near optimal\nfor online convex learning and for most cases, is also near optimal for\nstatistical convex learning. We next consider the problem of convex\noptimization and show that oracle complexity can be lower bounded by the so\ncalled fat-shattering dimension of the associated linear class. Thus we\nestablish a strong connection between offline convex optimization problems and\nstatistical learning problems. We also show that for a large class of high\ndimensional optimization problems, MD is in fact near optimal even for convex\noptimization.", "journal": ""}
{"doi": "10.48550/arXiv.2305.16650", "date": "2023-05-26", "title": "Data-Driven Optimization for Deposition with Degradable Tools", "authors": "Tony Zheng, Monimoy Bujarbaruah, Francesco Borrelli", "abstract": "We present a data-driven optimization approach for robotic controlled\ndeposition with a degradable tool. Existing methods make the assumption that\nthe tool tip is not changing or is replaced frequently. Errors can accumulate\nover time as the tool wears away and this leads to poor performance in the case\nwhere the tool degradation is unaccounted for during deposition. In the\nproposed approach, we utilize visual and force feedback to update the unknown\nmodel parameters of our tool-tip. Subsequently, we solve a constrained finite\ntime optimal control problem for tracking a reference deposition profile, where\nour robot plans with the learned tool degradation dynamics. We focus on a\nrobotic drawing problem as an illustrative example. Using real-world\nexperiments, we show that the error in target vs actual deposition decreases\nwhen learned degradation models are used in the control design.", "journal": ""}
{"doi": "10.48550/arXiv.2308.08765", "date": "2023-08-17", "title": "Explainable AI for tool wear prediction in turning", "authors": "Saleh Valizadeh Sotubadi, Rui Liu, Vinh Neguyen", "abstract": "This research aims develop an Explainable Artificial Intelligence (XAI)\nframework to facilitate human-understandable solutions for tool wear prediction\nduring turning. A random forest algorithm was used as the supervised Machine\nLearning (ML) classifier for training and binary classification using\nacceleration, acoustics, temperature, and spindle speed during the orthogonal\ntube turning process as input features. The ML classifier was used to predict\nthe condition of the tool after the cutting process, which was determined in a\nbinary class form indicating if the cutting tool was available or failed. After\nthe training process, the Shapley criterion was used to explain the predictions\nof the trained ML classifier. Specifically, the significance of each input\nfeature in the decision-making and classification was identified to explain the\nreasoning of the ML classifier predictions. After implementing the Shapley\ncriterion on all testing datasets, the tool temperature was identified as the\nmost significant feature in determining the classification of available versus\nfailed cutting tools. Hence, this research demonstrates capability of XAI to\nprovide machining operators the ability to diagnose and understand complex ML\nclassifiers in prediction of tool wear.", "journal": ""}
{"doi": "10.48550/arXiv.2312.10332", "date": "2023-12-16", "title": "ProTIP: Progressive Tool Retrieval Improves Planning", "authors": "Raviteja Anantha, Bortik Bandyopadhyay, Anirudh Kashi, Sayantan Mahinder, Andrew W Hill, Srinivas Chappidi", "abstract": "Large language models (LLMs) are increasingly employed for complex multi-step\nplanning tasks, where the tool retrieval (TR) step is crucial for achieving\nsuccessful outcomes. Two prevalent approaches for TR are single-step retrieval,\nwhich utilizes the complete query, and sequential retrieval using task\ndecomposition (TD), where a full query is segmented into discrete atomic\nsubtasks. While single-step retrieval lacks the flexibility to handle\n\"inter-tool dependency,\" the TD approach necessitates maintaining \"subtask-tool\natomicity alignment,\" as the toolbox can evolve dynamically. To address these\nlimitations, we introduce the Progressive Tool retrieval to Improve Planning\n(ProTIP) framework. ProTIP is a lightweight, contrastive learning-based\nframework that implicitly performs TD without the explicit requirement of\nsubtask labels, while simultaneously maintaining subtask-tool atomicity. On the\nToolBench dataset, ProTIP outperforms the ChatGPT task decomposition-based\napproach by a remarkable margin, achieving a 24% improvement in Recall@K=10 for\nTR and a 41% enhancement in tool accuracy for plan generation.", "journal": ""}
{"doi": "10.48550/arXiv.2402.14158", "date": "2024-02-21", "title": "TOOLVERIFIER: Generalization to New Tools via Self-Verification", "authors": "Dheeraj Mekala, Jason Weston, Jack Lanchantin, Roberta Raileanu, Maria Lomeli, Jingbo Shang, Jane Dwivedi-Yu", "abstract": "Teaching language models to use tools is an important milestone towards\nbuilding general assistants, but remains an open problem. While there has been\nsignificant progress on learning to use specific tools via fine-tuning,\nlanguage models still struggle with learning how to robustly use new tools from\nonly a few demonstrations. In this work we introduce a self-verification method\nwhich distinguishes between close candidates by self-asking contrastive\nquestions during (1) tool selection; and (2) parameter generation. We construct\nsynthetic, high-quality, self-generated data for this goal using Llama-2 70B,\nwhich we intend to release publicly. Extensive experiments on 4 tasks from the\nToolBench benchmark, consisting of 17 unseen tools, demonstrate an average\nimprovement of 22% over few-shot baselines, even in scenarios where the\ndistinctions between candidate tools are finely nuanced.", "journal": ""}
{"doi": "10.48550/arXiv.2405.08313", "date": "2024-05-14", "title": "Establishing Heuristics for Improving the Usability of GUI Machine Learning Tools for Novice Users", "authors": "Asma Yamani, Haifa Alshammare, Malak Baslyman", "abstract": "Machine learning (ML) tools with graphical user interfaces (GUI) are facing\ndemand from novice users who do not have the background of their underlying\nconcepts. These tools are frequently complex and pose unique challenges in\nterms of interaction and comprehension by novice users. There is yet to be an\nestablished set of usability heuristics to guide and assess GUI ML tool design.\nTo address this gap, in this paper, we extend Nielsen's heuristics for\nevaluating GUI ML Tools through a set of empirical evaluations. To validate the\nproposed heuristics, user testing was conducted by novice users on a prototype\nthat reflects those heuristics. Based on the results of the evaluations, our\nnew heuristics set improves upon existing heuristics in the context of ML\ntools. It can serve as a resource for practitioners designing and evaluating\nthese tools.", "journal": ""}
{"doi": "10.48550/arXiv.2506.03362", "date": "2025-06-03", "title": "Robustness-Aware Tool Selection and Manipulation Planning with Learned Energy-Informed Guidance", "authors": "Yifei Dong, Yan Zhang, Sylvain Calinon, Florian T. Pokorny", "abstract": "Humans subconsciously choose robust ways of selecting and using tools, based\non years of embodied experience -- for example, choosing a ladle instead of a\nflat spatula to serve meatballs. However, robustness under uncertainty remains\nunderexplored in robotic tool-use planning. This paper presents a\nrobustness-aware framework that jointly selects tools and plans contact-rich\nmanipulation trajectories, explicitly optimizing for robustness against\nenvironmental disturbances. At the core of our approach is a learned,\nenergy-based robustness metric, which guides the planner towards robust\nmanipulation behaviors. We formulate a hierarchical optimization pipeline that\nfirst identifies a tool and configuration that optimizes robustness, and then\nplans a corresponding manipulation trajectory that maintains robustness\nthroughout execution. We evaluate our approach across three representative\ntool-use tasks. Simulation and real-world results demonstrate that our approach\nconsistently selects robust tools and generates disturbance-resilient\nmanipulation plans.", "journal": ""}
{"doi": "10.48550/arXiv.1812.00048", "date": "2018-11-29", "title": "Mobile Learning Game Authoring Tools: Assessment, Synthesis and Proposals", "authors": "Aous Karoui, Iza Marfisi-Schottman, S\u00e9bastien George", "abstract": "Mobile Learning Games (MLGs) show great potential for increasing engagement,\ncreativity and authentic learning. Yet, despite their great potential for\neducation, the use of MLGs by teachers, remains limited. This is partly due to\nthe fact that MLGs are often designed to match a specific learning context, and\nthus cannot be reusable for other contexts. Therefore, researchers have\nrecently designed various types of MLG authoring tools. However, these\nauthoring tools are not always adapted to non-computer-scientists or\nnon-game-designers. Hence, we propose in this paper to focus on five existing\nMLG authoring tools, in order to assess their features and usability with the\nhelp of five teachers, who are used to organizing educational field trips. In\nthe second part of this paper, we present an approach for designing a MLG\nauthoring tool, based on the lacks identified through the analysis, and\ntailored to the teachers' different profiles and needs.", "journal": ""}
{"doi": "10.48550/arXiv.1812.06280", "date": "2018-12-15", "title": "Wikipedia2Vec: An Efficient Toolkit for Learning and Visualizing the Embeddings of Words and Entities from Wikipedia", "authors": "Ikuya Yamada, Akari Asai, Jin Sakuma, Hiroyuki Shindo, Hideaki Takeda, Yoshiyasu Takefuji, Yuji Matsumoto", "abstract": "The embeddings of entities in a large knowledge base (e.g., Wikipedia) are\nhighly beneficial for solving various natural language tasks that involve real\nworld knowledge. In this paper, we present Wikipedia2Vec, a Python-based\nopen-source tool for learning the embeddings of words and entities from\nWikipedia. The proposed tool enables users to learn the embeddings efficiently\nby issuing a single command with a Wikipedia dump file as an argument. We also\nintroduce a web-based demonstration of our tool that allows users to visualize\nand explore the learned embeddings. In our experiments, our tool achieved a\nstate-of-the-art result on the KORE entity relatedness dataset, and competitive\nresults on various standard benchmark datasets. Furthermore, our tool has been\nused as a key component in various recent studies. We publicize the source\ncode, demonstration, and the pretrained embeddings for 12 languages at\nhttps://wikipedia2vec.github.io.", "journal": ""}
{"doi": "10.48550/arXiv.2306.08323", "date": "2023-06-14", "title": "How to estimate carbon footprint when training deep learning models? A guide and review", "authors": "Lucia Bouza Heguerte, Aur\u00e9lie Bugeau, Lo\u00efc Lannelongue", "abstract": "Machine learning and deep learning models have become essential in the recent\nfast development of artificial intelligence in many sectors of the society. It\nis now widely acknowledge that the development of these models has an\nenvironmental cost that has been analyzed in many studies. Several online and\nsoftware tools have been developed to track energy consumption while training\nmachine learning models. In this paper, we propose a comprehensive introduction\nand comparison of these tools for AI practitioners wishing to start estimating\nthe environmental impact of their work. We review the specific vocabulary, the\ntechnical requirements for each tool. We compare the energy consumption\nestimated by each tool on two deep neural networks for image processing and on\ndifferent types of servers. From these experiments, we provide some advice for\nbetter choosing the right tool and infrastructure.", "journal": ""}
{"doi": "10.48550/arXiv.2312.12460", "date": "2023-12-16", "title": "Democratize with Care: The need for fairness specific features in user-interface based open source AutoML tools", "authors": "Sundaraparipurnan Narayanan", "abstract": "AI is increasingly playing a pivotal role in businesses and organizations,\nimpacting the outcomes and interests of human users. Automated Machine Learning\n(AutoML) streamlines the machine learning model development process by\nautomating repetitive tasks and making data-driven decisions, enabling even\nnon-experts to construct high-quality models efficiently. This democratization\nallows more users (including non-experts) to access and utilize\nstate-of-the-art machine-learning expertise. However, AutoML tools may also\npropagate bias in the way these tools handle the data, model choices, and\noptimization approaches adopted. We conducted an experimental study of\nUser-interface-based open source AutoML tools (DataRobot, H2O Studio, Dataiku,\nand Rapidminer Studio) to examine if they had features to assist users in\ndeveloping fairness-aware machine learning models. The experiments covered the\nfollowing considerations for the evaluation of features: understanding use case\ncontext, data representation, feature relevance and sensitivity, data bias and\npreprocessing techniques, data handling capabilities, training-testing split,\nhyperparameter handling, and constraints, fairness-oriented model development,\nexplainability and ability to download and edit models by the user. The results\nrevealed inadequacies in features that could support in fairness-aware model\ndevelopment. Further, the results also highlight the need to establish certain\nessential features for promoting fairness in AutoML tools.", "journal": ""}
{"doi": "10.48550/arXiv.1805.00367", "date": "2018-04-30", "title": "A Multi-State Diagnosis and Prognosis Framework with Feature Learning for Tool Condition Monitoring", "authors": "Chong Zhang, Geok Soon Hong, Jun-Hong Zhou, Kay Chen Tan, Haizhou Li, Huan Xu, Jihoon Hong, Hian-Leng Chan", "abstract": "In this paper, a multi-state diagnosis and prognosis (MDP) framework is\nproposed for tool condition monitoring via a deep belief network based\nmulti-state approach (DBNMS). For fault diagnosis, a cost-sensitive deep belief\nnetwork (namely ECS-DBN) is applied to deal with the imbalanced data problem\nfor tool state estimation. An appropriate prognostic degradation model is then\napplied for tool wear estimation based on the different tool states. The\nproposed framework has the advantage of automatic feature representation\nlearning and shows better performance in accuracy and robustness. The\neffectiveness of the proposed DBNMS is validated using a real-world dataset\nobtained from the gun drilling process. This dataset contains a large amount of\nmeasured signals involving different tool geometries under various operating\nconditions. The DBNMS is examined for both the tool state estimation and tool\nwear estimation tasks. In the experimental studies, the prediction results are\nevaluated and compared with popular machine learning approaches, which show the\nsuperior performance of the proposed DBNMS approach.", "journal": ""}
{"doi": "10.48550/arXiv.2501.12432", "date": "2025-01-21", "title": "Divide-Then-Aggregate: An Efficient Tool Learning Method via Parallel Tool Invocation", "authors": "Dongsheng Zhu, Weixian Shi, Zhengliang Shi, Zhaochun Ren, Shuaiqiang Wang, Lingyong Yan, Dawei Yin", "abstract": "Although current Large Language Models (LLMs) exhibit impressive\ncapabilities, performing complex real-world tasks still requires tool learning.\nMainstream methods, such as CoT/ReAct, rely on step-by-step tool invocation to\ninteract with external environments, but they are limited in perceptual scope\nand lack adequate task-planning capability. To address these limitations, other\nstudies introduce the first Search-based Decision Tree (DFSDT), which still\nsuffers from the high computational cost. In this paper, we introduce a novel\nparallel tool invocation paradigm, DTA-Llama (Divide-Then-Aggregate Llama).\nFirst, we transform traditional tree-based tool search paths into Directed\nAcyclic Graph (DAG) structure, generating a high-quality parallel tool\ninvocation dataset. The DTA-Llama is then trained on the dataset to learn to\niteratively divide the current task into several parallel tool invocation\nsub-tasks and aggregate the invocation results to decide the next actions.\nFurthermore, we introduce an efficient inference framework inspired by the\nProcess/Threads mechanism when applying the DTA-Llama to practical tasks.\nExperimental results show that our approach substantially enhances task\nperformance while reducing token consumption and inference time. Llama2-7B,\nusing our method, is comparable to the official parallel function calling\nmethod of GPT-3.5. The relevant code, dataset, and model weights are available\nat https://corn0205.github.io/", "journal": ""}
{"doi": "10.48550/arXiv.2504.13958", "date": "2025-04-16", "title": "ToolRL: Reward is All Tool Learning Needs", "authors": "Cheng Qian, Emre Can Acikgoz, Qi He, Hongru Wang, Xiusi Chen, Dilek Hakkani-T\u00fcr, Gokhan Tur, Heng Ji", "abstract": "Current Large Language Models (LLMs) often undergo supervised fine-tuning\n(SFT) to acquire tool use capabilities. However, SFT struggles to generalize to\nunfamiliar or complex tool use scenarios. Recent advancements in reinforcement\nlearning (RL), particularly with R1-like models, have demonstrated promising\nreasoning and generalization abilities. Yet, reward design for tool use\npresents unique challenges: multiple tools may be invoked with diverse\nparameters, and coarse-grained reward signals, such as answer matching, fail to\noffer the finegrained feedback required for effective learning. In this work,\nwe present the first comprehensive study on reward design for tool selection\nand application tasks within the RL paradigm. We systematically explore a wide\nrange of reward strategies, analyzing their types, scales, granularity, and\ntemporal dynamics. Building on these insights, we propose a principled reward\ndesign tailored for tool use tasks and apply it to train LLMs using Group\nRelative Policy Optimization (GRPO). Empirical evaluations across diverse\nbenchmarks demonstrate that our approach yields robust, scalable, and stable\ntraining, achieving a 17% improvement over base models and a 15% gain over SFT\nmodels. These results highlight the critical role of thoughtful reward design\nin enhancing the tool use capabilities and generalization performance of LLMs.\nAll the codes are released to facilitate future research.", "journal": ""}
{"doi": "10.48550/arXiv.1412.3131", "date": "2014-12-08", "title": "A tool for implementation of a domain model based on fuzzy relationships", "authors": "Ali Aajli, Karim Afdel", "abstract": "The domain model is one of the important components used by adaptive learning\nsystems to automatically generate customized courses for the learners. In this\npaper our contribution is to propose a new tool for implementation of a domain\nmodel based on fuzzy relationships among concepts. This tool allows the experts\nand teachers to find the best parameters in order to adapt the learners's\ndifferences.", "journal": ""}
{"doi": "10.48550/arXiv.2405.16533", "date": "2024-05-26", "title": "Tool Learning in the Wild: Empowering Language Models as Automatic Tool Agents", "authors": "Zhengliang Shi, Shen Gao, Lingyong Yan, Yue Feng, Xiuyi Chen, Zhumin Chen, Dawei Yin, Suzan Verberne, Zhaochun Ren", "abstract": "Augmenting large language models (LLMs) with external tools has emerged as a\npromising approach to extend their utility, enabling them to solve practical\ntasks. Previous methods manually parse tool documentation and create in-context\ndemonstrations, transforming tools into structured formats for LLMs to use in\ntheir step-by-step reasoning. However, this manual process requires domain\nexpertise and struggles to scale to large toolsets. Additionally, these methods\nrely heavily on ad-hoc inference techniques or special tokens to integrate\nfree-form LLM generation with tool-calling actions, limiting the LLM's\nflexibility in handling diverse tool specifications and integrating multiple\ntools.\n  In this work, we propose AutoTools, a framework that enables LLMs to automate\nthe tool-use workflow. Specifically, the LLM automatically transforms tool\ndocumentation into callable functions, verifying syntax and runtime\ncorrectness. Then, the LLM integrates these functions into executable programs\nto solve practical tasks, flexibly grounding tool-use actions into its\nreasoning processes. Extensive experiments on existing and newly collected,\nmore challenging benchmarks illustrate the superiority of our framework.\nInspired by these promising results, we further investigate how to improve the\nexpertise of LLMs, especially open-source LLMs with fewer parameters, within\nAutoTools. Thus, we propose the AutoTools-learning approach, training the LLMs\nwith three learning tasks on 34k instances of high-quality synthetic data,\nincluding documentation understanding, relevance learning, and function\nprogramming. Fine-grained results validate the effectiveness of our overall\ntraining approach and each individual task. Our methods are an important step\ntowards the use of LLMs for solving real-world tasks with external tools.", "journal": ""}
{"doi": "10.48550/arXiv.1911.04521", "date": "2019-11-11", "title": "Tool Substitution with Shape and Material Reasoning Using Dual Neural Networks", "authors": "Nithin Shrivatsav, Lakshmi Nair, Sonia Chernova", "abstract": "This paper explores the problem of tool substitution, namely, identifying\nsubstitute tools for performing a task from a given set of candidate tools. We\nintroduce a novel approach to tool substitution, that unlike prior work in the\narea, combines both shape and material reasoning to effectively identify\nsubstitute tools. Our approach combines the use of visual and spectral\nreasoning using dual neural networks. It takes as input, the desired action to\nbe performed, and outputs a ranking of the available candidate tools based on\ntheir suitability for performing the action. Our results on a test set of 30\nreal-world objects show that our approach is able to effectively match shape\nand material similarities, with improved tool substitution performance when\ncombining both.", "journal": ""}
{"doi": "10.48550/arXiv.2212.01714", "date": "2022-12-04", "title": "A survey on grading format of automated grading tools for programming assignments", "authors": "Aditi Agrawal, Benjamin Reed", "abstract": "The prevalence of online platforms and studies has generated the demand for\nautomated grading tools, and as a result, there are plenty in the market. Such\ntools are developed to grade coding assignments quickly, accurately, and\neffortlessly. Since there are varieties of tools available to cater to the\ndiverse options of programming languages and concepts, it is overwhelming for\nany instructor to decide which one suits one's requirements. There are several\nsurveys studying the tools and giving insights into how they function and what\nthey support. However other than knowing the functionality, it is important for\nan instructor to know how the assignments are graded and what is the format of\nthe test cases. This is crucial since the instructor has to design the grading\nformat and therefore requires a learning curve. This survey studies and\nevaluates the automated grading tools based on their evaluation format. This in\nturn helps a reader in deciding which tool to choose and provides an insight\ninto what are the assessment settings and approaches used in grading the coding\nassignment in any specific grading tool.", "journal": "15th annual International Conference of Education, Research and\n  Innovation (2022) 7506-7514"}
{"doi": "10.48550/arXiv.2406.19228", "date": "2024-06-27", "title": "Tools Fail: Detecting Silent Errors in Faulty Tools", "authors": "Jimin Sun, So Yeon Min, Yingshan Chang, Yonatan Bisk", "abstract": "Tools have become a mainstay of LLMs, allowing them to retrieve knowledge not\nin their weights, to perform tasks on the web, and even to control robots.\nHowever, most ontologies and surveys of tool-use have assumed the core\nchallenge for LLMs is choosing the tool. Instead, we introduce a framework for\ntools more broadly which guides us to explore a model's ability to detect\n\"silent\" tool errors, and reflect on how to plan. This more directly aligns\nwith the increasingly popular use of models as tools. We provide an initial\napproach to failure recovery with promising results both on a controlled\ncalculator setting and embodied agent planning.", "journal": ""}
{"doi": "10.48550/arXiv.2502.05867", "date": "2025-02-09", "title": "Self-Training Large Language Models for Tool-Use Without Demonstrations", "authors": "Ne Luo, Aryo Pradipta Gema, Xuanli He, Emile van Krieken, Pietro Lesci, Pasquale Minervini", "abstract": "Large language models (LLMs) remain prone to factual inaccuracies and\ncomputational errors, including hallucinations and mistakes in mathematical\nreasoning. Recent work augmented LLMs with tools to mitigate these\nshortcomings, but often requires curated gold tool-use demonstrations. In this\npaper, we investigate whether LLMs can learn to use tools without\ndemonstrations. First, we analyse zero-shot prompting strategies to guide LLMs\nin tool utilisation. Second, we propose a self-training method to synthesise\ntool-use traces using the LLM itself. We compare supervised fine-tuning and\npreference fine-tuning techniques for fine-tuning the model on datasets\nconstructed using existing Question Answering (QA) datasets, i.e., TriviaQA and\nGSM8K. Experiments show that tool-use enhances performance on a long-tail\nknowledge task: 3.7% on PopQA, which is used solely for evaluation, but leads\nto mixed results on other datasets, i.e., TriviaQA, GSM8K, and NQ-Open. Our\nfindings highlight the potential and challenges of integrating external tools\ninto LLMs without demonstrations.", "journal": ""}
{"doi": "10.48550/arXiv.2502.11271", "date": "2025-02-16", "title": "OctoTools: An Agentic Framework with Extensible Tools for Complex Reasoning", "authors": "Pan Lu, Bowen Chen, Sheng Liu, Rahul Thapa, Joseph Boen, James Zou", "abstract": "Solving complex reasoning tasks may involve visual understanding, domain\nknowledge retrieval, numerical calculation, and multi-step reasoning. Existing\nmethods augment large language models (LLMs) with external tools but are\nrestricted to specialized domains, limited tool types, or require additional\ntraining data. In this paper, we introduce OctoTools, a training-free,\nuser-friendly, and easily extensible open-source agentic framework designed to\ntackle complex reasoning across diverse domains. OctoTools introduces\nstandardized tool cards to encapsulate tool functionality, a planner for both\nhigh-level and low-level planning, and an executor to carry out tool usage. We\nvalidate OctoTools' generality across 16 diverse tasks (including MathVista,\nMMLU-Pro, MedQA, and GAIA-Text), achieving substantial average accuracy gains\nof 9.3% over GPT-4o. Furthermore, OctoTools outperforms AutoGen, GPT-Functions\nand LangChain by up to 10.6% when given the same set of tools. Through\ncomprehensive analysis and ablations, OctoTools demonstrates advantages in task\nplanning, effective tool usage, and multi-step problem solving.", "journal": ""}
{"doi": "10.48550/arXiv.2505.20016", "date": "2025-05-26", "title": "TTPA: Token-level Tool-use Preference Alignment Training Framework with Fine-grained Evaluation", "authors": "Chengrui Huang, Shen Gao, Zhengliang Shi, Dongsheng Wang, Shuo Shang", "abstract": "Existing tool-learning methods usually rely on supervised fine-tuning, they\noften overlook fine-grained optimization of internal tool call details, leading\nto limitations in preference alignment and error discrimination. To overcome\nthese challenges, we propose Token-level Tool-use Preference Alignment Training\nFramework (TTPA), a training paradigm for constructing token-level tool-use\npreference datasets that align LLMs with fine-grained preferences using a novel\nerror-oriented scoring mechanism. TTPA first introduces reversed dataset\nconstruction, a method for creating high-quality, multi-turn tool-use datasets\nby reversing the generation flow. Additionally, we propose Token-level\nPreference Sampling (TPS) to capture fine-grained preferences by modeling\ntoken-level differences during generation. To address biases in scoring, we\nintroduce the Error-oriented Scoring Mechanism (ESM), which quantifies\ntool-call errors and can be used as a training signal. Extensive experiments on\nthree diverse benchmark datasets demonstrate that TTPA significantly improves\ntool-using performance while showing strong generalization ability across\nmodels and datasets.", "journal": ""}
{"doi": "10.48550/arXiv.2404.04992", "date": "2024-04-07", "title": "Efficient Surgical Tool Recognition via HMM-Stabilized Deep Learning", "authors": "Haifeng Wang, Hao Xu, Jun Wang, Jian Zhou, Ke Deng", "abstract": "Recognizing various surgical tools, actions and phases from surgery videos is\nan important problem in computer vision with exciting clinical applications.\nExisting deep-learning-based methods for this problem either process each\nsurgical video as a series of independent images without considering their\ndependence, or rely on complicated deep learning models to count for dependence\nof video frames. In this study, we revealed from exploratory data analysis that\nsurgical videos enjoy relatively simple semantic structure, where the presence\nof surgical phases and tools can be well modeled by a compact hidden Markov\nmodel (HMM). Based on this observation, we propose an HMM-stabilized deep\nlearning method for tool presence detection. A wide range of experiments\nconfirm that the proposed approaches achieve better performance with lower\ntraining and running costs, and support more flexible ways to construct and\nutilize training data in scenarios where not all surgery videos of interest are\nextensively labelled. These results suggest that popular deep learning\napproaches with over-complicated model structures may suffer from inefficient\nutilization of data, and integrating ingredients of deep learning and\nstatistical learning wisely may lead to more powerful algorithms that enjoy\ncompetitive performance, transparent interpretation and convenient model\ntraining simultaneously.", "journal": ""}
{"doi": "10.48550/arXiv.2105.02595", "date": "2021-05-06", "title": "Scaling up Memory-Efficient Formal Verification Tools for Tree Ensembles", "authors": "John T\u00f6rnblom, Simin Nadjm-Tehrani", "abstract": "To guarantee that machine learning models yield outputs that are not only\naccurate, but also robust, recent works propose formally verifying robustness\nproperties of machine learning models. To be applicable to realistic\nsafety-critical systems, the used verification algorithms need to manage the\ncombinatorial explosion resulting from vast variations in the input domain, and\nbe able to verify correctness properties derived from versatile and\ndomain-specific requirements.\n  In this paper, we formalise the VoTE algorithm presented earlier as a tool\ndescription, and extend the tool set with mechanisms for systematic scalability\nstudies. In particular, we show a) how the separation of property checking from\nthe core verification engine enables verification of versatile requirements, b)\nthe scalability of the tool, both in terms of time taken for verification and\nuse of memory, and c) that the algorithm has attractive properties that lend\nthemselves well for massive parallelisation.\n  We demonstrate the application of the tool in two case studies, namely digit\nrecognition and aircraft collision avoidance, where the first case study serves\nto assess the resource utilisation of the tool, and the second to assess the\nability to verify versatile correctness properties.", "journal": ""}
{"doi": "10.48550/arXiv.2012.04700", "date": "2020-12-08", "title": "Emergence of Different Modes of Tool Use in a Reaching and Dragging Task", "authors": "Khuong Nguyen, Yoonsuck Choe", "abstract": "Tool use is an important milestone in the evolution of intelligence. In this\npaper, we investigate different modes of tool use that emerge in a reaching and\ndragging task. In this task, a jointed arm with a gripper must grab a tool (T,\nI, or L-shaped) and drag an object down to the target location (the bottom of\nthe arena). The simulated environment had real physics such as gravity and\nfriction. We trained a deep-reinforcement learning based controller (with raw\nvisual and proprioceptive input) with minimal reward shaping information to\ntackle this task. We observed the emergence of a wide range of unexpected\nbehaviors, not directly encoded in the motor primitives or reward functions.\nExamples include hitting the object to the target location, correcting error of\ninitial contact, throwing the tool toward the object, as well as normal\nexpected behavior such as wide sweep. Also, we further analyzed these behaviors\nbased on the type of tool and the initial position of the target object. Our\nresults show a rich repertoire of behaviors, beyond the basic built-in\nmechanisms of the deep reinforcement learning method we used.", "journal": ""}
{"doi": "10.48550/arXiv.2505.07512", "date": "2025-05-12", "title": "ToolACE-DEV: Self-Improving Tool Learning via Decomposition and EVolution", "authors": "Xu Huang, Weiwen Liu, Xingshan Zeng, Yuefeng Huang, Xinlong Hao, Yuxian Wang, Yirong Zeng, Chuhan Wu, Yasheng Wang, Ruiming Tang, Defu Lian", "abstract": "The tool-using capability of large language models (LLMs) enables them to\naccess up-to-date external information and handle complex tasks. Current\napproaches to enhancing this capability primarily rely on distilling advanced\nmodels by data synthesis. However, this method incurs significant costs\nassociated with advanced model usage and often results in data compatibility\nissues, led by the high discrepancy in the knowledge scope between the advanced\nmodel and the target model. To address these challenges, we propose\nToolACE-DEV, a self-improving framework for tool learning. First, we decompose\nthe tool-learning objective into sub-tasks that enhance basic tool-making and\ntool-using abilities. Then, we introduce a self-evolving paradigm that allows\nlightweight models to self-improve, reducing reliance on advanced LLMs.\nExtensive experiments validate the effectiveness of our approach across models\nof varying scales and architectures.", "journal": ""}
{"doi": "10.48550/arXiv.2506.11072", "date": "2025-06-02", "title": "Can We Trust Machine Learning? The Reliability of Features from Open-Source Speech Analysis Tools for Speech Modeling", "authors": "Tahiya Chowdhury, Veronica Romero", "abstract": "Machine learning-based behavioral models rely on features extracted from\naudio-visual recordings. The recordings are processed using open-source tools\nto extract speech features for classification models. These tools often lack\nvalidation to ensure reliability in capturing behaviorally relevant\ninformation. This gap raises concerns about reproducibility and fairness across\ndiverse populations and contexts. Speech processing tools, when used outside of\ntheir design context, can fail to capture behavioral variations equitably and\ncan then contribute to bias. We evaluate speech features extracted from two\nwidely used speech analysis tools, OpenSMILE and Praat, to assess their\nreliability when considering adolescents with autism. We observed considerable\nvariation in features across tools, which influenced model performance across\ncontext and demographic groups. We encourage domain-relevant verification to\nenhance the reliability of machine learning models in clinical applications.", "journal": ""}
{"doi": "10.48550/arXiv.1912.05796", "date": "2019-12-12", "title": "Automatic Layout Generation with Applications in Machine Learning Engine Evaluation", "authors": "Haoyu Yang, Wen Chen, Piyush Pathak, Frank Gennari, Ya-Chieh Lai, Bei Yu", "abstract": "Machine learning-based lithography hotspot detection has been deeply studied\nrecently, from varies feature extraction techniques to efficient learning\nmodels. It has been observed that such machine learning-based frameworks are\nproviding satisfactory metal layer hotspot prediction results on known public\nmetal layer benchmarks. In this work, we seek to evaluate how these machine\nlearning-based hotspot detectors generalize to complicated patterns. We first\nintroduce a automatic layout generation tool that can synthesize varies layout\npatterns given a set of design rules. The tool currently supports both metal\nlayer and via layer generation. As a case study, we conduct hotspot detection\non the generated via layer layouts with representative machine learning-based\nhotspot detectors, which shows that continuous study on model robustness and\ngenerality is necessary to prototype and integrate the learning engines in DFM\nflows. The source code of the layout generation tool will be available at\nhttps://github. com/phdyang007/layout-generation.", "journal": ""}
{"doi": "10.48550/arXiv.2405.16159", "date": "2024-05-25", "title": "A Declarative Query Language for Scientific Machine Learning", "authors": "Hasan M Jamil", "abstract": "The popularity of data science as a discipline and its importance in the\nemerging economy and industrial progress dictate that machine learning be\ndemocratized for the masses. This also means that the current practice of\nworkforce training using machine learning tools, which requires low-level\nstatistical and algorithmic details, is a barrier that needs to be addressed.\nSimilar to data management languages such as SQL, machine learning needs to be\npracticed at a conceptual level to help make it a staple tool for general\nusers. In particular, the technical sophistication demanded by existing machine\nlearning frameworks is prohibitive for many scientists who are not\ncomputationally savvy or well versed in machine learning techniques. The\nlearning curve to use the needed machine learning tools is also too high for\nthem to take advantage of these powerful platforms to rapidly advance science.\nIn this paper, we introduce a new declarative machine learning query language,\ncalled {\\em MQL}, for naive users. We discuss its merit and possible ways of\nimplementing it over a traditional relational database system. We discuss two\nmaterials science experiments implemented using MQL on a materials science\nworkflow system called MatFlow.", "journal": ""}
{"doi": "10.48550/arXiv.2311.00754", "date": "2023-11-01", "title": "Learning to Design and Use Tools for Robotic Manipulation", "authors": "Ziang Liu, Stephen Tian, Michelle Guo, C. Karen Liu, Jiajun Wu", "abstract": "When limited by their own morphologies, humans and some species of animals\nhave the remarkable ability to use objects from the environment toward\naccomplishing otherwise impossible tasks. Robots might similarly unlock a range\nof additional capabilities through tool use. Recent techniques for jointly\noptimizing morphology and control via deep learning are effective at designing\nlocomotion agents. But while outputting a single morphology makes sense for\nlocomotion, manipulation involves a variety of strategies depending on the task\ngoals at hand. A manipulation agent must be capable of rapidly prototyping\nspecialized tools for different goals. Therefore, we propose learning a\ndesigner policy, rather than a single design. A designer policy is conditioned\non task information and outputs a tool design that helps solve the task. A\ndesign-conditioned controller policy can then perform manipulation using these\ntools. In this work, we take a step towards this goal by introducing a\nreinforcement learning framework for jointly learning these policies. Through\nsimulated manipulation tasks, we show that this framework is more sample\nefficient than prior methods in multi-goal or multi-variant settings, can\nperform zero-shot interpolation or fine-tuning to tackle previously unseen\ngoals, and allows tradeoffs between the complexity of design and control\npolicies under practical constraints. Finally, we deploy our learned policies\nonto a real robot. Please see our supplementary video and website at\nhttps://robotic-tool-design.github.io/ for visualizations.", "journal": ""}
{"doi": "10.48550/arXiv.2501.10363", "date": "2024-12-08", "title": "A Web-Based IDE for DevOps Learning in Software Engineering Higher Education", "authors": "Ganesh Neelakanta Iyer, Andrew Goh Yisheng, Metilda Chee Heng Er, Weng Xian Choong, Shao Wei Koh", "abstract": "DevOps can be best explained as people working together to conceive, build\nand deliver secure software at top speed. DevOps practices enable software\ndevelopment (dev) and operations (ops) teams to accelerate delivery through\nautomation, collaboration, fast feedback, and iterative improvement. It is now\nan integral part of the information technology industry, and students should be\naware of it before they start their careers. However, teaching DevOps in a\nuniversity curriculum has many challenges as it involves many tools and\ntechnologies. This paper presents an innovative online Integrated Development\nEnvironment (IDE) designed to facilitate DevOps learning within university\ncurricula. The devised tool offers a standardized, accessible learning\nenvironment, equipped with devcontainers and engaging tutorials to simplify\nlearning DevOps. Research findings highlight a marked preference among students\nfor self-paced learning approaches, with experienced DevOps practitioners also\nnoting the value of the tool. With barriers such as limited hardware/software\naccess becoming evident, the necessity for cloud-based learning solutions is\nfurther underscored. User feedback emphasizes the tool's user-friendliness and\nthe imperative of automated installation procedures. We recommend additional\nexploration into the tool's extensibility and potential for continuous\nimprovement, especially regarding the development of Dev Containers. The study\nconcludes by emphasizing the pivotal role of practical learning tools in the\ndynamic field of DevOps education and research.", "journal": ""}
{"doi": "10.48550/arXiv.2007.05577", "date": "2020-07-10", "title": "Vizarel: A System to Help Better Understand RL Agents", "authors": "Shuby Deshpande, Jeff Schneider", "abstract": "Visualization tools for supervised learning have allowed users to interpret,\nintrospect, and gain intuition for the successes and failures of their models.\nWhile reinforcement learning practitioners ask many of the same questions,\nexisting tools are not applicable to the RL setting. In this work, we describe\nour initial attempt at constructing a prototype of these ideas, through\nidentifying possible features that such a system should encapsulate. Our design\nis motivated by envisioning the system to be a platform on which to experiment\nwith interpretable reinforcement learning.", "journal": ""}
{"doi": "10.48550/arXiv.2408.12655", "date": "2024-08-22", "title": "Improving Radiography Machine Learning Workflows via Metadata Management for Training Data Selection", "authors": "Mirabel Reid, Christine Sweeney, Oleg Korobkin", "abstract": "Most machine learning models require many iterations of hyper-parameter\ntuning, feature engineering, and debugging to produce effective results. As\nmachine learning models become more complicated, this pipeline becomes more\ndifficult to manage effectively. In the physical sciences, there is an\never-increasing pool of metadata that is generated by the scientific research\ncycle. Tracking this metadata can reduce redundant work, improve\nreproducibility, and aid in the feature and training dataset engineering\nprocess. In this case study, we present a tool for machine learning metadata\nmanagement in dynamic radiography. We evaluate the efficacy of this tool\nagainst the initial research workflow and discuss extensions to general machine\nlearning pipelines in the physical sciences.", "journal": ""}
{"doi": "10.48550/arXiv.2409.09537", "date": "2024-09-14", "title": "Deep Fast Machine Learning Utils: A Python Library for Streamlined Machine Learning Prototyping", "authors": "Fabi Prezja", "abstract": "Machine learning (ML) research and application often involve time-consuming\nsteps such as model architecture prototyping, feature selection, and dataset\npreparation. To support these tasks, we introduce the Deep Fast Machine\nLearning Utils (DFMLU) library, which provides tools designed to automate and\nenhance aspects of these processes. Compatible with frameworks like TensorFlow,\nKeras, and Scikit-learn, DFMLU offers functionalities that support model\ndevelopment and data handling. The library includes methods for dense neural\nnetwork search, advanced feature selection, and utilities for data management\nand visualization of training outcomes. This manuscript presents an overview of\nDFMLU's functionalities, providing Python examples for each tool.", "journal": ""}
{"doi": "10.48550/arXiv.2503.09576", "date": "2025-03-12", "title": "Manify: A Python Library for Learning Non-Euclidean Representations", "authors": "Philippe Chlenski, Kaizhu Du, Dylan Satow, Itsik Pe'er", "abstract": "We present Manify, an open-source Python library for non-Euclidean\nrepresentation learning. Leveraging manifold learning techniques, Manify\nprovides tools for learning embeddings in (products of) non-Euclidean spaces,\nperforming classification and regression with data that lives in such spaces,\nand estimating the curvature of a manifold. Manify aims to advance research and\napplications in machine learning by offering a comprehensive suite of tools for\nmanifold-based data analysis. Our source code, examples, datasets, results, and\ndocumentation are available at https://github.com/pchlenski/manify", "journal": ""}
{"doi": "10.48550/arXiv.2307.05029", "date": "2023-07-11", "title": "FairLay-ML: Intuitive Remedies for Unfairness in Data-Driven Social-Critical Algorithms", "authors": "Normen Yu, Gang Tan, Saeid Tizpaz-Niari", "abstract": "This thesis explores open-sourced machine learning (ML) model explanation\ntools to understand whether these tools can allow a layman to visualize,\nunderstand, and suggest intuitive remedies to unfairness in ML-based\ndecision-support systems. Machine learning models trained on datasets biased\nagainst minority groups are increasingly used to guide life-altering social\ndecisions, prompting the urgent need to study their logic for unfairness. Due\nto this problem's impact on vast populations of the general public, it is\ncritical for the layperson -- not just subject matter experts in social justice\nor machine learning experts -- to understand the nature of unfairness within\nthese algorithms and the potential trade-offs. Existing research on fairness in\nmachine learning focuses mostly on the mathematical definitions and tools to\nunderstand and remedy unfair models, with some directly citing user-interactive\ntools as necessary for future work. This thesis presents FairLay-ML, a\nproof-of-concept GUI integrating some of the most promising tools to provide\nintuitive explanations for unfair logic in ML models by integrating existing\nresearch tools (e.g. Local Interpretable Model-Agnostic Explanations) with\nexisting ML-focused GUI (e.g. Python Streamlit). We test FairLay-ML using\nmodels of various accuracy and fairness generated by an unfairness detector\ntool, Parfait-ML, and validate our results using Themis. Our study finds that\nthe technology stack used for FairLay-ML makes it easy to install and provides\nreal-time black-box explanations of pre-trained models to users. Furthermore,\nthe explanations provided translate to actionable remedies.", "journal": ""}
{"doi": "10.48550/arXiv.2401.08326", "date": "2024-01-16", "title": "RoTBench: A Multi-Level Benchmark for Evaluating the Robustness of Large Language Models in Tool Learning", "authors": "Junjie Ye, Yilong Wu, Songyang Gao, Caishuang Huang, Sixian Li, Guanyu Li, Xiaoran Fan, Qi Zhang, Tao Gui, Xuanjing Huang", "abstract": "Tool learning has generated widespread interest as a vital means of\ninteraction between Large Language Models (LLMs) and the physical world.\nCurrent research predominantly emphasizes LLMs' capacity to utilize tools in\nwell-structured environments while overlooking their stability when confronted\nwith the inevitable noise of the real world. To bridge this gap, we introduce\nRoTBench, a multi-level benchmark for evaluating the robustness of LLMs in tool\nlearning. Specifically, we establish five external environments, each featuring\nvarying levels of noise (i.e., Clean, Slight, Medium, Heavy, and Union),\nproviding an in-depth analysis of the model's resilience across three critical\nphases: tool selection, parameter identification, and content filling.\nExperiments involving six widely-used models underscore the urgent necessity\nfor enhancing the robustness of LLMs in tool learning. For instance, the\nperformance of GPT-4 even drops significantly from 80.00 to 58.10 when there is\nno substantial change in manual accuracy. More surprisingly, the noise\ncorrection capability inherent in the GPT family paradoxically impedes its\nadaptability in the face of mild noise. In light of these findings, we propose\nRoTTuning, a strategy that enriches the diversity of training environments to\nbolster the robustness of LLMs in tool learning. The code and data are\navailable at https://github.com/Junjie-Ye/RoTBench.", "journal": ""}
{"doi": "10.48550/arXiv.2308.00675", "date": "2023-08-01", "title": "Tool Documentation Enables Zero-Shot Tool-Usage with Large Language Models", "authors": "Cheng-Yu Hsieh, Si-An Chen, Chun-Liang Li, Yasuhisa Fujii, Alexander Ratner, Chen-Yu Lee, Ranjay Krishna, Tomas Pfister", "abstract": "Today, large language models (LLMs) are taught to use new tools by providing\na few demonstrations of the tool's usage. Unfortunately, demonstrations are\nhard to acquire, and can result in undesirable biased usage if the wrong\ndemonstration is chosen. Even in the rare scenario that demonstrations are\nreadily available, there is no principled selection protocol to determine how\nmany and which ones to provide. As tasks grow more complex, the selection\nsearch grows combinatorially and invariably becomes intractable. Our work\nprovides an alternative to demonstrations: tool documentation. We advocate the\nuse of tool documentation, descriptions for the individual tool usage, over\ndemonstrations. We substantiate our claim through three main empirical findings\non 6 tasks across both vision and language modalities. First, on existing\nbenchmarks, zero-shot prompts with only tool documentation are sufficient for\neliciting proper tool usage, achieving performance on par with few-shot\nprompts. Second, on a newly collected realistic tool-use dataset with hundreds\nof available tool APIs, we show that tool documentation is significantly more\nvaluable than demonstrations, with zero-shot documentation significantly\noutperforming few-shot without documentation. Third, we highlight the benefits\nof tool documentations by tackling image generation and video tracking using\njust-released unseen state-of-the-art models as tools. Finally, we highlight\nthe possibility of using tool documentation to automatically enable new\napplications: by using nothing more than the documentation of GroundingDino,\nStable Diffusion, XMem, and SAM, LLMs can re-invent the functionalities of the\njust-released Grounded-SAM and Track Anything models.", "journal": ""}
{"doi": "10.48550/arXiv.1909.13868", "date": "2019-09-30", "title": "Deep learning tools for the measurement of animal behavior in neuroscience", "authors": "Mackenzie W. Mathis, Alexander Mathis", "abstract": "Recent advances in computer vision have made accurate, fast and robust\nmeasurement of animal behavior a reality. In the past years powerful tools\nspecifically designed to aid the measurement of behavior have come to fruition.\nHere we discuss how capturing the postures of animals - pose estimation - has\nbeen rapidly advancing with new deep learning methods. While challenges still\nremain, we envision that the fast-paced development of new deep learning tools\nwill rapidly change the landscape of realizable real-world neuroscience.", "journal": "Current Opinion in Neurobiology Volume 60, February 2020, Pages\n  1-11"}
{"doi": "10.48550/arXiv.1906.04837", "date": "2019-06-11", "title": "Toward Best Practices for Explainable B2B Machine Learning", "authors": "Kit Kuksenok", "abstract": "To design tools and data pipelines for explainable B2B machine learning (ML)\nsystems, we need to recognize not only the immediate audience of such tools and\ndata, but also (1) their organizational context and (2) secondary audiences.\nOur learnings are based on building custom ML-based chatbots for recruitment.\nWe believe that in the B2B context, \"explainable\" ML means not only a system\nthat can \"explain itself\" through tools and data pipelines, but also enables\nits domain-expert users to explain it to other stakeholders.", "journal": ""}
{"doi": "10.48550/arXiv.2004.09263", "date": "2020-04-14", "title": "Reinforcement Learning Approach to Vibration Compensation for Dynamic Feed Drive Systems", "authors": "Ralf Gulde, Marc Tuscher, Akos Csiszar, Oliver Riedel, Alexander Verl", "abstract": "Vibration compensation is important for many domains. For the machine tool\nindustry it translates to higher machining precision and longer component\nlifetime. Current methods for vibration damping have their shortcomings (e.g.\nneed for accurate dynamic models). In this paper we present a reinforcement\nlearning based approach to vibration compensation applied to a machine tool\naxis. The work describes the problem formulation, the solution, the\nimplementation and experiments using industrial machine tool hardware and\ncontrol system.", "journal": ""}
{"doi": "10.48550/arXiv.2106.15247", "date": "2021-06-29", "title": "Unsupervised Technique To Conversational Machine Reading", "authors": "Peter Ochieng, Dennis Mugambi", "abstract": "Conversational machine reading (CMR) tools have seen a rapid progress in the\nrecent past. The current existing tools rely on the supervised learning\ntechnique which require labeled dataset for their training. The supervised\ntechnique necessitates that for every new rule text, a manually labeled dataset\nmust be created. This is tedious and error prone. This paper introduces and\ndemonstrates how unsupervised learning technique can be applied in the\ndevelopment of CMR. Specifically, we demonstrate how unsupervised learning can\nbe used in rule extraction and entailment modules of CMR. Compared to the\ncurrent best CMR tool, our developed framework reports 3.3% improvement in\nmicro averaged accuracy and 1.4 % improvement in macro averaged accuracy.", "journal": ""}
{"doi": "10.48550/arXiv.2305.16849", "date": "2023-05-26", "title": "Green Runner: A tool for efficient model selection from model repositories", "authors": "Jai Kannan, Scott Barnett, Anj Simmons, Taylan Selvi, Luis Cruz", "abstract": "Deep learning models have become essential in software engineering, enabling\nintelligent features like image captioning and document generation. However,\ntheir popularity raises concerns about environmental impact and inefficient\nmodel selection. This paper introduces GreenRunnerGPT, a novel tool for\nefficiently selecting deep learning models based on specific use cases. It\nemploys a large language model to suggest weights for quality indicators,\noptimizing resource utilization. The tool utilizes a multi-armed bandit\nframework to evaluate models against target datasets, considering tradeoffs. We\ndemonstrate that GreenRunnerGPT is able to identify a model suited to a target\nuse case without wasteful computations that would occur under a brute-force\napproach to model selection.", "journal": ""}
{"doi": "10.48550/arXiv.2311.17277", "date": "2023-11-28", "title": "An Online Optimization-Based Decision Support Tool for Small Farmers in India: Learning in Non-stationary Environments", "authors": "Tuxun Lu, Aviva Prins", "abstract": "Crop management decision support systems are specialized tools for farmers\nthat reduce the riskiness of revenue streams, especially valuable for use under\nthe current climate changes that impact agricultural productivity.\nUnfortunately, small farmers in India, who could greatly benefit from these\ntools, do not have access to them. In this paper, we model an individual\ngreenhouse as a Markov Decision Process (MDP) and adapt Li and Li (2019)'s\nFollow the Weighted Leader (FWL) online learning algorithm to offer crop\nplanning advice. We successfully produce utility-preserving cropping pattern\nsuggestions in simulations. When we compare against an offline planning\nalgorithm, we achieve the same cumulative revenue with greatly reduced runtime.", "journal": ""}
{"doi": "10.48550/arXiv.2410.03470", "date": "2024-10-04", "title": "Vulnerability Detection via Topological Analysis of Attention Maps", "authors": "Pavel Snopov, Andrey Nikolaevich Golubinskiy", "abstract": "Recently, deep learning (DL) approaches to vulnerability detection have\ngained significant traction. These methods demonstrate promising results, often\nsurpassing traditional static code analysis tools in effectiveness.\n  In this study, we explore a novel approach to vulnerability detection\nutilizing the tools from topological data analysis (TDA) on the attention\nmatrices of the BERT model. Our findings reveal that traditional machine\nlearning (ML) techniques, when trained on the topological features extracted\nfrom these attention matrices, can perform competitively with pre-trained\nlanguage models (LLMs) such as CodeBERTa. This suggests that TDA tools,\nincluding persistent homology, are capable of effectively capturing semantic\ninformation critical for identifying vulnerabilities.", "journal": ""}
{"doi": "10.48550/arXiv.2201.07959", "date": "2022-01-20", "title": "APIRO: A Framework for Automated Security Tools API Recommendation", "authors": "Zarrin Tasnim Sworna, Chadni Islam, Muhammad Ali Babar", "abstract": "Security Orchestration, Automation, and Response (SOAR) platforms integrate\nand orchestrate a wide variety of security tools to accelerate the operational\nactivities of Security Operation Center (SOC). Integration of security tools in\na SOAR platform is mostly done manually using APIs, plugins, and scripts. SOC\nteams need to navigate through API calls of different security tools to find a\nsuitable API to define or update an incident response action. Analyzing various\ntypes of API documentation with diverse API format and presentation structure\ninvolves significant challenges such as data availability, data heterogeneity,\nand semantic variation for automatic identification of security tool APIs\nspecific to a particular task. Given these challenges can have negative impact\non SOC team's ability to handle security incident effectively and efficiently,\nwe consider it important to devise suitable automated support solutions to\naddress these challenges. We propose a novel learning-based framework for\nautomated security tool API Recommendation for security Orchestration,\nautomation, and response, APIRO. To mitigate data availability constraint,\nAPIRO enriches security tool API description by applying a wide variety of data\naugmentation techniques. To learn data heterogeneity of the security tools and\nsemantic variation in API descriptions, APIRO consists of an API-specific word\nembedding model and a Convolutional Neural Network (CNN) model that are used\nfor prediction of top 3 relevant APIs for a task. We experimentally demonstrate\nthe effectiveness of APIRO in recommending APIs for different tasks using 3\nsecurity tools and 36 augmentation techniques. Our experimental results\ndemonstrate the feasibility of APIRO for achieving 91.9% Top-1 Accuracy.", "journal": ""}
{"doi": "10.48550/arXiv.2011.07785", "date": "2020-11-16", "title": "Autonomously Navigating a Surgical Tool Inside the Eye by Learning from Demonstration", "authors": "Ji Woong Kim, Changyan He, Muller Urias, Peter Gehlbach, Gregory D. Hager, Iulian Iordachita, Marin Kobilarov", "abstract": "A fundamental challenge in retinal surgery is safely navigating a surgical\ntool to a desired goal position on the retinal surface while avoiding damage to\nsurrounding tissues, a procedure that typically requires tens-of-microns\naccuracy. In practice, the surgeon relies on depth-estimation skills to\nlocalize the tool-tip with respect to the retina in order to perform the\ntool-navigation task, which can be prone to human error. To alleviate such\nuncertainty, prior work has introduced ways to assist the surgeon by estimating\nthe tool-tip distance to the retina and providing haptic or auditory feedback.\nHowever, automating the tool-navigation task itself remains unsolved and\nlargely unexplored. Such a capability, if reliably automated, could serve as a\nbuilding block to streamline complex procedures and reduce the chance for\ntissue damage. Towards this end, we propose to automate the tool-navigation\ntask by learning to mimic expert demonstrations of the task. Specifically, a\ndeep network is trained to imitate expert trajectories toward various locations\non the retina based on recorded visual servoing to a given goal specified by\nthe user. The proposed autonomous navigation system is evaluated in simulation\nand in physical experiments using a silicone eye phantom. We show that the\nnetwork can reliably navigate a needle surgical tool to various desired\nlocations within 137 microns accuracy in physical experiments and 94 microns in\nsimulation on average, and generalizes well to unseen situations such as in the\npresence of auxiliary surgical tools, variable eye backgrounds, and brightness\nconditions.", "journal": ""}
{"doi": "10.48550/arXiv.2405.15114", "date": "2024-05-24", "title": "Let Me Do It For You: Towards LLM Empowered Recommendation via Tool Learning", "authors": "Yuyue Zhao, Jiancan Wu, Xiang Wang, Wei Tang, Dingxian Wang, Maarten de Rijke", "abstract": "Conventional recommender systems (RSs) face challenges in precisely capturing\nusers' fine-grained preferences. Large language models (LLMs) have shown\ncapabilities in commonsense reasoning and leveraging external tools that may\nhelp address these challenges. However, existing LLM-based RSs suffer from\nhallucinations, misalignment between the semantic space of items and the\nbehavior space of users, or overly simplistic control strategies (e.g., whether\nto rank or directly present existing results). To bridge these gap, we\nintroduce ToolRec, a framework for LLM-empowered recommendations via tool\nlearning that uses LLMs as surrogate users, thereby guiding the recommendation\nprocess and invoking external tools to generate a recommendation list that\naligns closely with users' nuanced preferences.\n  We formulate the recommendation process as a process aimed at exploring user\ninterests in attribute granularity. The process factors in the nuances of the\ncontext and user preferences. The LLM then invokes external tools based on a\nuser's attribute instructions and probes different segments of the item pool.\nWe consider two types of attribute-oriented tools: rank tools and retrieval\ntools. Through the integration of LLMs, ToolRec enables conventional\nrecommender systems to become external tools with a natural language interface.\nExtensive experiments verify the effectiveness of ToolRec, particularly in\nscenarios that are rich in semantic content.", "journal": ""}
{"doi": "10.48550/arXiv.2411.06146", "date": "2024-11-09", "title": "AI-Compass: A Comprehensive and Effective Multi-module Testing Tool for AI Systems", "authors": "Zhiyu Zhu, Zhibo Jin, Hongsheng Hu, Minhui Xue, Ruoxi Sun, Seyit Camtepe, Praveen Gauravaram, Huaming Chen", "abstract": "AI systems, in particular with deep learning techniques, have demonstrated\nsuperior performance for various real-world applications. Given the need for\ntailored optimization in specific scenarios, as well as the concerns related to\nthe exploits of subsurface vulnerabilities, a more comprehensive and in-depth\ntesting AI system becomes a pivotal topic. We have seen the emergence of\ntesting tools in real-world applications that aim to expand testing\ncapabilities. However, they often concentrate on ad-hoc tasks, rendering them\nunsuitable for simultaneously testing multiple aspects or components.\nFurthermore, trustworthiness issues arising from adversarial attacks and the\nchallenge of interpreting deep learning models pose new challenges for\ndeveloping more comprehensive and in-depth AI system testing tools. In this\nstudy, we design and implement a testing tool, \\tool, to comprehensively and\neffectively evaluate AI systems. The tool extensively assesses multiple\nmeasurements towards adversarial robustness, model interpretability, and\nperforms neuron analysis. The feasibility of the proposed testing tool is\nthoroughly validated across various modalities, including image classification,\nobject detection, and text classification. Extensive experiments demonstrate\nthat \\tool is the state-of-the-art tool for a comprehensive assessment of the\nrobustness and trustworthiness of AI systems. Our research sheds light on a\ngeneral solution for AI systems testing landscape.", "journal": ""}
{"doi": "10.48550/arXiv.2412.12152", "date": "2024-12-11", "title": "GraphTool-Instruction: Revolutionizing Graph Reasoning in LLMs through Decomposed Subtask Instruction", "authors": "Rongzheng Wang, Shuang Liang, Qizhi Chen, Jiasheng Zhang, Ke Qin", "abstract": "Large language models (LLMs) have been demonstrated to possess the\ncapabilities to understand fundamental graph properties and address various\ngraph reasoning tasks. Existing methods fine-tune LLMs to understand and\nexecute graph reasoning tasks by specially designed task instructions. However,\nthese Text-Instruction methods generally exhibit poor performance. Inspired by\ntool learning, researchers propose Tool-Instruction methods to solve various\ngraph problems by special tool calling (e.g., function, API and model),\nachieving significant improvements in graph reasoning tasks. Nevertheless,\ncurrent Tool-Instruction approaches focus on the tool information and ignore\nthe graph structure information, which leads to significantly inferior\nperformance on small-scale LLMs (less than 13B). To tackle this issue, we\npropose GraphTool-Instruction, an innovative Instruction-tuning approach that\ndecomposes the graph reasoning task into three distinct subtasks (i.e., graph\nextraction, tool name identification and tool parameter extraction), and design\nspecialized instructions for each subtask. Our GraphTool-Instruction can be\nused as a plug-and-play prompt for different LLMs without fine-tuning.\nMoreover, building on GraphTool-Instruction, we develop GTools, a dataset that\nincludes twenty graph reasoning tasks, and create a graph reasoning LLM called\nGraphForge based on Llama3-8B. We conduct extensive experiments on twenty graph\nreasoning tasks with different graph types (e.g., graph size or graph\ndirection), and we find that GraphTool-Instruction achieves SOTA compared to\nText-Instruction and Tool-Instruction methods. Fine-tuned on GTools, GraphForge\ngets further improvement of over 30% compared to the Tool-Instruction enhanced\nGPT-3.5-turbo, and it performs comparably to the high-cost GPT-4o. Our codes\nand data are available at\nhttps://anonymous.4open.science/r/GraphTool-Instruction.", "journal": ""}
{"doi": "10.48550/arXiv.1801.02620", "date": "2018-01-06", "title": "A Machine Learning Framework for Register Placement Optimization in Digital Circuit Design", "authors": "Karthik Airani, Rohit Guttal", "abstract": "In modern digital circuit back-end design, designers heavily rely on\nelectronic-design-automoation (EDA) tool to close timing. However, the\nheuristic algorithms used in the place and route tool usually does not result\nin optimal solution. Thus, significant design effort is used to tune parameters\nor provide user constraints or guidelines to improve the tool performance. In\nthis paper, we targeted at those optimization space left behind by the EDA\ntools and propose a machine learning framework that helps to define what are\nthe guidelines and constraints for registers placement, which can yield better\nperformance and quality for back-end design. In other words, the framework is\ntrying to learn what are the flaws of the existing EDA tools and tries to\noptimize it by providing additional information. We discuss what is the proper\ninput feature vector to be extracted, and what is metric to be used for\nreference output. We also develop a scheme to generate perturbed training\nsamples using existing design based on Gaussian randomization. By applying our\nmethodology, we are able to improve the design runtime by up to 36% and timing\nquality by up to 23%.", "journal": ""}
{"doi": "10.48550/arXiv.2110.13264", "date": "2021-10-25", "title": "Memory visualization tool for training neural network", "authors": "Mahendran N", "abstract": "Software developed helps world a better place ranging from system software,\nopen source, application software and so on. Software engineering does have\nneural network models applied to code suggestion, bug report summarizing and so\non to demonstrate their effectiveness at a real SE task. Software and machine\nlearning algorithms combine to make software give better solutions and\nunderstanding of environment. In software, there are both generalized\napplications which helps solve problems for entire world and also some specific\napplications which helps one particular community. To address the computational\nchallenge in deep learning, many tools exploit hardware features such as\nmulti-core CPUs and many-core GPUs to shorten the training time. Machine\nlearning algorithms have a greater impact in the world but there is a\nconsiderable amount of memory utilization during the process. We propose a new\ntool for analysis of memory utilized for developing and training deep learning\nmodels. Our tool results in visual utilization of memory concurrently. Various\nparameters affecting the memory utilization are analysed while training. This\ntool helps in knowing better idea of processes or models which consumes more\nmemory.", "journal": ""}
{"doi": "10.48550/arXiv.2502.11404", "date": "2025-02-17", "title": "ToolCoder: A Systematic Code-Empowered Tool Learning Framework for Large Language Models", "authors": "Hanxing Ding, Shuchang Tao, Liang Pang, Zihao Wei, Jinyang Gao, Bolin Ding, Huawei Shen, Xueqi Cheng", "abstract": "Tool learning has emerged as a crucial capability for large language models\n(LLMs) to solve complex real-world tasks through interaction with external\ntools. Existing approaches face significant challenges, including reliance on\nhand-crafted prompts, difficulty in multi-step planning, and lack of precise\nerror diagnosis and reflection mechanisms. We propose ToolCoder, a novel\nframework that reformulates tool learning as a code generation task. Inspired\nby software engineering principles, ToolCoder transforms natural language\nqueries into structured Python function scaffold and systematically breaks down\ntasks with descriptive comments, enabling LLMs to leverage coding paradigms for\ncomplex reasoning and planning. It then generates and executes function\nimplementations to obtain final responses. Additionally, ToolCoder stores\nsuccessfully executed functions in a repository to promote code reuse, while\nleveraging error traceback mechanisms for systematic debugging, optimizing both\nexecution efficiency and robustness. Experiments demonstrate that ToolCoder\nachieves superior performance in task completion accuracy and execution\nreliability compared to existing approaches, establishing the effectiveness of\ncode-centric approaches in tool learning.", "journal": ""}
{"doi": "10.48550/arXiv.1808.05347", "date": "2018-08-16", "title": "Tool Breakage Detection using Deep Learning", "authors": "Guang Li, Xin Yang, Duanbing Chen, Anxing Song, Yuke Fang, Junlin Zhou", "abstract": "In manufacture, steel and other metals are mainly cut and shaped during the\nfabrication process by computer numerical control (CNC) machines. To keep high\nproductivity and efficiency of the fabrication process, engineers need to\nmonitor the real-time process of CNC machines, and the lifetime management of\nmachine tools. In a real manufacturing process, breakage of machine tools\nusually happens without any indication, this problem seriously affects the\nfabrication process for many years. Previous studies suggested many different\napproaches for monitoring and detecting the breakage of machine tools. However,\nthere still exists a big gap between academic experiments and the complex real\nfabrication processes such as the high demands of real-time detections, the\ndifficulty in data acquisition and transmission. In this work, we use the\nspindle current approach to detect the breakage of machine tools, which has the\nhigh performance of real-time monitoring, low cost, and easy to install. We\nanalyze the features of the current of a milling machine spindle through tools\nwearing processes, and then we predict the status of tool breakage by a\nconvolutional neural network(CNN). In addition, we use a BP neural network to\nunderstand the reliability of the CNN. The results show that our CNN approach\ncan detect tool breakage with an accuracy of 93%, while the best performance of\nBP is 80%.", "journal": ""}
{"doi": "10.48550/arXiv.2012.09013", "date": "2020-12-16", "title": "An Assessment of the Usability of Machine Learning Based Tools for the Security Operations Center", "authors": "Sean Oesch, Robert Bridges, Jared Smith, Justin Beaver, John Goodall, Kelly Huffer, Craig Miles, Dan Scofield", "abstract": "Gartner, a large research and advisory company, anticipates that by 2024 80%\nof security operation centers (SOCs) will use machine learning (ML) based\nsolutions to enhance their operations. In light of such widespread adoption, it\nis vital for the research community to identify and address usability concerns.\nThis work presents the results of the first in situ usability assessment of\nML-based tools. With the support of the US Navy, we leveraged the national\ncyber range, a large, air-gapped cyber testbed equipped with state-of-the-art\nnetwork and user emulation capabilities, to study six US Naval SOC analysts'\nusage of two tools. Our analysis identified several serious usability issues,\nincluding multiple violations of established usability heuristics form user\ninterface design. We also discovered that analysts lacked a clear mental model\nof how these tools generate scores, resulting in mistrust and/or misuse of the\ntools themselves. Surprisingly, we found no correlation between analysts' level\nof education or years of experience and their performance with either tool,\nsuggesting that other factors such as prior background knowledge or personality\nplay a significant role in ML-based tool usage. Our findings demonstrate that\nML-based security tool vendors must put a renewed focus on working with\nanalysts, both experienced and inexperienced, to ensure that their systems are\nusable and useful in real-world security operations settings.", "journal": ""}
{"doi": "10.48550/arXiv.2308.02613", "date": "2023-08-04", "title": "Interoperable synthetic health data with SyntHIR to enable the development of CDSS tools", "authors": "Pavitra Chauhan, Mohsen Gamal Saad Askar, Bj\u00f8rn Fjukstad, Lars Ailo Bongo, Edvard Pedersen", "abstract": "There is a great opportunity to use high-quality patient journals and health\nregisters to develop machine learning-based Clinical Decision Support Systems\n(CDSS). To implement a CDSS tool in a clinical workflow, there is a need to\nintegrate, validate and test this tool on the Electronic Health Record (EHR)\nsystems used to store and manage patient data. However, it is often not\npossible to get the necessary access to an EHR system due to legal compliance.\nWe propose an architecture for generating and using synthetic EHR data for CDSS\ntool development. The architecture is implemented in a system called SyntHIR.\nThe SyntHIR system uses the Fast Healthcare Interoperability Resources (FHIR)\nstandards for data interoperability, the Gretel framework for generating\nsynthetic data, the Microsoft Azure FHIR server as the FHIR-based EHR system\nand SMART on FHIR framework for tool transportability. We demonstrate the\nusefulness of SyntHIR by developing a machine learning-based CDSS tool using\ndata from the Norwegian Patient Register (NPR) and Norwegian Patient\nPrescriptions (NorPD). We demonstrate the development of the tool on the\nSyntHIR system and then lift it to the Open DIPS environment. In conclusion,\nSyntHIR provides a generic architecture for CDSS tool development using\nsynthetic FHIR data and a testing environment before implementing it in a\nclinical setting. However, there is scope for improvement in terms of the\nquality of the synthetic data generated. The code is open source and available\nat https://github.com/potter-coder89/SyntHIR.git.", "journal": ""}
{"doi": "10.48550/arXiv.2311.06261", "date": "2023-09-26", "title": "With ChatGPT, do we have to rewrite our learning objectives -- CASE study in Cybersecurity", "authors": "Peter Jamieson, Suman Bhunia, Dhananjai M. Rao", "abstract": "With the emergence of Artificial Intelligent chatbot tools such as ChatGPT\nand code writing AI tools such as GitHub Copilot, educators need to question\nwhat and how we should teach our courses and curricula in the future. In\nreality, automated tools may result in certain academic fields being deeply\nreduced in the number of employable people. In this work, we make a case study\nof cybersecurity undergrad education by using the lens of ``Understanding by\nDesign'' (UbD). First, we provide a broad understanding of learning objectives\n(LOs) in cybersecurity from a computer science perspective. Next, we dig a\nlittle deeper into a curriculum with an undergraduate emphasis on cybersecurity\nand examine the major courses and their LOs for our cybersecurity program at\nMiami University. With these details, we perform a thought experiment on how\nattainable the LOs are with the above-described tools, asking the key question\n``what needs to be enduring concepts?'' learned in this process. If an LO\nbecomes something that the existence of automation tools might be able to do,\nwe then ask ``what level is attainable for the LO that is not a simple query to\nthe tools?''. With this exercise, we hope to establish an example of how to\nprompt ChatGPT to accelerate students in their achievements of LOs given the\nexistence of these new AI tools, and our goal is to push all of us to leverage\nand teach these tools as powerful allies in our quest to improve human\nexistence and knowledge.", "journal": ""}
{"doi": "10.48550/arXiv.1606.08531", "date": "2016-06-28", "title": "A Learning Algorithm for Relational Logistic Regression: Preliminary Results", "authors": "Bahare Fatemi, Seyed Mehran Kazemi, David Poole", "abstract": "Relational logistic regression (RLR) is a representation of conditional\nprobability in terms of weighted formulae for modelling multi-relational data.\nIn this paper, we develop a learning algorithm for RLR models. Learning an RLR\nmodel from data consists of two steps: 1- learning the set of formulae to be\nused in the model (a.k.a. structure learning) and learning the weight of each\nformula (a.k.a. parameter learning). For structure learning, we deploy Schmidt\nand Murphy's hierarchical assumption: first we learn a model with simple\nformulae, then more complex formulae are added iteratively only if all their\nsub-formulae have proven effective in previous learned models. For parameter\nlearning, we convert the problem into a non-relational learning problem and use\nan off-the-shelf logistic regression learning algorithm from Weka, an\nopen-source machine learning tool, to learn the weights. We also indicate how\nhidden features about the individuals can be incorporated into RLR to boost the\nlearning performance. We compare our learning algorithm to other structure and\nparameter learning algorithms in the literature, and compare the performance of\nRLR models to standard logistic regression and RDN-Boost on a modified version\nof the MovieLens data-set.", "journal": ""}
{"doi": "10.48550/arXiv.2003.07680", "date": "2020-03-13", "title": "Designing Tools for Semi-Automated Detection of Machine Learning Biases: An Interview Study", "authors": "Po-Ming Law, Sana Malik, Fan Du, Moumita Sinha", "abstract": "Machine learning models often make predictions that bias against certain\nsubgroups of input data. When undetected, machine learning biases can\nconstitute significant financial and ethical implications. Semi-automated tools\nthat involve humans in the loop could facilitate bias detection. Yet, little is\nknown about the considerations involved in their design. In this paper, we\nreport on an interview study with 11 machine learning practitioners for\ninvestigating the needs surrounding semi-automated bias detection tools. Based\non the findings, we highlight four considerations in designing to guide system\ndesigners who aim to create future tools for bias detection.", "journal": ""}
{"doi": "10.48550/arXiv.1804.01825", "date": "2018-04-04", "title": "Evaluating Hospital Case Cost Prediction Models Using Azure Machine Learning Studio", "authors": "Alexei Botchkarev", "abstract": "Ability for accurate hospital case cost modelling and prediction is critical\nfor efficient health care financial management and budgetary planning. A\nvariety of regression machine learning algorithms are known to be effective for\nhealth care cost predictions. The purpose of this experiment was to build an\nAzure Machine Learning Studio tool for rapid assessment of multiple types of\nregression models. The tool offers environment for comparing 14 types of\nregression models in a unified experiment: linear regression, Bayesian linear\nregression, decision forest regression, boosted decision tree regression,\nneural network regression, Poisson regression, Gaussian processes for\nregression, gradient boosted machine, nonlinear least squares regression,\nprojection pursuit regression, random forest regression, robust regression,\nrobust regression with mm-type estimators, support vector regression. The tool\npresents assessment results arranged by model accuracy in a single table using\nfive performance metrics. Evaluation of regression machine learning models for\nperforming hospital case cost prediction demonstrated advantage of robust\nregression model, boosted decision tree regression and decision forest\nregression. The operational tool has been published to the web and openly\navailable for experiments and extensions.", "journal": ""}
{"doi": "10.48550/arXiv.1906.03060", "date": "2019-06-02", "title": "Comparison of block-based and hybrid-based environments in transferring programming skills to text-based environments", "authors": "Hussein Alrubaye, Stephanie Ludi, Mohamed Wiem Mkaouer", "abstract": "Teachers face several challenges when presenting the fundamental concepts of\nprogramming in the classroom. Several tools are introduced to give a visual\ndimension to support the learning process. These tools rely on code blocks,\neasily manipulated in a plug and play fashion, to build a program. These\nblock-based tools intend to familiarize students with programming logic, before\ndiving into text-based programming languages such as Java, Python, etc.\nHowever; when transitioning from block-based to text-based programming,\nstudents often encounter a gap in their learning. The student may not be able\nto apply block-based foundations in a text-based environment. To bridge the gap\nbetween both environments, we developed a hybrid-based learning approach. We\nfound that on average a hybrid-based approach increases the students\nunderstanding of programming foundations, memorization, and ease of transition\nby more than 30% when compared to a block-based to text-based learning\napproach. Finally, we provide the community with an open source, hybrid-based\nlearning tool that can be used by students when learning programming concepts\nor for future studies.", "journal": ""}
{"doi": "10.48550/arXiv.2012.10056", "date": "2020-12-18", "title": "Transfer Learning Based Automatic Model Creation Tool For Resource Constraint Devices", "authors": "Karthik Bhat, Manan Bhandari, ChangSeok Oh, Sujin Kim, Jeeho Yoo", "abstract": "With the enhancement of Machine Learning, many tools are being designed to\nassist developers to easily create their Machine Learning models. In this\npaper, we propose a novel method for auto creation of such custom models for\nconstraint devices using transfer learning without the need to write any\nmachine learning code. We share the architecture of our automatic model\ncreation tool and the CNN Model created by it using pretrained models such as\nYAMNet and MobileNetV2 as feature extractors. Finally, we demonstrate accuracy\nand memory footprint of the model created from the tool by creating an\nAutomatic Image and Audio classifier and report the results of our experiments\nusing Stanford Cars and ESC-50 dataset.", "journal": ""}
{"doi": "10.48550/arXiv.1712.01674", "date": "2017-12-05", "title": "On Benchmarking the Capability of Symbolic Execution Tools with Logic Bombs", "authors": "Hui Xu, Zirui Zhao, Yangfan Zhou, Michael R. Lyu", "abstract": "Symbolic execution now becomes an indispensable technique for software\ntesting and program analysis. There are several symbolic execution tools\navailable off-the-shelf, and we need a practical benchmark approach to learn\ntheir capabilities. Therefore, this paper introduces a novel approach to\nbenchmark symbolic execution tools in a fine-grained and efficient manner. In\nparticular, our approach evaluates the performance of such tools against the\nknown challenges faced by general symbolic execution techniques, such as\nfloating-point numbers and symbolic memories. To this end, we first survey\nrelated papers and systematize the challenges of symbolic execution. We extract\n12 distinct challenges from the literature and categorize them into two\ncategories: symbolic-reasoning challenges and path-explosion challenges. Then,\nwe develop a dataset of logic bombs and a framework to benchmark symbolic\nexecution tools automatically. For each challenge, our dataset contains several\nlogic bombs, each of which is guarded by a specific challenging problem. If a\nsymbolic execution tool can find test cases to trigger logic bombs, it\nindicates that the tool can handle the corresponding problems. We have\nconducted real-world experiments with three popular symbolic execution tools:\nKLEE, Angr, and Triton. Experimental results show that our approach can reveal\ntheir capabilities and limitations in handling particular issues accurately and\nefficiently. The benchmark process generally takes only dozens of minutes to\nevaluate a tool. We release our dataset on GitHub as open source, with an aim\nto better facilitate the community to conduct future work on benchmarking\nsymbolic execution tools.", "journal": ""}
{"doi": "10.48550/arXiv.2204.10370", "date": "2022-04-21", "title": "Passport: Improving Automated Formal Verification Using Identifiers", "authors": "Alex Sanchez-Stern, Emily First, Timothy Zhou, Zhanna Kaufman, Yuriy Brun, Talia Ringer", "abstract": "Formally verifying system properties is one of the most effective ways of\nimproving system quality, but its high manual effort requirements often render\nit prohibitively expensive. Tools that automate formal verification, by\nlearning from proof corpora to suggest proofs, have just begun to show their\npromise. These tools are effective because of the richness of the data the\nproof corpora contain. This richness comes from the stylistic conventions\nfollowed by communities of proof developers, together with the logical systems\nbeneath proof assistants. However, this richness remains underexploited, with\nmost work thus far focusing on architecture rather than making the most of the\nproof data.\n  In this paper, we develop Passport, a fully-automated proof-synthesis tool\nthat systematically explores how to most effectively exploit one aspect of that\nproof data: identifiers. Passport enriches a predictive Coq model with three\nnew encoding mechanisms for identifiers: category vocabulary indexing, subword\nsequence modeling, and path elaboration. We compare Passport to three existing\nbase tools which Passport can enhance: ASTactic, Tac, and Tok. In head-to-head\ncomparisons, Passport automatically proves 29% more theorems than the\nbest-performing of these base tools. Combining the three Passport-enhanced\ntools automatically proves 38% more theorems than the three base tools\ntogether, without Passport's enhancements. Finally, together, these base tools\nand Passport-enhanced tools prove 45% more theorems than the combined base\ntools without Passport's enhancements. Overall, our findings suggest that\nmodeling identifiers can play a significant role in improving proof synthesis,\nleading to higher-quality software.", "journal": "ACM Transactions on Programming Languages and Systems (TOPLAS),\n  45(2):12:1-12:30, June 2023"}
{"doi": "10.48550/arXiv.2309.16090", "date": "2023-09-28", "title": "TPE: Towards Better Compositional Reasoning over Conceptual Tools with Multi-persona Collaboration", "authors": "Hongru Wang, Huimin Wang, Lingzhi Wang, Minda Hu, Rui Wang, Boyang Xue, Hongyuan Lu, Fei Mi, Kam-Fai Wong", "abstract": "Large language models (LLMs) have demonstrated exceptional performance in\nplanning the use of various functional tools, such as calculators and\nretrievers, particularly in question-answering tasks. In this paper, we expand\nthe definition of these tools, centering on conceptual tools within the context\nof dialogue systems. A conceptual tool specifies a cognitive concept that aids\nsystematic or investigative thought. These conceptual tools play important\nroles in practice, such as multiple psychological or tutoring strategies being\ndynamically applied in a single turn to compose helpful responses. To further\nenhance the reasoning and planning capability of LLMs with these conceptual\ntools, we introduce a multi-persona collaboration framework: Think-Plan-Execute\n(TPE). This framework decouples the response generation process into three\ndistinct roles: Thinker, Planner, and Executor. Specifically, the Thinker\nanalyzes the internal status exhibited in the dialogue context, such as user\nemotions and preferences, to formulate a global guideline. The Planner then\ngenerates executable plans to call different conceptual tools (e.g., sources or\nstrategies), while the Executor compiles all intermediate results into a\ncoherent response. This structured approach not only enhances the\nexplainability and controllability of responses but also reduces token\nredundancy. We demonstrate the effectiveness of TPE across various dialogue\nresponse generation tasks, including multi-source (FoCus) and multi-strategy\ninteractions (CIMA and PsyQA). This reveals its potential to handle real-world\ndialogue interactions that require more complicated tool learning beyond just\nfunctional tools. The full code and data will be released for reproduction.", "journal": ""}
{"doi": "10.48550/arXiv.1711.03386", "date": "2017-11-09", "title": "Performance Evaluation of Deep Learning Tools in Docker Containers", "authors": "Pengfei Xu, Shaohuai Shi, Xiaowen Chu", "abstract": "With the success of deep learning techniques in a broad range of application\ndomains, many deep learning software frameworks have been developed and are\nbeing updated frequently to adapt to new hardware features and software\nlibraries, which bring a big challenge for end users and system administrators.\nTo address this problem, container techniques are widely used to simplify the\ndeployment and management of deep learning software. However, it remains\nunknown whether container techniques bring any performance penalty to deep\nlearning applications. The purpose of this work is to systematically evaluate\nthe impact of docker container on the performance of deep learning\napplications. We first benchmark the performance of system components (IO, CPU\nand GPU) in a docker container and the host system and compare the results to\nsee if there's any difference. According to our results, we find that\ncomputational intensive jobs, either running on CPU or GPU, have small overhead\nindicating docker containers can be applied to deep learning programs. Then we\nevaluate the performance of some popular deep learning tools deployed in a\ndocker container and the host system. It turns out that the docker container\nwill not cause noticeable drawbacks while running those deep learning tools. So\nencapsulating deep learning tool in a container is a feasible solution.", "journal": ""}
{"doi": "10.48550/arXiv.2108.03487", "date": "2021-08-07", "title": "Augmented Reality and Gamification: A Framework for Developing Supplementary Learning Tool", "authors": "Carlo H. Godoy Jr", "abstract": "The main purpose of the study is to develop a supplementary learning tool\nframework by the use of a dynamic mobile application using Unity AR and Vuforia\nfor Senior High School (SHS) students and teachers to help the learning process\nin SHS Earth Science. The researchers will be using the Software Development\nLife Cycle (SDLC) Model of methodology to ensure the quality of the software as\nwell as the correctness of the development process. The expected result of the\nstudy is that Augmented Reality and Gamification will now be used as a\nsupplementary learning tool in SHS Earth Science. Augmented Reality and\nGamification can now be used as a supplementary learning tool in SHS Earth\nScience using the designed framework. Future studies will focus on the\ndevelopment of the framework and the mobile application. Since the system has a\nlot of potential in the education sector and due to the effects of COVID-19,\nthe software will serve as a pioneer to show that a supplementary tool will\nhelp students learn logically and entertainingly especially since schools\nnowadays are transitioning with either distance learning or blended learning.", "journal": "International Journal of Computing Sciences Research, 5(1),\n  595-612 (2021)"}
{"doi": "10.48550/arXiv.1606.00518", "date": "2016-06-02", "title": "The challenge of engaging all students via self-paced interactive e-learning tutorials for introductory physics", "authors": "Seth DeVore, Emily Marshman, Chandralekha Singh", "abstract": "As research-based self-paced e-learning tools become increasingly available,\na critical issue educators encounter is implementing strategies to ensure that\nall students engage with them as intended. Here, we discuss the effectiveness\nof research-based e-learning tutorials as self-paced learning tools in large\nenrollment brick and mortar introductory physics courses. These interactive\ntutorials were developed via research in physics education and were found to be\neffective for a diverse group of introductory physics students in one-on-one\nimplementation. Instructors encouraged the use of these self-paced tools in a\nself-paced learning environment by telling students that they would be helpful\nfor solving the assigned homework problems and that the underlying physics\nprinciples in the tutorial problems would be similar to those in the in-class\nquizzes (which we call paired problems). We find that many students, who\nstruggled in the courses in which these adaptive e-learning tutorials were\nassigned as a self-study tool, performed poorly on the paired problems. In\ncontrast, a majority of student volunteers in one-on-one implementation greatly\nbenefited from the tutorials and performed well on the paired problems. This\nsuggests that many students enrolled in introductory physics courses did not\neffectively engage with the self-paced tutorials outside of class and may have\nonly used them superficially. The findings suggest that many students in need\nof out-of-class remediation via self-paced learning tools may have difficulty\nmotivating themselves and may lack the self-regulation and time-management\nskills to engage effectively with tools specially designed to help them learn\nat their own pace. We conclude by proposing a theoretical framework to help\nstudents with diverse prior preparations engage effectively with self-study\ntools.", "journal": ""}
{"doi": "10.48550/arXiv.1807.10129", "date": "2018-07-26", "title": "A Benchmark of Selected Algorithmic Differentiation Tools on Some Problems in Computer Vision and Machine Learning", "authors": "Filip \u0160rajer, Zuzana Kukelova, Andrew Fitzgibbon", "abstract": "Algorithmic differentiation (AD) allows exact computation of derivatives\ngiven only an implementation of an objective function. Although many AD tools\nare available, a proper and efficient implementation of AD methods is not\nstraightforward. The existing tools are often too different to allow for a\ngeneral test suite. In this paper, we compare fifteen ways of computing\nderivatives including eleven automatic differentiation tools implementing\nvarious methods and written in various languages (C++, F#, MATLAB, Julia and\nPython), two symbolic differentiation tools, finite differences, and\nhand-derived computation.\n  We look at three objective functions from computer vision and machine\nlearning. These objectives are for the most part simple, in the sense that no\niterative loops are involved, and conditional statements are encapsulated in\nfunctions such as {\\tt abs} or {\\tt logsumexp}. However, it is important for\nthe success of algorithmic differentiation that such `simple' objective\nfunctions are handled efficiently, as so many problems in computer vision and\nmachine learning are of this form.\n  Of course, our results depend on programmer skill, and familiarity with the\ntools. However, we contend that this paper presents an important datapoint: a\nskilled programmer devoting roughly a week to each tool produced the timings we\npresent. We have made our implementations available as open source to allow the\ncommunity to replicate and update these benchmarks.", "journal": ""}
{"doi": "10.48550/arXiv.2312.10908", "date": "2023-12-18", "title": "CLOVA: A Closed-Loop Visual Assistant with Tool Usage and Update", "authors": "Zhi Gao, Yuntao Du, Xintong Zhang, Xiaojian Ma, Wenjuan Han, Song-Chun Zhu, Qing Li", "abstract": "Utilizing large language models (LLMs) to compose off-the-shelf visual tools\nrepresents a promising avenue of research for developing robust visual\nassistants capable of addressing diverse visual tasks. However, these methods\noften overlook the potential for continual learning, typically by freezing the\nutilized tools, thus limiting their adaptation to environments requiring new\nknowledge. To tackle this challenge, we propose CLOVA, a Closed-Loop Visual\nAssistant, which operates within a framework encompassing inference,\nreflection, and learning phases. During the inference phase, LLMs generate\nprograms and execute corresponding tools to complete assigned tasks. In the\nreflection phase, a multimodal global-local reflection scheme analyzes human\nfeedback to determine which tools require updating. Lastly, the learning phase\nemploys three flexible approaches to automatically gather training data and\nintroduces a novel prompt tuning scheme to update the tools, allowing CLOVA to\nefficiently acquire new knowledge. Experimental findings demonstrate that CLOVA\nsurpasses existing tool-usage methods by 5% in visual question answering and\nmultiple-image reasoning, by 10% in knowledge tagging, and by 20% in image\nediting. These results underscore the significance of the continual learning\ncapability in general visual assistants.", "journal": ""}
{"doi": "10.48550/arXiv.2408.06689", "date": "2024-08-13", "title": "Comparative Analysis of Digital Tools and Traditional Teaching Methods in Educational Effectiveness", "authors": "Aarush Kandukoori, Aditya Kandukoori, Faizan Wajid", "abstract": "In today's world technology comprises a large aspect of our lives so this\nstudy aimed to investigate if using computers and digital tools are better than\ntraditional methods like using textbooks and worksheets for learning math. This\nstudy was done at Clarksburg Elementary School with help from MoCo Innovation\nwhich is a club that focuses on fostering an interest in technology among\nstudents. A major question that sparked our minds was: Are digital tools like\nlearning on computers better than traditional methods for improving students\nmath skills? We believe students who use digital tools might improve more in\ntheir math skills. To find out we worked with 30 students from the school. We\nsplit them into two groups and gave each group a pre assessment and post\nassessment. One group learned math using computers and were able to use\ninteractive math websites such as Khan Academy while the other group used\nworksheets. After some learning we gave them a post assessment to see how much\nthey had improved. Our results showed that the students who used the digital\ntools improved test scores averages by 24.2 percent from 70 percent to 87\npercent while the students who used traditional methods only improved by 8.3\npercent from 72 percent to 78 percent in math. These results show that digital\ntools are superior to regular teaching methods especially for subjects like\nmath. But more research is required to see if digital tools are the main reason\nfor this improvement. This research is definitely important to help schools\ndecide if they want to use more technology.", "journal": ""}
{"doi": "10.48550/arXiv.2408.15128", "date": "2024-08-27", "title": "Evaluating the Energy Consumption of Machine Learning: Systematic Literature Review and Experiments", "authors": "Charlotte Rodriguez, Laura Degioanni, Laetitia Kameni, Richard Vidal, Giovanni Neglia", "abstract": "Monitoring, understanding, and optimizing the energy consumption of Machine\nLearning (ML) are various reasons why it is necessary to evaluate the energy\nusage of ML. However, there exists no universal tool that can answer this\nquestion for all use cases, and there may even be disagreement on how to\nevaluate energy consumption for a specific use case. Tools and methods are\nbased on different approaches, each with their own advantages and drawbacks,\nand they need to be mapped out and explained in order to select the most\nsuitable one for a given situation. We address this challenge through two\napproaches. First, we conduct a systematic literature review of all tools and\nmethods that permit to evaluate the energy consumption of ML (both at training\nand at inference), irrespective of whether they were originally designed for\nmachine learning or general software. Second, we develop and use an\nexperimental protocol to compare a selection of these tools and methods. The\ncomparison is both qualitative and quantitative on a range of ML tasks of\ndifferent nature (vision, language) and computational complexity. The\nsystematic literature review serves as a comprehensive guide for understanding\nthe array of tools and methods used in evaluating energy consumption of ML, for\nvarious use cases going from basic energy monitoring to consumption\noptimization. Two open-source repositories are provided for further\nexploration. The first one contains tools that can be used to replicate this\nwork or extend the current review. The second repository houses the\nexperimental protocol, allowing users to augment the protocol with new ML\ncomputing tasks and additional energy evaluation tools.", "journal": ""}
{"doi": "10.48550/arXiv.2501.18361", "date": "2025-01-30", "title": "Video-based Surgical Tool-tip and Keypoint Tracking using Multi-frame Context-driven Deep Learning Models", "authors": "Bhargav Ghanekar, Lianne R. Johnson, Jacob L. Laughlin, Marcia K. O'Malley, Ashok Veeraraghavan", "abstract": "Automated tracking of surgical tool keypoints in robotic surgery videos is an\nessential task for various downstream use cases such as skill assessment,\nexpertise assessment, and the delineation of safety zones. In recent years, the\nexplosion of deep learning for vision applications has led to many works in\nsurgical instrument segmentation, while lesser focus has been on tracking\nspecific tool keypoints, such as tool tips. In this work, we propose a novel,\nmulti-frame context-driven deep learning framework to localize and track tool\nkeypoints in surgical videos. We train and test our models on the annotated\nframes from the 2015 EndoVis Challenge dataset, resulting in state-of-the-art\nperformance. By leveraging sophisticated deep learning models and multi-frame\ncontext, we achieve 90\\% keypoint detection accuracy and a localization RMS\nerror of 5.27 pixels. Results on a self-annotated JIGSAWS dataset with more\nchallenging scenarios also show that the proposed multi-frame models can\naccurately track tool-tip and tool-base keypoints, with ${<}4.2$-pixel RMS\nerror overall. Such a framework paves the way for accurately tracking surgical\ninstrument keypoints, enabling further downstream use cases. Project and\ndataset webpage: https://tinyurl.com/mfc-tracker", "journal": ""}
{"doi": "10.48550/arXiv.2504.01400", "date": "2025-04-02", "title": "ToolACE-R: Tool Learning with Adaptive Self-Refinement", "authors": "Xingshan Zeng, Weiwen Liu, Xu Huang, Zezhong Wang, Lingzhi Wang, Liangyou Li, Yasheng Wang, Lifeng Shang, Xin Jiang, Ruiming Tang, Qun Liu", "abstract": "Tool learning, which allows Large Language Models (LLMs) to leverage external\ntools for solving complex user tasks, has emerged as a promising avenue for\nextending model capabilities. However, current approaches primarily focus on\ndata synthesis for fine-tuning LLMs to invoke tools effectively, largely\nignoring how to fully stimulate the potential of the model. In this paper, we\npropose ToolACE-R, a novel method that introduces adaptive self-refinement for\ntool invocations. Our approach features a model-aware iterative training\nprocedure that progressively incorporates more training samples based on the\nmodel's evolving capabilities. Additionally, it allows LLMs to iteratively\nrefine their tool calls, optimizing performance without requiring external\nfeedback. To further enhance computational efficiency, we integrate an adaptive\nmechanism when scaling the inference time, enabling the model to autonomously\ndetermine when to stop the refinement process. We conduct extensive experiments\nacross several benchmark datasets, showing that ToolACE-R achieves competitive\nperformance compared to advanced API-based models, even without any refinement.\nFurthermore, its performance can be further improved efficiently through\nadaptive self-refinement. Our results demonstrate the effectiveness of the\nproposed method, which is compatible with base models of various sizes,\noffering a promising direction for more efficient tool learning.", "journal": ""}
{"doi": "10.48550/arXiv.1104.4461", "date": "2011-04-22", "title": "Computer algebra systems as tools for learning Physics and Mathematics", "authors": "Danilo T. Alves, Silvio C. F. Pereira Filho", "abstract": "In the present paper, we describe some experiences in using programming,\ncommands and graphical interfaces based on computer algebra systems, as tools\nfor learning Physics and Mathematics.", "journal": ""}
{"doi": "10.48550/arXiv.2208.11792", "date": "2022-08-24", "title": "A Survey of Open Source Automation Tools for Data Science Predictions", "authors": "Nicholas Hoell", "abstract": "We present an expository overview of technical and cultural challenges to the\ndevelopment and adoption of automation at various stages in the data science\nprediction lifecycle, restricting focus to supervised learning with structured\ndatasets. In addition, we review popular open source Python tools implementing\ncommon solution patterns for the automation challenges and highlight gaps where\nwe feel progress still demands to be made.", "journal": ""}
{"doi": "10.48550/arXiv.2107.07886", "date": "2021-07-16", "title": "Tracing Halpha Fibrils through Bayesian Deep Learning", "authors": "Haodi Jiang, Ju Jing, Jiasheng Wang, Chang Liu, Qin Li, Yan Xu, Jason T. L. Wang, Haimin Wang", "abstract": "We present a new deep learning method, dubbed FibrilNet, for tracing\nchromospheric fibrils in Halpha images of solar observations. Our method\nconsists of a data pre-processing component that prepares training data from a\nthreshold-based tool, a deep learning model implemented as a Bayesian\nconvolutional neural network for probabilistic image segmentation with\nuncertainty quantification to predict fibrils, and a post-processing component\ncontaining a fibril-fitting algorithm to determine fibril orientations. The\nFibrilNet tool is applied to high-resolution Halpha images from an active\nregion (AR 12665) collected by the 1.6 m Goode Solar Telescope (GST) equipped\nwith high-order adaptive optics at the Big Bear Solar Observatory (BBSO). We\nquantitatively assess the FibrilNet tool, comparing its image segmentation\nalgorithm and fibril-fitting algorithm with those employed by the\nthreshold-based tool. Our experimental results and major findings are\nsummarized as follows. First, the image segmentation results (i.e., detected\nfibrils) of the two tools are quite similar, demonstrating the good learning\ncapability of FibrilNet. Second, FibrilNet finds more accurate and smoother\nfibril orientation angles than the threshold-based tool. Third, FibrilNet is\nfaster than the threshold-based tool and the uncertainty maps produced by\nFibrilNet not only provide a quantitative way to measure the confidence on each\ndetected fibril, but also help identify fibril structures that are not detected\nby the threshold-based tool but are inferred through machine learning. Finally,\nwe apply FibrilNet to full-disk Halpha images from other solar observatories\nand additional high-resolution Halpha images collected by BBSO/GST,\ndemonstrating the tool's usability in diverse datasets.", "journal": ""}
{"doi": "10.48550/arXiv.1611.00201", "date": "2016-11-01", "title": "Towards Lifelong Self-Supervision: A Deep Learning Direction for Robotics", "authors": "Jay M. Wong", "abstract": "Despite outstanding success in vision amongst other domains, many of the\nrecent deep learning approaches have evident drawbacks for robots. This\nmanuscript surveys recent work in the literature that pertain to applying deep\nlearning systems to the robotics domain, either as means of estimation or as a\ntool to resolve motor commands directly from raw percepts. These recent\nadvances are only a piece to the puzzle. We suggest that deep learning as a\ntool alone is insufficient in building a unified framework to acquire general\nintelligence. For this reason, we complement our survey with insights from\ncognitive development and refer to ideas from classical control theory,\nproducing an integrated direction for a lifelong learning architecture.", "journal": ""}
{"doi": "10.48550/arXiv.1911.06578", "date": "2019-11-15", "title": "Smarter Features, Simpler Learning?", "authors": "Sarah Winkler, Georg Moser", "abstract": "Earlier work on machine learning for automated reasoning mostly relied on\nsimple, syntactic features combined with sophisticated learning techniques.\nUsing ideas adopted in the software verification community, we propose the\ninvestigation of more complex, structural features to learn from. These may be\nexploited to either learn beneficial strategies for tools, or build a portfolio\nsolver that chooses the most suitable tool for a given problem. We present some\nideas for features of term rewrite systems and theorem proving problems.", "journal": "EPTCS 311, 2019, pp. 25-31"}
{"doi": "10.48550/arXiv.2307.11777", "date": "2023-07-20", "title": "Prediction of Handball Matches with Statistically Enhanced Learning via Estimated Team Strengths", "authors": "Florian Felice, Christophe Ley", "abstract": "We propose a Statistically Enhanced Learning (aka. SEL) model to predict\nhandball games. Our Machine Learning model augmented with SEL features\noutperforms state-of-the-art models with an accuracy beyond 80%. In this work,\nwe show how we construct the data set to train Machine Learning models on past\nfemale club matches. We then compare different models and evaluate them to\nassess their performance capabilities. Finally, explainability methods allow us\nto change the scope of our tool from a purely predictive solution to a highly\ninsightful analytical tool. This can become a valuable asset for handball\nteams' coaches providing valuable statistical and predictive insights to\nprepare future competitions.", "journal": ""}
{"doi": "10.48550/arXiv.2312.05582", "date": "2023-12-09", "title": "Robotics as a Simulation Educational Tool", "authors": "Athanasios Karagounis", "abstract": "In the evolving landscape of education, robotics has emerged as a powerful\ntool for fostering creativity, critical thinking, and problem-solving skills\namong students of all ages. This innovative approach to learning seamlessly\nintegrates STEM (Science, Technology, Engineering, and Mathematics) concepts,\ncreating an engaging and immersive learning experience. Educational robotics\ntranscends traditional classroom settings, transforming learning into a\nhands-on, experiential endeavor. Students are actively involved in the design,\nconstruction, and programming of robots, allowing them to apply theoretical\nconcepts to practical applications. This hands-on approach fosters deeper\nunderstanding and retention of knowledge, making learning more meaningful and\nenjoyable. In this paper, the potential of simulation robotics is evaluated as\na hands on interactive learning experience that goes beyond traditional robotic\nclassroom methods.", "journal": ""}
{"doi": "10.48550/arXiv.1610.00985", "date": "2016-10-01", "title": "Key attributes of a modern statistical computing tool", "authors": "Amelia McNamara", "abstract": "In the 1990s, statisticians began thinking in a principled way about how\ncomputation could better support the learning and doing of statistics. Since\nthen, the pace of software development has accelerated, advancements in\ncomputing and data science have moved the goalposts, and it is time to\nreassess. Software continues to be developed to help do and learn statistics,\nbut there is little critical evaluation of the resulting tools, and no accepted\nframework with which to critique them. This paper presents a set of attributes\nnecessary for a modern statistical computing tool. The framework was designed\nto be broadly applicable to both novice and expert users, with a particular\nfocus on making more supportive statistical computing environments. A modern\nstatistical computing tool should be accessible, provide easy entry, privilege\ndata as a first-order object, support exploratory and confirmatory analysis,\nallow for flexible plot creation, support randomization, be interactive,\ninclude inherent documentation, support narrative, publishing, and\nreproducibility, and be flexible to extensions. Ideally, all these attributes\ncould be incorporated into one tool, supporting users at all levels, but a more\nreasonable goal is for tools designed for novices and professionals to `reach\nacross the gap,' taking inspiration from each others' strengths.", "journal": ""}
{"doi": "10.48550/arXiv.1812.09178", "date": "2018-12-20", "title": "An Evaluation of Methods for Real-Time Anomaly Detection using Force Measurements from the Turning Process", "authors": "Yuanzhi Huang, Eamonn Ahearne, Szymon Baron, Andrew Parnell", "abstract": "We examined the use of three conventional anomaly detection methods and\nassess their potential for on-line tool wear monitoring. Through efficient data\nprocessing and transformation of the algorithm proposed here, in a real-time\nenvironment, these methods were tested for fast evaluation of cutting tools on\nCNC machines. The three-dimensional force data streams we used were extracted\nfrom a turning experiment of 21 runs for which a tool was run until it\ngenerally satisfied an end-of-life criterion. Our real-time anomaly detection\nalgorithm was scored and optimised according to how precisely it can predict\nthe progressive wear of the tool flank. Most of our tool wear predictions were\naccurate and reliable as illustrated in our off-line simulation results.\nParticularly when the multivariate analysis was applied, the algorithm we\ndevelop was found to be very robust across different scenarios and against\nparameter changes. It shall be reasonably easy to apply our approach elsewhere\nfor real-time tool wear analytics.", "journal": ""}
{"doi": "10.48550/arXiv.2008.01053", "date": "2020-07-24", "title": "Towards Leveraging End-of-Life Tools as an Asset: Value Co-Creation based on Deep Learning in the Machining Industry", "authors": "Jannis Walk, Niklas K\u00fchl, Jonathan Sch\u00e4fer", "abstract": "Sustainability is the key concept in the management of products that reached\ntheir end-of-life. We propose that end-of-life products have -- besides their\nvalue as recyclable assets -- additional value for producer and consumer. We\nargue this is especially true for the machining industry, where we illustrate\nan automatic characterization of worn cutting tools to foster value co-creation\nbetween tool manufacturer and tool user (customer) in the future. In the work\nat hand, we present a deep-learning-based computer vision system for the\nautomatic classification of worn tools regarding flank wear and chipping. The\nresulting Matthews Correlation Coefficient of 0.878 and 0.644 confirms the\nfeasibility of our system based on the VGG-16 network and Gradient Boosting.\nBased on these first results we derive a research agenda which addresses the\nneed for a more holistic tool characterization by semantic segmentation and\nassesses the perceived business impact and usability by different user groups.", "journal": ""}
{"doi": "10.48550/arXiv.2207.07987", "date": "2022-07-16", "title": "A tool for emulating neuromorphic architectures with memristive models and devices", "authors": "Jinqi Huang, Spyros Stathopoulos, Alex Serb, Themis Prodromakis", "abstract": "Memristors have shown promising features for enhancing neuromorphic computing\nconcepts and AI hardware accelerators. In this paper, we present a\nuser-friendly software infrastructure that allows emulating a wide range of\nneuromorphic architectures with memristor models. This tool empowers studies\nthat exploit memristors for online learning and online classification tasks,\npredicting memristor resistive state changes during the training process. The\nversatility of the tool is showcased through the capability for users to\ncustomise parameters in the employed memristor and neuronal models as well as\nthe employed learning rules. This further allows users to validate concepts and\ntheir sensitivity across a wide range of parameters. We demonstrate the use of\nthe tool via an MNIST classification task. Finally, we show how this tool can\nalso be used to emulate the concepts under study in-silico with practical\nmemristive devices via appropriate interfacing with commercially available\ncharacterisation tools.", "journal": ""}
{"doi": "10.48550/arXiv.2210.11991", "date": "2022-10-21", "title": "Real-time Detection of 2D Tool Landmarks with Synthetic Training Data", "authors": "Bram Vanherle, Jeroen Put, Nick Michiels, Frank Van Reeth", "abstract": "In this paper a deep learning architecture is presented that can, in real\ntime, detect the 2D locations of certain landmarks of physical tools, such as a\nhammer or screwdriver. To avoid the labor of manual labeling, the network is\ntrained on synthetically generated data. Training computer vision models on\ncomputer generated images, while still achieving good accuracy on real images,\nis a challenge due to the difference in domain. The proposed method uses an\nadvanced rendering method in combination with transfer learning and an\nintermediate supervision architecture to address this problem. It is shown that\nthe model presented in this paper, named Intermediate Heatmap Model (IHM),\ngeneralizes to real images when trained on synthetic data. To avoid the need\nfor an exact textured 3D model of the tool in question, it is shown that the\nmodel will generalize to an unseen tool when trained on a set of different 3D\nmodels of the same type of tool. IHM is compared to two existing approaches to\nkeypoint detection and it is shown that it outperforms those at detecting tool\nlandmarks, trained on synthetic data.", "journal": "ROBOVIS, 2021, 40-47"}
{"doi": "10.48550/arXiv.2211.09306", "date": "2022-11-17", "title": "Integration of discrete-event dynamics and machining dynamics for machine tool: modeling, analysis and algorithms", "authors": "Mason Ma, Alisa Ren, Christopher Tyler, Jaydeep Karandikar, Michael Gomez, Tony Shi, Tony Schmitz", "abstract": "Machining dynamics research lays a solid foundation for machining operations\nby providing stable combinations of spindle speed and depth of cut.\nFurthermore, machine learning has been applied to predict tool life as a\nfunction of cutting speed. However, the existing research does not consider the\ndiscrete-event dynamics in machine shop, i.e., the machine tool needs to\nprocess a series of parts in queue under various practical production\nrequirements. This paper addresses the integration of discrete-event dynamics\nand machining dynamics to achieve cost savings in machining. We first propose a\nlearning-based cost function for the studied integrated optimization problem of\nmachine tool. The proposed cost function utilizes the predicted tool life under\ndifferent stable cutting speeds for further optimizing speed selection of\nmachine tool to deal with the discrete-event dynamics in machine shop. Then,\naccording to the practical production requirements, we develop several\nmathematical optimization models for the related integrated optimization\nproblems with the consideration of cost, makespan and due date. Numerical\nresults show the effectiveness of our proposed methods and also the potential\nto be used in practice.", "journal": ""}
{"doi": "10.48550/arXiv.1806.05573", "date": "2018-06-14", "title": "Weakly-Supervised Learning for Tool Localization in Laparoscopic Videos", "authors": "Armine Vardazaryan, Didier Mutter, Jacques Marescaux, Nicolas Padoy", "abstract": "Surgical tool localization is an essential task for the automatic analysis of\nendoscopic videos. In the literature, existing methods for tool localization,\ntracking and segmentation require training data that is fully annotated,\nthereby limiting the size of the datasets that can be used and the\ngeneralization of the approaches. In this work, we propose to circumvent the\nlack of annotated data with weak supervision. We propose a deep architecture,\ntrained solely on image level annotations, that can be used for both tool\npresence detection and localization in surgical videos. Our architecture relies\non a fully convolutional neural network, trained end-to-end, enabling us to\nlocalize surgical tools without explicit spatial annotations. We demonstrate\nthe benefits of our approach on a large public dataset, Cholec80, which is\nfully annotated with binary tool presence information and of which 5 videos\nhave been fully annotated with bounding boxes and tool centers for the\nevaluation.", "journal": ""}
{"doi": "10.48550/arXiv.1703.09613", "date": "2017-03-28", "title": "Documenting API Input/Output Examples", "authors": "Siyuan Jiang, Ameer Armaly, Collin McMillan, Qiyu Zhi, Ronald Metoyer", "abstract": "When learning to use an Application Programming Interface (API), programmers\nneed to understand the inputs and outputs (I/O) of the API functions. Current\ndocumentation tools automatically document the static information of I/O, such\nas parameter types and names. What is missing from these tools is dynamic\ninformation, such as I/O examples---actual valid values of inputs that produce\ncertain outputs. In this paper, we demonstrate a prototype toolset we built to\ngenerate I/O examples. Our tool logs I/O values when API functions are\nexecuted, for example in running test suites. Then, the tool puts I/O values\ninto API documents as I/O examples. Our tool has three programs: 1) funcWatch,\nwhich collects I/O values when API developers run test suites, 2) ioSelect,\nwhich selects one I/O example from a set of I/O values, and 3) ioPresent, which\nembeds the I/O examples into documents. In a preliminary evaluation, we used\nour tool to generate four hundred I/O examples for three C libraries: ffmpeg,\nlibssh, and protobuf-c.", "journal": ""}
{"doi": "10.48550/arXiv.2301.04713", "date": "2023-01-11", "title": "How do \"technical\" design-choices made when building algorithmic decision-making tools for criminal justice authorities create constitutional dangers?", "authors": "Karen Yeung, Adam Harkens", "abstract": "This two part paper argues that seemingly \"technical\" choices made by\ndevelopers of machine-learning based algorithmic tools used to inform decisions\nby criminal justice authorities can create serious constitutional dangers,\nenhancing the likelihood of abuse of decision-making power and the scope and\nmagnitude of injustice. Drawing on three algorithmic tools in use, or recently\nused, to assess the \"risk\" posed by individuals to inform how they should be\ntreated by criminal justice authorities, we integrate insights from data\nscience and public law scholarship to show how public law principles and more\nspecific legal duties that are rooted in these principles, are routinely\noverlooked in algorithmic tool-building and implementation. We argue that\ntechnical developers must collaborate closely with public law experts to ensure\nthat if algorithmic decision-support tools are to inform criminal justice\ndecisions, those tools are configured and implemented in a manner that is\ndemonstrably compliant with public law principles and doctrine, including\nrespect for human rights, throughout the tool-building process.", "journal": ""}
{"doi": "10.48550/arXiv.2301.04715", "date": "2023-01-11", "title": "How do \"technical\" design-choices made when building algorithmic decision-making tools for criminal justice authorities create constitutional dangers? Part II", "authors": "Karen Yeung, Adam Harkens", "abstract": "This two-part paper argues that seemingly \"technical\" choices made by\ndevelopers of machine-learning based algorithmic tools used to inform decisions\nby criminal justice authorities can create serious constitutional dangers,\nenhancing the likelihood of abuse of decision-making power and the scope and\nmagnitude of injustice. Drawing on three algorithmic tools in use, or recently\nused, to assess the \"risk\" posed by individuals to inform how they should be\ntreated by criminal justice authorities, we integrate insights from data\nscience and public law scholarship to show how public law principles and more\nspecific legal duties that are rooted in these principles, are routinely\noverlooked in algorithmic tool-building and implementation. We argue that\ntechnical developers must collaborate closely with public law experts to ensure\nthat if algorithmic decision-support tools are to inform criminal justice\ndecisions, those tools are configured and implemented in a manner that is\ndemonstrably compliant with public law principles and doctrine, including\nrespect for human rights, throughout the tool-building process.", "journal": ""}
{"doi": "10.48550/arXiv.2304.13419", "date": "2023-04-26", "title": "Are Explainability Tools Gender Biased? A Case Study on Face Presentation Attack Detection", "authors": "Marco Huber, Meiling Fang, Fadi Boutros, Naser Damer", "abstract": "Face recognition (FR) systems continue to spread in our daily lives with an\nincreasing demand for higher explainability and interpretability of FR systems\nthat are mainly based on deep learning. While bias across demographic groups in\nFR systems has already been studied, the bias of explainability tools has not\nyet been investigated. As such tools aim at steering further development and\nenabling a better understanding of computer vision problems, the possible\nexistence of bias in their outcome can lead to a chain of biased decisions. In\nthis paper, we explore the existence of bias in the outcome of explainability\ntools by investigating the use case of face presentation attack detection. By\nutilizing two different explainability tools on models with different levels of\nbias, we investigate the bias in the outcome of such tools. Our study shows\nthat these tools show clear signs of gender bias in the quality of their\nexplanations.", "journal": ""}
{"doi": "10.48550/arXiv.2311.10775", "date": "2023-11-15", "title": "ToolTalk: Evaluating Tool-Usage in a Conversational Setting", "authors": "Nicholas Farn, Richard Shin", "abstract": "Large language models (LLMs) have displayed massive improvements in reasoning\nand decision-making skills and can hold natural conversations with users. Many\nrecent works seek to augment LLM-based assistants with external tools so they\ncan access private or up-to-date information and carry out actions on behalf of\nusers. To better measure the performance of these assistants, this paper\nintroduces ToolTalk, a benchmark consisting of complex user intents requiring\nmulti-step tool usage specified through dialogue. ToolTalk contains 28 tools\ngrouped into 7 plugins, and includes a complete simulated implementation of\neach tool, allowing for fully automated evaluation of assistants that rely on\nexecution feedback. ToolTalk also emphasizes tools that externally affect the\nworld rather than only tools for referencing or searching information. We\nevaluate GPT-3.5 and GPT-4 on ToolTalk resulting in success rates of 26% and\n50% respectively. Our analysis of the errors reveals three major categories and\nsuggests some future directions for improvement. We release ToolTalk at\nhttps://github.com/microsoft/ToolTalk.", "journal": ""}
{"doi": "10.48550/arXiv.2312.05708", "date": "2023-12-09", "title": "Context Tuning for Retrieval Augmented Generation", "authors": "Raviteja Anantha, Tharun Bethi, Danil Vodianik, Srinivas Chappidi", "abstract": "Large language models (LLMs) have the remarkable ability to solve new tasks\nwith just a few examples, but they need access to the right tools. Retrieval\nAugmented Generation (RAG) addresses this problem by retrieving a list of\nrelevant tools for a given task. However, RAG's tool retrieval step requires\nall the required information to be explicitly present in the query. This is a\nlimitation, as semantic search, the widely adopted tool retrieval method, can\nfail when the query is incomplete or lacks context. To address this limitation,\nwe propose Context Tuning for RAG, which employs a smart context retrieval\nsystem to fetch relevant information that improves both tool retrieval and plan\ngeneration. Our lightweight context retrieval model uses numerical,\ncategorical, and habitual usage signals to retrieve and rank context items. Our\nempirical results demonstrate that context tuning significantly enhances\nsemantic search, achieving a 3.5-fold and 1.5-fold improvement in Recall@K for\ncontext retrieval and tool retrieval tasks respectively, and resulting in an\n11.6% increase in LLM-based planner accuracy. Additionally, we show that our\nproposed lightweight model using Reciprocal Rank Fusion (RRF) with LambdaMART\noutperforms GPT-4 based retrieval. Moreover, we observe context augmentation at\nplan generation, even after tool retrieval, reduces hallucination.", "journal": ""}
{"doi": "10.48550/arXiv.2506.11045", "date": "2025-05-21", "title": "Procedural Environment Generation for Tool-Use Agents", "authors": "Michael Sullivan, Mareike Hartmann, Alexander Koller", "abstract": "Although the power of LLM tool-use agents has ignited a flurry of recent\nresearch in this area, the curation of tool-use training data remains an open\nproblem$-$especially for online RL training. Existing approaches to synthetic\ntool-use data generation tend to be non-interactive, and/or non-compositional.\nWe introduce RandomWorld, a pipeline for the procedural generation of\ninteractive tools and compositional tool-use data. We show that models tuned\nvia SFT and RL on synthetic RandomWorld data improve on a range of tool-use\nbenchmarks, and set the new SoTA for two metrics on the NESTFUL dataset.\nFurther experiments show that downstream performance scales with the amount of\nRandomWorld-generated training data, opening up the possibility of further\nimprovement through the use of entirely synthetic data.", "journal": ""}
{"doi": "10.48550/arXiv.1411.4345", "date": "2014-11-17", "title": "Constructing Strategy of Online Learning in Higher Education: Transaction Cost Economy", "authors": "Yabit Alas, Muhammad Anshari", "abstract": "The online learning tools and management also known as Learning Management\nSystem (LMS) have been adopted by higher education as it allows convenient and\nflexibility in learning process between students and instructors or tutors with\nminimal cost. The adoption of online learning tools in university has allowed\nusers (students and instructors) to interact, share and discuss\nanytime-anywhere conveniently. Many students nowadays rely on online resources\nbased using their mobile devices, substituting traditional learning\ninteractions. Universities need strategy to sustain in providing intensive\ninteractions and spreading word out mouth of good services through online\nlearning tools by focusing on niche markets and creating close relationship\nwith their stakeholders. The study presented in this paper analyses how\nuniversities design best practices in adopting LMS and evaluate its current\nstate for future improvement. In fact, with proper strategies of LMS,\nuniversities have opportunities to sustain their business by offering\ninteresting packages and to improve their services through intensive\ninteractions with their users. In this study, we deploy Transaction Cost\nEconomics (TCE) to understand the change business environment and to construct\na model for higher institution to regulate their scenario on online learning\nstrategies in fast changing and threatening business environment.", "journal": ""}
{"doi": "10.48550/arXiv.2504.03048", "date": "2025-04-03", "title": "LLM Library Learning Fails: A LEGO-Prover Case Study", "authors": "Ian Berlot-Attwell, Frank Rudzicz, Xujie Si", "abstract": "Recent advancements in the coding, reasoning, and tool-using abilities of\nLLMs have spurred interest in library learning (i.e., online learning through\nthe creation, storage, and retrieval of reusable and composable functions,\nknowledge, checklists, or lemmas). Such systems often promise improved task\nperformance through the automatic creation of broadly applicable tools, as well\nas superior computational performance through the caching of reasoning (i.e.,\nthe storage of generated tools). However, we find strong reason to be\nskeptical. We perform a deep dive into one such system, LEGO-Prover, which\npurports to learn reusable lemmas for mathematical reasoning. We find no\nevidence of the direct reuse of learned lemmas, and find evidence against the\nsoft reuse of learned lemmas (i.e., reuse by modifying relevant examples).\nCrucially, we find that LEGO-Prover does not in fact improve over the simple\nbaseline of prompting the model - the improvements in task accuracy vanish once\ncomputational cost is accounted for. Our findings suggest that serious\nmisconceptions exist as to the effectiveness of these techniques, that a\nserious re-examination of the state of LLM-based library learning is required,\nand that we require much stronger standards for evaluation including\nbehavioural analysis and ensuring that an equal computational budget is used\nfor baselines.", "journal": ""}
{"doi": "10.48550/arXiv.2005.05265", "date": "2020-05-11", "title": "Federated Learning and Wireless Communications", "authors": "Zhijin Qin, Geoffrey Ye Li, Hao Ye", "abstract": "Federated learning becomes increasingly attractive in the areas of wireless\ncommunications and machine learning due to its powerful functions and potential\napplications. In contrast to other machine learning tools that require no\ncommunication resources, federated learning exploits communications between the\ncentral server and the distributed local clients to train and optimize a\nmachine learning model. Therefore, how to efficiently assign limited\ncommunication resources to train a federated learning model becomes critical to\nperformance optimization. On the other hand, federated learning, as a brand new\ntool, can potentially enhance the intelligence of wireless networks. In this\narticle, we provide a comprehensive overview on the relationship between\nfederated learning and wireless communications, including basic principle of\nfederated learning, efficient communications for training a federated learning\nmodel, and federated learning for intelligent wireless applications. We also\nidentify some future research challenges and directions at the end of this\narticle.", "journal": ""}
{"doi": "10.48550/arXiv.2201.12465", "date": "2022-01-29", "title": "Flashlight: Enabling Innovation in Tools for Machine Learning", "authors": "Jacob Kahn, Vineel Pratap, Tatiana Likhomanenko, Qiantong Xu, Awni Hannun, Jeff Cai, Paden Tomasello, Ann Lee, Edouard Grave, Gilad Avidov, Benoit Steiner, Vitaliy Liptchinsky, Gabriel Synnaeve, Ronan Collobert", "abstract": "As the computational requirements for machine learning systems and the size\nand complexity of machine learning frameworks increases, essential framework\ninnovation has become challenging. While computational needs have driven recent\ncompiler, networking, and hardware advancements, utilization of those\nadvancements by machine learning tools is occurring at a slower pace. This is\nin part due to the difficulties involved in prototyping new computational\nparadigms with existing frameworks. Large frameworks prioritize machine\nlearning researchers and practitioners as end users and pay comparatively\nlittle attention to systems researchers who can push frameworks forward -- we\nargue that both are equally important stakeholders. We introduce Flashlight, an\nopen-source library built to spur innovation in machine learning tools and\nsystems by prioritizing open, modular, customizable internals and\nstate-of-the-art, research-ready models and training setups across a variety of\ndomains. Flashlight allows systems researchers to rapidly prototype and\nexperiment with novel ideas in machine learning computation and has low\noverhead, competing with and often outperforming other popular machine learning\nframeworks. We see Flashlight as a tool enabling research that can benefit\nwidely used libraries downstream and bring machine learning and systems\nresearchers closer together. Flashlight is available at\nhttps://github.com/flashlight/flashlight .", "journal": ""}
{"doi": "10.48550/arXiv.2305.02555", "date": "2023-05-04", "title": "Should ChatGPT and Bard Share Revenue with Their Data Providers? A New Business Model for the AI Era", "authors": "Dong Zhang", "abstract": "With various AI tools such as ChatGPT becoming increasingly popular, we are\nentering a true AI era. We can foresee that exceptional AI tools will soon reap\nconsiderable profits. A crucial question arise: should AI tools share revenue\nwith their training data providers in additional to traditional stakeholders\nand shareholders? The answer is Yes. Large AI tools, such as large language\nmodels, always require more and better quality data to continuously improve,\nbut current copyright laws limit their access to various types of data. Sharing\nrevenue between AI tools and their data providers could transform the current\nhostile zero-sum game relationship between AI tools and a majority of\ncopyrighted data owners into a collaborative and mutually beneficial one, which\nis necessary to facilitate the development of a virtuous cycle among AI tools,\ntheir users and data providers that drives forward AI technology and builds a\nhealthy AI ecosystem. However, current revenue-sharing business models do not\nwork for AI tools in the forthcoming AI era, since the most widely used metrics\nfor website-based traffic and action, such as clicks, will be replaced by new\nmetrics such as prompts and cost per prompt for generative AI tools. A\ncompletely new revenue-sharing business model, which must be almost independent\nof AI tools and be easily explained to data providers, needs to establish a\nprompt-based scoring system to measure data engagement of each data provider.\nThis paper systematically discusses how to build such a scoring system for all\ndata providers for AI tools based on classification and content similarity\nmodels, and outlines the requirements for AI tools or third parties to build\nit. Sharing revenue with data providers using such a scoring system would\nencourage more data owners to participate in the revenue-sharing program. This\nwill be a utilitarian AI era where all parties benefit.", "journal": ""}
{"doi": "10.48550/arXiv.1610.08854", "date": "2016-10-27", "title": "Tool and Phase recognition using contextual CNN features", "authors": "Manish Sahu, Anirban Mukhopadhyay, Angelika Szengel, Stefan Zachow", "abstract": "A transfer learning method for generating features suitable for surgical\ntools and phase recognition from the ImageNet classification features [1] is\nproposed here. In addition, methods are developed for generating contextual\nfeatures and combining them with time series analysis for final classification\nusing multi-class random forest. The proposed pipeline is tested over the\ntraining and testing datasets of M2CAI16 challenges: tool and phase detection.\nEncouraging results are obtained by leave-one-out cross validation evaluation\non the training dataset.", "journal": ""}
{"doi": "10.48550/arXiv.2006.11330", "date": "2020-06-19", "title": "Pycro-manager: open-source software for integrated microscopy hardware control and image processing", "authors": "Henry Pinkard, Nico Stuurman, Laura Waller", "abstract": "{\\mu}Manager, an open-source microscopy acquisition software, has been an\nessential tool for many microscopy experiments over the past 15 years, but is\nnot easy to use for experiments in which image acquisition and analysis are\nclosely coupled. This is because {\\mu}Manager libraries are written in C++ and\nJava, whereas image processing is increasingly carried out with data science\nand machine learning tools most easily accessible through the Python\nprogramming language. We present Pycro-Manager, a tool that enables rapid\ndevelopment of such experiments, while also providing access to the wealth of\nexisting tools within {\\mu}Manager through Python.", "journal": ""}
{"doi": "10.48550/arXiv.1503.04937", "date": "2015-03-17", "title": "Interactive MCQs as a tool for Knowledge Acquisition", "authors": "Kisor Ray, Saumen Sarkar", "abstract": "Multiple Choice Questions or MCQs are very important for e-learning.\nGenerally, MCQs are used as a tool for the assessment of student performance at\nthe end of their learning sessions. Can MCQs become an important tool in the\nprocess of knowledge acquisition while attending a course? This paper intends\nto find out how MCQs could be used as a tool for the better understanding,\ncoverage as well as knowledge acquisition.", "journal": ""}
{"doi": "10.48550/arXiv.2004.03351", "date": "2020-04-06", "title": "An Image Labeling Tool and Agricultural Dataset for Deep Learning", "authors": "Patrick Wspanialy, Justin Brooks, Medhat Moussa", "abstract": "We introduce a labeling tool and dataset aimed to facilitate computer vision\nresearch in agriculture. The annotation tool introduces novel methods for\nlabeling with a variety of manual, semi-automatic, and fully-automatic tools.\nThe dataset includes original images collected from commercial greenhouses,\nimages from PlantVillage, and images from Google Images. Images were annotated\nwith segmentations for foreground leaf, fruit, and stem instances, and diseased\nleaf area. Labels were in an extended COCO format. In total the dataset\ncontained 10k tomatoes, 7k leaves, 2k stems, and 2k diseased leaf annotations.", "journal": ""}
{"doi": "10.48550/arXiv.2005.08772", "date": "2020-05-18", "title": "Color Visual Illusions: A Statistics-based Computational Model", "authors": "Elad Hirsch, Ayellet Tal", "abstract": "Visual illusions may be explained by the likelihood of patches in real-world\nimages, as argued by input-driven paradigms in Neuro-Science. However, neither\nthe data nor the tools existed in the past to extensively support these\nexplanations. The era of big data opens a new opportunity to study input-driven\napproaches. We introduce a tool that computes the likelihood of patches, given\na large dataset to learn from. Given this tool, we present a model that\nsupports the approach and explains lightness and color visual illusions in a\nunified manner. Furthermore, our model generates visual illusions in natural\nimages, by applying the same tool, reversely.", "journal": ""}
{"doi": "10.48550/arXiv.2112.01516", "date": "2021-12-02", "title": "Ownership and Creativity in Generative Models", "authors": "Omri Avrahami, Bar Tamir", "abstract": "Machine learning generated content such as image artworks, textual poems and\nmusic become prominent in recent years. These tools attract much attention from\nthe media, artists, researchers, and investors. Because these tools are\ndata-driven, they are inherently different than the traditional creative tools\nwhich arises the question - who may own the content that is generated by these\ntools? In this paper we aim to address this question, we start by providing a\nbackground to this problem, raising several candidates that may own the content\nand arguments for each one of them. Then we propose a possible algorithmic\nsolution in the vision-based model's regime. Finally, we discuss the broader\nimplications of this problem.", "journal": ""}
{"doi": "10.48550/arXiv.2303.01839", "date": "2023-03-03", "title": "Automating Constraint-Aware Datapath Optimization using E-Graphs", "authors": "Samuel Coward, George A. Constantinides, Theo Drane", "abstract": "Numerical hardware design requires aggressive optimization, where designers\nexploit branch constraints, creating optimization opportunities that are valid\nonly on a sub-domain of input space. We developed an RTL optimization tool that\nautomatically learns the consequences of conditional branches and exploits that\nknowledge to enable deep optimization. The tool deploys custom built program\nanalysis based on abstract interpretation theory, which when combined with a\ndata-structure known as an e-graph simplifies complex reasoning about program\nproperties. Our tool fully-automatically discovers known floating-point\narchitectures from the computer arithmetic literature and out-performs baseline\nEDA tools, generating up to 33% faster and 41% smaller circuits.", "journal": ""}
{"doi": "10.48550/arXiv.2307.14206", "date": "2023-07-26", "title": "AI and Education: An Investigation into the Use of ChatGPT for Systems Thinking", "authors": "Holger Arndt", "abstract": "This exploratory study investigates the potential of the artificial\nintelligence tool, ChatGPT, to support systems thinking (ST) in various\nsubjects. Using both general and subject specific prompts, the study assesses\nthe accuracy, helpfulness, and reliability of ChatGPT's responses across\ndifferent versions of the tool. The results indicate that ChatGPT can provide\nlargely correct and very helpful responses in various subjects, demonstrating\nits potential as a tool for enhancing ST skills. However, occasional\ninaccuracies highlight the need for users to remain critical of ChatGPT's\nresponses. Despite some limitations, this study suggests that with careful use\nand attention to its idiosyncrasies, ChatGPT can be a valuable tool for\nteaching and learning ST.", "journal": ""}
{"doi": "10.48550/arXiv.2310.16249", "date": "2023-10-24", "title": "A clustering tool for interrogating finite element models based on eigenvectors of graph adjacency", "authors": "Ramaseshan Kannan", "abstract": "This note introduces an unsupervised learning algorithm to debug errors in\nfinite element (FE) simulation models and details how it was productionised.\nThe algorithm clusters degrees of freedom in the FE model using numerical\nproperties of the adjacency of its stiffness matrix. The algorithm has been\ndeployed as a tool called `Model Stability Analysis' tool within the commercial\nstructural FE suite Oasys GSA (www.oasys-software.com/gsa). It has been used\nsuccessfully by end-users for debugging real world FE models and we present\nexamples of the tool in action.", "journal": ""}
{"doi": "10.48550/arXiv.2206.01335", "date": "2022-06-02", "title": "Code Generation Tools (Almost) for Free? A Study of Few-Shot, Pre-Trained Language Models on Code", "authors": "Patrick Barei\u00df, Beatriz Souza, Marcelo d'Amorim, Michael Pradel", "abstract": "Few-shot learning with large-scale, pre-trained language models is a powerful\nway to answer questions about code, e.g., how to complete a given code example,\nor even generate code snippets from scratch. The success of these models raises\nthe question whether they could serve as a basis for building a wide range code\ngeneration tools. Traditionally, such tools are built manually and separately\nfor each task. Instead, few-shot learning may allow to obtain different tools\nfrom a single pre-trained language model by simply providing a few examples or\na natural language description of the expected tool behavior. This paper\nstudies to what extent a state-of-the-art, pre-trained language model of code,\nCodex, may serve this purpose. We consider three code manipulation and code\ngeneration tasks targeted by a range of traditional tools: (i) code mutation;\n(ii) test oracle generation from natural language documentation; and (iii) test\ncase generation. For each task, we compare few-shot learning to a manually\nbuilt tool. Our results show that the model-based tools complement (code\nmutation), are on par (test oracle generation), or even outperform their\nrespective traditionally built tool (test case generation), while imposing far\nless effort to develop them. By comparing the effectiveness of different\nvariants of the model-based tools, we provide insights on how to design an\nappropriate input (\"prompt\") to the model and what influence the size of the\nmodel has. For example, we find that providing a small natural language\ndescription of the code generation task is an easy way to improve predictions.\nOverall, we conclude that few-shot language models are surprisingly effective,\nyet there is still more work to be done, such as exploring more diverse ways of\nprompting and tackling even more involved tasks.", "journal": ""}
{"doi": "10.48550/arXiv.2208.04287", "date": "2022-08-08", "title": "Continual Reinforcement Learning with TELLA", "authors": "Neil Fendley, Cash Costello, Eric Nguyen, Gino Perrotta, Corey Lowman", "abstract": "Training reinforcement learning agents that continually learn across multiple\nenvironments is a challenging problem. This is made more difficult by a lack of\nreproducible experiments and standard metrics for comparing different continual\nlearning approaches. To address this, we present TELLA, a tool for the Test and\nEvaluation of Lifelong Learning Agents. TELLA provides specified, reproducible\ncurricula to lifelong learning agents while logging detailed data for\nevaluation and standardized analysis. Researchers can define and share their\nown curricula over various learning environments or run against a curriculum\ncreated under the DARPA Lifelong Learning Machines (L2M) Program.", "journal": ""}
{"doi": "10.48550/arXiv.2305.00520", "date": "2023-04-30", "title": "The ART of Transfer Learning: An Adaptive and Robust Pipeline", "authors": "Boxiang Wang, Yunan Wu, Chenglong Ye", "abstract": "Transfer learning is an essential tool for improving the performance of\nprimary tasks by leveraging information from auxiliary data resources. In this\nwork, we propose Adaptive Robust Transfer Learning (ART), a flexible pipeline\nof performing transfer learning with generic machine learning algorithms. We\nestablish the non-asymptotic learning theory of ART, providing a provable\ntheoretical guarantee for achieving adaptive transfer while preventing negative\ntransfer. Additionally, we introduce an ART-integrated-aggregating machine that\nproduces a single final model when multiple candidate algorithms are\nconsidered. We demonstrate the promising performance of ART through extensive\nempirical studies on regression, classification, and sparse learning. We\nfurther present a real-data analysis for a mortality study.", "journal": ""}
{"doi": "10.48550/arXiv.1807.11694", "date": "2018-07-31", "title": "Spectrum concentration in deep residual learning: a free probability approach", "authors": "Zenan Ling, Xing He, Robert C. Qiu", "abstract": "We revisit the initialization of deep residual networks (ResNets) by\nintroducing a novel analytical tool in free probability to the community of\ndeep learning. This tool deals with non-Hermitian random matrices, rather than\ntheir conventional Hermitian counterparts in the literature. As a consequence,\nthis new tool enables us to evaluate the singular value spectrum of the\ninput-output Jacobian of a fully-connected deep ResNet for both linear and\nnonlinear cases. With the powerful tool of free probability, we conduct an\nasymptotic analysis of the spectrum on the single-layer case, and then extend\nthis analysis to the multi-layer case of an arbitrary number of layers. In\nparticular, we propose to rescale the classical random initialization by the\nnumber of residual units, so that the spectrum has the order of $O(1)$, when\ncompared with the large width and depth of the network. We empirically\ndemonstrate that the proposed initialization scheme learns at a speed of orders\nof magnitudes faster than the classical ones, and thus attests a strong\npractical relevance of this investigation.", "journal": ""}
{"doi": "10.48550/arXiv.1904.00747", "date": "2019-03-25", "title": "A Novel Pixel-Averaging Technique for Extracting Training Data from a Single Image, Used in ML-Based Image Enlargement", "authors": "Amir Rastar", "abstract": "Size of the training dataset is an important factor in the performance of a\nmachine learning algorithms and tools used in medical image processing are not\nexceptions. Machine learning tools normally require a decent amount of training\ndata before they could efficiently predict a target. For image processing and\ncomputer vision, the number of images determines the validity and reliability\nof the training set. Medical images in some cases, suffer from poor quality and\ninadequate quantity required for a suitable training set. The proposed\nalgorithm in this research obviates the need for large or even small image\ndatasets used in machine learning based image enlargement techniques by\nextracting the required data from a single image. The extracted data was then\nintroduced to a decision tree regressor for upscaling greyscale medical images\nat different zoom levels. Results from the algorithm are relatively acceptable\ncompared to third-party applications and promising for future research. This\ntechnique could be tailored to the requirements of other machine learning tools\nand the results may be improved by further tweaking of the tools\nhyperparameters.", "journal": ""}
{"doi": "10.48550/arXiv.2002.04760", "date": "2020-02-12", "title": "DeepMutation: A Neural Mutation Tool", "authors": "Michele Tufano, Jason Kimko, Shiya Wang, Cody Watson, Gabriele Bavota, Massimiliano Di Penta, Denys Poshyvanyk", "abstract": "Mutation testing can be used to assess the fault-detection capabilities of a\ngiven test suite. To this aim, two characteristics of mutation testing\nframeworks are of paramount importance: (i) they should generate mutants that\nare representative of real faults; and (ii) they should provide a complete tool\nchain able to automatically generate, inject, and test the mutants. To address\nthe first point, we recently proposed an approach using a Recurrent Neural\nNetwork Encoder-Decoder architecture to learn mutants from ~787k faults mined\nfrom real programs. The empirical evaluation of this approach confirmed its\nability to generate mutants representative of real faults. In this paper, we\naddress the second point, presenting DeepMutation, a tool wrapping our deep\nlearning model into a fully automated tool chain able to generate, inject, and\ntest mutants learned from real faults. Video:\nhttps://sites.google.com/view/learning-mutation/deepmutation", "journal": ""}
{"doi": "10.48550/arXiv.2307.04080", "date": "2023-07-09", "title": "Automatic Static Bug Detection for Machine Learning Libraries: Are We There Yet?", "authors": "Nima Shiri harzevili, Jiho Shin, Junjie Wang, Song Wang, Nachiappan Nagappan", "abstract": "Automatic detection of software bugs is a critical task in software security.\nMany static tools that can help detect bugs have been proposed. While these\nstatic bug detectors are mainly evaluated on general software projects call\ninto question their practical effectiveness and usefulness for machine learning\nlibraries. In this paper, we address this question by analyzing five popular\nand widely used static bug detectors, i.e., Flawfinder, RATS, Cppcheck,\nFacebook Infer, and Clang static analyzer on a curated dataset of software bugs\ngathered from four popular machine learning libraries including Mlpack, MXNet,\nPyTorch, and TensorFlow with a total of 410 known bugs. Our research provides a\ncategorization of these tools' capabilities to better understand the strengths\nand weaknesses of the tools for detecting software bugs in machine learning\nlibraries. Overall, our study shows that static bug detectors find a negligible\namount of all bugs accounting for 6/410 bugs (0.01%), Flawfinder and RATS are\nthe most effective static checker for finding software bugs in machine learning\nlibraries. Based on our observations, we further identify and discuss\nopportunities to make the tools more effective and practical.", "journal": ""}
{"doi": "10.48550/arXiv.2310.03618", "date": "2023-10-05", "title": "CLASSify: A Web-Based Tool for Machine Learning", "authors": "Aaron D. Mullen, Samuel E. Armstrong, Jeff Talbert, V. K. Cody Bumgardner", "abstract": "Machine learning classification problems are widespread in bioinformatics,\nbut the technical knowledge required to perform model training, optimization,\nand inference can prevent researchers from utilizing this technology. This\narticle presents an automated tool for machine learning classification problems\nto simplify the process of training models and producing results while\nproviding informative visualizations and insights into the data. This tool\nsupports both binary and multiclass classification problems, and it provides\naccess to a variety of models and methods. Synthetic data can be generated\nwithin the interface to fill missing values, balance class labels, or generate\nentirely new datasets. It also provides support for feature evaluation and\ngenerates explainability scores to indicate which features influence the output\nthe most. We present CLASSify, an open-source tool for simplifying the user\nexperience of solving classification problems without the need for knowledge of\nmachine learning.", "journal": ""}
{"doi": "10.48550/arXiv.2312.09548", "date": "2023-12-15", "title": "Integrating AI and Learning Analytics for Data-Driven Pedagogical Decisions and Personalized Interventions in Education", "authors": "Ramteja Sajja, Yusuf Sermet, David Cwiertny, Ibrahim Demir", "abstract": "This research study explores the conceptualization, development, and\ndeployment of an innovative learning analytics tool, leveraging OpenAI's GPT-4\nmodel to quantify student engagement, map learning progression, and evaluate\ndiverse instructional strategies within an educational context. By analyzing\ncritical data points such as students' stress levels, curiosity, confusion,\nagitation, topic preferences, and study methods, the tool provides a\ncomprehensive view of the learning environment. It also employs Bloom's\ntaxonomy to assess cognitive development based on student inquiries. In\naddition to technical evaluation through synthetic data, feedback from a survey\nof teaching faculty at the University of Iowa was collected to gauge perceived\nbenefits and challenges. Faculty recognized the tool's potential to enhance\ninstructional decision-making through real-time insights but expressed concerns\nabout data security and the accuracy of AI-generated insights. The study\noutlines the design, implementation, and evaluation of the tool, highlighting\nits contributions to educational outcomes, practical integration within\nlearning management systems, and future refinements needed to address privacy\nand accuracy concerns. This research underscores AI's role in shaping\npersonalized, data-driven education.", "journal": ""}
{"doi": "10.48550/arXiv.2403.07501", "date": "2024-03-12", "title": "Detecting Security-Relevant Methods using Multi-label Machine Learning", "authors": "Oshando Johnson, Goran Piskachev, Ranjith Krishnamurthy, Eric Bodden", "abstract": "To detect security vulnerabilities, static analysis tools need to be\nconfigured with security-relevant methods. Current approaches can automatically\nidentify such methods using binary relevance machine learning approaches.\nHowever, they ignore dependencies among security-relevant methods,\nover-generalize and perform poorly in practice. Additionally, users have to\nnevertheless manually configure static analysis tools using the detected\nmethods. Based on feedback from users and our observations, the excessive\nmanual steps can often be tedious, error-prone and counter-intuitive.\n  In this paper, we present Dev-Assist, an IntelliJ IDEA plugin that detects\nsecurity-relevant methods using a multi-label machine learning approach that\nconsiders dependencies among labels. The plugin can automatically generate\nconfigurations for static analysis tools, run the static analysis, and show the\nresults in IntelliJ IDEA. Our experiments reveal that Dev-Assist's machine\nlearning approach has a higher F1-Measure than related approaches. Moreover,\nthe plugin reduces and simplifies the manual effort required when configuring\nand using static analysis tools.", "journal": ""}
{"doi": "10.48550/arXiv.2411.02143", "date": "2024-11-04", "title": "CryptoEL: A Novel Experiential Learning Tool for Enhancing K-12 Cryptography Education", "authors": "Pranathi Rayavaram, Ukaegbu Onyinyechukwu, Maryam Abbasalizadeh, Krishnaa Vellamchetty, Sashank Narain", "abstract": "This paper presents an educational tool designed to enhance cryptography\neducation for K-12 students, utilizing Kolb's Experiential Learning (EL) model\nand engaging visual components. Our tool incorporates the four stages of EL --\nConcrete Experience, Reflective Observation, Abstract Conceptualization, and\nActive Experimentation -- to teach key cryptographic concepts, including\nhashing, symmetric cryptography, and asymmetric cryptography. The learning\nexperience is enriched with real-world simulations, customized AI-based\nconversation agents, video demonstrations, interactive scenarios, and a\nsimplified Python coding terminal focused on cryptography. Targeted at\nbeginners in cybersecurity, the tool encourages independent learning with\nminimal instructor involvement. An evaluation with 51 middle and high school\nstudents showed positive feedback from 93% of participants, who found the\nsimulations, visualizations, AI reflections, scenarios, and coding capabilities\nengaging and conducive to learning. Comprehension surveys indicated a high\nunderstanding of cryptography concepts: hashing (middle school: 89%, high\nschool: 92%), symmetric cryptography (middle school: 93%, high school: 97%),\nand asymmetric cryptography (middle school: 91%, high school: 94%).", "journal": ""}
{"doi": "10.48550/arXiv.2501.11035", "date": "2025-01-19", "title": "From Arabic Text to Puzzles: LLM-Driven Development of Arabic Educational Crosswords", "authors": "Kamyar Zeinalipour, Mohamed Zaky Saad, Marco Maggini, Marco Gori", "abstract": "We present an Arabic crossword puzzle generator from a given text that\nutilizes advanced language models such as GPT-4-Turbo, GPT-3.5-Turbo and\nLlama3-8B-Instruct, specifically developed for educational purposes, this\ninnovative generator leverages a meticulously compiled dataset named\nArabic-Clue-Instruct with over 50,000 entries encompassing text, answers,\nclues, and categories. This dataset is intricately designed to aid in the\ngeneration of pertinent clues linked to specific texts and keywords within\ndefined categories. This project addresses the scarcity of advanced educational\ntools tailored for the Arabic language, promoting enhanced language learning\nand cognitive development. By providing a culturally and linguistically\nrelevant tool, our objective is to make learning more engaging and effective\nthrough gamification and interactivity. Integrating state-of-the-art artificial\nintelligence with contemporary learning methodologies, this tool can generate\ncrossword puzzles from any given educational text, thereby facilitating an\ninteractive and enjoyable learning experience. This tool not only advances\neducational paradigms but also sets a new standard in interactive and cognitive\nlearning technologies. The model and dataset are publicly available.", "journal": ""}
{"doi": "10.48550/arXiv.1904.12462", "date": "2019-04-29", "title": "Deep Learning-Based Video Coding: A Review and A Case Study", "authors": "Dong Liu, Yue Li, Jianping Lin, Houqiang Li, Feng Wu", "abstract": "The past decade has witnessed great success of deep learning technology in\nmany disciplines, especially in computer vision and image processing. However,\ndeep learning-based video coding remains in its infancy. This paper reviews the\nrepresentative works about using deep learning for image/video coding, which\nhas been an actively developing research area since the year of 2015. We divide\nthe related works into two categories: new coding schemes that are built\nprimarily upon deep networks (deep schemes), and deep network-based coding\ntools (deep tools) that shall be used within traditional coding schemes or\ntogether with traditional coding tools. For deep schemes, pixel probability\nmodeling and auto-encoder are the two approaches, that can be viewed as\npredictive coding scheme and transform coding scheme, respectively. For deep\ntools, there have been several proposed techniques using deep learning to\nperform intra-picture prediction, inter-picture prediction, cross-channel\nprediction, probability distribution prediction, transform, post- or in-loop\nfiltering, down- and up-sampling, as well as encoding optimizations. In the\nhope of advocating the research of deep learning-based video coding, we present\na case study of our developed prototype video codec, namely Deep Learning Video\nCoding (DLVC). DLVC features two deep tools that are both based on\nconvolutional neural network (CNN), namely CNN-based in-loop filter (CNN-ILF)\nand CNN-based block adaptive resolution coding (CNN-BARC). Both tools help\nimprove the compression efficiency by a significant margin. With the two deep\ntools as well as other non-deep coding tools, DLVC is able to achieve on\naverage 39.6\\% and 33.0\\% bits saving than HEVC, under random-access and\nlow-delay configurations, respectively. The source code of DLVC has been\nreleased for future researches.", "journal": "ACM Computing Surveys, vol 53, no 1, article no 11, Feb 2020"}
{"doi": "10.48550/arXiv.2208.03143", "date": "2022-08-05", "title": "Deep Learning and Health Informatics for Smart Monitoring and Diagnosis", "authors": "Amin Gasmi", "abstract": "The connection between the design and delivery of health care services using\ninformation technology is known as health informatics. It involves data usage,\nvalidation, and transfer of an integrated medical analysis using neural\nnetworks of multi-layer deep learning techniques to analyze complex data. For\ninstance, Google incorporated ''DeepMind'' health mobile tool that integrates\n\\& leverage medical data needed to enhance professional healthcare delivery to\npatients. Moorfield Eye Hospital London introduced DeepMind Research Algorithms\nwith dozens of retinal scans attributes while DeepMind UCL handled the\nidentification of cancerous tissues using CT \\& MRI Scan tools. Atomise\nanalyzed drugs and chemicals with Deep Learning Neural Networks to identify\naccurate pre-clinical prescriptions. Health informatics makes medical care\nintelligent, interactive, cost-effective, and accessible; especially with DL\napplication tools for detecting the actual cause of diseases. The extensive use\nof neural network tools leads to the expansion of different medical disciplines\nwhich mitigates data complexity and enhances 3-4D overlap images using target\npoint label data detectors that support data augmentation, un-semi-supervised\nlearning, multi-modality and transfer learning architecture. Health science\nover the years focused on artificial intelligence tools for care delivery,\nchronic care management, prevention/wellness, clinical supports, and diagnosis.\nThe outcome of their research leads to cardiac arrest diagnosis through Heart\nSignal Computer-Aided Diagnostic tool (CADX) and other multifunctional deep\nlearning techniques that offer care, diagnosis \\& treatment. Health informatics\nprovides monitored outcomes of human body organs through medical images that\nclassify interstitial lung disease, detects image nodules for reconstruction \\&\ntumor segmentation. The emergent medical research applications gave rise to\nclinical-pathological human-level performing tools for handling Radiological,\nOphthalmological, and Dental diagnosis. This research will evaluate\nmethodologies, Deep learning architectures, approaches, bio-informatics,\nspecified function requirements, monitoring tools, ANN (artificial neural\nnetwork), data labeling \\& annotation algorithms that control data validation,\nmodeling, and diagnosis of different diseases using smart monitoring health\ninformatics applications.", "journal": ""}
{"doi": "10.48550/arXiv.1010.2138", "date": "2010-10-11", "title": "Combiner suivi de l'activite? et partage d'exp\u00e9riences en apprentissage par projet pour les acteurs tuteurs et apprenants", "authors": "Christine Michel, Elise Lavou\u00e9", "abstract": "Our work aims to study tools offered to students and tutors involved in\nface-to-face or blended project- based learning activities. Project-based\nlearning is often applied in the case of complex learning (i.e. which aims at\nmaking learners acquire various linked skills or develop their behaviours). In\ncomparison to traditional learning, this type of learning relies on\nco-development, collective responsibility and co-operation. Learners are the\nprincipal actors of their learning. These trainings rest on rich and complex\norganizations, particularly for tutors, and it is difficult to apply innovative\neducational strategies. Our aim, in a bottom-up approach, is (1) to observe,\naccording to Knowledge Management methods, a course characterized by these\nthree criteria. The observed course concerns project management learning. Its\nobservation allows us (2) to highlight and to analyze the problems encountered\nby the actors (students, tutors, designers) and (3) to propose tools to solve\nor improve them. We particularly study the relevance and the limits of the\nexisting monitoring and experience sharing tools. We finally propose a result\nin the form of the tool MEShaT (Monitoring and Experience Sharing Tool) and end\non the perspectives offered by these researches.", "journal": "7\\`eme Colloque Technologies de l'Information et de la\n  Communication pour l'Enseignement (TICE 2010), Nancy : France (2010)"}
{"doi": "10.48550/arXiv.2301.06975", "date": "2023-01-17", "title": "Vision Based Machine Learning Algorithms for Out-of-Distribution Generalisation", "authors": "Hamza Riaz, Alan F. Smeaton", "abstract": "There are many computer vision applications including object segmentation,\nclassification, object detection, and reconstruction for which machine learning\n(ML) shows state-of-the-art performance. Nowadays, we can build ML tools for\nsuch applications with real-world accuracy. However, each tool works well\nwithin the domain in which it has been trained and developed. Often, when we\ntrain a model on a dataset in one specific domain and test on another unseen\ndomain known as an out of distribution (OOD) dataset, models or ML tools show a\ndecrease in performance. For instance, when we train a simple classifier on\nreal-world images and apply that model on the same classes but with a different\ndomain like cartoons, paintings or sketches then the performance of ML tools\ndisappoints. This presents serious challenges of domain generalisation (DG),\ndomain adaptation (DA), and domain shifting. To enhance the power of ML tools,\nwe can rebuild and retrain models from scratch or we can perform transfer\nlearning. In this paper, we present a comparison study between vision-based\ntechnologies for domain-specific and domain-generalised methods. In this\nresearch we highlight that simple convolutional neural network (CNN) based deep\nlearning methods perform poorly when they have to tackle domain shifting.\nExperiments are conducted on two popular vision-based benchmarks, PACS and\nOffice-Home. We introduce an implementation pipeline for domain generalisation\nmethods and conventional deep learning models. The outcome confirms that\nCNN-based deep learning models show poor generalisation compare to other\nextensive methods.", "journal": ""}
{"doi": "10.48550/arXiv.2402.12867", "date": "2024-02-20", "title": "Towards MLOps: A DevOps Tools Recommender System for Machine Learning System", "authors": "Pir Sami Ullah Shah, Naveed Ahmad, Mirza Omer Beg", "abstract": "Applying DevOps practices to machine learning system is termed as MLOps and\nmachine learning systems evolve on new data unlike traditional systems on\nrequirements. The objective of MLOps is to establish a connection between\ndifferent open-source tools to construct a pipeline that can automatically\nperform steps to construct a dataset, train the machine learning model and\ndeploy the model to the production as well as store different versions of model\nand dataset. Benefits of MLOps is to make sure the fast delivery of the new\ntrained models to the production to have accurate results. Furthermore, MLOps\npractice impacts the overall quality of the software products and is completely\ndependent on open-source tools and selection of relevant open-source tools is\nconsidered as challenged while a generalized method to select an appropriate\nopen-source tools is desirable. In this paper, we present a framework for\nrecommendation system that processes the contextual information (e.g., nature\nof data, type of the data) of the machine learning project and recommends a\nrelevant toolchain (tech-stack) for the operationalization of machine learning\nsystems. To check the applicability of the proposed framework, four different\napproaches i.e., rule-based, random forest, decision trees and k-nearest\nneighbors were investigated where precision, recall and f-score is measured,\nthe random forest out classed other approaches with highest f-score value of\n0.66.", "journal": ""}
{"doi": "10.48550/arXiv.1606.01348", "date": "2016-06-04", "title": "A Conceptual Framework to Assess the Effectiveness of Rubric Tool", "authors": "Phil Smith, Mohan John Blooma, Jayan Kurian", "abstract": "Rubrics are being used in a wide variety of disciplines in higher education\nto evaluate assessments and provide feedback to students. Rubrics are\ntraditionally implemented as paper-based table format to grade assessments and\nprovide feedback. The advancement of technology has integrated Rubric Tool into\nLearning Management Systems (LMS) like Blackboard to facilitate online marking.\nIn this study, we aim to evaluate the effectiveness of Rubric Tool. We propose\nan integrated conceptual framework using various theories from extant\nliterature to assess the effectiveness of Rubric Tool. Further, this framework\nhas been applied in an undergraduate course at the Asian campus of an\nAustralian International University. This is a research in progress paper which\nadds to the literature on integrating information systems theory, learning\ntheory, political philosophy and communication theory to examine the\neffectiveness of Rubric Tool in student learning.", "journal": ""}
{"doi": "10.48550/arXiv.1712.06034", "date": "2017-12-16", "title": "Study on the Best Uses of Technology in Support of Project-Based Learning", "authors": "James Taylor", "abstract": "Project-Based Learning (PBL) is a teaching technique in which authentic,\nreal-world projects are used as the primary vehicle to drive the student's\nlearning experience. This technique has been found to be very effective, but\nits overall adoption rate is relatively low, in part due to teachers'\nunfamiliarity with how to best use technology to successfully implement it.\nThis research study involved a comprehensive survey of supportive technology\ntools, as well as secondary survey research from students and teachers with\nactual experience in PBL. The goal was to determine which types of technology\ntools were most supportive of PBL. Overall, the study found that teachers and\nstudents are mostly aligned with regards to the importance of technology and\nthe effectiveness of various types of tools. Tools which fostered collaboration\namongst teacher and students were ultimately deemed the most effective, but\ncontent-development and assessment tools were also found to be particularly\nhelpful.", "journal": ""}
{"doi": "10.48550/arXiv.2406.19614", "date": "2024-06-28", "title": "A Survey on Data Quality Dimensions and Tools for Machine Learning", "authors": "Yuhan Zhou, Fengjiao Tu, Kewei Sha, Junhua Ding, Haihua Chen", "abstract": "Machine learning (ML) technologies have become substantial in practically all\naspects of our society, and data quality (DQ) is critical for the performance,\nfairness, robustness, safety, and scalability of ML models. With the large and\ncomplex data in data-centric AI, traditional methods like exploratory data\nanalysis (EDA) and cross-validation (CV) face challenges, highlighting the\nimportance of mastering DQ tools. In this survey, we review 17 DQ evaluation\nand improvement tools in the last 5 years. By introducing the DQ dimensions,\nmetrics, and main functions embedded in these tools, we compare their strengths\nand limitations and propose a roadmap for developing open-source DQ tools for\nML. Based on the discussions on the challenges and emerging trends, we further\nhighlight the potential applications of large language models (LLMs) and\ngenerative AI in DQ evaluation and improvement for ML. We believe this\ncomprehensive survey can enhance understanding of DQ in ML and could drive\nprogress in data-centric AI. A complete list of the literature investigated in\nthis survey is available on GitHub at:\nhttps://github.com/haihua0913/awesome-dq4ml.", "journal": ""}
{"doi": "10.48550/arXiv.2502.01700", "date": "2025-02-03", "title": "EdgeMark: An Automation and Benchmarking System for Embedded Artificial Intelligence Tools", "authors": "Mohammad Amin Hasanpour, Mikkel Kirkegaard, Xenofon Fafoutis", "abstract": "The integration of artificial intelligence (AI) into embedded devices, a\nparadigm known as embedded artificial intelligence (eAI) or tiny machine\nlearning (TinyML), is transforming industries by enabling intelligent data\nprocessing at the edge. However, the many tools available in this domain leave\nresearchers and developers wondering which one is best suited to their needs.\nThis paper provides a review of existing eAI tools, highlighting their\nfeatures, trade-offs, and limitations. Additionally, we introduce EdgeMark, an\nopen-source automation system designed to streamline the workflow for deploying\nand benchmarking machine learning (ML) models on embedded platforms. EdgeMark\nsimplifies model generation, optimization, conversion, and deployment while\npromoting modularity, reproducibility, and scalability. Experimental\nbenchmarking results showcase the performance of widely used eAI tools,\nincluding TensorFlow Lite Micro (TFLM), Edge Impulse, Ekkono, and Renesas eAI\nTranslator, across a wide range of models, revealing insights into their\nrelative strengths and weaknesses. The findings provide guidance for\nresearchers and developers in selecting the most suitable tools for specific\napplication requirements, while EdgeMark lowers the barriers to adoption of eAI\ntechnologies.", "journal": ""}
{"doi": "10.48550/arXiv.2502.03635", "date": "2025-02-05", "title": "UX Challenges in Implementing an Interactive B2B Customer Segmentation Tool", "authors": "Muhammad Raees, Vassilis-Javed Khan, Konstantinos Papangelis", "abstract": "In our effort to implement an interactive customer segmentation tool for a\nglobal manufacturing company, we identified user experience (UX) challenges\nwith technical implications. The main challenge relates to domain users'\neffort, in our case sales experts, to interpret the clusters produced by an\nunsupervised Machine Learning (ML) algorithm, for creating a customer\nsegmentation. An additional challenge is what sort of interactions should such\na tool support to enable meaningful interpretations of the output of clustering\nmodels. In this case study, we describe what we learned from implementing an\nInteractive Machine Learning (IML) prototype to address such UX challenges. We\nleverage a multi-year real-world dataset and domain experts' feedback from a\nglobal manufacturing company to evaluate our tool. We report what we found to\nbe effective and wish to inform designers of IML systems in the context of\ncustomer segmentation and other related unsupervised ML tools.", "journal": ""}
{"doi": "10.48550/arXiv.2501.16945", "date": "2025-01-28", "title": "ToolFactory: Automating Tool Generation by Leveraging LLM to Understand REST API Documentations", "authors": "Xinyi Ni, Qiuyang Wang, Yukun Zhang, Pengyu Hong", "abstract": "LLM-based tool agents offer natural language interfaces, enabling users to\nseamlessly interact with computing services. While REST APIs are valuable\nresources for building such agents, they must first be transformed into\nAI-compatible tools. Automatically generating AI-compatible tools from REST API\ndocuments can greatly streamline tool agent development and minimize user\nlearning curves. However, API documentation often suffers from a lack of\nstandardization, inconsistent schemas, and incomplete information. To address\nthese issues, we developed \\textbf{ToolFactory}, an open-source pipeline for\nautomating tool generation from unstructured API documents. To enhance the\nreliability of the developed tools, we implemented an evaluation method to\ndiagnose errors. Furthermore, we built a knowledge base of verified tools,\nwhich we leveraged to infer missing information from poorly documented APIs. We\ndeveloped the API Extraction Benchmark, comprising 167 API documents and 744\nendpoints in various formats, and designed a JSON schema to annotate them. This\nannotated dataset was utilized to train and validate ToolFactory. The\nexperimental results highlight the effectiveness of ToolFactory. We also\ndemonstrated ToolFactory by creating a domain-specific AI agent for\nglycomaterials research. ToolFactory exhibits significant potential for\nfacilitating the seamless integration of scientific REST APIs into AI\nworkflows.", "journal": ""}
{"doi": "10.48550/arXiv.2105.04556", "date": "2021-05-05", "title": "TANGO: Commonsense Generalization in Predicting Tool Interactions for Mobile Manipulators", "authors": "Shreshth Tuli, Rajas Bansal, Rohan Paul, Mausam", "abstract": "Robots assisting us in factories or homes must learn to make use of objects\nas tools to perform tasks, e.g., a tray for carrying objects. We consider the\nproblem of learning commonsense knowledge of when a tool may be useful and how\nits use may be composed with other tools to accomplish a high-level task\ninstructed by a human. We introduce a novel neural model, termed TANGO, for\npredicting task-specific tool interactions, trained using demonstrations from\nhuman teachers instructing a virtual robot. TANGO encodes the world state,\ncomprising objects and symbolic relationships between them, using a graph\nneural network. The model learns to attend over the scene using knowledge of\nthe goal and the action history, finally decoding the symbolic action to\nexecute. Crucially, we address generalization to unseen environments where some\nknown tools are missing, but alternative unseen tools are present. We show that\nby augmenting the representation of the environment with pre-trained embeddings\nderived from a knowledge-base, the model can generalize effectively to novel\nenvironments. Experimental results show a 60.5-78.9% absolute improvement over\nthe baseline in predicting successful symbolic plans in unseen settings for a\nsimulated mobile manipulator.", "journal": ""}
{"doi": "10.48550/arXiv.2009.10993", "date": "2020-09-23", "title": "Deep Learning-Based Reconstruction of Interventional Tools from Four X-Ray Projections for Tomographic Interventional Guidance", "authors": "Elias Eulig, Joscha Maier, Michael Knaup, N. Robert Bennett, Klaus H\u00f6rndler, Adam S. Wang, Marc Kachelrie\u00df", "abstract": "Image guidance for minimally invasive interventions is usually performed by\nacquiring fluoroscopic images using a C-arm system. However, the projective\ndata provide only limited information about the spatial structure and position\nof interventional tools such as stents, guide wires or coils. In this work we\npropose a deep learning-based pipeline for real-time tomographic\n(four-dimensional) interventional guidance at acceptable dose levels. In the\nfirst step, interventional tools are extracted from four cone-beam CT\nprojections using a deep convolutional neural network (CNN). These projections\nare then reconstructed and fed into a second CNN, which maps this highly\nundersampled reconstruction to a segmentation of the interventional tools. Our\npipeline is capable of reconstructing interventional tools from only four x-ray\nprojections without the need for a patient prior with very high accuracy.\nTherefore, the proposed approach is capable of overcoming the drawbacks of\ntoday's interventional guidance and could enable the development of new\nminimally invasive radiological interventions by providing full spatiotemporal\ninformation about the interventional tools.", "journal": "Eulig, E., et al. Deep learning-based reconstruction of\n  interventional tools and devices from four X-ray projections for tomographic\n  interventional guidance. Medical Physics. 2021; 48: 5837-5850"}
{"doi": "10.48550/arXiv.2310.16390", "date": "2023-10-25", "title": "Evaluating Pre-trained Language Models for Repairing API Misuses", "authors": "Ting Zhang, Ivana Clairine Irsan, Ferdian Thung, David Lo, Asankhaya Sharma, Lingxiao Jiang", "abstract": "API misuses often lead to software bugs, crashes, and vulnerabilities. While\nseveral API misuse detectors have been proposed, there are no automatic repair\ntools specifically designed for this purpose. In a recent study,\ntest-suite-based automatic program repair (APR) tools were found to be\nineffective in repairing API misuses. Still, since the study focused on\nnon-learning-aided APR tools, it remains unknown whether learning-aided APR\ntools are capable of fixing API misuses. In recent years, pre-trained language\nmodels (PLMs) have succeeded greatly in many natural language processing tasks.\nThere is a rising interest in applying PLMs to APR. However, there has not been\nany study that investigates the effectiveness of PLMs in repairing API misuse.\n  To fill this gap, we conduct a comprehensive empirical study on 11\nlearning-aided APR tools, which include 9 of the state-of-the-art\ngeneral-purpose PLMs and two APR tools. We evaluate these models with an\nAPI-misuse repair dataset, consisting of two variants. Our results show that\nPLMs perform better than the studied APR tools in repairing API misuses. Among\nthe 9 pre-trained models tested, CodeT5 is the best performer in the exact\nmatch. We also offer insights and potential exploration directions for future\nresearch.", "journal": ""}
{"doi": "10.48550/arXiv.2504.06766", "date": "2025-04-09", "title": "FamilyTool: A Multi-hop Personalized Tool Use Benchmark", "authors": "Yuxin Wang, Yiran Guo, Yining Zheng, Zhangyue Yin, Shuo Chen, Jie Yang, Jiajun Chen, Yuan Li, Xuanjing Huang, Xipeng Qiu", "abstract": "The integration of tool learning with Large Language Models (LLMs) has\nexpanded their capabilities in handling complex tasks by leveraging external\ntools. However, existing benchmarks for tool learning inadequately address\ncritical real-world personalized scenarios, particularly those requiring\nmulti-hop reasoning and inductive knowledge adaptation in dynamic environments.\nTo bridge this gap, we introduce FamilyTool, a novel benchmark grounded in a\nfamily-based knowledge graph (KG) that simulates personalized, multi-hop tool\nuse scenarios. FamilyTool, including base and extended datasets, challenges\nLLMs with queries spanning from 1 to 4 relational hops (e.g., inferring\nfamilial connections and preferences) and 2 to 6 hops respectively, and\nincorporates an inductive KG setting where models must adapt to unseen user\npreferences and relationships without re-training, a common limitation in prior\napproaches that compromises generalization. We further propose KGETool: a\nsimple KG-augmented evaluation pipeline to systematically assess LLMs' tool use\nability in these settings. Experiments reveal significant performance gaps in\nstate-of-the-art LLMs, with accuracy dropping sharply as hop complexity\nincreases and inductive scenarios exposing severe generalization deficits.\nThese findings underscore the limitations of current LLMs in handling\npersonalized, evolving real-world contexts and highlight the urgent need for\nadvancements in tool-learning frameworks. FamilyTool serves as a critical\nresource for evaluating and advancing LLM agents' reasoning, adaptability, and\nscalability in complex, dynamic environments. Code and dataset are available at\n\\href{https://github.com/yxzwang/FamilyTool}{https://github.com/yxzwang/FamilyTool}.", "journal": ""}
{"doi": "10.48550/arXiv.2504.21520", "date": "2025-04-30", "title": "Padding Matters -- Exploring Function Detection in PE Files", "authors": "Raphael Springer, Alexander Schmitz, Artur Leinweber, Tobias Urban, Christian Dietrich", "abstract": "Function detection is a well-known problem in binary analysis. While previous\nresearch has primarily focused on Linux/ELF, Windows/PE binaries have been\noverlooked or only partially considered. This paper introduces FuncPEval, a new\ndataset for Windows x86 and x64 PE files, featuring Chromium and the Conti\nransomware, along with ground truth data for 1,092,820 function starts.\nUtilizing FuncPEval, we evaluate five heuristics-based (Ghidra, IDA, Nucleus,\nrev.ng, SMDA) and three machine-learning-based (DeepDi, RNN, XDA) function\nstart detection tools. Among the tested tools, IDA achieves the highest\nF1-score (98.44%) for Chromium x64, while DeepDi closely follows (97%) but\nstands out as the fastest by a significant margin. Working towards\nexplainability, we examine the impact of padding between functions on the\ndetection results. Our analysis shows that all tested tools, except rev.ng, are\nsusceptible to randomized padding. The randomized padding significantly\ndiminishes the effectiveness for the RNN, XDA, and Nucleus. Among the\nlearning-based tools, DeepDi exhibits the least sensitivity and demonstrates\noverall the fastest performance, while Nucleus is the most adversely affected\namong non-learning-based tools. In addition, we improve the recurrent neural\nnetwork (RNN) proposed by Shin et al. and enhance the XDA tool, increasing the\nF1-score by approximately 10%.", "journal": ""}
{"doi": "10.48550/arXiv.1910.05522", "date": "2019-10-12", "title": "RiPPLE: A Crowdsourced Adaptive Platform for Recommendation of Learning Activities", "authors": "Hassan Khosravi, Kirsty Kitto, Joseph Jay Williams", "abstract": "This paper presents a platform called RiPPLE (Recommendation in Personalised\nPeer-Learning Environments) that recommends personalized learning activities to\nstudents based on their knowledge state from a pool of crowdsourced learning\nactivities that are generated by educators and the students themselves. RiPPLE\nintegrates insights from crowdsourcing, learning sciences, and adaptive\nlearning, aiming to narrow the gap between these large bodies of research while\nproviding a practical platform-based implementation that instructors can easily\nuse in their courses. This paper provides a design overview of RiPPLE, which\ncan be employed as a standalone tool or embedded into any learning management\nsystem (LMS) or online platform that supports the Learning Tools\nInteroperability (LTI) standard. The platform has been evaluated based on a\npilot in an introductory course with 453 students at The University of\nQueensland. Initial results suggest that the use of the \\name platform led to\nmeasurable learning gains and that students perceived the platform as\nbeneficially supporting their learning.", "journal": ""}
{"doi": "10.48550/arXiv.2304.01316", "date": "2023-04-03", "title": "Matched Machine Learning: A Generalized Framework for Treatment Effect Inference With Learned Metrics", "authors": "Marco Morucci, Cynthia Rudin, Alexander Volfovsky", "abstract": "We introduce Matched Machine Learning, a framework that combines the\nflexibility of machine learning black boxes with the interpretability of\nmatching, a longstanding tool in observational causal inference.\nInterpretability is paramount in many high-stakes application of causal\ninference. Current tools for nonparametric estimation of both average and\nindividualized treatment effects are black-boxes that do not allow for human\nauditing of estimates. Our framework uses machine learning to learn an optimal\nmetric for matching units and estimating outcomes, thus achieving the\nperformance of machine learning black-boxes, while being interpretable. Our\ngeneral framework encompasses several published works as special cases. We\nprovide asymptotic inference theory for our proposed framework, enabling users\nto construct approximate confidence intervals around estimates of both\nindividualized and average treatment effects. We show empirically that\ninstances of Matched Machine Learning perform on par with black-box machine\nlearning methods and better than existing matching methods for similar\nproblems. Finally, in our application we show how Matched Machine Learning can\nbe used to perform causal inference even when covariate data are highly\ncomplex: we study an image dataset, and produce high quality matches and\nestimates of treatment effects.", "journal": ""}
{"doi": "10.48550/arXiv.1903.11854", "date": "2019-03-28", "title": "Knowledge Management in Medium-Sized Software Consulting Companies: An investigation of Intranet-based Knowledge Management Tools for Knowledge Cartography and Knowledge Repositories for Learning Software Organisations", "authors": "Torgeir Dings\u00f8yr", "abstract": "Companies that develop software have a pressure from customers to deliver\nbetter solutions, and to deliver solutions faster and cheaper. Many researchers\nhave worked with suggestions on how to improve the development process;\nsoftware process improvement. As software development is a very knowledge\nintensive task, both researchers and industry have recently turned their\nattention to knowledge management as a means to improve software development.\nThis often involves developing technical tools, which many companies have spent\nresources on. But the tools are often not used in practise by developers and\nmanagers in the companies, and it is often unknown if the tools improve how\nknowledge is managed. In order to build efficient knowledge management tools,\nwe need a better understanding of how the tools that exist are applied and used\nin software development. We present and analyse eight case studies of knowledge\nmanagement initiatives from the literature. We found evidence of improved\nsoftware quality, reduced development costs and evidence of a better working\nenvironment for developers as a result of these initiatives. Further, we\nexamine success criteria in knowledge management codification initiatives,\nbased on Intranet tools in medium-sized software companies. In addition, we\ninvestigate how knowledge management tools are used for different purposes by\ndifferent groups of users in two software consulting companies. They use tools\nboth as support for personalization and codification strategies. The consulting\ncompanies are two medium-sized Norwegian companies with 40 and 150 employees,\nwhich work in development projects that lasts from a few weeks to several\nyears.", "journal": ""}
{"doi": "10.48550/arXiv.2210.00831", "date": "2022-09-30", "title": "NLP-based classification of software tools for metagenomics sequencing data analysis into EDAM semantic annotation", "authors": "Kaoutar Daoud Hiri, Matja\u017e Hren, Toma\u017e Curk", "abstract": "Motivation: The rapid growth of metagenomics sequencing data makes\nmetagenomics increasingly dependent on computational and statistical methods\nfor fast and efficient analysis. Consequently, novel analysis tools for\nbig-data metagenomics are constantly emerging. One of the biggest challenges\nfor researchers occurs in the analysis planning stage: selecting the most\nsuitable metagenomics software tool to gain valuable insights from sequencing\ndata. The building process of data analysis pipelines is often laborious and\ntime-consuming since it requires a deep and critical understanding of how to\napply a particular tool to complete a specified metagenomics task.\n  Results: We have addressed this challenge by using machine learning methods\nto develop a classification system of metagenomics software tools into 13\nclasses (11 semantic annotations of EDAM and two virus-specific classes) based\non the descriptions of the tools. We trained three classifiers (Naive Bayes,\nLogistic Regression, and Random Forest) using 15 text feature extraction\ntechniques (TF-IDF, GloVe, BERT-based models, and others). The manually curated\ndataset includes 224 software tools and contains text from the abstract and the\nmethods section of the tools' publications. The best classification\nperformance, with an Area Under the Precision-Recall Curve score of 0.85, is\nachieved using Logistic regression, BioBERT for text embedding, and text from\nabstracts only. The proposed system provides accurate and unified\nidentification of metagenomics data analysis tools and tasks, which is a\ncrucial step in the construction of metagenomics data analysis pipelines.", "journal": ""}
{"doi": "10.48550/arXiv.2303.00303", "date": "2023-03-01", "title": "The Inversive Relationship Between Bugs and Patches: An Empirical Study", "authors": "Jinhan Kim, Jongchan Park, Shin Yoo", "abstract": "Software bugs pose an ever-present concern for developers, and patching such\nbugs requires a considerable amount of costs through complex operations. In\ncontrast, introducing bugs can be an effortless job, in that even a simple\nmutation can easily break the Program Under Test (PUT). Existing research has\nconsidered these two opposed activities largely separately, either trying to\nautomatically generate realistic patches to help developers, or to find\nrealistic bugs to simulate and prevent future defects. Despite the fundamental\ndifferences between them, however, we hypothesise that they do not\nsyntactically differ from each other when considered simply as code changes. To\nexamine this assumption systematically, we investigate the relationship between\npatches and buggy commits, both generated manually and automatically, using a\nclustering and pattern analysis. A large scale empirical evaluation reveals\nthat up to 70% of patches and faults can be clustered together based on the\nsimilarity between their lexical patterns; further, 44% of the code changes can\nbe abstracted into the identical change patterns. Moreover, we investigate\nwhether code mutation tools can be used as Automated Program Repair (APR)\ntools, and APR tools as code mutation tools. In both cases, the inverted use of\nmutation and APR tools can perform surprisingly well, or even better, when\ncompared to their original, intended uses. For example, 89% of patches found by\nSequenceR, a deep learning based APR tool, can also be found by its inversion,\ni.e., a model trained with faults and not patches. Similarly, real fault\ncoupling study of mutants reveals that TBar, a template based APR tool, can\ngenerate 14% and 3% more fault couplings than traditional mutation tools, PIT\nand Major respectively, when used as a mutation tool.", "journal": ""}
{"doi": "10.48550/arXiv.2310.01045", "date": "2023-10-02", "title": "Tool-Augmented Reward Modeling", "authors": "Lei Li, Yekun Chai, Shuohuan Wang, Yu Sun, Hao Tian, Ningyu Zhang, Hua Wu", "abstract": "Reward modeling (a.k.a., preference modeling) is instrumental for aligning\nlarge language models with human preferences, particularly within the context\nof reinforcement learning from human feedback (RLHF). While conventional reward\nmodels (RMs) have exhibited remarkable scalability, they oft struggle with\nfundamental functionality such as arithmetic computation, code execution, and\nfactual lookup. In this paper, we propose a tool-augmented preference modeling\napproach, named Themis, to address these limitations by empowering RMs with\naccess to external environments, including calculators and search engines. This\napproach not only fosters synergy between tool utilization and reward grading\nbut also enhances interpretive capacity and scoring reliability. Our study\ndelves into the integration of external tools into RMs, enabling them to\ninteract with diverse external sources and construct task-specific tool\nengagement and reasoning traces in an autoregressive manner. We validate our\napproach across a wide range of domains, incorporating seven distinct external\ntools. Our experimental results demonstrate a noteworthy overall improvement of\n17.7% across eight tasks in preference ranking. Furthermore, our approach\noutperforms Gopher 280B by 7.3% on TruthfulQA task in zero-shot evaluation. In\nhuman evaluations, RLHF trained with Themis attains an average win rate of 32%\nwhen compared to baselines across four distinct tasks. Additionally, we provide\na comprehensive collection of tool-related RM datasets, incorporating data from\nseven distinct tool APIs, totaling 15,000 instances. We have made the code,\ndata, and model checkpoints publicly available to facilitate and inspire\nfurther research\nadvancements\\footnote{\\url{https://github.com/ernie-research/Tool-Augmented-Reward-Model}}.", "journal": ""}
{"doi": "10.48550/arXiv.2409.13202", "date": "2024-09-20", "title": "CITI: Enhancing Tool Utilizing Ability in Large Language Models without Sacrificing General Performance", "authors": "Yupu Hao, Pengfei Cao, Zhuoran Jin, Huanxuan Liao, Yubo Chen, Kang Liu, Jun Zhao", "abstract": "Tool learning enables the Large Language Models (LLMs) to interact with the\nexternal environment by invoking tools, enriching the accuracy and capability\nscope of LLMs. However, previous works predominantly focus on improving model's\ntool-utilizing accuracy and the ability to generalize to new, unseen tools,\nexcessively forcing LLMs to adjust specific tool-invoking pattern without\nconsidering the harm to model's general performance. This deviates from the\nactual applications and original intention of integrating tools to enhance\nmodel. To tackle this problem, we dissect the capability trade-offs by\nexamining the hidden representation changes and the gradient-based importance\nscore of model's components. Based on the analysis result, we propose a\nComponent Importance-based Tool-utilizing ability Injection method (CITI).\nAccording to the gradient-based importance score of different components, it\nalleviates the capability conflicts caused by fine-tuning process by applying\ndistinct training strategies to different components. CITI applies\nMixture-Of-LoRA (MOLoRA) for important components. Meanwhile, it fine-tunes the\nparameters of few components deemed less important in the backbone of the LLM,\nwhile keeping other parameters frozen. CITI can effectively enhance the model's\ntool-utilizing capability without excessively compromising its general\nperformance. Experimental results demonstrate that our approach achieves\noutstanding performance across a range of evaluation metrics.", "journal": ""}
{"doi": "10.48550/arXiv.2303.16140", "date": "2023-03-09", "title": "Machine learning tools to improve nonlinear modeling parameters of RC columns", "authors": "Hamid Khodadadi Koodiani, Elahe Jafari, Arsalan Majlesi, Mohammad Shahin, Adolfo Matamoros, Adel Alaeddini", "abstract": "Modeling parameters are essential to the fidelity of nonlinear models of\nconcrete structures subjected to earthquake ground motions, especially when\nsimulating seismic events strong enough to cause collapse. This paper addresses\ntwo of the most significant barriers to improving nonlinear modeling provisions\nin seismic evaluation standards using experimental data sets: identifying the\nmost likely mode of failure of structural components, and implementing data\nfitting techniques capable of recognizing interdependencies between input\nparameters and nonlinear relationships between input parameters and model\noutputs. Machine learning tools in the Scikit-learn and Pytorch libraries were\nused to calibrate equations and black-box numerical models for nonlinear\nmodeling parameters (MP) a and b of reinforced concrete columns defined in the\nASCE 41 and ACI 369.1 standards, and to estimate their most likely mode of\nfailure. It was found that machine learning regression models and machine\nlearning black-boxes were more accurate than current provisions in the ACI\n369.1/ASCE 41 Standards. Among the regression models, Regularized Linear\nRegression was the most accurate for estimating MP a, and Polynomial Regression\nwas the most accurate for estimating MP b. The two black-box models evaluated,\nnamely the Gaussian Process Regression and the Neural Network (NN), provided\nthe most accurate estimates of MPs a and b. The NN model was the most accurate\nmachine learning tool of all evaluated. A multi-class classification tool from\nthe Scikit-learn machine learning library correctly identified column mode of\nfailure with 79% accuracy for rectangular columns and with 81% accuracy for\ncircular columns, a substantial improvement over the classification rules in\nASCE 41-13.", "journal": ""}
{"doi": "10.48550/arXiv.2405.04861", "date": "2024-05-08", "title": "Insights into Deep Learning Refactoring: Bridging the Gap Between Practices and Expectations", "authors": "SiQi Wang, Xing Hu, Bei Wang, WenXin Yao, Xin Xia, XingYu Wang", "abstract": "With the rapid development of deep learning, the implementation of intricate\nalgorithms and substantial data processing have become standard elements of\ndeep learning projects. As a result, the code has become progressively complex\nas the software evolves, which is difficult to maintain and understand.\nExisting studies have investigated the impact of refactoring on software\nquality within traditional software. However, the insight of code refactoring\nin the context of deep learning is still unclear. This study endeavors to fill\nthis knowledge gap by empirically examining the current state of code\nrefactoring in deep learning realm, and practitioners' views on refactoring. We\nfirst manually analyzed the commit history of five popular and well-maintained\ndeep learning projects (e.g., PyTorch). We mined 4,921 refactoring practices in\nhistorical commits and measured how different types and elements of refactoring\noperations are distributed and found that refactoring operation types'\ndistribution in deep learning projects is different from it in traditional Java\nsoftware. We then surveyed 159 practitioners about their views of code\nrefactoring in deep learning projects and their expectations of current\nrefactoring tools. The result of the survey showed that refactoring research\nand the development of related tools in the field of deep learning are crucial\nfor improving project maintainability and code quality, and that current\nrefactoring tools do not adequately meet the needs of practitioners. Lastly, we\nprovided our perspective on the future advancement of refactoring tools and\noffered suggestions for developers' development practices.", "journal": ""}
{"doi": "10.48550/arXiv.1703.02910", "date": "2017-03-08", "title": "Deep Bayesian Active Learning with Image Data", "authors": "Yarin Gal, Riashat Islam, Zoubin Ghahramani", "abstract": "Even though active learning forms an important pillar of machine learning,\ndeep learning tools are not prevalent within it. Deep learning poses several\ndifficulties when used in an active learning setting. First, active learning\n(AL) methods generally rely on being able to learn and update models from small\namounts of data. Recent advances in deep learning, on the other hand, are\nnotorious for their dependence on large amounts of data. Second, many AL\nacquisition functions rely on model uncertainty, yet deep learning methods\nrarely represent such model uncertainty. In this paper we combine recent\nadvances in Bayesian deep learning into the active learning framework in a\npractical way. We develop an active learning framework for high dimensional\ndata, a task which has been extremely challenging so far, with very sparse\nexisting literature. Taking advantage of specialised models such as Bayesian\nconvolutional neural networks, we demonstrate our active learning techniques\nwith image data, obtaining a significant improvement on existing active\nlearning approaches. We demonstrate this on both the MNIST dataset, as well as\nfor skin cancer diagnosis from lesion images (ISIC2016 task).", "journal": ""}
{"doi": "10.48550/arXiv.1403.6006", "date": "2014-03-24", "title": "A Tablet Based Learning Environment", "authors": "Hong Cao", "abstract": "Pen computing tools such as Tablet PC, Tablet Monitors and its various\nsupporting software tool offer another dimension to enhance our today's\ndigitally integrated and connected classroom learning environment. This paper\nfirst reviews the various state-of-the-art pen-computing hardware and software\nthat have been applied in the classroom setting to introduce student-centric\nlearning, collaboration and making annotations and designing classroom\nactivities easier. We then propose a new classroom environment which is fully\nequipped with Tablet devices and the supporting software tools for the goals of\n1) easy electronic ink annotations with least constraints; 2) enhanced active\nlearning with timely feedback; 3) enhanced student collaborations and 4)\nlecture recording. The classroom has been put into practical teaching and\nlearning environment as a pilot project in our higher learning environment.\nAfter overcoming the initial learning curves, the environment received positive\nfeedbacks from the teaching faculties as well as from the students.", "journal": ""}
{"doi": "10.48550/arXiv.2007.09339", "date": "2020-07-18", "title": "ML Privacy Meter: Aiding Regulatory Compliance by Quantifying the Privacy Risks of Machine Learning", "authors": "Sasi Kumar Murakonda, Reza Shokri", "abstract": "When building machine learning models using sensitive data, organizations\nshould ensure that the data processed in such systems is adequately protected.\nFor projects involving machine learning on personal data, Article 35 of the\nGDPR mandates it to perform a Data Protection Impact Assessment (DPIA). In\naddition to the threats of illegitimate access to data through security\nbreaches, machine learning models pose an additional privacy risk to the data\nby indirectly revealing about it through the model predictions and parameters.\nGuidances released by the Information Commissioner's Office (UK) and the\nNational Institute of Standards and Technology (US) emphasize on the threat to\ndata from models and recommend organizations to account for and estimate these\nrisks to comply with data protection regulations. Hence, there is an immediate\nneed for a tool that can quantify the privacy risk to data from models.\n  In this paper, we focus on this indirect leakage about training data from\nmachine learning models. We present ML Privacy Meter, a tool that can quantify\nthe privacy risk to data from models through state of the art membership\ninference attack techniques. We discuss how this tool can help practitioners in\ncompliance with data protection regulations, when deploying machine learning\nmodels.", "journal": ""}
{"doi": "10.48550/arXiv.2303.12395", "date": "2023-03-22", "title": "Open Learning Analytics: A Systematic Literature Review and Future Perspectives", "authors": "Arham Muslim, Mohamed Amine Chatti, Mouadh Guesmi", "abstract": "Open Learning Analytics (OLA) is an emerging research area that aims at\nimproving learning efficiency and effectiveness in lifelong learning\nenvironments. OLA employs multiple methods to draw value from a wide range of\neducational data coming from various learning environments and contexts in\norder to gain insight into the learning processes of different stakeholders. As\nthe research field is still relatively young, only a few technical platforms\nare available and a common understanding of requirements is lacking. This paper\nprovides a systematic literature review of tools available in the learning\nanalytics literature from 2011-2019 with an eye on their support for openness.\n137 tools from nine academic databases are collected to form the base for this\nreview. The analysis of selected tools is performed based on four dimensions,\nnamely 'Data, Environments, Context (What?)', 'Stakeholders (Who?)',\n'Objectives (Why?)', and 'Methods (How?)'. Moreover, five well-known OLA\nframeworks available in the community are systematically compared. The review\nconcludes by eliciting the main requirements for an effective OLA platform and\nby identifying key challenges and future lines of work in this emerging field.", "journal": ""}
{"doi": "10.48550/arXiv.2404.04201", "date": "2024-04-05", "title": "V-Star: Learning Visibly Pushdown Grammars from Program Inputs", "authors": "Xiaodong Jia, Gang Tan", "abstract": "Accurate description of program inputs remains a critical challenge in the\nfield of programming languages. Active learning, as a well-established field,\nachieves exact learning for regular languages. We offer an innovative grammar\ninference tool, V-Star, based on the active learning of visibly pushdown\nautomata. V-Star deduces nesting structures of program input languages from\nsample inputs, employing a novel inference mechanism based on nested patterns.\nThis mechanism identifies token boundaries and converts languages such as XML\ndocuments into VPLs. We then adapted Angluin's L-Star, an exact learning\nalgorithm, for VPA learning, which improves the precision of our tool. Our\nevaluation demonstrates that V-Star effectively and efficiently learns a\nvariety of practical grammars, including S-Expressions, JSON, and XML, and\noutperforms other state-of-the-art tools.", "journal": ""}
{"doi": "10.48550/arXiv.2410.01392", "date": "2024-10-02", "title": "Causal Inference Tools for a Better Evaluation of Machine Learning", "authors": "Micha\u00ebl Soumm", "abstract": "We present a comprehensive framework for applying rigorous statistical\ntechniques from econometrics to analyze and improve machine learning systems.\nWe introduce key statistical methods such as Ordinary Least Squares (OLS)\nregression, Analysis of Variance (ANOVA), and logistic regression, explaining\ntheir theoretical foundations and practical applications in machine learning\nevaluation. The document serves as a guide for researchers and practitioners,\ndetailing how these techniques can provide deeper insights into model behavior,\nperformance, and fairness. We cover the mathematical principles behind each\nmethod, discuss their assumptions and limitations, and provide step-by-step\ninstructions for their implementation. The paper also addresses how to\ninterpret results, emphasizing the importance of statistical significance and\neffect size. Through illustrative examples, we demonstrate how these tools can\nreveal subtle patterns and interactions in machine learning models that are not\napparent from traditional evaluation metrics. By connecting the fields of\neconometrics and machine learning, this work aims to equip readers with\npowerful analytical tools for more rigorous and comprehensive evaluation of AI\nsystems. The framework presented here contributes to developing more robust,\ninterpretable, and fair machine learning technologies.", "journal": ""}
{"doi": "10.48550/arXiv.2502.13281", "date": "2025-02-18", "title": "Beyond Training: Social Dynamics of AI Adoption in Industry", "authors": "Riya Sahni, Lydia B. Chilton", "abstract": "While organizations continue to invest in AI tools like M365 Copilot, little\nis known about how individual employees engage with these technologies once\ndeployed. This study examines M365 Copilot adoption behaviors among a group of\n10 experienced users across many industries in the United States. Findings\nreveal a strong preference for informal learning methods over structured\ntraining. Even though 9 out of 10 participants acknowledged that formal\ntraining for Copilot tools would be useful, 7 out of 10 stated that they\nignored the Copilot onboarding videos provided to them, citing reasons such as\ntime constraints, preference for self-guided learning, or reliance on external\nresources like ChatGPT. No participants used formal training as their primary\nlearning method. Instead, experiential learning (trial and error, 8\nparticipants) and social learning (peer discussions, 6 participants) emerged as\ndominant learning strategies. We discuss opportunities for promoting social\nlearning of AI tools in the workplace.", "journal": ""}
{"doi": "10.48550/arXiv.1904.02679", "date": "2019-04-04", "title": "Visualizing Attention in Transformer-Based Language Representation Models", "authors": "Jesse Vig", "abstract": "We present an open-source tool for visualizing multi-head self-attention in\nTransformer-based language representation models. The tool extends earlier work\nby visualizing attention at three levels of granularity: the attention-head\nlevel, the model level, and the neuron level. We describe how each of these\nviews can help to interpret the model, and we demonstrate the tool on the BERT\nmodel and the OpenAI GPT-2 model. We also present three use cases for analyzing\nGPT-2: detecting model bias, identifying recurring patterns, and linking\nneurons to model behavior.", "journal": ""}
{"doi": "10.48550/arXiv.2003.05048", "date": "2020-03-11", "title": "Auditing ML Models for Individual Bias and Unfairness", "authors": "Songkai Xue, Mikhail Yurochkin, Yuekai Sun", "abstract": "We consider the task of auditing ML models for individual bias/unfairness. We\nformalize the task in an optimization problem and develop a suite of\ninferential tools for the optimal value. Our tools permit us to obtain\nasymptotic confidence intervals and hypothesis tests that cover the\ntarget/control the Type I error rate exactly. To demonstrate the utility of our\ntools, we use them to reveal the gender and racial biases in Northpointe's\nCOMPAS recidivism prediction instrument.", "journal": ""}
{"doi": "10.48550/arXiv.2008.02778", "date": "2020-08-06", "title": "Mixed-Initiative Level Design with RL Brush", "authors": "Omar Delarosa, Hang Dong, Mindy Ruan, Ahmed Khalifa, Julian Togelius", "abstract": "This paper introduces RL Brush, a level-editing tool for tile-based games\ndesigned for mixed-initiative co-creation. The tool uses\nreinforcement-learning-based models to augment manual human level-design\nthrough the addition of AI-generated suggestions. Here, we apply RL Brush to\ndesigning levels for the classic puzzle game Sokoban. We put the tool online\nand tested it in 39 different sessions. The results show that users using the\nAI suggestions stay around longer and their created levels on average are more\nplayable and more complex than without.", "journal": ""}
{"doi": "10.48550/arXiv.2105.05012", "date": "2021-05-11", "title": "Robotic Assistant Agent for Student and Machine Co-Learning on AI-FML Practice with AIoT Application", "authors": "Chang-Shing Lee, Mei-Hui Wang, Zong-Han Ciou, Rin-Pin Chang, Chun-Hao Tsai, Shen-Chien Chen, Tzong-Xiang Huang, Eri Sato-Shimokawara, Toru Yamaguchi", "abstract": "In this paper, the Robotic Assistant Agent for student and machine\nco-learning on AI-FML practice with AIoT application is presented. The\nstructure of AI-FML contains three parts, including fuzzy logic, neural\nnetwork, and evolutionary computation. Besides, the Robotic Assistant Agent\n(RAA) can assist students and machines in co-learning English and AI-FML\npractice based on the robot Kebbi Air and AIoT-FML learning tool. Since Sept.\n2019, we have introduced an Intelligent Speaking English Assistant (ISEA) App\nand AI-FML platform to English and computer science learning classes at two\nelementary schools in Taiwan. We use the collected English-learning data to\ntrain a predictive regression model based on students' monthly examination\nscores. In Jan. 2021, we further combined the developed AI-FML platform with a\nnovel AIoT-FML learning tool to enhance students' interests in learning English\nand AI-FML with basic hands-on practice. The proposed RAA is responsible for\nreasoning students' learning performance and showing the results on the\nAIoT-FML learning tool after communicating with the AI-FML platform. The\nexperimental results and the collection of students' feedback show that this\nkind of learning model is popular with elementary-school and high-school\nstudents, and the learning performance of elementary-school students is\nimproved.", "journal": ""}
{"doi": "10.48550/arXiv.2109.01183", "date": "2021-09-02", "title": "roadscene2vec: A Tool for Extracting and Embedding Road Scene-Graphs", "authors": "Arnav Vaibhav Malawade, Shih-Yuan Yu, Brandon Hsu, Harsimrat Kaeley, Anurag Karra, Mohammad Abdullah Al Faruque", "abstract": "Recently, road scene-graph representations used in conjunction with graph\nlearning techniques have been shown to outperform state-of-the-art deep\nlearning techniques in tasks including action classification, risk assessment,\nand collision prediction. To enable the exploration of applications of road\nscene-graph representations, we introduce roadscene2vec: an open-source tool\nfor extracting and embedding road scene-graphs. The goal of roadscene2vec is to\nenable research into the applications and capabilities of road scene-graphs by\nproviding tools for generating scene-graphs, graph learning models to generate\nspatio-temporal scene-graph embeddings, and tools for visualizing and analyzing\nscene-graph-based methodologies. The capabilities of roadscene2vec include (i)\ncustomized scene-graph generation from either video clips or data from the\nCARLA simulator, (ii) multiple configurable spatio-temporal graph embedding\nmodels and baseline CNN-based models, (iii) built-in functionality for using\ngraph and sequence embeddings for risk assessment and collision prediction\napplications, (iv) tools for evaluating transfer learning, and (v) utilities\nfor visualizing scene-graphs and analyzing the explainability of graph learning\nmodels. We demonstrate the utility of roadscene2vec for these use cases with\nexperimental results and qualitative evaluations for both graph learning models\nand CNN-based models. roadscene2vec is available at\nhttps://github.com/AICPS/roadscene2vec.", "journal": ""}
{"doi": "10.48550/arXiv.2308.00107", "date": "2023-07-23", "title": "Validation of a Zero-Shot Learning Natural Language Processing Tool for Data Abstraction from Unstructured Healthcare Data", "authors": "Basil Kaufmann, Dallin Busby, Chandan Krushna Das, Neeraja Tillu, Mani Menon, Ashutosh K. Tewari, Michael A. Gorin", "abstract": "Objectives: To describe the development and validation of a zero-shot\nlearning natural language processing (NLP) tool for abstracting data from\nunstructured text contained within PDF documents, such as those found within\nelectronic health records. Materials and Methods: A data abstraction tool based\non the GPT-3.5 model from OpenAI was developed and compared to three physician\nhuman abstractors in terms of time to task completion and accuracy for\nabstracting data on 14 unique variables from a set of 199 de-identified radical\nprostatectomy pathology reports. The reports were processed by the software\ntool in vectorized and scanned formats to establish the impact of optical\ncharacter recognition on data abstraction. The tool was assessed for\nsuperiority for data abstraction speed and non-inferiority for accuracy.\nResults: The human abstractors required a mean of 101s per report for data\nabstraction, with times varying from 15 to 284 s. In comparison, the software\ntool required a mean of 12.8 s to process the vectorized reports and a mean of\n15.8 to process the scanned reports (P < 0.001). The overall accuracies of the\nthree human abstractors were 94.7%, 97.8%, and 96.4% for the combined set of\n2786 datapoints. The software tool had an overall accuracy of 94.2% for the\nvectorized reports, proving to be non-inferior to the human abstractors at a\nmargin of -10% ($\\alpha$=0.025). The tool had a slightly lower accuracy of\n88.7% using the scanned reports, proving to be non-inferiority to 2 out of 3\nhuman abstractors. Conclusion: The developed zero-shot learning NLP tool\naffords researchers comparable levels of accuracy to that of human abstractors,\nwith significant time savings benefits. Because of the lack of need for\ntask-specific model training, the developed tool is highly generalizable and\ncan be used for a wide variety of data abstraction tasks, even outside the\nfield of medicine.", "journal": ""}
{"doi": "10.48550/arXiv.2111.12423", "date": "2021-11-24", "title": "xFuzz: Machine Learning Guided Cross-Contract Fuzzing", "authors": "Yinxing Xue, Jiaming Ye, Wei Zhang, Jun Sun, Lei Ma, Haijun Wang, Jianjun Zhao", "abstract": "Smart contract transactions are increasingly interleaved by cross-contract\ncalls. While many tools have been developed to identify a common set of\nvulnerabilities, the cross-contract vulnerability is overlooked by existing\ntools. Cross-contract vulnerabilities are exploitable bugs that manifest in the\npresence of more than two interacting contracts. Existing methods are however\nlimited to analyze a maximum of two contracts at the same time. Detecting\ncross-contract vulnerabilities is highly non-trivial. With multiple interacting\ncontracts, the search space is much larger than that of a single contract. To\naddress this problem, we present xFuzz, a machine learning guided smart\ncontract fuzzing framework. The machine learning models are trained with novel\nfeatures (e.g., word vectors and instructions) and are used to filter likely\nbenign program paths. Comparing with existing static tools, machine learning\nmodel is proven to be more robust, avoiding directly adopting manually-defined\nrules in specific tools. We compare xFuzz with three state-of-the-art tools on\n7,391 contracts. xFuzz detects 18 exploitable cross-contract vulnerabilities,\nof which 15 vulnerabilities are exposed for the first time. Furthermore, our\napproach is shown to be efficient in detecting non-cross-contract\nvulnerabilities as well -- using less than 20% time as that of other fuzzing\ntools, xFuzz detects twice as many vulnerabilities.", "journal": ""}
{"doi": "10.48550/arXiv.1804.01557", "date": "2018-04-04", "title": "Qualit\u00e4tsma\u00dfe bin\u00e4rer Klassifikationen im Bereich kriminalprognostischer Instrumente der vierten Generation", "authors": "Tobias D. Krafft", "abstract": "This master's thesis discusses an important issue regarding how algorithmic\ndecision making (ADM) is used in crime forecasting. In America forecasting\ntools are widely used by judiciary systems for making decisions about risk\noffenders based on criminal justice for risk offenders. By making use of such\ntools, the judiciary relies on ADM in order to make error free judgement on\noffenders. For this purpose, one of the quality measures for machine learning\ntechniques which is widly used, the $AUC$ (area under curve), is compared to\nand contrasted for results with the $PPV_k$ (positive predictive value).\nKeeping in view the criticality of judgement along with a high dependency on\ntools offering ADM, it is necessary to evaluate risk tools that aid in decision\nmaking based on algorithms. In this methodology, such an evaluation is\nconducted by implementing a common machine learning approach called binary\nclassifier, as it determines the binary outcome of the underlying juristic\nquestion. This thesis showed that the $PPV_k$ (positive predictive value)\ntechnique models the decision of judges much better than the $AUC$. Therefore,\nthis research has investigated whether there exists a classifier for which the\n$PPV_k$ deviates from $AUC$ by a large proportion. It could be shown that the\ndeviation can rise up to 0.75. In order to test this deviation on an already in\nused Classifier, data from the fourth generation risk assement tool COMPAS was\nused. The result were were quite alarming as the two measures derivate from\neach other by 0.48. In this study, the risk assessment evaluation of the\nforecasting tools was successfully conducted, carefully reviewed and examined.\nAdditionally, it is also discussed whether such systems used for the purpose of\nmaking decisions should be socially accepted or not.", "journal": ""}
{"doi": "10.48550/arXiv.2312.01093", "date": "2023-12-02", "title": "Predicting Postoperative Nausea And Vomiting Using Machine Learning: A Model Development and Validation Study", "authors": "Maxim Glebov, Teddy Lazebnik, Boris Orkin, Haim Berkenstadt, Svetlana Bunimovich-Mendrazitsky", "abstract": "Background: Postoperative nausea and vomiting (PONV) is a frequently observed\ncomplication in patients undergoing surgery under general anesthesia. Moreover,\nit is a frequent cause of distress and dissatisfaction during the early\npostoperative period. The tools used for predicting PONV at present have not\nyielded satisfactory results. Therefore, prognostic tools for the prediction of\nearly and delayed PONV were developed in this study with the aim of achieving\nsatisfactory predictive performance.\n  Methods: The retrospective data of adult patients admitted to the\npost-anesthesia care unit after undergoing surgical procedures under general\nanesthesia at the Sheba Medical Center, Israel, between September 1, 2018, and\nSeptember 1, 2023, were used in this study. An ensemble model of machine\nlearning algorithms trained on the data of 54848 patients was developed. The\nk-fold cross-validation method was used followed by splitting the data to train\nand test sets that optimally preserve the sociodemographic features of the\npatients, such as age, sex, and smoking habits, using the Bee Colony algorithm.\n  Findings: Among the 54848 patients, early and delayed PONV were observed in\n2706 (4.93%) and 8218 (14.98%) patients, respectively. The proposed PONV\nprediction tools could correctly predict early and delayed PONV in 84.0% and\n77.3% of cases, respectively, outperforming the second-best PONV prediction\ntool (Koivuranta score) by 13.4% and 12.9%, respectively. Feature importance\nanalysis revealed that the performance of the proposed prediction tools aligned\nwith previous clinical knowledge, indicating their utility.\n  Interpretation: The machine learning-based tools developed in this study\nenabled improved PONV prediction, thereby facilitating personalized care and\nimproved patient outcomes.", "journal": ""}
{"doi": "10.48550/arXiv.2401.11366", "date": "2024-01-21", "title": "A Multivocal Literature Review on the Benefits and Limitations of Automated Machine Learning Tools", "authors": "Kelly Azevedo, Luigi Quaranta, Fabio Calefato, Marcos Kalinowski", "abstract": "Context. Advancements in Machine Learning (ML) are revolutionizing every\napplication domain, driving unprecedented transformations and fostering\ninnovation. However, despite these advances, several organizations are\nexperiencing friction in the adoption of ML-based technologies, mainly due to\nthe shortage of ML professionals. In this context, Automated Machine Learning\n(AutoML) techniques have been presented as a promising solution to democratize\nML adoption. Objective. We aim to provide an overview of the evidence on the\nbenefits and limitations of using AutoML tools. Method. We conducted a\nmultivocal literature review, which allowed us to identify 54 sources from the\nacademic literature and 108 sources from the grey literature reporting on\nAutoML benefits and limitations. We extracted reported benefits and limitations\nfrom the papers and applied thematic analysis. Results. We identified 18\nbenefits and 25 limitations. Concerning the benefits, we highlight that AutoML\ntools can help streamline the core steps of ML workflows, namely data\npreparation, feature engineering, model construction, and hyperparameter\ntuning, with concrete benefits on model performance, efficiency, and\nscalability. In addition, AutoML empowers both novice and experienced data\nscientists, promoting ML accessibility. On the other hand, we highlight several\nlimitations that may represent obstacles to the widespread adoption of AutoML.\nFor instance, AutoML tools may introduce barriers to transparency and\ninteroperability, exhibit limited flexibility for complex scenarios, and offer\ninconsistent coverage of the ML workflow. Conclusions. The effectiveness of\nAutoML in facilitating the adoption of machine learning by users may vary\ndepending on the tool and the context in which it is used. As of today, AutoML\ntools are used to increase human expertise rather than replace it, and, as\nsuch, they require skilled users.", "journal": ""}
{"doi": "10.48550/arXiv.2403.10013", "date": "2024-03-15", "title": "LyZNet: A Lightweight Python Tool for Learning and Verifying Neural Lyapunov Functions and Regions of Attraction", "authors": "Jun Liu, Yiming Meng, Maxwell Fitzsimmons, Ruikun Zhou", "abstract": "In this paper, we describe a lightweight Python framework that provides\nintegrated learning and verification of neural Lyapunov functions for stability\nanalysis. The proposed tool, named LyZNet, learns neural Lyapunov functions\nusing physics-informed neural networks (PINNs) to solve Zubov's equation and\nverifies them using satisfiability modulo theories (SMT) solvers. What\ndistinguishes this tool from others in the literature is its ability to provide\nverified regions of attraction close to the domain of attraction. This is\nachieved by encoding Zubov's partial differential equation (PDE) into the PINN\napproach. By embracing the non-convex nature of the underlying optimization\nproblems, we demonstrate that in cases where convex optimization, such as\nsemidefinite programming, fails to capture the domain of attraction, our neural\nnetwork framework proves more successful. The tool also offers automatic\ndecomposition of coupled nonlinear systems into a network of low-dimensional\nsubsystems for compositional verification. We illustrate the tool's usage and\neffectiveness with several numerical examples, including both non-trivial\nlow-dimensional nonlinear systems and high-dimensional systems. The repository\nof the tool can be found at https://git.uwaterloo.ca/hybrid-systems-lab/lyznet.", "journal": ""}
{"doi": "10.48550/arXiv.1602.05619", "date": "2016-02-17", "title": "Cognitive Issues in Learning Advanced Physics: An Example from Quantum Mechanics", "authors": "Chandralekha Singh, Guangtian Zhu", "abstract": "We are investigating cognitive issues in learning quantum mechanics in order\nto develop effective teaching and learning tools. The analysis of cognitive\nissues is particularly important for bridging the gap between the quantitative\nand conceptual aspects of quantum mechanics and for ensuring that the learning\ntools help students build a robust knowledge structure. We discuss the\ncognitive aspects of quantum mechanics that are similar or different from those\nof introductory physics and their implications for developing strategies to\nhelp students develop a good grasp of quantum mechanics.", "journal": "Proceedings of the Physics Education Research Conference, AIP\n  Conf. Proc., Melville, New York 1179, 63-66 (2009)"}
{"doi": "10.48550/arXiv.1603.03108", "date": "2016-03-10", "title": "Interactive learning tutorials on quantum mechanics", "authors": "Chandralekha Singh", "abstract": "We discuss the development and evaluation of quantum interactive learning\ntutorials (QuILTs) which are suitable for undergraduate courses in quantum\nmechanics. QuILTs are based on the investigation of student difficulties in\nlearning quantum physics. They exploit computer-based visualization tools and\nhelp students build links between the formal and conceptual aspects of quantum\nphysics without compromising the technical content. They can be used both as\nsupplements to lectures or as a self-study tool.", "journal": "Am. J. Phys., 76(4), 400-405, (2008)"}
{"doi": "10.48550/arXiv.1906.10991", "date": "2019-06-26", "title": "Verifying Robustness of Gradient Boosted Models", "authors": "Gil Einziger, Maayan Goldstein, Yaniv Sa'ar, Itai Segall", "abstract": "Gradient boosted models are a fundamental machine learning technique.\nRobustness to small perturbations of the input is an important quality measure\nfor machine learning models, but the literature lacks a method to prove the\nrobustness of gradient boosted models. This work introduces VeriGB, a tool for\nquantifying the robustness of gradient boosted models. VeriGB encodes the model\nand the robustness property as an SMT formula, which enables state of the art\nverification tools to prove the model's robustness. We extensively evaluate\nVeriGB on publicly available datasets and demonstrate a capability for\nverifying large models. Finally, we show that some model configurations tend to\nbe inherently more robust than others.", "journal": ""}
{"doi": "10.48550/arXiv.2209.11123", "date": "2022-09-22", "title": "Modern Machine Learning Tools for Monitoring and Control of Industrial Processes: A Survey", "authors": "R. Bhushan Gopaluni, Aditya Tulsyan, Benoit Chachuat, Biao Huang, Jong Min Lee, Faraz Amjad, Seshu Kumar Damarla, Jong Woo Kim, Nathan P. Lawrence", "abstract": "Over the last ten years, we have seen a significant increase in industrial\ndata, tremendous improvement in computational power, and major theoretical\nadvances in machine learning. This opens up an opportunity to use modern\nmachine learning tools on large-scale nonlinear monitoring and control\nproblems. This article provides a survey of recent results with applications in\nthe process industry.", "journal": ""}
{"doi": "10.48550/arXiv.2402.15278", "date": "2024-02-23", "title": "Economic and Financial Learning with Artificial Intelligence: A Mixed-Methods Study on ChatGPT", "authors": "Holger Arndt", "abstract": "In the evolving landscape of digital education, chatbots have emerged as\npotential game-changers, promising personalized and adaptive learning\nexperiences. This research undertook an in-depth exploration of ChatGPT's\npotential as an educational tool, focusing on user perceptions, experiences and\nlearning outcomes. Through a mixed-methods approach, a diverse group of 102\nparticipants engaged with ChatGPT, providing insights pre- and postinteraction.\nThe study reveals a notable positive shift in perceptions after exposure,\nunderscoring the efficacy of ChatGPT. However, challenges such as prompting\neffectiveness and information accuracy emerged as pivotal concerns. Introducing\nthe concept of 'AI-learning-competence', this study lays the groundwork for\nfuture research, emphasizing the need for formal training and pedagogical\nintegration of AI tools.", "journal": ""}
{"doi": "10.48550/arXiv.2410.11221", "date": "2024-10-15", "title": "Multi-objective Reinforcement Learning: A Tool for Pluralistic Alignment", "authors": "Peter Vamplew, Conor F Hayes, Cameron Foale, Richard Dazeley, Hadassah Harland", "abstract": "Reinforcement learning (RL) is a valuable tool for the creation of AI\nsystems. However it may be problematic to adequately align RL based on scalar\nrewards if there are multiple conflicting values or stakeholders to be\nconsidered. Over the last decade multi-objective reinforcement learning (MORL)\nusing vector rewards has emerged as an alternative to standard, scalar RL. This\npaper provides an overview of the role which MORL can play in creating\npluralistically-aligned AI.", "journal": ""}
{"doi": "10.48550/arXiv.1006.1138", "date": "2010-06-06", "title": "Online Learning via Sequential Complexities", "authors": "Alexander Rakhlin, Karthik Sridharan, Ambuj Tewari", "abstract": "We consider the problem of sequential prediction and provide tools to study\nthe minimax value of the associated game. Classical statistical learning theory\nprovides several useful complexity measures to study learning with i.i.d. data.\nOur proposed sequential complexities can be seen as extensions of these\nmeasures to the sequential setting. The developed theory is shown to yield\nprecise learning guarantees for the problem of sequential prediction. In\nparticular, we show necessary and sufficient conditions for online learnability\nin the setting of supervised learning. Several examples show the utility of our\nframework: we can establish learnability without having to exhibit an explicit\nonline learning algorithm.", "journal": ""}
{"doi": "10.48550/arXiv.1711.03343", "date": "2017-11-09", "title": "Analysis of Dropout in Online Learning", "authors": "Kazuyuki Hara", "abstract": "Deep learning is the state-of-the-art in fields such as visual object\nrecognition and speech recognition. This learning uses a large number of layers\nand a huge number of units and connections. Therefore, overfitting is a serious\nproblem with it, and the dropout which is a kind of regularization tool is\nused. However, in online learning, the effect of dropout is not well known.\nThis paper presents our investigation on the effect of dropout in online\nlearning. We analyzed the effect of dropout on convergence speed near the\nsingular point. Our results indicated that dropout is effective in online\nlearning. Dropout tends to avoid the singular point for convergence speed near\nthat point.", "journal": "IEICE Technical Report IBIS2017-61"}
{"doi": "10.48550/arXiv.2202.13800", "date": "2022-02-28", "title": "Differential equation and probability inspired graph neural networks for latent variable learning", "authors": "Zhuangwei Shi", "abstract": "Probabilistic theory and differential equation are powerful tools for the\ninterpretability and guidance of the design of machine learning models,\nespecially for illuminating the mathematical motivation of learning latent\nvariable from observation. Subspace learning maps high-dimensional features on\nlow-dimensional subspace to capture efficient representation. Graphs are widely\napplied for modeling latent variable learning problems, and graph neural\nnetworks implement deep learning architectures on graphs. Inspired by\nprobabilistic theory and differential equations, this paper conducts notes and\nproposals about graph neural networks to solve subspace learning problems by\nvariational inference and differential equation.", "journal": ""}
{"doi": "10.48550/arXiv.2106.05842", "date": "2021-06-03", "title": "Causality in Neural Networks -- An Extended Abstract", "authors": "Abbavaram Gowtham Reddy", "abstract": "Causal reasoning is the main learning and explanation tool used by humans. AI\nsystems should possess causal reasoning capabilities to be deployed in the real\nworld with trust and reliability. Introducing the ideas of causality to machine\nlearning helps in providing better learning and explainable models.\nExplainability, causal disentanglement are some important aspects of any\nmachine learning model. Causal explanations are required to believe in a\nmodel's decision and causal disentanglement learning is important for transfer\nlearning applications. We exploit the ideas of causality to be used in deep\nlearning models to achieve better and causally explainable models that are\nuseful in fairness, disentangled representation, etc.", "journal": ""}
{"doi": "10.48550/arXiv.1802.01168", "date": "2018-02-04", "title": "Machine Learning vs. Rules and Out-of-the-Box vs. Retrained: An Evaluation of Open-Source Bibliographic Reference and Citation Parsers", "authors": "Dominika Tkaczyk, Andrew Collins, Paraic Sheridan, Joeran Beel", "abstract": "Bibliographic reference parsing refers to extracting machine-readable\nmetadata, such as the names of the authors, the title, or journal name, from\nbibliographic reference strings. Many approaches to this problem have been\nproposed so far, including regular expressions, knowledge bases and supervised\nmachine learning. Many open source reference parsers based on various\nalgorithms are also available. In this paper, we apply, evaluate and compare\nten reference parsing tools in a specific business use case. The tools are\nAnystyle-Parser, Biblio, CERMINE, Citation, Citation-Parser, GROBID, ParsCit,\nPDFSSA4MET, Reference Tagger and Science Parse, and we compare them in both\ntheir out-of-the-box versions and versions tuned to the project-specific data.\nAccording to our evaluation, the best performing out-of-the-box tool is GROBID\n(F1 0.89), followed by CERMINE (F1 0.83) and ParsCit (F1 0.75). We also found\nthat even though machine learning-based tools and tools based on rules or\nregular expressions achieve on average similar precision (0.77 for ML-based\ntools vs. 0.76 for non-ML-based tools), applying machine learning-based tools\nresults in a recall three times higher than in the case of non-ML-based tools\n(0.66 vs. 0.22). Our study also confirms that tuning the models to the\ntask-specific data results in the increase in the quality. The retrained\nversions of reference parsers are in all cases better than their out-of-the-box\ncounterparts; for GROBID F1 increased by 3% (0.92 vs. 0.89), for CERMINE by 11%\n(0.92 vs. 0.83), and for ParsCit by 16% (0.87 vs. 0.75).", "journal": ""}
{"doi": "10.48550/arXiv.1711.05233", "date": "2017-11-14", "title": "A visual search engine for Bangladeshi laws", "authors": "Manash Kumar Mandal, Pinku Deb Nath, Arpeeta Shams Mizan, Nazmus Saquib", "abstract": "Browsing and finding relevant information for Bangladeshi laws is a challenge\nfaced by all law students and researchers in Bangladesh, and by citizens who\nwant to learn about any legal procedure. Some law archives in Bangladesh are\ndigitized, but lack proper tools to organize the data meaningfully. We present\na text visualization tool that utilizes machine learning techniques to make the\nsearching of laws quicker and easier. Using Doc2Vec to layout law article\nnodes, link mining techniques to visualize relevant citation networks, and\nnamed entity recognition to quickly find relevant sections in long law\narticles, our tool provides a faster and better search experience to the users.\nQualitative feedback from law researchers, students, and government officials\nshow promise for visually intuitive search tools in the context of\ngovernmental, legal, and constitutional data in developing countries, where\ndigitized data does not necessarily pave the way towards an easy access to\ninformation.", "journal": ""}
{"doi": "10.48550/arXiv.1711.10585", "date": "2017-11-15", "title": "A Survey of Learning Management Systems and Synchronous Distance Education Tools", "authors": "Khondkar Islam, Pouyan Ahmadi, Salman Yousaf", "abstract": "Although compelling assessments have been quite frequently examined in recent\nyears, more studies are required to yield a better understanding of several\nDistance Learning (DL) methods where Learning Management Systems (LMSs)\nsignificantly affect student learning process. Most studies in this area do not\nconsider the effect of varying web-facilitated DL application tools. To address\nthese drawbacks, the objective of our study is to compare two LMSs and four\nsynchronous distance education tools (SDET). The comparisons confirm the\nsuperiority of Moodle Integrated Synchrotrons Teaching Conferencing (MIST/C),\nwhich seems to be the most practical, convenient and modest distance education\ntool offered in the market today because it is open source and has a second\nmirrored whiteboard for simulteaching that is not available with any other\nsystem.", "journal": ""}
{"doi": "10.48550/arXiv.1807.05317", "date": "2018-07-14", "title": "LeFlow: Enabling Flexible FPGA High-Level Synthesis of Tensorflow Deep Neural Networks", "authors": "Daniel H. Noronha, Bahar Salehpour, Steven J. E. Wilton", "abstract": "Recent work has shown that Field-Programmable Gate Arrays (FPGAs) play an\nimportant role in the acceleration of Machine Learning applications. Initial\nspecification of machine learning applications are often done using a\nhigh-level Python-oriented framework such as Tensorflow, followed by a manual\ntranslation to either C or RTL for synthesis using vendor tools. This manual\ntranslation step is time-consuming and requires expertise that limit the\napplicability of FPGAs in this important domain. In this paper, we present an\nopen-source tool-flow that maps numerical computation models written in\nTensorflow to synthesizable hardware. Unlike other tools, which are often\nconstrained by a small number of inflexible templates, our flow uses Google's\nXLA compiler which emits LLVM code directly from a Tensorflow specification.\nThis LLVM code can then be used with a high-level synthesis tool to\nautomatically generate hardware. We show that our flow allows users to generate\nDeep Neural Networks with very few lines of Python code.", "journal": ""}
{"doi": "10.48550/arXiv.1903.04174", "date": "2019-03-11", "title": "Gathering Insights from Teenagers' Hacking Experience with Authentic Cybersecurity Tools", "authors": "Valdemar \u0160v\u00e1bensk\u00fd, Jan Vykopal", "abstract": "This Work-In-Progress Paper for the Innovative Practice Category presents a\nnovel experiment in active learning of cybersecurity. We introduced a new\nworkshop on hacking for an existing science-popularizing program at our\nuniversity. The workshop participants, 28 teenagers, played a cybersecurity\ngame designed for training undergraduates and professionals in penetration\ntesting. Unlike in learning environments that are simplified for young\nlearners, the game features a realistic virtual network infrastructure. This\nallows exploring security tools in an authentic scenario, which is complemented\nby a background story. Our research aim is to examine how young players\napproach using cybersecurity tools by interacting with the professional game. A\npreliminary analysis of the game session showed several challenges that the\nworkshop participants faced. Nevertheless, they reported learning about\nsecurity tools and exploits, and 61% of them reported wanting to learn more\nabout cybersecurity after the workshop. Our results support the notion that\nyoung learners should be allowed more hands-on experience with security topics,\nboth in formal education and informal extracurricular events.", "journal": ""}
{"doi": "10.48550/arXiv.1908.08623", "date": "2019-08-22", "title": "Exact inference under the perfect phylogeny model", "authors": "Surjyendu Ray, Bei Jia, Sam Safavi, Tim van Opijnen, Ralph Isberg, Jason Rosch, Jos\u00e9 Bento", "abstract": "Motivation: Many inference tools use the Perfect Phylogeny Model (PPM) to\nlearn trees from noisy variant allele frequency (VAF) data. Learning in this\nsetting is hard, and existing tools use approximate or heuristic algorithms. An\nalgorithmic improvement is important to help disentangle the limitations of the\nPPM's assumptions from the limitations in our capacity to learn under it.\nResults: We make such improvement in the scenario, where the mutations that are\nrelevant for evolution can be clustered into a small number of groups, and the\ntrees to be reconstructed have a small number of nodes. We use a careful\ncombination of algorithms, software, and hardware, to develop EXACT: a tool\nthat can explore the space of all possible phylogenetic trees, and performs\nexact inference under the PPM with noisy data. EXACT allows users to obtain not\njust the most-likely tree for some input data, but exact statistics about the\ndistribution of trees that might explain the data. We show that EXACT\noutperforms several existing tools for this same task. Availability:\nhttps://github.com/surjray-repos/EXACT", "journal": ""}
{"doi": "10.48550/arXiv.2003.11003", "date": "2020-03-24", "title": "Learn to Schedule (LEASCH): A Deep reinforcement learning approach for radio resource scheduling in the 5G MAC layer", "authors": "F. AL-Tam, N. Correia, J. Rodriguez", "abstract": "Network management tools are usually inherited from one generation to\nanother. This was successful since these tools have been kept in check and\nupdated regularly to fit new networking goals and service requirements.\nUnfortunately, new networking services will render this approach obsolete and\nhandcrafting new tools or upgrading the current ones may lead to complicated\nsystems that will be extremely difficult to maintain and improve. Fortunately,\nrecent advances in AI have provided new promising tools that can help solving\nmany network management problems. Following this interesting trend, the current\narticle presents LEASCH, a deep reinforcement learning model able to solve the\nradio resource scheduling problem in the MAC layer of 5G networks. LEASCH is\ndeveloped and trained in a sand-box and then deployed in a 5G network. The\nexperimental results validate the effectiveness of LEASCH compared to\nconventional baseline methods in many key performance indicators.", "journal": "IEEE access 2020"}
{"doi": "10.48550/arXiv.1907.04135", "date": "2019-07-09", "title": "The What-If Tool: Interactive Probing of Machine Learning Models", "authors": "James Wexler, Mahima Pushkarna, Tolga Bolukbasi, Martin Wattenberg, Fernanda Viegas, Jimbo Wilson", "abstract": "A key challenge in developing and deploying Machine Learning (ML) systems is\nunderstanding their performance across a wide range of inputs. To address this\nchallenge, we created the What-If Tool, an open-source application that allows\npractitioners to probe, visualize, and analyze ML systems, with minimal coding.\nThe What-If Tool lets practitioners test performance in hypothetical\nsituations, analyze the importance of different data features, and visualize\nmodel behavior across multiple models and subsets of input data. It also lets\npractitioners measure systems according to multiple ML fairness metrics. We\ndescribe the design of the tool, and report on real-life usage at different\norganizations.", "journal": ""}
{"doi": "10.48550/arXiv.2108.00078", "date": "2021-07-27", "title": "Technical Report for HW2VEC -- A Graph Learning Tool for Automating Hardware Security", "authors": "Yasamin Moghaddas, Tommy Nguyen, Shih-Yuan Yu, Rozhin Yasaei, Mohammad Abdullah Al Faruque", "abstract": "In this technical report, we present HW2VEC [11], an open-source graph\nlearning tool for hardware security, and its implementation details (Figure 1).\nHW2VEC provides toolboxes for graph representation extraction in the form of\nData Flow Graphs (DFGs) or Abstract Syntax Trees (ASTs) from hardware designs\nat RTL and GLN levels. Besides, HW2VEC also offers graph learning tools for\nrepresenting hardware designs in vectors that preserve both structural features\nand behavioral features. To the best of our knowledge, HW2VEC is the first\nopen-source research tool that supports applying graph learning methods to\nhardware designs in different abstraction levels for hardware security. We\norganize the remainder of this technical report as follows: Section 2\nintroduces the architecture of HW2VEC; Section 3 gives information about the\nuse-case implementations; Section 4 provides the experimental results and\ndemonstrates the performance of HW2VEC for two hardware security applications:\nHT detection and IP piracy detection; finally, Section 5 will conclude this\nreport.", "journal": ""}
{"doi": "10.48550/arXiv.2303.07546", "date": "2023-03-14", "title": "Constrained Adversarial Learning for Automated Software Testing: a literature review", "authors": "Jo\u00e3o Vitorino, Tiago Dias, Tiago Fonseca, Eva Maia, Isabel Pra\u00e7a", "abstract": "It is imperative to safeguard computer applications and information systems\nagainst the growing number of cyber-attacks. Automated software testing tools\ncan be developed to quickly analyze many lines of code and detect\nvulnerabilities by generating function-specific testing data. This process\ndraws similarities to the constrained adversarial examples generated by\nadversarial machine learning methods, so there could be significant benefits to\nthe integration of these methods in testing tools to identify possible attack\nvectors. Therefore, this literature review is focused on the current\nstate-of-the-art of constrained data generation approaches applied for\nadversarial learning and software testing, aiming to guide researchers and\ndevelopers to enhance their software testing tools with adversarial testing\nmethods and improve the resilience and robustness of their information systems.\nThe found approaches were systematized, and the advantages and limitations of\nthose specific for white-box, grey-box, and black-box testing were analyzed,\nidentifying research gaps and opportunities to automate the testing tools with\ndata generated by adversarial attacks.", "journal": ""}
{"doi": "10.48550/arXiv.2303.16725", "date": "2023-03-29", "title": "Machine Learning for Uncovering Biological Insights in Spatial Transcriptomics Data", "authors": "Alex J. Lee, Robert Cahill, Reza Abbasi-Asl", "abstract": "Development and homeostasis in multicellular systems both require exquisite\ncontrol over spatial molecular pattern formation and maintenance. Advances in\nspatially-resolved and high-throughput molecular imaging methods such as\nmultiplexed immunofluorescence and spatial transcriptomics (ST) provide\nexciting new opportunities to augment our fundamental understanding of these\nprocesses in health and disease. The large and complex datasets resulting from\nthese techniques, particularly ST, have led to rapid development of innovative\nmachine learning (ML) tools primarily based on deep learning techniques. These\nML tools are now increasingly featured in integrated experimental and\ncomputational workflows to disentangle signals from noise in complex biological\nsystems. However, it can be difficult to understand and balance the different\nimplicit assumptions and methodologies of a rapidly expanding toolbox of\nanalytical tools in ST. To address this, we summarize major ST analysis goals\nthat ML can help address and current analysis trends. We also describe four\nmajor data science concepts and related heuristics that can help guide\npractitioners in their choices of the right tools for the right biological\nquestions.", "journal": ""}
{"doi": "10.48550/arXiv.2310.01603", "date": "2023-10-02", "title": "A Review of Digital Learning Environments for Teaching Natural Language Processing in K-12 Education", "authors": "Xiaoyi Tian, Kristy Elizabeth Boyer", "abstract": "Natural Language Processing (NLP) plays a significant role in our daily lives\nand has become an essential part of Artificial Intelligence (AI) education in\nK-12. As children grow up with NLP-powered applications, it is crucial to\nintroduce NLP concepts to them, fostering their understanding of language\nprocessing, language generation, and ethical implications of AI and NLP. This\npaper presents a comprehensive review of digital learning environments for\nteaching NLP in K-12. Specifically, it explores existing digital learning\ntools, discusses how they support specific NLP tasks and procedures, and\ninvestigates their explainability and evaluation results in educational\ncontexts. By examining the strengths and limitations of these tools, this\nliterature review sheds light on the current state of NLP learning tools in\nK-12 education. It aims to guide future research efforts to refine existing\ntools, develop new ones, and explore more effective and inclusive strategies\nfor integrating NLP into K-12 educational contexts.", "journal": ""}
{"doi": "10.48550/arXiv.2404.06572", "date": "2024-04-09", "title": "Detecting Refactoring Commits in Machine Learning Python Projects: A Machine Learning-Based Approach", "authors": "Shayan Noei, Heng Li, Ying Zou", "abstract": "Refactoring enhances software quality without altering its functional\nbehaviors. Understanding the refactoring activities of developers is crucial to\nimproving software maintainability. With the increasing use of machine learning\n(ML) libraries and frameworks, maximizing their maintainability is crucial. Due\nto the data-driven nature of ML projects, they often undergo different\nrefactoring operations (e.g., data manipulation), for which existing\nrefactoring tools lack ML-specific detection capabilities. Furthermore, a large\nnumber of ML libraries are written in Python, which has limited tools for\nrefactoring detection. PyRef, a rule-based and state-of-the-art tool for Python\nrefactoring detection, can identify 11 types of refactoring operations. In\ncomparison, Rminer can detect 99 types of refactoring for Java projects. We\nintroduce MLRefScanner, a prototype tool that applies machine-learning\ntechniques to detect refactoring commits in ML Python projects. MLRefScanner\nidentifies commits with both ML-specific and general refactoring operations.\nEvaluating MLRefScanner on 199 ML projects demonstrates its superior\nperformance compared to state-of-the-art approaches, achieving an overall 94%\nprecision and 82% recall. Combining it with PyRef further boosts performance to\n95% precision and 99% recall. Our study highlights the potential of ML-driven\napproaches in detecting refactoring across diverse programming languages and\ntechnical domains, addressing the limitations of rule-based detection methods.", "journal": ""}
{"doi": "10.48550/arXiv.2405.18871", "date": "2024-05-29", "title": "DFAMiner: Mining minimal separating DFAs from labelled samples", "authors": "Daniele Dell'Erba, Yong Li, Sven Schewe", "abstract": "We propose DFAMiner, a passive learning tool for learning minimal separating\ndeterministic finite automata (DFA) from a set of labelled samples. Separating\nautomata are an interesting class of automata that occurs generally in regular\nmodel checking and has raised interest in foundational questions of parity game\nsolving. We first propose a simple and linear-time algorithm that incrementally\nconstructs a three-valued DFA (3DFA) from a set of labelled samples given in\nthe usual lexicographical order. This 3DFA has accepting and rejecting states\nas well as don't-care states, so that it can exactly recognise the labelled\nexamples. We then apply our tool to mining a minimal separating DFA for the\nlabelled samples by minimising the constructed automata via a reduction to\nsolving SAT problems. Empirical evaluation shows that our tool outperforms\ncurrent state-of-the-art tools significantly on standard benchmarks for\nlearning minimal separating DFAs from samples. Progress in the efficient\nconstruction of separating DFAs can also lead to finding the lower bound of\nparity game solving, where we show that DFAMiner can create optimal separating\nautomata for simple languages with up to 7 colours. Future improvements might\noffer inroads to better data structures.", "journal": ""}
{"doi": "10.48550/arXiv.2408.04619", "date": "2024-08-08", "title": "Transformer Explainer: Interactive Learning of Text-Generative Models", "authors": "Aeree Cho, Grace C. Kim, Alexander Karpekov, Alec Helbling, Zijie J. Wang, Seongmin Lee, Benjamin Hoover, Duen Horng Chau", "abstract": "Transformers have revolutionized machine learning, yet their inner workings\nremain opaque to many. We present Transformer Explainer, an interactive\nvisualization tool designed for non-experts to learn about Transformers through\nthe GPT-2 model. Our tool helps users understand complex Transformer concepts\nby integrating a model overview and enabling smooth transitions across\nabstraction levels of mathematical operations and model structures. It runs a\nlive GPT-2 instance locally in the user's browser, empowering users to\nexperiment with their own input and observe in real-time how the internal\ncomponents and parameters of the Transformer work together to predict the next\ntokens. Our tool requires no installation or special hardware, broadening the\npublic's education access to modern generative AI techniques. Our open-sourced\ntool is available at https://poloclub.github.io/transformer-explainer/. A video\ndemo is available at https://youtu.be/ECR4oAwocjs.", "journal": ""}
{"doi": "10.48550/arXiv.2502.08555", "date": "2025-02-12", "title": "A Machine Learning-Ready Data Processing Tool for Near Real-Time Forecasting", "authors": "Maher A Dayeh, Michael J Starkey, Subhamoy Chatterjee, Heather Elliott, Samuel Hart, Kimberly Moreland", "abstract": "Space weather forecasting is critical for mitigating radiation risks in space\nexploration and protecting Earth-based technologies from geomagnetic\ndisturbances. This paper presents the development of a Machine Learning (ML)-\nready data processing tool for Near Real-Time (NRT) space weather forecasting.\nBy merging data from diverse NRT sources such as solar imagery, magnetic field\nmeasurements, and energetic particle fluxes, the tool addresses key gaps in\ncurrent space weather prediction capabilities. The tool processes and\nstructures the data for machine learning models, focusing on time-series\nforecasting and event detection for extreme solar events. It provides users\nwith a framework to download, process, and label data for ML applications,\nstreamlining the workflow for improved NRT space weather forecasting and\nscientific research.", "journal": "IAC-24,D5,IP,12,x89662, 2024"}
{"doi": "10.48550/arXiv.2505.12101", "date": "2025-05-17", "title": "Designing Scaffolded Interfaces for Enhanced Learning and Performance in Professional Software", "authors": "Yimeng Liu, Misha Sra", "abstract": "Professional software offers immense power but also presents significant\nlearning challenges. Its complex interfaces, as well as insufficient built-in\nstructured guidance and unfamiliar terminology, often make newcomers struggle\nwith task completion. To address these challenges, we introduce ScaffoldUI, a\nmethod for scaffolded interface design to reduce interface complexity, provide\nstructured guidance, and enhance software learnability. The scaffolded\ninterface presents task-relevant tools, progressively discloses tool\ncomplexity, and organizes tools based on domain concepts, aiming to assist task\nperformance and software learning. To evaluate the feasibility of our interface\ndesign method, we present a technical pipeline for scaffolded interface\nimplementation in professional 3D software, i.e., Blender, and conduct user\nstudies with beginners (N=32) and experts (N=8). Study results demonstrate that\nour scaffolded interfaces significantly reduce perceived task load caused by\ninterface complexity, support task performance through structured guidance, and\naugment learning by clearly connecting concepts and tools within the taskflow\ncontext. Based on a discussion of the user study findings, we offer insights\nfor future research on designing scaffolded interfaces to support instruction,\nproductivity, creativity, and cross-software workflows.", "journal": ""}
{"doi": "10.48550/arXiv.1611.03969", "date": "2016-11-12", "title": "An Introduction to MM Algorithms for Machine Learning and Statistical", "authors": "Hien D. Nguyen", "abstract": "MM (majorization--minimization) algorithms are an increasingly popular tool\nfor solving optimization problems in machine learning and statistical\nestimation. This article introduces the MM algorithm framework in general and\nvia three popular example applications: Gaussian mixture regressions,\nmultinomial logistic regressions, and support vector machines. Specific\nalgorithms for the three examples are derived and numerical demonstrations are\npresented. Theoretical and practical aspects of MM algorithm design are\ndiscussed.", "journal": ""}
{"doi": "10.48550/arXiv.1904.01957", "date": "2019-04-02", "title": "A Game of Dice: Machine Learning and the Question Concerning Art", "authors": "Paul Todorov", "abstract": "We review some practical and philosophical questions raised by the use of\nmachine learning in creative practice. Beyond the obvious problems regarding\nplagiarism and authorship, we argue that the novelty in AI Art relies mostly on\na narrow machine learning contribution : manifold approximation. Nevertheless,\nthis contribution creates a radical shift in the way we have to consider this\nmovement. Is this omnipotent tool a blessing or a curse for the artists?", "journal": ""}
{"doi": "10.48550/arXiv.2207.05548", "date": "2022-07-12", "title": "Practical Attacks on Machine Learning: A Case Study on Adversarial Windows Malware", "authors": "Luca Demetrio, Battista Biggio, Fabio Roli", "abstract": "While machine learning is vulnerable to adversarial examples, it still lacks\nsystematic procedures and tools for evaluating its security in different\napplication contexts. In this article, we discuss how to develop automated and\nscalable security evaluations of machine learning using practical attacks,\nreporting a use case on Windows malware detection.", "journal": "IEEE Security & Privacy, 2022"}
{"doi": "10.48550/arXiv.2403.02467", "date": "2024-03-04", "title": "Applied Causal Inference Powered by ML and AI", "authors": "Victor Chernozhukov, Christian Hansen, Nathan Kallus, Martin Spindler, Vasilis Syrgkanis", "abstract": "An introduction to the emerging fusion of machine learning and causal\ninference. The book presents ideas from classical structural equation models\n(SEMs) and their modern AI equivalent, directed acyclical graphs (DAGs) and\nstructural causal models (SCMs), and covers Double/Debiased Machine Learning\nmethods to do inference in such models using modern predictive tools.", "journal": ""}
{"doi": "10.48550/arXiv.1811.06711", "date": "2018-11-16", "title": "An Algorithmic Perspective on Imitation Learning", "authors": "Takayuki Osa, Joni Pajarinen, Gerhard Neumann, J. Andrew Bagnell, Pieter Abbeel, Jan Peters", "abstract": "As robots and other intelligent agents move from simple environments and\nproblems to more complex, unstructured settings, manually programming their\nbehavior has become increasingly challenging and expensive. Often, it is easier\nfor a teacher to demonstrate a desired behavior rather than attempt to manually\nengineer it. This process of learning from demonstrations, and the study of\nalgorithms to do so, is called imitation learning. This work provides an\nintroduction to imitation learning. It covers the underlying assumptions,\napproaches, and how they relate; the rich set of algorithms developed to tackle\nthe problem; and advice on effective tools and implementation.\n  We intend this paper to serve two audiences. First, we want to familiarize\nmachine learning experts with the challenges of imitation learning,\nparticularly those arising in robotics, and the interesting theoretical and\npractical distinctions between it and more familiar frameworks like statistical\nsupervised learning theory and reinforcement learning. Second, we want to give\nroboticists and experts in applied artificial intelligence a broader\nappreciation for the frameworks and tools available for imitation learning.", "journal": ""}
{"doi": "10.48550/arXiv.2102.07112", "date": "2021-02-14", "title": "A New Algorithm for Hidden Markov Models Learning Problem", "authors": "Taha Mansouri, Mohamadreza Sadeghimoghadam, Iman Ghasemian Sahebi", "abstract": "This research focuses on the algorithms and approaches for learning Hidden\nMarkov Models (HMMs) and compares HMM learning methods and algorithms. HMM is a\nstatistical Markov model in which the system being modeled is assumed to be a\nMarkov process. One of the essential characteristics of HMMs is their learning\ncapabilities. Learning algorithms are introduced to overcome this\ninconvenience. One of the main problems of the newly proposed algorithms is\ntheir validation. This research aims by using the theoretical and experimental\nanalysis to 1) compare HMMs learning algorithms proposed in the literature, 2)\nprovide a validation tool for new HMM learning algorithms, and 3) present a new\nalgorithm called Asexual Reproduction Optimization (ARO) with one of its\nextensions - Modified ARO (MARO) - as a novel HMM learning algorithm to use the\nvalidation tool proposed. According to the literature findings, it seems that\npopulationbased algorithms perform better among HMMs learning approaches than\nother algorithms. Also, the testing was done in nine benchmark datasets. The\nresults show that MARO outperforms different algorithms in objective functions\nin terms of accuracy and robustness.", "journal": ""}
{"doi": "10.48550/arXiv.2310.13240", "date": "2023-10-20", "title": "Transparency challenges in policy evaluation with causal machine learning -- improving usability and accountability", "authors": "Patrick Rehill, Nicholas Biddle", "abstract": "Causal machine learning tools are beginning to see use in real-world policy\nevaluation tasks to flexibly estimate treatment effects. One issue with these\nmethods is that the machine learning models used are generally black boxes,\ni.e., there is no globally interpretable way to understand how a model makes\nestimates. This is a clear problem in policy evaluation applications,\nparticularly in government, because it is difficult to understand whether such\nmodels are functioning in ways that are fair, based on the correct\ninterpretation of evidence and transparent enough to allow for accountability\nif things go wrong. However, there has been little discussion of transparency\nproblems in the causal machine learning literature and how these might be\novercome. This paper explores why transparency issues are a problem for causal\nmachine learning in public policy evaluation applications and considers ways\nthese problems might be addressed through explainable AI tools and by\nsimplifying models in line with interpretable AI principles. It then applies\nthese ideas to a case-study using a causal forest model to estimate conditional\naverage treatment effects for a hypothetical change in the school leaving age\nin Australia. It shows that existing tools for understanding black-box\npredictive models are poorly suited to causal machine learning and that\nsimplifying the model to make it interpretable leads to an unacceptable\nincrease in error (in this application). It concludes that new tools are needed\nto properly understand causal machine learning models and the algorithms that\nfit them.", "journal": ""}
{"doi": "10.48550/arXiv.2410.16392", "date": "2024-10-21", "title": "Training of Scaffolded Language Models with Language Supervision: A Survey", "authors": "Matthieu Lin, Jenny Sheng, Andrew Zhao, Shenzhi Wang, Yang Yue, Victor Shea Jay Huang, Huan Liu, Jun Liu, Gao Huang, Yong-Jin Liu", "abstract": "This survey organizes the intricate literature on the design and optimization\nof emerging structures around post-trained LMs. We refer to this overarching\nstructure as scaffolded LMs and focus on LMs that are integrated into\nmulti-step processes with tools. We view scaffolded LMs as semi-parametric\nmodels wherein we train non-parametric variables, including the prompt, tools,\nand scaffold's code. In particular, they interpret instructions, use tools, and\nreceive feedback all in language. Recent works use an LM as an optimizer to\ninterpret language supervision and update non-parametric variables according to\nintricate objectives. In this survey, we refer to this paradigm as training of\nscaffolded LMs with language supervision. A key feature of non-parametric\ntraining is the ability to learn from language. Parametric training excels in\nlearning from demonstration (supervised learning), exploration (reinforcement\nlearning), or observations (unsupervised learning), using well-defined loss\nfunctions. Language-based optimization enables rich, interpretable, and\nexpressive objectives, while mitigating issues like catastrophic forgetting and\nsupporting compatibility with closed-source models. Furthermore, agents are\nincreasingly deployed as co-workers in real-world applications such as Copilot\nin Office tools or software development. In these mixed-autonomy settings,\nwhere control and decision-making are shared between human and AI, users point\nout errors or suggest corrections. Accordingly, we discuss agents that\ncontinuously improve by learning from this real-time, language-based feedback\nand refer to this setting as streaming learning from language supervision.", "journal": ""}
{"doi": "10.48550/arXiv.2504.14870", "date": "2025-04-21", "title": "Acting Less is Reasoning More! Teaching Model to Act Efficiently", "authors": "Hongru Wang, Cheng Qian, Wanjun Zhong, Xiusi Chen, Jiahao Qiu, Shijue Huang, Bowen Jin, Mengdi Wang, Kam-Fai Wong, Heng Ji", "abstract": "Tool-integrated reasoning (TIR) augments large language models (LLMs) with\nthe ability to invoke external tools during long-form reasoning, such as search\nengines and code interpreters, to solve tasks beyond the capabilities of\ninternal reasoning. While reinforcement learning (RL) has shown promise in\ntraining such agents, most of existing approaches typically optimize only for\nfinal correctness without considering the efficiency or necessity of external\ntool use. This often leads to excessive tool calling, incurring high\ncomputational costs and hindering the development of internal reasoning\ncapabilities - a phenomenon known as \\textit{cognitive offloading}. To this\nend, we propose Optimal Tool Call-controlled Policy Optimization (OTC-PO), a\nsimple yet effective RL-based framework that encourages models to produce\naccurate answers with minimal tool calls. Our method introduces a\ntool-integrated reward that jointly considers answer correctness and\ncorresponding tool use behavior of model to reach that answer. To validate the\neffectiveness, we introduce the metric of \\textit{tool productivity}, defined\nas the ratio between the number of correct answers and the total number of tool\ncalls across all test cases. This metric reflects how efficiently tool usage\ncontributes to successful task completion, with higher values indicating\nsmarter and more autonomous reasoning. We instantiate this framework within\nboth Proximal Policy Optimization (PPO) and Group Relative Preference\nOptimization (GRPO), resulting in OTC-PPO and OTC-GRPO. Experiments with\nQwen-2.5 and Qwen-Math across multiple QA benchmarks show that our approach\nreduces tool calls by up to 68.3\\% and improves tool productivity by up to\n215.4\\%, while maintaining comparable answer accuracy.", "journal": ""}
{"doi": "10.48550/arXiv.2301.13617", "date": "2023-01-31", "title": "A Closer Look into Recent Video-based Learning Research: A Comprehensive Review of Video Characteristics, Tools, Technologies, and Learning Effectiveness", "authors": "Evelyn Navarrete, Andreas Nehring, Sascha Schanze, Ralph Ewerth, Anett Hoppe", "abstract": "People increasingly use videos on the Web as a source for learning. To\nsupport this way of learning, researchers and developers are continuously\ndeveloping tools, proposing guidelines, analyzing data, and conducting\nexperiments. However, it is still not clear what characteristics a video should\nhave to be an effective learning medium. In this paper, we present a\ncomprehensive review of 257 articles on video-based learning for the period\nfrom 2016 to 2021. One of the aims of the review is to identify the video\ncharacteristics that have been explored by previous work. Based on our\nanalysis, we suggest a taxonomy which organizes the video characteristics and\ncontextual aspects into eight categories: (1) audio features, (2) visual\nfeatures, (3) textual features, (4) instructor behavior, (5) learners\nactivities, (6) interactive features (quizzes, etc.), (7) production style, and\n(8) instructional design. Also, we identify four representative research\ndirections: (1) proposals of tools to support video-based learning, (2) studies\nwith controlled experiments, (3) data analysis studies, and (4) proposals of\ndesign guidelines for learning videos. We find that the most explored\ncharacteristics are textual features followed by visual features, learner\nactivities, and interactive features. Text of transcripts, video frames, and\nimages (figures and illustrations) are most frequently used by tools that\nsupport learning through videos. The learner activity is heavily explored\nthrough log files in data analysis studies, and interactive features have been\nfrequently scrutinized in controlled experiments. We complement our review by\ncontrasting research findings that investigate the impact of video\ncharacteristics on the learning effectiveness, report on tasks and technologies\nused to develop tools that support learning, and summarize trends of design\nguidelines to produce learning videos", "journal": ""}
{"doi": "10.48550/arXiv.1809.01587", "date": "2018-09-05", "title": "GAN Lab: Understanding Complex Deep Generative Models using Interactive Visual Experimentation", "authors": "Minsuk Kahng, Nikhil Thorat, Duen Horng Chau, Fernanda Vi\u00e9gas, Martin Wattenberg", "abstract": "Recent success in deep learning has generated immense interest among\npractitioners and students, inspiring many to learn about this new technology.\nWhile visual and interactive approaches have been successfully developed to\nhelp people more easily learn deep learning, most existing tools focus on\nsimpler models. In this work, we present GAN Lab, the first interactive\nvisualization tool designed for non-experts to learn and experiment with\nGenerative Adversarial Networks (GANs), a popular class of complex deep\nlearning models. With GAN Lab, users can interactively train generative models\nand visualize the dynamic training process's intermediate results. GAN Lab\ntightly integrates an model overview graph that summarizes GAN's structure, and\na layered distributions view that helps users interpret the interplay between\nsubmodels. GAN Lab introduces new interactive experimentation features for\nlearning complex deep learning models, such as step-by-step training at\nmultiple levels of abstraction for understanding intricate training dynamics.\nImplemented using TensorFlow.js, GAN Lab is accessible to anyone via modern web\nbrowsers, without the need for installation or specialized hardware, overcoming\na major practical challenge in deploying interactive tools for deep learning.", "journal": ""}
{"doi": "10.48550/arXiv.1911.05567", "date": "2019-11-13", "title": "DARTS: DenseUnet-based Automatic Rapid Tool for brain Segmentation", "authors": "Aakash Kaku, Chaitra V. Hegde, Jeffrey Huang, Sohae Chung, Xiuyuan Wang, Matthew Young, Alireza Radmanesh, Yvonne W. Lui, Narges Razavian", "abstract": "Quantitative, volumetric analysis of Magnetic Resonance Imaging (MRI) is a\nfundamental way researchers study the brain in a host of neurological\nconditions including normal maturation and aging. Despite the availability of\nopen-source brain segmentation software, widespread clinical adoption of\nvolumetric analysis has been hindered due to processing times and reliance on\nmanual corrections. Here, we extend the use of deep learning models from\nproof-of-concept, as previously reported, to present a comprehensive\nsegmentation of cortical and deep gray matter brain structures matching the\nstandard regions of aseg+aparc included in the commonly used open-source tool,\nFreesurfer. The work presented here provides a real-life, rapid deep\nlearning-based brain segmentation tool to enable clinical translation as well\nas research application of quantitative brain segmentation. The advantages of\nthe presented tool include short (~1 minute) processing time and improved\nsegmentation quality. This is the first study to perform quick and accurate\nsegmentation of 102 brain regions based on the surface-based protocol (DMK\nprotocol), widely used by experts in the field. This is also the first work to\ninclude an expert reader study to assess the quality of the segmentation\nobtained using a deep-learning-based model. We show the superior performance of\nour deep-learning-based models over the traditional segmentation tool,\nFreesurfer. We refer to the proposed deep learning-based tool as DARTS\n(DenseUnet-based Automatic Rapid Tool for brain Segmentation). Our tool and\ntrained models are available at https://github.com/NYUMedML/DARTS", "journal": ""}
{"doi": "10.48550/arXiv.1603.06212", "date": "2016-03-20", "title": "Evaluation of a Tree-based Pipeline Optimization Tool for Automating Data Science", "authors": "Randal S. Olson, Nathan Bartley, Ryan J. Urbanowicz, Jason H. Moore", "abstract": "As the field of data science continues to grow, there will be an\never-increasing demand for tools that make machine learning accessible to\nnon-experts. In this paper, we introduce the concept of tree-based pipeline\noptimization for automating one of the most tedious parts of machine\nlearning---pipeline design. We implement an open source Tree-based Pipeline\nOptimization Tool (TPOT) in Python and demonstrate its effectiveness on a\nseries of simulated and real-world benchmark data sets. In particular, we show\nthat TPOT can design machine learning pipelines that provide a significant\nimprovement over a basic machine learning analysis while requiring little to no\ninput nor prior knowledge from the user. We also address the tendency for TPOT\nto design overly complex pipelines by integrating Pareto optimization, which\nproduces compact pipelines without sacrificing classification accuracy. As\nsuch, this work represents an important step toward fully automating machine\nlearning pipeline design.", "journal": ""}
{"doi": "10.48550/arXiv.1807.07023", "date": "2018-07-18", "title": "Cross-layer Optimization for High Speed Adders: A Pareto Driven Machine Learning Approach", "authors": "Yuzhe Ma, Subhendu Roy, Jin Miao, Jiamin Chen, Bei Yu", "abstract": "In spite of maturity to the modern electronic design automation (EDA) tools,\noptimized designs at architectural stage may become sub-optimal after going\nthrough physical design flow. Adder design has been such a long studied\nfundamental problem in VLSI industry yet designers cannot achieve optimal\nsolutions by running EDA tools on the set of available prefix adder\narchitectures. In this paper, we enhance a state-of-the-art prefix adder\nsynthesis algorithm to obtain a much wider solution space in architectural\ndomain. On top of that, a machine learning-based design space exploration\nmethodology is applied to predict the Pareto frontier of the adders in physical\ndomain, which is infeasible by exhaustively running EDA tools for innumerable\narchitectural solutions. Considering the high cost of obtaining the true values\nfor learning, an active learning algorithm is utilized to select the\nrepresentative data during learning process, which uses less labeled data while\nachieving better quality of Pareto frontier. Experimental results demonstrate\nthat our framework can achieve Pareto frontier of high quality over a wide\ndesign space, bridging the gap between architectural and physical designs.", "journal": ""}
{"doi": "10.48550/arXiv.1808.02081", "date": "2018-07-29", "title": "The system of cloud oriented learning tools as an element of educational and scientific environment of high school", "authors": "Andrii M. Striuk, Maryna V. Rassovytska", "abstract": "The aim of this research is to design and implementation of cloud based\nlearning environment for separate division of the university. The analysis of\nexisting approaches to the construction of cloud based learning environments,\nthe formation of requirements cloud based learning tools, the selection on the\nbasis of these requirements, cloud ICT training and pilot their use for\nbuilding cloud based learning environment for separate division of the\nuniversity with the use of open source software and resources its own IT\ninfrastructure of the institution. Results of the study is planned to\ngeneralize to develop recommendations for the design of cloud based environment\nof high school.", "journal": "Information Technologies and Learning Tools 42 (2014) 150-158"}
{"doi": "10.48550/arXiv.2003.00108", "date": "2020-02-28", "title": "Deep Learning in Mining Biological Data", "authors": "Mufti Mahmud, M Shamim Kaiser, Amir Hussain", "abstract": "Recent technological advancements in data acquisition tools allowed life\nscientists to acquire multimodal data from different biological application\ndomains. Broadly categorized in three types (i.e., sequences, images, and\nsignals), these data are huge in amount and complex in nature. Mining such an\nenormous amount of data for pattern recognition is a big challenge and requires\nsophisticated data-intensive machine learning techniques. Artificial neural\nnetwork-based learning systems are well known for their pattern recognition\ncapabilities and lately their deep architectures - known as deep learning (DL)\n- have been successfully applied to solve many complex pattern recognition\nproblems. Highlighting the role of DL in recognizing patterns in biological\ndata, this article provides - applications of DL to biological sequences,\nimages, and signals data; overview of open access sources of these data;\ndescription of open source DL tools applicable on these data; and comparison of\nthese tools from qualitative and quantitative perspectives. At the end, it\noutlines some open research challenges in mining biological data and puts\nforward a number of possible future perspectives.", "journal": ""}
{"doi": "10.48550/arXiv.2201.05445", "date": "2022-01-14", "title": "Machine Learning of polymer types from the spectral signature of Raman spectroscopy microplastics data", "authors": "Sheela Ramanna, Danila Morozovskii, Sam Swanson, Jennifer Bruneau", "abstract": "The tools and technology that are currently used to analyze chemical compound\nstructures that identify polymer types in microplastics are not well-calibrated\nfor environmentally weathered microplastics. Microplastics that have been\ndegraded by environmental weathering factors can offer less analytic certainty\nthan samples of microplastics that have not been exposed to weathering\nprocesses. Machine learning tools and techniques allow us to better calibrate\nthe research tools for certainty in microplastics analysis. In this paper, we\ninvestigate whether the signatures (Raman shift values) are distinct enough\nsuch that well studied machine learning (ML) algorithms can learn to identify\npolymer types using a relatively small amount of labeled input data when the\nsamples have not been impacted by environmental degradation. Several ML models\nwere trained on a well-known repository, Spectral Libraries of Plastic\nParticles (SLOPP), that contain Raman shift and intensity results for a range\nof plastic particles, then tested on environmentally aged plastic particles\n(SloPP-E) consisting of 22 polymer types. After extensive preprocessing and\naugmentation, the trained random forest model was then tested on the SloPP-E\ndataset resulting in an improvement in classification accuracy of 93.81% from\n89%.", "journal": "Advances in Artificial Intelligence and Machine Learning (AAIML)\n  2023"}
{"doi": "10.48550/arXiv.1912.10597", "date": "2019-12-23", "title": "The Labeling Distribution Matrix (LDM): A Tool for Estimating Machine Learning Algorithm Capacity", "authors": "Pedro Sandoval Segura, Julius Lauw, Daniel Bashir, Kinjal Shah, Sonia Sehra, Dominique Macias, George Montanez", "abstract": "Algorithm performance in supervised learning is a combination of\nmemorization, generalization, and luck. By estimating how much information an\nalgorithm can memorize from a dataset, we can set a lower bound on the amount\nof performance due to other factors such as generalization and luck. With this\ngoal in mind, we introduce the Labeling Distribution Matrix (LDM) as a tool for\nestimating the capacity of learning algorithms. The method attempts to\ncharacterize the diversity of possible outputs by an algorithm for different\ntraining datasets, using this to measure algorithm flexibility and\nresponsiveness to data. We test the method on several supervised learning\nalgorithms, and find that while the results are not conclusive, the LDM does\nallow us to gain potentially valuable insight into the prediction behavior of\nalgorithms. We also introduce the Label Recorder as an additional tool for\nestimating algorithm capacity, with more promising initial results.", "journal": ""}
{"doi": "10.48550/arXiv.2008.12080", "date": "2020-08-27", "title": "Identifying and Tracking Solar Magnetic Flux Elements with Deep Learning", "authors": "Haodi Jiang, Jiasheng Wang, Chang Liu, Ju Jing, Hao Liu, Jason T. L. Wang, Haimin Wang", "abstract": "Deep learning has drawn a lot of interest in recent years due to its\neffectiveness in processing big and complex observational data gathered from\ndiverse instruments. Here we propose a new deep learning method, called\nSolarUnet, to identify and track solar magnetic flux elements or features in\nobserved vector magnetograms based on the Southwest Automatic Magnetic\nIdentification Suite (SWAMIS). Our method consists of a data pre-processing\ncomponent that prepares training data from the SWAMIS tool, a deep learning\nmodel implemented as a U-shaped convolutional neural network for fast and\naccurate image segmentation, and a post-processing component that prepares\ntracking results. SolarUnet is applied to data from the 1.6 meter Goode Solar\nTelescope at the Big Bear Solar Observatory. When compared to the widely used\nSWAMIS tool, SolarUnet is faster while agreeing mostly with SWAMIS on feature\nsize and flux distributions, and complementing SWAMIS in tracking long-lifetime\nfeatures. Thus, the proposed physics-guided deep learning-based tool can be\nconsidered as an alternative method for solar magnetic tracking.", "journal": "The Astrophysical Journal Supplement Series, 250:5 (13pp), 2020"}
{"doi": "10.48550/arXiv.2101.02377", "date": "2021-01-07", "title": "Eth2Vec: Learning Contract-Wide Code Representations for Vulnerability Detection on Ethereum Smart Contracts", "authors": "Nami Ashizawa, Naoto Yanai, Jason Paul Cruz, Shingo Okamura", "abstract": "Ethereum smart contracts are programs that run on the Ethereum blockchain,\nand many smart contract vulnerabilities have been discovered in the past\ndecade. Many security analysis tools have been created to detect such\nvulnerabilities, but their performance decreases drastically when codes to be\nanalyzed are being rewritten. In this paper, we propose Eth2Vec, a\nmachine-learning-based static analysis tool for vulnerability detection, with\nrobustness against code rewrites in smart contracts. Existing\nmachine-learning-based static analysis tools for vulnerability detection need\nfeatures, which analysts create manually, as inputs. In contrast, Eth2Vec\nautomatically learns features of vulnerable Ethereum Virtual Machine (EVM)\nbytecodes with tacit knowledge through a neural network for language\nprocessing. Therefore, Eth2Vec can detect vulnerabilities in smart contracts by\ncomparing the code similarity between target EVM bytecodes and the EVM\nbytecodes it already learned. We conducted experiments with existing open\ndatabases, such as Etherscan, and our results show that Eth2Vec outperforms the\nexisting work in terms of well-known metrics, i.e., precision, recall, and\nF1-score. Moreover, Eth2Vec can detect vulnerabilities even in rewritten codes.", "journal": ""}
{"doi": "10.48550/arXiv.2106.06524", "date": "2021-06-11", "title": "WAX-ML: A Python library for machine learning and feedback loops on streaming data", "authors": "Emmanuel S\u00e9ri\u00e9", "abstract": "Wax is what you put on a surfboard to avoid slipping. It is an essential tool\nto go surfing... We introduce WAX-ML a research-oriented Python library\nproviding tools to design powerful machine learning algorithms and feedback\nloops working on streaming data. It strives to complement JAX with tools\ndedicated to time series. WAX-ML makes JAX-based programs easy to use for\nend-users working with pandas and xarray for data manipulation. It provides a\nsimple mechanism for implementing feedback loops, allows the implementation of\nonline learning and reinforcement learning algorithms with functions, and makes\nthem easy to integrate by end-users working with the object-oriented\nreinforcement learning framework from the Gym library. It is released with an\nApache open-source license on GitHub at https://github.com/eserie/wax-ml.", "journal": ""}
{"doi": "10.48550/arXiv.2110.03999", "date": "2021-10-08", "title": "Graphs as Tools to Improve Deep Learning Methods", "authors": "Carlos Lassance, Myriam Bontonou, Mounia Hamidouche, Bastien Pasdeloup, Lucas Drumetz, Vincent Gripon", "abstract": "In recent years, deep neural networks (DNNs) have known an important rise in\npopularity. However, although they are state-of-the-art in many machine\nlearning challenges, they still suffer from several limitations. For example,\nDNNs require a lot of training data, which might not be available in some\npractical applications. In addition, when small perturbations are added to the\ninputs, DNNs are prone to misclassification errors. DNNs are also viewed as\nblack-boxes and as such their decisions are often criticized for their lack of\ninterpretability.\n  In this chapter, we review recent works that aim at using graphs as tools to\nimprove deep learning methods. These graphs are defined considering a specific\nlayer in a deep learning architecture. Their vertices represent distinct\nsamples, and their edges depend on the similarity of the corresponding\nintermediate representations. These graphs can then be leveraged using various\nmethodologies, many of which built on top of graph signal processing.\n  This chapter is composed of four main parts: tools for visualizing\nintermediate layers in a DNN, denoising data representations, optimizing graph\nobjective functions and regularizing the learning process.", "journal": ""}
{"doi": "10.48550/arXiv.2308.03864", "date": "2023-08-07", "title": "Storyfier: Exploring Vocabulary Learning Support with Text Generation Models", "authors": "Zhenhui Peng, Xingbo Wang, Qiushi Han, Junkai Zhu, Xiaojuan Ma, Huamin Qu", "abstract": "Vocabulary learning support tools have widely exploited existing materials,\ne.g., stories or video clips, as contexts to help users memorize each target\nword. However, these tools could not provide a coherent context for any target\nwords of learners' interests, and they seldom help practice word usage. In this\npaper, we work with teachers and students to iteratively develop Storyfier,\nwhich leverages text generation models to enable learners to read a generated\nstory that covers any target words, conduct a story cloze test, and use these\nwords to write a new story with adaptive AI assistance. Our within-subjects\nstudy (N=28) shows that learners generally favor the generated stories for\nconnecting target words and writing assistance for easing their learning\nworkload. However, in the read-cloze-write learning sessions, participants\nusing Storyfier perform worse in recalling and using target words than learning\nwith a baseline tool without our AI features. We discuss insights into\nsupporting learning tasks with generative models.", "journal": ""}
{"doi": "10.48550/arXiv.2405.14751", "date": "2024-05-23", "title": "AGILE: A Novel Reinforcement Learning Framework of LLM Agents", "authors": "Peiyuan Feng, Yichen He, Guanhua Huang, Yuan Lin, Hanchong Zhang, Yuchen Zhang, Hang Li", "abstract": "We introduce a novel reinforcement learning framework of LLM agents named\nAGILE (AGent that Interacts and Learns from Environments) designed to perform\ncomplex conversational tasks with users, leveraging LLMs, memory, tools, and\ninteractions with experts. The agent possesses capabilities beyond\nconversation, including reflection, tool usage, and expert consultation. We\nformulate the construction of such an LLM agent as a reinforcement learning\n(RL) problem, in which the LLM serves as the policy model. We fine-tune the LLM\nusing labeled data of actions and the PPO algorithm. We focus on question\nanswering and release a dataset for agents called ProductQA, comprising\nchallenging questions in online shopping. Our extensive experiments on\nProductQA, MedMCQA and HotPotQA show that AGILE agents based on 7B and 13B LLMs\ntrained with PPO can outperform GPT-4 agents. Our ablation study highlights the\nindispensability of memory, tools, consultation, reflection, and reinforcement\nlearning in achieving the agent's strong performance. Datasets and code are\navailable at https://github.com/bytarnish/AGILE.", "journal": ""}
{"doi": "10.48550/arXiv.2405.17839", "date": "2024-05-28", "title": "PeerFL: A Simulator for Peer-to-Peer Federated Learning at Scale", "authors": "Alka Luqman, Shivanshu Shekhar, Anupam Chattopadhyay", "abstract": "This work integrates peer-to-peer federated learning tools with NS3, a widely\nused network simulator, to create a novel simulator designed to allow\nheterogeneous device experiments in federated learning. This cross-platform\nadaptability addresses a critical gap in existing simulation tools, enhancing\nthe overall utility and user experience. NS3 is leveraged to simulate WiFi\ndynamics to facilitate federated learning experiments with participants that\nmove around physically during training, leading to dynamic network\ncharacteristics. Our experiments showcase the simulator's efficiency in\ncomputational resource utilization at scale, with a maximum of 450\nheterogeneous devices modelled as participants in federated learning. This\npositions it as a valuable tool for simulation-based investigations in\npeer-to-peer federated learning. The framework is open source and available for\nuse and extension to the community.", "journal": ""}
{"doi": "10.48550/arXiv.2405.20091", "date": "2024-05-30", "title": "VAAD: Visual Attention Analysis Dashboard applied to e-Learning", "authors": "Miriam Navarro, \u00c1lvaro Becerra, Roberto Daza, Ruth Cobos, Aythami Morales, Julian Fierrez", "abstract": "In this paper, we present an approach in the Multimodal Learning Analytics\nfield. Within this approach, we have developed a tool to visualize and analyze\neye movement data collected during learning sessions in online courses. The\ntool is named VAAD, an acronym for Visual Attention Analysis Dashboard. These\neye movement data have been gathered using an eye-tracker and subsequently\nprocessed and visualized for interpretation. The purpose of the tool is to\nconduct a descriptive analysis of the data by facilitating its visualization,\nenabling the identification of differences and learning patterns among various\nlearner populations. Additionally, it integrates a predictive module capable of\nanticipating learner activities during a learning session. Consequently, VAAD\nholds the potential to offer valuable insights into online learning behaviors\nfrom both descriptive and predictive perspectives.", "journal": ""}
{"doi": "10.48550/arXiv.1111.4407", "date": "2011-11-16", "title": "Proceedings Fifth Transformation Tool Contest", "authors": "Pieter Van Gorp, Steffen Mazanek, Louis Rose", "abstract": "The aim of the Transformation Tool Contest (TTC) series is to compare the\nexpressiveness, the usability and the performance of graph and model\ntransformation tools along a number of selected case studies. Participants want\nto learn about the pros and cons of each tool considering different\napplications. A deeper understanding of the relative merits of different tool\nfeatures will help to further improve graph and model transformation tools and\nto indicate open problems.\n  TTC 2011 involved 25 offline case study solutions: 12 solutions to the Hello\nWorld case, 2 solutions to the GMF Model Migration case, 5 solutions to the\nCompiler Optimization case, and 7 solutions to the Reengineering (i.e., Program\nUnderstanding) case. This volume contains the submissions that have passed an\nadditional (post-workshop) reviewing round.", "journal": ""}
{"doi": "10.48550/arXiv.1606.07461", "date": "2016-06-23", "title": "LSTMVis: A Tool for Visual Analysis of Hidden State Dynamics in Recurrent Neural Networks", "authors": "Hendrik Strobelt, Sebastian Gehrmann, Hanspeter Pfister, Alexander M. Rush", "abstract": "Recurrent neural networks, and in particular long short-term memory (LSTM)\nnetworks, are a remarkably effective tool for sequence modeling that learn a\ndense black-box hidden representation of their sequential input. Researchers\ninterested in better understanding these models have studied the changes in\nhidden state representations over time and noticed some interpretable patterns\nbut also significant noise. In this work, we present LSTMVIS, a visual analysis\ntool for recurrent neural networks with a focus on understanding these hidden\nstate dynamics. The tool allows users to select a hypothesis input range to\nfocus on local state changes, to match these states changes to similar patterns\nin a large data set, and to align these results with structural annotations\nfrom their domain. We show several use cases of the tool for analyzing specific\nhidden state properties on dataset containing nesting, phrase structure, and\nchord progressions, and demonstrate how the tool can be used to isolate\npatterns for further statistical analysis. We characterize the domain, the\ndifferent stakeholders, and their goals and tasks.", "journal": ""}
{"doi": "10.48550/arXiv.1311.7536", "date": "2013-11-29", "title": "Proceedings Sixth Transformation Tool Contest", "authors": "Pieter Van Gorp, Louis M. Rose, Christian Krause", "abstract": "The aim of the Transformation Tool Contest (TTC) series is to compare the\nexpressiveness, the usability and the performance of graph and model\ntransformation tools along a number of selected case studies. Participants want\nto learn about the pros and cons of each tool considering different\napplications. A deeper understanding of the relative merits of different tool\nfeatures will help to further improve graph and model transformation tools and\nto indicate open problems.\n  TTC 2013 involved 18 offline case study solutions: 6 solutions to the\nFlowGraphs case, 9 solutions to the Petri Nets to Statecharts case and 3\nsolutions to the Restructuring case. 13 of the 18 solutions have undergone a\nnon-blind peer review before the workshop and were presented and evaluated\nduring the workshop in Budapest. This volume contains the submissions that have\npassed an additional (post-workshop, blind) reviewing round.", "journal": "EPTCS 135, 2013"}
{"doi": "10.48550/arXiv.2201.11105", "date": "2021-12-15", "title": "Do You See What I See? Capabilities and Limits of Automated Multimedia Content Analysis", "authors": "Carey Shenkman, Dhanaraj Thakur, Emma Llans\u00f3", "abstract": "The ever-increasing amount of user-generated content online has led, in\nrecent years, to an expansion in research and investment in automated content\nanalysis tools. Scrutiny of automated content analysis has accelerated during\nthe COVID-19 pandemic, as social networking services have placed a greater\nreliance on these tools due to concerns about health risks to their moderation\nstaff from in-person work. At the same time, there are important policy debates\naround the world about how to improve content moderation while protecting free\nexpression and privacy. In order to advance these debates, we need to\nunderstand the potential role of automated content analysis tools.\n  This paper explains the capabilities and limitations of tools for analyzing\nonline multimedia content and highlights the potential risks of using these\ntools at scale without accounting for their limitations. It focuses on two main\ncategories of tools: matching models and computer prediction models. Matching\nmodels include cryptographic and perceptual hashing, which compare\nuser-generated content with existing and known content. Predictive models\n(including computer vision and computer audition) are machine learning\ntechniques that aim to identify characteristics of new or previously unknown\ncontent.", "journal": ""}
{"doi": "10.48550/arXiv.2101.03263", "date": "2021-01-09", "title": "SyReNN: A Tool for Analyzing Deep Neural Networks", "authors": "Matthew Sotoudeh, Aditya V. Thakur", "abstract": "Deep Neural Networks (DNNs) are rapidly gaining popularity in a variety of\nimportant domains. Formally, DNNs are complicated vector-valued functions which\ncome in a variety of sizes and applications. Unfortunately, modern DNNs have\nbeen shown to be vulnerable to a variety of attacks and buggy behavior. This\nhas motivated recent work in formally analyzing the properties of such DNNs.\nThis paper introduces SyReNN, a tool for understanding and analyzing a DNN by\ncomputing its symbolic representation. The key insight is to decompose the DNN\ninto linear functions. Our tool is designed for analyses using low-dimensional\nsubsets of the input space, a unique design point in the space of DNN analysis\ntools. We describe the tool and the underlying theory, then evaluate its use\nand performance on three case studies: computing Integrated Gradients,\nvisualizing a DNN's decision boundaries, and patching a DNN.", "journal": ""}
{"doi": "10.48550/arXiv.2106.07483", "date": "2021-06-14", "title": "Can Explainable AI Explain Unfairness? A Framework for Evaluating Explainable AI", "authors": "Kiana Alikhademi, Brianna Richardson, Emma Drobina, Juan E. Gilbert", "abstract": "Many ML models are opaque to humans, producing decisions too complex for\nhumans to easily understand. In response, explainable artificial intelligence\n(XAI) tools that analyze the inner workings of a model have been created.\nDespite these tools' strength in translating model behavior, critiques have\nraised concerns about the impact of XAI tools as a tool for `fairwashing` by\nmisleading users into trusting biased or incorrect models. In this paper, we\ncreated a framework for evaluating explainable AI tools with respect to their\ncapabilities for detecting and addressing issues of bias and fairness as well\nas their capacity to communicate these results to their users clearly. We found\nthat despite their capabilities in simplifying and explaining model behavior,\nmany prominent XAI tools lack features that could be critical in detecting\nbias. Developers can use our framework to suggest modifications needed in their\ntoolkits to reduce issues likes fairwashing.", "journal": ""}
{"doi": "10.48550/arXiv.2106.14973", "date": "2021-06-28", "title": "GIFT: Generalizable Interaction-aware Functional Tool Affordances without Labels", "authors": "Dylan Turpin, Liquan Wang, Stavros Tsogkas, Sven Dickinson, Animesh Garg", "abstract": "Tool use requires reasoning about the fit between an object's affordances and\nthe demands of a task. Visual affordance learning can benefit from\ngoal-directed interaction experience, but current techniques rely on human\nlabels or expert demonstrations to generate this data. In this paper, we\ndescribe a method that grounds affordances in physical interactions instead,\nthus removing the need for human labels or expert policies. We use an efficient\nsampling-based method to generate successful trajectories that provide contact\ndata, which are then used to reveal affordance representations. Our framework,\nGIFT, operates in two phases: first, we discover visual affordances from\ngoal-directed interaction with a set of procedurally generated tools; second,\nwe train a model to predict new instances of the discovered affordances on\nnovel tools in a self-supervised fashion. In our experiments, we show that GIFT\ncan leverage a sparse keypoint representation to predict grasp and interaction\npoints to accommodate multiple tasks, such as hooking, reaching, and hammering.\nGIFT outperforms baselines on all tasks and matches a human oracle on two of\nthree tasks using novel tools.", "journal": ""}
{"doi": "10.48550/arXiv.2206.00101", "date": "2022-05-31", "title": "MAD-EN: Microarchitectural Attack Detection through System-wide Energy Consumption", "authors": "Debopriya Roy Dipta, Berk Gulmezoglu", "abstract": "Microarchitectural attacks have become more threatening the hardware security\nthan before with the increasing diversity of attacks such as Spectre and\nMeltdown. Vendor patches cannot keep up with the pace of the new threats, which\nmakes the dynamic anomaly detection tools more evident than before.\nUnfortunately, previous studies utilize hardware performance counters that lead\nto high performance overhead and profile limited number of microarchitectural\nattacks due to the small number of counters that can be profiled concurrently.\nThis yields those detection tools inefficient in real-world scenarios.\n  In this study, we introduce MAD-EN dynamic detection tool that leverages\nsystem-wide energy consumption traces collected from a generic Intel RAPL tool\nto detect ongoing anomalies in a system. In our experiments, we show that\nCNN-based MAD-EN can detect 10 different microarchitectural attacks with a\ntotal of 15 variants with the highest F1 score of 0.999, which makes our tool\nthe most generic attack detection tool so far. Moreover, individual attacks can\nbe distinguished with a 98% accuracy after an anomaly is detected in a system.\nWe demonstrate that MAD-EN introduces 69.3% less performance overhead compared\nto performance counter-based detection mechanisms.", "journal": ""}
{"doi": "10.48550/arXiv.2210.09427", "date": "2022-10-17", "title": "A Pilot Study on Teacher-Facing Real-Time Classroom Game Dashboards", "authors": "Luke Swanson, David Gagnon, Jennifer Scianna", "abstract": "Educational games are an increasingly popular teaching tool in modern\nclassrooms. However, the development of complementary tools for teachers\nfacilitating classroom gameplay is lacking. We present the results of a\nparticipatory design process for a teacher-facing, real-time game data\ndashboard. This two-phase process included a workshop to elicit teachers'\nrequirements for such a tool, and a pilot study of our dashboard prototype. We\nanalyze post-gameplay survey and interview data to understand teachers'\nexperiences with the tool in terms of evidence of co-design, feasibility, and\neffectiveness. Our results indicate the participatory design yielded a tool\nboth useful for and usable by teachers within the context of a real class\ngameplay session. We advocate for the continued development of data-driven\nteacher tools to improve the effectiveness of games deployed in the classroom.", "journal": ""}
{"doi": "10.48550/arXiv.2303.13418", "date": "2023-03-23", "title": "GiveMeLabeledIssues: An Open Source Issue Recommendation System", "authors": "Joseph Vargovich, Fabio Santos, Jacob Penney, Marco A. Gerosa, Igor Steinmacher", "abstract": "Developers often struggle to navigate an Open Source Software (OSS) project's\nissue-tracking system and find a suitable task. Proper issue labeling can aid\ntask selection, but current tools are limited to classifying the issues\naccording to their type (e.g., bug, question, good first issue, feature, etc.).\nIn contrast, this paper presents a tool (GiveMeLabeledIssues) that mines\nproject repositories and labels issues based on the skills required to solve\nthem. We leverage the domain of the APIs involved in the solution (e.g., User\nInterface (UI), Test, Databases (DB), etc.) as a proxy for the required skills.\nGiveMeLabeledIssues facilitates matching developers' skills to tasks, reducing\nthe burden on project maintainers. The tool obtained a precision of 83.9% when\npredicting the API domains involved in the issues. The replication package\ncontains instructions on executing the tool and including new projects. A demo\nvideo is available at https://www.youtube.com/watch?v=ic2quUue7i8", "journal": "IEEE/ACM 20th International Conference on Mining Software\n  Repositories (MSR 2023), Data and Tool Showcase"}
{"doi": "10.48550/arXiv.2309.15723", "date": "2023-09-27", "title": "Where Are We So Far? Understanding Data Storytelling Tools from the Perspective of Human-AI Collaboration", "authors": "Haotian Li, Yun Wang, Huamin Qu", "abstract": "Data storytelling is powerful for communicating data insights, but it\nrequires diverse skills and considerable effort from human creators. Recent\nresearch has widely explored the potential for artificial intelligence (AI) to\nsupport and augment humans in data storytelling. However, there lacks a\nsystematic review to understand data storytelling tools from the perspective of\nhuman-AI collaboration, which hinders researchers from reflecting on the\nexisting collaborative tool designs that promote humans' and AI's advantages\nand mitigate their shortcomings. This paper investigated existing tools with a\nframework from two perspectives: the stages in the storytelling workflow where\na tool serves, including analysis, planning, implementation, and communication,\nand the roles of humans and AI in each stage, such as creators, assistants,\noptimizers, and reviewers. Through our analysis, we recognize the common\ncollaboration patterns in existing tools, summarize lessons learned from these\npatterns, and further illustrate research opportunities for human-AI\ncollaboration in data storytelling.", "journal": ""}
{"doi": "10.48550/arXiv.2312.07763", "date": "2023-12-12", "title": "Can LLM find the green circle? Investigation and Human-guided tool manipulation for compositional generalization", "authors": "Min Zhang, Jianfeng He, Shuo Lei, Murong Yue, Linhang Wang, Chang-Tien Lu", "abstract": "The meaning of complex phrases in natural language is composed of their\nindividual components. The task of compositional generalization evaluates a\nmodel's ability to understand new combinations of components. Previous studies\ntrained smaller, task-specific models, which exhibited poor generalization.\nWhile large language models (LLMs) exhibit impressive generalization abilities\non many tasks through in-context learning (ICL), their potential for\ncompositional generalization remains unexplored. In this paper, we first\nempirically investigate prevailing ICL methods in compositional generalization.\nWe find that they struggle with complex compositional questions due to\ncumulative errors in long reasoning steps and intricate logic required for\ntool-making. Consequently, we propose a human-guided tool manipulation\nframework (HTM) that generates tools for sub-questions and integrates multiple\ntools. Our method enhances the effectiveness of tool creation and usage with\nminimal human effort. Experiments show that our method achieves\nstate-of-the-art performance on two compositional generalization benchmarks and\noutperforms existing methods on the most challenging test split by 70%.", "journal": ""}
{"doi": "10.48550/arXiv.2401.17150", "date": "2024-01-30", "title": "GAISSALabel: A tool for energy labeling of ML models", "authors": "Pau Duran, Joel Casta\u00f1o, Cristina G\u00f3mez, Silverio Mart\u00ednez-Fern\u00e1ndez", "abstract": "Background: The increasing environmental impact of Information Technologies,\nparticularly in Machine Learning (ML), highlights the need for sustainable\npractices in software engineering. The escalating complexity and energy\nconsumption of ML models need tools for assessing and improving their energy\nefficiency. Goal: This paper introduces GAISSALabel, a web-based tool designed\nto evaluate and label the energy efficiency of ML models. Method: GAISSALabel\nis a technology transfer development from a former research on energy\nefficiency classification of ML, consisting of a holistic tool for assessing\nboth the training and inference phases of ML models, considering various\nmetrics such as power draw, model size efficiency, CO2e emissions and more.\nResults: GAISSALabel offers a labeling system for energy efficiency, akin to\nlabels on consumer appliances, making it accessible to ML stakeholders of\nvarying backgrounds. The tool's adaptability allows for customization in the\nproposed labeling system, ensuring its relevance in the rapidly evolving ML\nfield. Conclusions: GAISSALabel represents a significant step forward in\nsustainable software engineering, offering a solution for balancing\nhigh-performance ML models with environmental impacts. The tool's effectiveness\nand market relevance will be further assessed through planned evaluations using\nthe Technology Acceptance Model.", "journal": ""}
{"doi": "10.48550/arXiv.2406.13436", "date": "2024-06-19", "title": "What's Next? Exploring Utilization, Challenges, and Future Directions of AI-Generated Image Tools in Graphic Design", "authors": "Yuying Tang, Mariana Ciancia, Zhigang Wang, Ze Gao", "abstract": "Recent advancements in artificial intelligence, such as computer vision and\ndeep learning, have led to the emergence of numerous generative AI platforms,\nparticularly for image generation. However, the application of AI-generated\nimage tools in graphic design has not been extensively explored. This study\nconducted semi-structured interviews with seven designers of varying experience\nlevels to understand their current usage, challenges, and future functional\nneeds for AI-generated image tools in graphic design. As our findings suggest,\nAI tools serve as creative partners in design, enhancing human creativity,\noffering strategic insights, and fostering team collaboration and\ncommunication. The findings provide guiding recommendations for the future\ndevelopment of AI-generated image tools, aimed at helping engineers optimize\nthese tools to better meet the needs of graphic designers.", "journal": ""}
{"doi": "10.48550/arXiv.2408.04682", "date": "2024-08-08", "title": "ToolSandbox: A Stateful, Conversational, Interactive Evaluation Benchmark for LLM Tool Use Capabilities", "authors": "Jiarui Lu, Thomas Holleis, Yizhe Zhang, Bernhard Aumayer, Feng Nan, Felix Bai, Shuang Ma, Shen Ma, Mengyu Li, Guoli Yin, Zirui Wang, Ruoming Pang", "abstract": "Recent large language models (LLMs) advancements sparked a growing research\ninterest in tool assisted LLMs solving real-world challenges, which calls for\ncomprehensive evaluation of tool-use capabilities. While previous works focused\non either evaluating over stateless web services (RESTful API), based on a\nsingle turn user prompt, or an off-policy dialog trajectory, ToolSandbox\nincludes stateful tool execution, implicit state dependencies between tools, a\nbuilt-in user simulator supporting on-policy conversational evaluation and a\ndynamic evaluation strategy for intermediate and final milestones over an\narbitrary trajectory. We show that open source and proprietary models have a\nsignificant performance gap, and complex tasks like State Dependency,\nCanonicalization and Insufficient Information defined in ToolSandbox are\nchallenging even the most capable SOTA LLMs, providing brand-new insights into\ntool-use LLM capabilities. ToolSandbox evaluation framework is released at\nhttps://github.com/apple/ToolSandbox", "journal": ""}
{"doi": "10.48550/arXiv.2412.03096", "date": "2024-12-04", "title": "TOOL-ED: Enhancing Empathetic Response Generation with the Tool Calling Capability of LLM", "authors": "Huiying Cao, Yiqun Zhang, Shi Feng, Xiaocui Yang, Daling Wang, Yifei Zhang", "abstract": "Empathetic conversation is a crucial characteristic in daily conversations\nbetween individuals. Nowadays, Large Language models (LLMs) have shown\noutstanding performance in generating empathetic responses. Knowledge bases\nlike COMET can assist LLMs in mitigating illusions and enhancing the\nunderstanding of users' intentions and emotions. However, models remain heavily\nreliant on fixed knowledge bases and unrestricted incorporation of external\nknowledge can introduce noise. Tool learning is a flexible end-to-end approach\nthat assists LLMs in handling complex problems. In this paper, we propose\nEmotional Knowledge Tool Calling (EKTC) framework, which encapsulates the\ncommonsense knowledge bases as empathetic tools, enabling LLMs to integrate\nexternal knowledge flexibly through tool calling. In order to adapt the models\nto the new task, we construct a novel dataset TOOL-ED based on the\nEMPATHETICMPATHETIC DIALOGUE (ED) dataset. We validate EKTC on the ED dataset,\nand the experimental results demonstrate that our framework can enhance the\nability of LLMs to generate empathetic responses effectively.", "journal": ""}
{"doi": "10.48550/arXiv.2412.12151", "date": "2024-12-11", "title": "SMARTCAL: An Approach to Self-Aware Tool-Use Evaluation and Calibration", "authors": "Yuanhao Shen, Xiaodan Zhu, Lei Chen", "abstract": "The tool-use ability of Large Language Models (LLMs) has a profound impact on\na wide range of industrial applications. However, LLMs' self-control and\ncalibration capability in appropriately using tools remains understudied. The\nproblem is consequential as it raises potential risks of degraded performance\nand poses a threat to the trustworthiness of the models. In this paper, we\nconduct a study on a family of state-of-the-art LLMs on three datasets with two\nmainstream tool-use frameworks. Our study reveals the tool-abuse behavior of\nLLMs, a tendency for models to misuse tools with overconfidence. We also find\nthat this is a common issue regardless of model capability. Accordingly, we\npropose a novel approach, \\textit{SMARTCAL}, to mitigate the observed issues,\nand our results show an average of 8.6 percent increase in the QA performance\nand a 21.6 percent decrease in Expected Calibration Error (ECE) compared to\nbaseline models.", "journal": ""}
{"doi": "10.48550/arXiv.2505.07064", "date": "2025-05-11", "title": "ParaView-MCP: An Autonomous Visualization Agent with Direct Tool Use", "authors": "Shusen Liu, Haichao Miao, Peer-Timo Bremer", "abstract": "While powerful and well-established, tools like ParaView present a steep\nlearning curve that discourages many potential users. This work introduces\nParaView-MCP, an autonomous agent that integrates modern multimodal large\nlanguage models (MLLMs) with ParaView to not only lower the barrier to entry\nbut also augment ParaView with intelligent decision support. By leveraging the\nstate-of-the-art reasoning, command execution, and vision capabilities of\nMLLMs, ParaView-MCP enables users to interact with ParaView through natural\nlanguage and visual inputs. Specifically, our system adopted the Model Context\nProtocol (MCP) - a standardized interface for model-application communication -\nthat facilitates direct interaction between MLLMs with ParaView's Python API to\nallow seamless information exchange between the user, the language model, and\nthe visualization tool itself. Furthermore, by implementing a visual feedback\nmechanism that allows the agent to observe the viewport, we unlock a range of\nnew capabilities, including recreating visualizations from examples,\nclosed-loop visualization parameter updates based on user-defined goals, and\neven cross-application collaboration involving multiple tools. Broadly, we\nbelieve such an agent-driven visualization paradigm can profoundly change the\nway we interact with visualization tools. We expect a significant uptake in the\ndevelopment of such visualization tools, in both visualization research and\nindustry.", "journal": ""}
{"doi": "10.48550/arXiv.2505.21569", "date": "2025-05-27", "title": "ChemHAS: Hierarchical Agent Stacking for Enhancing Chemistry Tools", "authors": "Zhucong Li, Bowei Zhang, Jin Xiao, Zhijian Zhou, Fenglei Cao, Jiaqing Liang, Yuan Qi", "abstract": "Large Language Model (LLM)-based agents have demonstrated the ability to\nimprove performance in chemistry-related tasks by selecting appropriate tools.\nHowever, their effectiveness remains limited by the inherent prediction errors\nof chemistry tools. In this paper, we take a step further by exploring how\nLLMbased agents can, in turn, be leveraged to reduce prediction errors of the\ntools. To this end, we propose ChemHAS (Chemical Hierarchical Agent Stacking),\na simple yet effective method that enhances chemistry tools through optimizing\nagent-stacking structures from limited data. ChemHAS achieves state-of-the-art\nperformance across four fundamental chemistry tasks, demonstrating that our\nmethod can effectively compensate for prediction errors of the tools.\nFurthermore, we identify and characterize four distinct agent-stacking\nbehaviors, potentially improving interpretability and revealing new\npossibilities for AI agent applications in scientific research. Our code and\ndataset are publicly available at https:\n//anonymous.4open.science/r/ChemHAS-01E4/README.md.", "journal": ""}
{"doi": "10.48550/arXiv.2506.01276", "date": "2025-06-02", "title": "Schema as Parameterized Tools for Universal Information Extraction", "authors": "Sheng Liang, Yongyue Zhang, Yaxiong Wu, Ruiming Tang, Yong Liu", "abstract": "Universal information extraction (UIE) primarily employs an extractive\ngeneration approach with large language models (LLMs), typically outputting\nstructured information based on predefined schemas such as JSON or tables. UIE\nsuffers from a lack of adaptability when selecting between predefined schemas\nand on-the-fly schema generation within the in-context learning paradigm,\nespecially when there are numerous schemas to choose from. In this paper, we\npropose a unified adaptive text-to-structure generation framework, called\nSchema as Parameterized Tools (SPT), which reimagines the tool-calling\ncapability of LLMs by treating predefined schemas as parameterized tools for\ntool selection and parameter filling. Specifically, our SPT method can be\napplied to unify closed, open, and on-demand IE tasks by adopting Schema\nRetrieval by fetching the relevant schemas from a predefined pool, Schema\nFilling by extracting information and filling slots as with tool parameters, or\nSchema Generation by synthesizing new schemas with uncovered cases. Experiments\nshow that the SPT method can handle four distinct IE tasks adaptively,\ndelivering robust schema retrieval and selection performance. SPT also achieves\ncomparable extraction performance to LoRA baselines and current leading UIE\nsystems with significantly fewer trainable parameters.", "journal": ""}
{"doi": "10.48550/arXiv.0712.1800", "date": "2007-12-11", "title": "Conception d'outils de communication sp\u00e9cifiques au contexte \u00e9ducatif", "authors": "S\u00e9bastien George, C\u00e9cile Bothorel", "abstract": "In a distance learning context, providing usual communication tools (forum,\nchat, ...) is not always enough to create efficient interactions between\nlearners and to favour collective knowledge building. A solution consists in\nsetting-up collective activities which encourage learners to communicate. But,\neven in that case, tools can sometimes become a barrier to communication. We\npresent in this paper examples of specific tools that are designed in order to\nfavour and to guide communications in an educational context, but also to\nfoster interactions during learning activities that are not inherently\ncollaborative. We describe synchronous communication tools (semi-structured\nchat), asynchronous tools (temporally structured forum, contextual forum) and a\nsystem which promotes mutual aid between learners.", "journal": "Sciences et Technologies de l'Information et de la Communication\n  pour l'Education et la Formation 13 (2007) 317-344"}
{"doi": "10.48550/arXiv.1110.6755", "date": "2011-10-31", "title": "PAC-Bayes-Bernstein Inequality for Martingales and its Application to Multiarmed Bandits", "authors": "Yevgeny Seldin, Nicol\u00f2 Cesa-Bianchi, Peter Auer, Fran\u00e7ois Laviolette, John Shawe-Taylor", "abstract": "We develop a new tool for data-dependent analysis of the\nexploration-exploitation trade-off in learning under limited feedback. Our tool\nis based on two main ingredients. The first ingredient is a new concentration\ninequality that makes it possible to control the concentration of weighted\naverages of multiple (possibly uncountably many) simultaneously evolving and\ninterdependent martingales. The second ingredient is an application of this\ninequality to the exploration-exploitation trade-off via importance weighted\nsampling. We apply the new tool to the stochastic multiarmed bandit problem,\nhowever, the main importance of this paper is the development and understanding\nof the new tool rather than improvement of existing algorithms for stochastic\nmultiarmed bandits. In the follow-up work we demonstrate that the new tool can\nimprove over state-of-the-art in structurally richer problems, such as\nstochastic multiarmed bandits with side information (Seldin et al., 2011a).", "journal": ""}
{"doi": "10.48550/arXiv.1506.06579", "date": "2015-06-22", "title": "Understanding Neural Networks Through Deep Visualization", "authors": "Jason Yosinski, Jeff Clune, Anh Nguyen, Thomas Fuchs, Hod Lipson", "abstract": "Recent years have produced great advances in training large, deep neural\nnetworks (DNNs), including notable successes in training convolutional neural\nnetworks (convnets) to recognize natural images. However, our understanding of\nhow these models work, especially what computations they perform at\nintermediate layers, has lagged behind. Progress in the field will be further\naccelerated by the development of better tools for visualizing and interpreting\nneural nets. We introduce two such tools here. The first is a tool that\nvisualizes the activations produced on each layer of a trained convnet as it\nprocesses an image or video (e.g. a live webcam stream). We have found that\nlooking at live activations that change in response to user input helps build\nvaluable intuitions about how convnets work. The second tool enables\nvisualizing features at each layer of a DNN via regularized optimization in\nimage space. Because previous versions of this idea produced less recognizable\nimages, here we introduce several new regularization methods that combine to\nproduce qualitatively clearer, more interpretable visualizations. Both tools\nare open source and work on a pre-trained convnet with minimal setup.", "journal": ""}
{"doi": "10.48550/arXiv.1703.00757", "date": "2017-03-02", "title": "Predicting Rankings of Software Verification Competitions", "authors": "Mike Czech, Eyke H\u00fcllermeier, Marie-Christine Jakobs, Heike Wehrheim", "abstract": "Software verification competitions, such as the annual SV-COMP, evaluate\nsoftware verification tools with respect to their effectivity and efficiency.\nTypically, the outcome of a competition is a (possibly category-specific)\nranking of the tools. For many applications, such as building portfolio\nsolvers, it would be desirable to have an idea of the (relative) performance of\nverification tools on a given verification task beforehand, i.e., prior to\nactually running all tools on the task.\n  In this paper, we present a machine learning approach to predicting rankings\nof tools on verification tasks. The method builds upon so-called label ranking\nalgorithms, which we complement with appropriate kernels providing a similarity\nmeasure for verification tasks. Our kernels employ a graph representation for\nsoftware source code that mixes elements of control flow and program dependence\ngraphs with abstract syntax trees. Using data sets from SV-COMP, we demonstrate\nour rank prediction technique to generalize well and achieve a rather high\npredictive accuracy. In particular, our method outperforms a recently proposed\nfeature-based approach of Demyanova et al. (when applied to rank predictions).", "journal": ""}
{"doi": "10.48550/arXiv.1904.08007", "date": "2019-04-16", "title": "Metamorphic Testing for Quality Assurance of Protein Function Prediction Tools", "authors": "Morteza Pourreza Shahri, Madhusudan Srinivasan, Gillian Reynolds, Diane Bimczok, Indika Kahanda, Upulee Kanewala", "abstract": "Proteins are the workhorses of life and gaining insight on their functions is\nof paramount importance for applications such as drug design. However, the\nexperimental validation of functions of proteins is highly-resource consuming.\nTherefore, recently, automated protein function prediction (AFP) using machine\nlearning has gained significant interest. Many of these AFP tools are based on\nsupervised learning models trained using existing gold-standard functional\nannotations, which are known to be incomplete. The main challenge associated\nwith conducting systematic testing on AFP software is the lack of a test\noracle, which determines passing or failing of a test case; unfortunately, due\nto the incompleteness of gold-standard data, the exact expected outcomes are\nnot well defined for the AFP task. Thus, AFP tools face the \\emph{oracle\nproblem}. In this work, we use metamorphic testing (MT) to test nine\nstate-of-the-art AFP tools by defining a set of metamorphic relations (MRs)\nthat apply input transformations to protein sequences. According to our\nresults, we observe that several AFP tools fail all the test cases causing\nconcerns over the quality of their predictions.", "journal": ""}
{"doi": "10.48550/arXiv.2201.05647", "date": "2022-01-14", "title": "Tools and Practices for Responsible AI Engineering", "authors": "Ryan Soklaski, Justin Goodwin, Olivia Brown, Michael Yee, Jason Matterer", "abstract": "Responsible Artificial Intelligence (AI) - the practice of developing,\nevaluating, and maintaining accurate AI systems that also exhibit essential\nproperties such as robustness and explainability - represents a multifaceted\nchallenge that often stretches standard machine learning tooling, frameworks,\nand testing methods beyond their limits. In this paper, we present two new\nsoftware libraries - hydra-zen and the rAI-toolbox - that address critical\nneeds for responsible AI engineering. hydra-zen dramatically simplifies the\nprocess of making complex AI applications configurable, and their behaviors\nreproducible. The rAI-toolbox is designed to enable methods for evaluating and\nenhancing the robustness of AI-models in a way that is scalable and that\ncomposes naturally with other popular ML frameworks. We describe the design\nprinciples and methodologies that make these tools effective, including the use\nof property-based testing to bolster the reliability of the tools themselves.\nFinally, we demonstrate the composability and flexibility of the tools by\nshowing how various use cases from adversarial robustness and explainable AI\ncan be concisely implemented with familiar APIs.", "journal": ""}
{"doi": "10.48550/arXiv.1906.03277", "date": "2019-06-07", "title": "xBIT: an easy to use scanning tool with machine learning abilities", "authors": "Florian Staub", "abstract": "xBIT is a tool for performing parameter scans in beyond the Standard Model\ntheories. It's written in Python and fully open source. The main purpose of\nxBIT is to provide an easy to use tool to help phenomenologists with their\ndaily task: exploring the parameter space of new models. It was developed under\nthe impression of the SARAH/SPheno framework, but should be use-able with other\ntools as well that use the SLHA format to transfer data. It also supports by\ndefault MicrOmegas for dark matter calculations, HiggsBounds and HiggsSignals\nfor checking the Higgs properties, and Vevacious for testing the vacuum\nstability. Classes for other tools can be added if necessary. In order to\nimprove the efficiency of the parameter scans, the recently proposed 'Machine\nLearning Scan' approach is included. For this purpose, xBIT uses pyTorch to\ndeal with artificial neural networks.", "journal": ""}
{"doi": "10.48550/arXiv.2005.14545", "date": "2020-05-27", "title": "Seamlessly Integrating Loops That Matter into Model Development and Analysis", "authors": "William Schoenberg, Robert Eberlein", "abstract": "Understanding why models behave the way they do is critical to learning from\nthem, and to conveying the insights they offer to a broad audience. The Loops\nthat Matter methodology automatically shows which loops are dominating behavior\nat each point in time and generates simplified causal loop diagrams from a user\nadjustable set of important loops. This paper describes the challenges of\nimplementing these tools into a fully functioning model development environment\nalong with the solutions developed. The promise of the tools has, if anything,\nbeen amplified by the results of this implementation, and we give several\nexamples of using the tools. For pedagogical models Loops that Matter can ease\ncommunication while speeding and deepening learning. For complex models the\ntools allow the extraction of realistic explanations of behavior in the form of\nanimated simplified causal loop diagrams. For models with discrete and\ndiscontinuous elements, the bigger feedback picture is still easily\ndiscoverable. While there will doubtless be refinements and enhancement to the\ndelivered tools, they represent a large step forward in our ability to\nunderstand models from conceptualization through delivery.", "journal": ""}
{"doi": "10.48550/arXiv.2007.04848", "date": "2020-07-09", "title": "Guru, Partner, or Pencil Sharpener? Understanding Designers' Attitudes Towards Intelligent Creativity Support Tools", "authors": "Angus Main, Mick Grierson", "abstract": "Creativity Support Tools (CST) aim to enhance human creativity, but the\ndeeply personal and subjective nature of creativity makes the design of\nuniversal support tools challenging. Individuals develop personal approaches to\ncreativity, particularly in the context of commercial design where signature\nstyles and techniques are valuable commodities. Artificial Intelligence (AI)\nand Machine Learning (ML) techniques could provide a means of creating\n'intelligent' CST which learn and adapt to personal styles of creativity.\nIdentifying what kind of role such tools could play in the design process\nrequires a better understanding of designers' attitudes towards working with\nAI, and their willingness to include it in their personal creative process.\nThis paper details the results of a survey of professional designers which\nindicates a positive and pragmatic attitude towards collaborating with AI\ntools, and a particular opportunity for incorporating them in the research\nstages of a design project.", "journal": ""}
{"doi": "10.48550/arXiv.2206.14998", "date": "2022-06-30", "title": "Understanding Physical Effects for Effective Tool-use", "authors": "Zeyu Zhang, Ziyuan Jiao, Weiqi Wang, Yixin Zhu, Song-Chun Zhu, Hangxin Liu", "abstract": "We present a robot learning and planning framework that produces an effective\ntool-use strategy with the least joint efforts, capable of handling objects\ndifferent from training. Leveraging a Finite Element Method (FEM)-based\nsimulator that reproduces fine-grained, continuous visual and physical effects\ngiven observed tool-use events, the essential physical properties contributing\nto the effects are identified through the proposed Iterative Deepening Symbolic\nRegression (IDSR) algorithm. We further devise an optimal control-based motion\nplanning scheme to integrate robot- and tool-specific kinematics and dynamics\nto produce an effective trajectory that enacts the learned properties. In\nsimulation, we demonstrate that the proposed framework can produce more\neffective tool-use strategies, drastically different from the observed ones in\ntwo exemplar tasks.", "journal": "IEEE Robotics and Automation Letters, 2022"}
{"doi": "10.48550/arXiv.2212.10376", "date": "2022-12-20", "title": "The Third International Verification of Neural Networks Competition (VNN-COMP 2022): Summary and Results", "authors": "Mark Niklas M\u00fcller, Christopher Brix, Stanley Bak, Changliu Liu, Taylor T. Johnson", "abstract": "This report summarizes the 3rd International Verification of Neural Networks\nCompetition (VNN-COMP 2022), held as a part of the 5th Workshop on Formal\nMethods for ML-Enabled Autonomous Systems (FoMLAS), which was collocated with\nthe 34th International Conference on Computer-Aided Verification (CAV).\nVNN-COMP is held annually to facilitate the fair and objective comparison of\nstate-of-the-art neural network verification tools, encourage the\nstandardization of tool interfaces, and bring together the neural network\nverification community. To this end, standardized formats for networks (ONNX)\nand specification (VNN-LIB) were defined, tools were evaluated on equal-cost\nhardware (using an automatic evaluation pipeline based on AWS instances), and\ntool parameters were chosen by the participants before the final test sets were\nmade public. In the 2022 iteration, 11 teams participated on a diverse set of\n12 scored benchmarks. This report summarizes the rules, benchmarks,\nparticipating tools, results, and lessons learned from this iteration of this\ncompetition.", "journal": ""}
{"doi": "10.48550/arXiv.2311.12806", "date": "2023-09-19", "title": "MatGD: Materials Graph Digitizer", "authors": "Jaewoong Lee, Wonseok Lee, Jihan Kim", "abstract": "We have developed MatGD (Material Graph Digitizer), which is a tool for\ndigitizing a data line from scientific graphs. The algorithm behind the tool\nconsists of four steps: (1) identifying graphs within subfigures, (2)\nseparating axes and data sections, (3) discerning the data lines by eliminating\nirrelevant graph objects and matching with the legend, and (4) data extraction\nand saving. From the 62,534 papers in the areas of batteries, catalysis, and\nMOFs, 501,045 figures were mined. Remarkably, our tool showcased performance\nwith over 99% accuracy in legend marker and text detection. Moreover, its\ncapability for data line separation stood at 66%, which is much higher compared\nto other existing figure mining tools. We believe that this tool will be\nintegral to collecting both past and future data from publications, and these\ndata can be used to train various machine learning models that can enhance\nmaterial predictions and new materials discovery.", "journal": ""}
{"doi": "10.48550/arXiv.2312.16760", "date": "2023-12-28", "title": "The Fourth International Verification of Neural Networks Competition (VNN-COMP 2023): Summary and Results", "authors": "Christopher Brix, Stanley Bak, Changliu Liu, Taylor T. Johnson", "abstract": "This report summarizes the 4th International Verification of Neural Networks\nCompetition (VNN-COMP 2023), held as a part of the 6th Workshop on Formal\nMethods for ML-Enabled Autonomous Systems (FoMLAS), that was collocated with\nthe 35th International Conference on Computer-Aided Verification (CAV).\nVNN-COMP is held annually to facilitate the fair and objective comparison of\nstate-of-the-art neural network verification tools, encourage the\nstandardization of tool interfaces, and bring together the neural network\nverification community. To this end, standardized formats for networks (ONNX)\nand specification (VNN-LIB) were defined, tools were evaluated on equal-cost\nhardware (using an automatic evaluation pipeline based on AWS instances), and\ntool parameters were chosen by the participants before the final test sets were\nmade public. In the 2023 iteration, 7 teams participated on a diverse set of 10\nscored and 4 unscored benchmarks. This report summarizes the rules, benchmarks,\nparticipating tools, results, and lessons learned from this iteration of this\ncompetition.", "journal": ""}
{"doi": "10.48550/arXiv.2403.07562", "date": "2024-03-12", "title": "A Flexible Cell Classification for ML Projects in Jupyter Notebooks", "authors": "Miguel Perez, Selin Aydin, Horst Lichter", "abstract": "Jupyter Notebook is an interactive development environment commonly used for\nrapid experimentation of machine learning (ML) solutions. Describing the ML\nactivities performed along code cells improves the readability and\nunderstanding of Notebooks. Manual annotation of code cells is time-consuming\nand error-prone. Therefore, tools have been developed that classify the cells\nof a notebook concerning the ML activity performed in them. However, the\ncurrent tools are not flexible, as they work based on look-up tables that have\nbeen created, which map function calls of commonly used ML libraries to ML\nactivities. These tables must be manually adjusted to account for new or\nchanged libraries.\n  This paper presents a more flexible approach to cell classification based on\na hybrid classification approach that combines a rule-based and a decision tree\nclassifier. We discuss the design rationales and describe the developed\nclassifiers in detail. We implemented the new flexible cell classification\napproach in a tool called JupyLabel. Its evaluation and the obtained metric\nscores regarding precision, recall, and F1-score are discussed. Additionally,\nwe compared JupyLabel with HeaderGen, an existing cell classification tool. We\nwere able to show that the presented flexible cell classification approach\noutperforms this tool significantly.", "journal": ""}
{"doi": "10.48550/arXiv.2403.07714", "date": "2024-03-12", "title": "StableToolBench: Towards Stable Large-Scale Benchmarking on Tool Learning of Large Language Models", "authors": "Zhicheng Guo, Sijie Cheng, Hao Wang, Shihao Liang, Yujia Qin, Peng Li, Zhiyuan Liu, Maosong Sun, Yang Liu", "abstract": "Large Language Models (LLMs) have witnessed remarkable advancements in recent\nyears, prompting the exploration of tool learning, which integrates LLMs with\nexternal tools to address diverse real-world challenges. Assessing the\ncapability of LLMs to utilise tools necessitates large-scale and stable\nbenchmarks. However, previous works relied on either hand-crafted online tools\nwith limited scale, or large-scale real online APIs suffering from instability\nof API status. To address this problem, we introduce StableToolBench, a\nbenchmark evolving from ToolBench, proposing a virtual API server and stable\nevaluation system. The virtual API server contains a caching system and API\nsimulators which are complementary to alleviate the change in API status.\nMeanwhile, the stable evaluation system designs solvable pass and win rates\nusing GPT-4 as the automatic evaluator to eliminate the randomness during\nevaluation. Experimental results demonstrate the stability of StableToolBench,\nand further discuss the effectiveness of API simulators, the caching system,\nand the evaluator system.", "journal": ""}
{"doi": "10.48550/arXiv.2408.04032", "date": "2024-08-07", "title": "The Evolution of Information Seeking in Software Development: Understanding the Role and Impact of AI Assistants", "authors": "Ebtesam Al Haque, Chris Brown, Thomas D. LaToza, Brittany Johnson", "abstract": "About 32% of a software practitioners' day involves seeking and using\ninformation to support task completion. Although the information needs of\nsoftware practitioners have been studied extensively, the impact of AI-assisted\ntools on their needs and information-seeking behaviors remains largely\nunexplored. To addresses this gap, we conducted a mixed-method study to\nunderstand AI-assisted information seeking behavior of practitioners and its\nimpact on their perceived productivity and skill development. We found that\ndevelopers are increasingly using AI tools to support their information\nseeking, citing increased efficiency as a key benefit. Our findings also\namplify caveats that come with effectively using AI tools for information\nseeking, especially for learning and skill development, such as the importance\nof foundational developer knowledge that can guide and inform the information\nprovided by AI tools. Our efforts have implications for the effective\nintegration of AI tools into developer workflows as information retrieval\nsystems and learning aids.", "journal": ""}
{"doi": "10.48550/arXiv.2410.01096", "date": "2024-10-01", "title": "Mechanic Maker: Accessible Game Development Via Symbolic Learning Program Synthesis", "authors": "Megan Sumner, Vardan Saini, Matthew Guzdial", "abstract": "Game development is a highly technical practice that traditionally requires\nprogramming skills. This serves as a barrier to entry for would-be developers\nor those hoping to use games as part of their creative expression. While there\nhave been prior game development tools focused on accessibility, they generally\nstill require programming, or have major limitations in terms of the kinds of\ngames they can make. In this paper we introduce Mechanic Maker, a tool for\ncreating a wide-range of game mechanics without programming. It instead relies\non a backend symbolic learning system to synthesize game mechanics from\nexamples. We conducted a user study to evaluate the benefits of the tool for\nparticipants with a variety of programming and game development experience. Our\nresults demonstrated that participants' ability to use the tool was unrelated\nto programming ability. We conclude that tools like ours could help democratize\ngame development, making the practice accessible regardless of programming\nskills.", "journal": ""}
{"doi": "10.48550/arXiv.2412.16228", "date": "2024-12-18", "title": "TAACKIT: Track Annotation and Analytics with Continuous Knowledge Integration Tool", "authors": "Lily Lee, Julian Fontes, Andrew Weinert, Laura Schomacker, Daniel Stabile, Jonathan Hou", "abstract": "Machine learning (ML) is a powerful tool for efficiently analyzing data,\ndetecting patterns, and forecasting trends across various domains such as text,\naudio, and images. The availability of annotation tools to generate reliably\nannotated data is crucial for advances in ML applications. In the domain of\ngeospatial tracks, the lack of such tools to annotate and validate data impedes\nrapid and accessible ML application development. This paper presents Track\nAnnotation and Analytics with Continuous Knowledge Integration Tool (TAACKIT)\nto serve the critically important functions of annotating geospatial track data\nand validating ML models. We demonstrate an ML application use case in the air\ntraffic domain to illustrate its data annotation and model evaluation power and\nquantify the annotation effort reduction.", "journal": "AIxDKE 2024"}
{"doi": "10.48550/arXiv.2412.19985", "date": "2024-12-28", "title": "The Fifth International Verification of Neural Networks Competition (VNN-COMP 2024): Summary and Results", "authors": "Christopher Brix, Stanley Bak, Taylor T. Johnson, Haoze Wu", "abstract": "This report summarizes the 5th International Verification of Neural Networks\nCompetition (VNN-COMP 2024), held as a part of the 7th International Symposium\non AI Verification (SAIV), that was collocated with the 36th International\nConference on Computer-Aided Verification (CAV). VNN-COMP is held annually to\nfacilitate the fair and objective comparison of state-of-the-art neural network\nverification tools, encourage the standardization of tool interfaces, and bring\ntogether the neural network verification community. To this end, standardized\nformats for networks (ONNX) and specification (VNN-LIB) were defined, tools\nwere evaluated on equal-cost hardware (using an automatic evaluation pipeline\nbased on AWS instances), and tool parameters were chosen by the participants\nbefore the final test sets were made public. In the 2024 iteration, 8 teams\nparticipated on a diverse set of 12 regular and 8 extended benchmarks. This\nreport summarizes the rules, benchmarks, participating tools, results, and\nlessons learned from this iteration of this competition.", "journal": ""}
{"doi": "10.48550/arXiv.2503.19280", "date": "2025-03-25", "title": "LogicLearner: A Tool for the Guided Practice of Propositional Logic Proofs", "authors": "Amogh Inamdar, Uzay Macar, Michel Vazirani, Michael Tarnow, Zarina Mustapha, Natalia Dittren, Sam Sadeh, Nakul Verma, Ansaf Salleb-Aouissi", "abstract": "The study of propositional logic -- fundamental to the theory of computing --\nis a cornerstone of the undergraduate computer science curriculum. Learning to\nsolve logical proofs requires repeated guided practice, but undergraduate\nstudents often lack access to on-demand tutoring in a judgment-free\nenvironment. In this work, we highlight the need for guided practice tools in\nundergraduate mathematics education and outline the desiderata of an effective\npractice tool. We accordingly develop LogicLearner, a web application for\nguided logic proof practice. LogicLearner consists of an interface to attempt\nlogic proofs step-by-step and an automated proof solver to generate solutions\non the fly, allowing users to request guidance as needed. We pilot LogicLearner\nas a practice tool in two semesters of an undergraduate discrete mathematics\ncourse and receive strongly positive feedback for usability and pedagogical\nvalue in student surveys. To the best of our knowledge, LogicLearner is the\nonly learning tool that provides an end-to-end practice environment for logic\nproofs with immediate, judgment-free feedback.", "journal": ""}
{"doi": "10.48550/arXiv.2504.15185", "date": "2025-04-21", "title": "ForgeBench: A Machine Learning Benchmark Suite and Auto-Generation Framework for Next-Generation HLS Tools", "authors": "Andy Wanna, Hanqiu Chen, Cong Hao", "abstract": "Although High-Level Synthesis (HLS) has attracted considerable interest in\nhardware design, it has not yet become mainstream due to two primary\nchallenges. First, current HLS hardware design benchmarks are outdated as they\ndo not cover modern machine learning (ML) applications, preventing the rigorous\ndevelopment of HLS tools on ML-focused hardware design. Second, existing HLS\ntools are outdated because they predominantly target individual accelerator\ndesigns and lack an architecture-oriented perspective to support common\nhardware module extraction and reuse, limiting their adaptability and broader\napplicability. Motivated by these two limitations, we propose ForgeBench, an\nML-focused benchmark suite with a hardware design auto-generation framework for\nnext-generation HLS tools. In addition to the auto-generation framework, we\nprovide two ready-to-use benchmark suites. The first contains over 6,000\nrepresentative ML HLS designs. We envision future HLS tools being\narchitecture-oriented, capable of automatically identifying common\ncomputational modules across designs, and supporting flexible dataflow and\ncontrol. Accordingly, the second benchmark suite includes ML HLS designs with\npossible resource sharing manually implemented to highlight the necessity of\narchitecture-oriented design, ensuring it is future-HLS ready. ForgeBench is\nopen-sourced at https://github.com/hchen799/ForgeBench .", "journal": ""}
{"doi": "10.48550/arXiv.2506.14821", "date": "2025-06-10", "title": "Reinforcing VLMs to Use Tools for Detailed Visual Reasoning Under Resource Constraints", "authors": "Sunil Kumar, Bowen Zhao, Leo Dirac, Paulina Varshavskaya", "abstract": "Despite tremendous recent advances in large model reasoning ability,\nvision-language models (VLMs) still struggle with detailed visual reasoning,\nespecially when compute resources are limited. To address this challenge, we\ndraw inspiration from methods like Deepseek-r1 for VLMs and train smaller-scale\nmodels with Group Relative Policy Optimization (GRPO) to use external tools\nsuch as zoom. The greatest benefit is obtained with a combination of GRPO\nlearning, a simple reward structure, a simplified tool-calling interface,\nallocating additional tokens to the result of the tool call, and a training\ndata mix that over-represents visually difficult examples. Compared to\nsimilarly-sized baseline models, our method achieves better performance on some\nvisual question-answering (VQA) tasks, thanks to the detailed visual\ninformation gathered from the external tool.", "journal": ""}
{"doi": "10.48550/arXiv.1604.05799", "date": "2016-04-20", "title": "Mainstreaming video annotation software for critical video analysis", "authors": "Matthew Martin, James Charlton, Andy M. Connor", "abstract": "The range of video annotation software currently available is set within\ncommercially specialized professions, distributed via outdated sources or\nthrough online video hosting services. As video content becomes an increasingly\nsignificant tool for analysis, there is a demand for appropriate digital\nannotation techniques that offer equivalent functionality to tools used for\nannotation of text based literature sources. This paper argues for the\nimportance of video annotating as an effective method for research that is as\naccessible as literature annotation is. Video annotation has been shown to\ntrigger higher learning and engagement but research struggles to explain the\nabsence of video annotation in contemporary structures of education practice.\nIn both academic and informal settings the use of video playback as a\nmeaningful tool of analysis is apparent, yet the availability of supplementary\nannotation software is not within obvious grasp or even prevalent in\nstandardized computer software. Practical software tools produced by the\nresearcher have demonstrated effective video annotation in a short development\ntime. With software design programs available for rapid application creation,\nthis paper also highlights the absence of a development community. This paper\nargues that video annotation is an accessible tool, not just for academic\ncontexts, but also for wider practical video analysis applications, potentially\nbecoming a mainstream learning tool. This paper thus presents a practical\nmultimodal public approach to video research that potentially affords a deeper\nanalysis of media content. This is supported by an in-depth consideration of\nthe motivation for undertaking video annotation and a critical analysis of\ncurrently available tools.", "journal": "Journal of Technologies and Human Usability, 11(3), 1-13 (2015)"}
{"doi": "10.48550/arXiv.1902.05064", "date": "2019-02-12", "title": "PLIT: An alignment-free computational tool for identification of long non-coding RNAs in plant transcriptomic datasets", "authors": "S. Deshpande, J. Shuttleworth, J. Yang, S. Taramonli, M. England", "abstract": "Long non-coding RNAs (lncRNAs) are a class of non-coding RNAs which play a\nsignificant role in several biological processes. RNA-seq based transcriptome\nsequencing has been extensively used for identification of lncRNAs. However,\naccurate identification of lncRNAs in RNA-seq datasets is crucial for exploring\ntheir characteristic functions in the genome as most coding potential\ncomputation (CPC) tools fail to accurately identify them in transcriptomic\ndata. Well-known CPC tools such as CPC2, lncScore, CPAT are primarily designed\nfor prediction of lncRNAs based on the GENCODE, NONCODE and CANTATAdb\ndatabases. The prediction accuracy of these tools often drops when tested on\ntranscriptomic datasets. This leads to higher false positive results and\ninaccuracy in the function annotation process. In this study, we present a\nnovel tool, PLIT, for the identification of lncRNAs in plants RNA-seq datasets.\nPLIT implements a feature selection method based on L1 regularization and\niterative Random Forests (iRF) classification for selection of optimal\nfeatures. Based on sequence and codon-bias features, it classifies the RNA-seq\nderived FASTA sequences into coding or long non-coding transcripts. Using L1\nregularization, 31 optimal features were obtained based on lncRNA and\nprotein-coding transcripts from 8 plant species. The performance of the tool\nwas evaluated on 7 plant RNA-seq datasets using 10-fold cross-validation. The\nanalysis exhibited superior accuracy when evaluated against currently available\nstate-of-the-art CPC tools.", "journal": "Computers in Biology and Medicine, 105, pp. 169 - 181, Elevier,\n  2019"}
{"doi": "10.48550/arXiv.2102.06919", "date": "2021-02-13", "title": "Asset Management in Machine Learning: A Survey", "authors": "Samuel Idowu, Daniel Str\u00fcber, Thorsten Berger", "abstract": "Machine Learning (ML) techniques are becoming essential components of many\nsoftware systems today, causing an increasing need to adapt traditional\nsoftware engineering practices and tools to the development of ML-based\nsoftware systems. This need is especially pronounced due to the challenges\nassociated with the large-scale development and deployment of ML systems. Among\nthe most commonly reported challenges during the development, production, and\noperation of ML-based systems are experiment management, dependency management,\nmonitoring, and logging of ML assets. In recent years, we have seen several\nefforts to address these challenges as witnessed by an increasing number of\ntools for tracking and managing ML experiments and their assets. To facilitate\nresearch and practice on engineering intelligent systems, it is essential to\nunderstand the nature of the current tool support for managing ML assets. What\nkind of support is provided? What asset types are tracked? What operations are\noffered to users for managing those assets? We discuss and position ML asset\nmanagement as an important discipline that provides methods and tools for ML\nassets as structures and the ML development activities as their operations. We\npresent a feature-based survey of 17 tools with ML asset management support\nidentified in a systematic search. We overview these tools' features for\nmanaging the different types of assets used for engineering ML-based systems\nand performing experiments. We found that most of the asset management support\ndepends on traditional version control systems, while only a few tools support\nan asset granularity level that differentiates between important ML assets,\nsuch as datasets and models.", "journal": ""}
{"doi": "10.48550/arXiv.2106.04547", "date": "2021-06-08", "title": "Automatic Generation of Machine Learning Synthetic Data Using ROS", "authors": "Kyle M. Hart, Ari B. Goodman, Ryan P. O'Shea", "abstract": "Data labeling is a time intensive process. As such, many data scientists use\nvarious tools to aid in the data generation and labeling process. While these\ntools help automate labeling, many still require user interaction throughout\nthe process. Additionally, most target only a few network frameworks. Any\nresearchers exploring multiple frameworks must find additional tools orwrite\nconversion scripts. This paper presents an automated tool for generating\nsynthetic data in arbitrary network formats. It uses Robot Operating System\n(ROS) and Gazebo, which are common tools in the robotics community. Through ROS\nparadigms, it allows extensive user customization of the simulation environment\nand data generation process. Additionally, a plugin-like framework allows the\ndevelopment of arbitrary data format writers without the need to change the\nmain body of code. Using this tool, the authors were able to generate an\narbitrarily large image dataset for three unique training formats using\napproximately 15 min of user setup time and a variable amount of hands-off run\ntime, depending on the dataset size. The source code for this data generation\ntool is available at https://github.com/Navy-RISE-Lab/nn_data_collection", "journal": ""}
{"doi": "10.48550/arXiv.2301.03636", "date": "2023-01-09", "title": "OpenMP Advisor", "authors": "Alok Mishra, Abid M. Malik, Meifeng Lin, Barbara Chapman", "abstract": "With the increasing diversity of heterogeneous architecture in the HPC\nindustry, porting a legacy application to run on different architectures is a\ntough challenge. In this paper, we present OpenMP Advisor, a first of its kind\ncompiler tool that enables code offloading to a GPU with OpenMP using Machine\nLearning. Although the tool is currently limited to GPUs, it can be extended to\nsupport other OpenMP-capable devices. The tool has two modes: Training mode and\nPrediction mode. The training mode must be executed on the target hardware. It\ntakes benchmark codes as input, generates and executes every variant of the\ncode that could possibly run on the target device, and then collects data from\nall of the executed codes to train an ML-based cost model for use in prediction\nmode. However, in prediction mode the tool does not need any interaction with\nthe target device. It accepts a C code as input and returns the best code\nvariant that can be used to offload the code to the specified device. The tool\ncan determine the kernels that are best suited for offloading by predicting\ntheir runtime using a machine learning-based cost model. The main objective\nbehind this tool is to maintain the portability aspect of OpenMP. Using our\nAdvisor, we were able to generate code of multiple applications for seven\ndifferent architectures, and correctly predict the top ten best variants for\neach application on every architecture. Preliminary findings indicate that this\ntool can assist compiler developers and HPC application researchers in porting\ntheir legacy HPC codes to the upcoming heterogeneous computing environment.", "journal": ""}
{"doi": "10.48550/arXiv.2309.01723", "date": "2023-09-04", "title": "SAF-IS: a Spatial Annotation Free Framework for Instance Segmentation of Surgical Tools", "authors": "Luca Sestini, Benoit Rosa, Elena De Momi, Giancarlo Ferrigno, Nicolas Padoy", "abstract": "Instance segmentation of surgical instruments is a long-standing research\nproblem, crucial for the development of many applications for computer-assisted\nsurgery. This problem is commonly tackled via fully-supervised training of deep\nlearning models, requiring expensive pixel-level annotations to train. In this\nwork, we develop a framework for instance segmentation not relying on spatial\nannotations for training. Instead, our solution only requires binary tool\nmasks, obtainable using recent unsupervised approaches, and binary tool\npresence labels, freely obtainable in robot-assisted surgery. Based on the\nbinary mask information, our solution learns to extract individual tool\ninstances from single frames, and to encode each instance into a compact vector\nrepresentation, capturing its semantic features. Such representations guide the\nautomatic selection of a tiny number of instances (8 only in our experiments),\ndisplayed to a human operator for tool-type labelling. The gathered information\nis finally used to match each training instance with a binary tool presence\nlabel, providing an effective supervision signal to train a tool instance\nclassifier. We validate our framework on the EndoVis 2017 and 2018 segmentation\ndatasets. We provide results using binary masks obtained either by manual\nannotation or as predictions of an unsupervised binary segmentation model. The\nlatter solution yields an instance segmentation approach completely free from\nspatial annotations, outperforming several state-of-the-art fully-supervised\nsegmentation approaches.", "journal": ""}
{"doi": "10.48550/arXiv.2401.10599", "date": "2024-01-19", "title": "A research-informed graphical tool to visually approach Gauss' and Stokes' theorems in vector calculus", "authors": "Larissa Hahn, Simon Blaue, Pascal Klein", "abstract": "Gauss' and Stokes' theorems are fundamental results in vector calculus and\nimportant tools in physics and engineering. When students are asked to describe\nthe meaning of Gauss' divergence theorem, they often use statements like this:\n\"The sum of all sources of a vector field in a region gives the net flux out of\nthe region\". In order to raise this description to a mathematically sound level\nof understanding, we present an educational approach based on the visual\ninterpretation of the vector differential operators, i.e. divergence and curl.\nAs a starting point, we use simple vector field diagrams for a qualitative\napproach to connect both sides of the integral theorems, and present an\ninteractive graphical tool to support this connection. The tool allows to\nvisualise two-dimensional vector fields, to specify vector decomposition, to\nevaluate divergence and curl point wise, and to draw rectangles to determine\nsurface and line integrals. From a meta-perspective, we situate this\neducational approach into learning with (multiple) representations. Based on\nprior research, the graphical tool addresses various learning difficulties of\nvector fields that are connected to divergence and curl. The tool was\nincorporated into the weekly lecture-based recitations of Physics II\n(electromagnetism) in 2022 and 2023, and we assessed various educational\noutcome measures. The students overall reported the tool to be intuitive and\nuser-friendly (level of agreement $76\\%$, $N=125$), considered it helpful for\nunderstanding and recommended its use for introductory physics courses (level\nof agreement $65\\%$, $N=65$).", "journal": ""}
{"doi": "10.48550/arXiv.2405.04533", "date": "2024-05-07", "title": "ChatHuman: Chatting about 3D Humans with Tools", "authors": "Jing Lin, Yao Feng, Weiyang Liu, Michael J. Black", "abstract": "Numerous methods have been proposed to detect, estimate, and analyze\nproperties of people in images, including 3D pose, shape, contact, human-object\ninteraction, and emotion. While widely applicable in vision and other areas,\nsuch methods require expert knowledge to select, use, and interpret the\nresults. To address this, we introduce ChatHuman, a language-driven system that\nintegrates the capabilities of specialized methods into a unified framework.\nChatHuman functions as an assistant proficient in utilizing, analyzing, and\ninteracting with tools specific to 3D human tasks, adeptly discussing and\nresolving related challenges. Built on a Large Language Model (LLM) framework,\nChatHuman is trained to autonomously select, apply, and interpret a diverse set\nof tools in response to user inputs. Our approach overcomes significant hurdles\nin adapting LLMs to 3D human tasks, including the need for domain-specific\nknowledge and the ability to interpret complex 3D outputs. The innovations of\nChatHuman include leveraging academic publications to instruct the LLM on tool\nusage, employing a retrieval-augmented generation model to create in-context\nlearning examples for managing new tools, and effectively discriminating\nbetween and integrating tool results by transforming specialized 3D outputs\ninto comprehensible formats. Experiments demonstrate that ChatHuman surpasses\nexisting models in both tool selection accuracy and overall performance across\nvarious 3D human tasks, and it supports interactive chatting with users.\nChatHuman represents a significant step toward consolidating diverse analytical\nmethods into a unified, robust system for 3D human tasks.", "journal": ""}
{"doi": "10.48550/arXiv.2409.00557", "date": "2024-08-31", "title": "Learning to Ask: When LLM Agents Meet Unclear Instruction", "authors": "Wenxuan Wang, Juluan Shi, Zixuan Ling, Yuk-Kit Chan, Chaozheng Wang, Cheryl Lee, Youliang Yuan, Jen-tse Huang, Wenxiang Jiao, Michael R. Lyu", "abstract": "Equipped with the capability to call functions, modern large language models\n(LLMs) can leverage external tools for addressing a range of tasks unattainable\nthrough language skills alone. However, the effective execution of these tools\nrelies heavily not just on the advanced capabilities of LLMs but also on\nprecise user instructions, which often cannot be ensured in the real world. To\nevaluate the performance of LLMs tool-use under imperfect instructions, we\nmeticulously examine the real-world instructions queried from users, analyze\nthe error patterns, and build a challenging tool-use benchmark called Noisy\nToolBench (NoisyToolBench). We find that due to the next-token prediction\ntraining objective, LLMs tend to arbitrarily generate the missed argument,\nwhich may lead to hallucinations and risks. To address this issue, we propose a\nnovel framework, Ask-when-Needed (AwN), which prompts LLMs to ask questions to\nusers whenever they encounter obstacles due to unclear instructions. Moreover,\nto reduce the manual labor involved in user-LLM interaction and assess LLMs\nperformance in tool utilization from both accuracy and efficiency perspectives,\nwe design an automated evaluation tool named ToolEvaluator. Our experiments\ndemonstrate that the AwN significantly outperforms existing frameworks for tool\nlearning in the NoisyToolBench. We will release all related code and datasets\nto support future research.", "journal": ""}
{"doi": "10.48550/arXiv.2504.20168", "date": "2025-04-28", "title": "MICE for CATs: Model-Internal Confidence Estimation for Calibrating Agents with Tools", "authors": "Nishant Subramani, Jason Eisner, Justin Svegliato, Benjamin Van Durme, Yu Su, Sam Thomson", "abstract": "Tool-using agents that act in the world need to be both useful and safe.\nWell-calibrated model confidences can be used to weigh the risk versus reward\nof potential actions, but prior work shows that many models are poorly\ncalibrated. Inspired by interpretability literature exploring the internals of\nmodels, we propose a novel class of model-internal confidence estimators (MICE)\nto better assess confidence when calling tools. MICE first decodes from each\nintermediate layer of the language model using logitLens and then computes\nsimilarity scores between each layer's generation and the final output. These\nfeatures are fed into a learned probabilistic classifier to assess confidence\nin the decoded output. On the simulated trial and error (STE) tool-calling\ndataset using Llama3 models, we find that MICE beats or matches the baselines\non smoothed expected calibration error. Using MICE confidences to determine\nwhether to call a tool significantly improves over strong baselines on a new\nmetric, expected tool-calling utility. Further experiments show that MICE is\nsample-efficient, can generalize zero-shot to unseen APIs, and results in\nhigher tool-calling utility in scenarios with varying risk levels. Our code is\nopen source, available at https://github.com/microsoft/mice_for_cats.", "journal": ""}
{"doi": "10.48550/arXiv.1909.13561", "date": "2019-09-30", "title": "Imagine That! Leveraging Emergent Affordances for 3D Tool Synthesis", "authors": "Yizhe Wu, Sudhanshu Kasewa, Oliver Groth, Sasha Salter, Li Sun, Oiwi Parker Jones, Ingmar Posner", "abstract": "In this paper we explore the richness of information captured by the latent\nspace of a vision-based generative model. The model combines unsupervised\ngenerative learning with a task-based performance predictor to learn and to\nexploit task-relevant object affordances given visual observations from a\nreaching task, involving a scenario and a stick-like tool. While the learned\nembedding of the generative model captures factors of variation in 3D tool\ngeometry (e.g. length, width, and shape), the performance predictor identifies\nsub-manifolds of the embedding that correlate with task success. Within a\nvariety of scenarios, we demonstrate that traversing the latent space via\nbackpropagation from the performance predictor allows us to imagine tools\nappropriate for the task at hand. Our results indicate that affordances-like\nthe utility for reaching-are encoded along smooth trajectories in latent space.\nAccessing these emergent affordances by considering only high-level performance\ncriteria (such as task success) enables an agent to manipulate tool geometries\nin a targeted and deliberate way.", "journal": ""}
{"doi": "10.48550/arXiv.2101.01506", "date": "2021-01-05", "title": "Structured Machine Learning Tools for Modelling Characteristics of Guided Waves", "authors": "Marcus Haywood-Alexander, Nikolaos Dervilis, Keith Worden, Elizabeth J. Cross, Robin S. Mills, Timothy J. Rogers", "abstract": "The use of ultrasonic guided waves to probe the materials/structures for\ndamage continues to increase in popularity for non-destructive evaluation (NDE)\nand structural health monitoring (SHM). The use of high-frequency waves such as\nthese offers an advantage over low-frequency methods from their ability to\ndetect damage on a smaller scale. However, in order to assess damage in a\nstructure, and implement any NDE or SHM tool, knowledge of the behaviour of a\nguided wave throughout the material/structure is important (especially when\ndesigning sensor placement for SHM systems). Determining this behaviour is\nextremely diffcult in complex materials, such as fibre-matrix composites, where\nunique phenomena such as continuous mode conversion takes place. This paper\nintroduces a novel method for modelling the feature-space of guided waves in a\ncomposite material. This technique is based on a data-driven model, where prior\nphysical knowledge can be used to create structured machine learning tools;\nwhere constraints are applied to provide said structure. The method shown makes\nuse of Gaussian processes, a full Bayesian analysis tool, and in this paper it\nis shown how physical knowledge of the guided waves can be utilised in\nmodelling using an ML tool. This paper shows that through careful consideration\nwhen applying machine learning techniques, more robust models can be generated\nwhich offer advantages such as extrapolation ability and physical\ninterpretation.", "journal": ""}
{"doi": "10.48550/arXiv.2104.11593", "date": "2021-04-21", "title": "Assessing Validity of Static Analysis Warnings using Ensemble Learning", "authors": "Anshul Tanwar, Hariharan Manikandan, Krishna Sundaresan, Prasanna Ganesan, Sathish Kumar Chandrasekaran, Sriram Ravi", "abstract": "Static Analysis (SA) tools are used to identify potential weaknesses in code\nand fix them in advance, while the code is being developed. In legacy codebases\nwith high complexity, these rules-based static analysis tools generally report\na lot of false warnings along with the actual ones. Though the SA tools uncover\nmany hidden bugs, they are lost in the volume of fake warnings reported. The\ndevelopers expend large hours of time and effort in identifying the true\nwarnings. Other than impacting the developer productivity, true bugs are also\nmissed out due to this challenge. To address this problem, we propose a Machine\nLearning (ML)-based learning process that uses source codes, historic commit\ndata, and classifier-ensembles to prioritize the True warnings from the given\nlist of warnings. This tool is integrated into the development workflow to\nfilter out the false warnings and prioritize actual bugs. We evaluated our\napproach on the networking C codes, from a large data pool of static analysis\nwarnings reported by the tools. Time-to-time these warnings are addressed by\nthe developers, labelling them as authentic bugs or fake alerts. The ML model\nis trained with full supervision over the code features. Our results confirm\nthat applying deep learning over the traditional static analysis reports is an\nassuring approach for drastically reducing the false positive rates.", "journal": ""}
{"doi": "10.48550/arXiv.2404.07594", "date": "2024-04-11", "title": "Weakly-Supervised Learning via Multi-Lateral Decoder Branching for Tool Segmentation in Robot-Assisted Cardiovascular Catheterization", "authors": "Olatunji Mumini Omisore, Toluwanimi Akinyemi, Anh Nguyen, Lei Wang", "abstract": "Robot-assisted catheterization has garnered a good attention for its\npotentials in treating cardiovascular diseases. However, advancing\nsurgeon-robot collaboration still requires further research, particularly on\ntask-specific automation. For instance, automated tool segmentation can assist\nsurgeons in visualizing and tracking of endovascular tools during cardiac\nprocedures. While learning-based models have demonstrated state-of-the-art\nsegmentation performances, generating ground-truth labels for fully-supervised\nmethods is both labor-intensive time consuming, and costly. In this study, we\npropose a weakly-supervised learning method with multi-lateral pseudo labeling\nfor tool segmentation in cardiovascular angiogram datasets. The method utilizes\na modified U-Net architecture featuring one encoder and multiple laterally\nbranched decoders. The decoders generate diverse pseudo labels under different\nperturbations, augmenting available partial labels. The pseudo labels are\nself-generated using a mixed loss function with shared consistency across the\ndecoders. The weakly-supervised model was trained end-to-end and validated\nusing partially annotated angiogram data from three cardiovascular\ncatheterization procedures. Validation results show that the model could\nperform closer to fully-supervised models. Also, the proposed weakly-supervised\nmulti-lateral method outperforms three well known methods used for\nweakly-supervised learning, offering the highest segmentation performance\nacross the three angiogram datasets. Furthermore, numerous ablation studies\nconfirmed the model's consistent performance under different parameters.\nFinally, the model was applied for tool segmentation in a robot-assisted\ncatheterization experiments. The model enhanced visualization with high\nconnectivity indices for guidewire and catheter, and a mean processing time of\n35 ms per frame.", "journal": ""}
{"doi": "10.48550/arXiv.2505.19255", "date": "2025-05-25", "title": "VTool-R1: VLMs Learn to Think with Images via Reinforcement Learning on Multimodal Tool Use", "authors": "Mingyuan Wu, Jingcheng Yang, Jize Jiang, Meitang Li, Kaizhuo Yan, Hanchao Yu, Minjia Zhang, Chengxiang Zhai, Klara Nahrstedt", "abstract": "Reinforcement Learning Finetuning (RFT) has significantly advanced the\nreasoning capabilities of large language models (LLMs) by enabling long chains\nof thought, self-correction, and effective tool use. While recent works attempt\nto extend RFT to vision-language models (VLMs), these efforts largely produce\ntext-only reasoning conditioned on static image inputs, falling short of true\nmultimodal reasoning in the response. In contrast, test-time methods like\nVisual Sketchpad incorporate visual steps but lack training mechanisms.\n  We introduce VTool-R1, the first framework that trains VLMs to generate\nmultimodal chains of thought by interleaving text and intermediate visual\nreasoning steps. VTool-R1 integrates Python-based visual editing tools into the\nRFT process, enabling VLMs to learn when and how to generate visual reasoning\nsteps that benefit final reasoning. Trained with outcome-based rewards tied to\ntask accuracy, our approach elicits strategic visual tool use for reasoning\nwithout relying on process-based supervision. Experiments on structured visual\nquestion answering over charts and tables show that VTool-R1 enhances reasoning\nperformance by teaching VLMs to \"think with images\" and generate multimodal\nchain of thoughts with tools.", "journal": ""}
{"doi": "10.48550/arXiv.2505.24480", "date": "2025-05-30", "title": "Towards Effective Code-Integrated Reasoning", "authors": "Fei Bai, Yingqian Min, Beichen Zhang, Zhipeng Chen, Wayne Xin Zhao, Lei Fang, Zheng Liu, Zhongyuan Wang, Ji-Rong Wen", "abstract": "In this paper, we investigate code-integrated reasoning, where models\ngenerate code when necessary and integrate feedback by executing it through a\ncode interpreter. To acquire this capability, models must learn when and how to\nuse external code tools effectively, which is supported by tool-augmented\nreinforcement learning (RL) through interactive learning. Despite its benefits,\ntool-augmented RL can still suffer from potential instability in the learning\ndynamics. In light of this challenge, we present a systematic approach to\nimproving the training effectiveness and stability of tool-augmented RL for\ncode-integrated reasoning. Specifically, we develop enhanced training\nstrategies that balance exploration and stability, progressively building\ntool-use capabilities while improving reasoning performance. Through extensive\nexperiments on five mainstream mathematical reasoning benchmarks, our model\ndemonstrates significant performance improvements over multiple competitive\nbaselines. Furthermore, we conduct an in-depth analysis of the mechanism and\neffect of code-integrated reasoning, revealing several key insights, such as\nthe extension of model's capability boundaries and the simultaneous improvement\nof reasoning efficiency through code integration. All data and code for\nreproducing this work are available at: https://github.com/RUCAIBox/CIR.", "journal": ""}
{"doi": "10.48550/arXiv.2011.02159", "date": "2020-11-04", "title": "Reverse engineering learned optimizers reveals known and novel mechanisms", "authors": "Niru Maheswaranathan, David Sussillo, Luke Metz, Ruoxi Sun, Jascha Sohl-Dickstein", "abstract": "Learned optimizers are algorithms that can themselves be trained to solve\noptimization problems. In contrast to baseline optimizers (such as momentum or\nAdam) that use simple update rules derived from theoretical principles, learned\noptimizers use flexible, high-dimensional, nonlinear parameterizations.\nAlthough this can lead to better performance in certain settings, their inner\nworkings remain a mystery. How is a learned optimizer able to outperform a well\ntuned baseline? Has it learned a sophisticated combination of existing\noptimization techniques, or is it implementing completely new behavior? In this\nwork, we address these questions by careful analysis and visualization of\nlearned optimizers. We study learned optimizers trained from scratch on three\ndisparate tasks, and discover that they have learned interpretable mechanisms,\nincluding: momentum, gradient clipping, learning rate schedules, and a new form\nof learning rate adaptation. Moreover, we show how the dynamics of learned\noptimizers enables these behaviors. Our results help elucidate the previously\nmurky understanding of how learned optimizers work, and establish tools for\ninterpreting future learned optimizers.", "journal": ""}
{"doi": "10.48550/arXiv.1701.08374", "date": "2017-01-29", "title": "Feature base fusion for splicing forgery detection based on neuro fuzzy", "authors": "Habib Ghaffari Hadigheh, Ghazali bin sulong", "abstract": "Most of researches on image forensics have been mainly focused on detection\nof artifacts introduced by a single processing tool. They lead in the\ndevelopment of many specialized algorithms looking for one or more particular\nfootprints under specific settings. Naturally, the performance of such\nalgorithms are not perfect, and accordingly the provided output might be noisy,\ninaccurate and only partially correct. Furthermore, a forged image in practical\nscenarios is often the result of utilizing several tools available by\nimage-processing software systems. Therefore, reliable tamper detection\nrequires developing more poweful tools to deal with various tempering\nscenarios. Fusion of forgery detection tools based on Fuzzy Inference System\nhas been used before for addressing this problem. Adjusting the membership\nfunctions and defining proper fuzzy rules for attaining to better results are\ntime-consuming processes. This can be accounted as main disadvantage of fuzzy\ninference systems. In this paper, a Neuro-Fuzzy inference system for fusion of\nforgery detection tools is developed. The neural network characteristic of\nthese systems provides appropriate tool for automatically adjusting the\nmembership functions. Moreover, initial fuzzy inference system is generated\nbased on fuzzy clustering techniques. The proposed framework is implemented and\nvalidated on a benchmark image splicing data set in which three forgery\ndetection tools are fused based on adaptive Neuro-Fuzzy inference system. The\noutcome of the proposed method reveals that applying Neuro Fuzzy inference\nsystems could be a better approach for fusion of forgery detection tools.", "journal": ""}
{"doi": "10.48550/arXiv.1710.04970", "date": "2017-10-13", "title": "Transfer of Tool Affordance and Manipulation Cues with 3D Vision Data", "authors": "Paulo Abelha, Frank Guerin", "abstract": "Future service robots working in human environments, such as kitchens, will\nface situations where they need to improvise. The usual tool for a given task\nmight not be available and the robot will have to use some substitute tool. The\nrobot needs to select an appropriate alternative tool from the candidates\navailable, and also needs to know where to grasp it, how to orient it and what\npart to use as the end-effector. We present a system which takes as input a\ncandidate tool's point cloud and weight, and outputs a score for how effective\nthat tool is for a task, and how to use it. Our key novelty is in taking a\ntask-driven approach, where the task exerts a top-down influence on how low\nlevel vision data is interpreted. This facilitates the type of 'everyday\ncreativity' where an object such as a wine bottle could be used as a rolling\npin, because the interpretation of the object is not fixed in advance, but\nrather results from the interaction between the bottom-up and top-down\npressures at run-time. The top-down influence is implemented by transfer: prior\nknowledge of geometric features that make a tool good for a task is used to\nseek similar features in a candidate tool. The prior knowledge is learned by\nsimulating Web models performing the tasks. We evaluate on a set of fifty\nhousehold objects and five tasks. We compare our system with the closest one in\nthe literature and show that we achieve significantly better results", "journal": ""}
{"doi": "10.48550/arXiv.2111.05796", "date": "2021-11-10", "title": "Human-Centric Decision Support Tools: Insights from Real-World Design and Implementation", "authors": "Narges Ahani, Andrew C. Trapp", "abstract": "Decision support tools enable improved decision-making for challenging\ndecision problems by empowering stakeholders to process, analyze, visualize,\nand otherwise make sense of a variety of key factors. Their intentional design\nis a critical component of the value they create. All decision-support tools\nshare in common that there is a complex decision problem to be solved for which\ndecision-support is useful, and moreover, that appropriate analytics expertise\nis available to produce solutions to the problem setting at hand. When\nwell-designed, decision support tools reduce friction and increase efficiency\nin providing support for the decision-making process, thereby improving the\nability of decision-makers to make quality decisions. On the other hand, the\npresence of overwhelming, superfluous, insufficient, or ill-fitting information\nand software features can have an adverse effect on the decision-making process\nand, consequently, outcomes. We advocate for an innovative, and perhaps\noverlooked, approach to designing effective decision support tools: genuinely\nlistening to the project stakeholders, to ascertain and appreciate their real\nneeds and perspectives. By prioritizing stakeholder needs, a foundation of\nmutual trust and understanding is established with the design team. We maintain\nthis trust is critical to eventual tool acceptance and adoption, and its\nabsence jeopardizes the future use of the tool, which would leave its\nanalytical insights for naught. We discuss examples across multiple contexts to\nunderscore our collective experience, highlight lessons learned, and present\nrecommended practices to improve the design and eventual adoption of decision\ndupport tools.", "journal": ""}
{"doi": "10.48550/arXiv.2004.14378", "date": "2020-04-29", "title": "Towards Faster Reasoners By Using Transparent Huge Pages", "authors": "Johannes K. Fichte, Norbert Manthey, Julian Stecklina, Andr\u00e9 Schidler", "abstract": "Various state-of-the-art automated reasoning (AR) tools are widely used as\nbackend tools in research of knowledge representation and reasoning as well as\nin industrial applications. In testing and verification, those tools often run\ncontinuously or nightly. In this work, we present an approach to reduce the\nruntime of AR tools by 10% on average and up to 20% for long running tasks. Our\nimprovement addresses the high memory usage that comes with the data structures\nused in AR tools, which are based on conflict driven no-good learning. We\nestablish a general way to enable faster memory access by using the memory\ncache line of modern hardware more effectively. Therefore, we extend the\nstandard C library (glibc) by dynamically allowing to use a memory management\nfeature called huge pages. Huge pages allow to reduce the overhead that is\nrequired to translate memory addresses between the virtual memory of the\noperating system and the physical memory of the hardware. In that way, we can\nreduce runtime, costs, and energy consumption of AR tools and applications with\nsimilar memory access patterns simply by linking the tool against this new\nglibc library when compiling it. In every day industrial applications this\neasily allows to be more eco-friendly in computation. To back up the claimed\nspeed-up, we present experimental results for tools that are commonly used in\nthe AR community, including the domains ASP, BMC, MaxSAT, SAT, and SMT.", "journal": ""}
{"doi": "10.48550/arXiv.2305.16504", "date": "2023-05-25", "title": "On the Tool Manipulation Capability of Open-source Large Language Models", "authors": "Qiantong Xu, Fenglu Hong, Bo Li, Changran Hu, Zhengyu Chen, Jian Zhang", "abstract": "Recent studies on software tool manipulation with large language models\n(LLMs) mostly rely on closed model APIs. The industrial adoption of these\nmodels is substantially constrained due to the security and robustness risks in\nexposing information to closed LLM API services. In this paper, we ask can we\nenhance open-source LLMs to be competitive to leading closed LLM APIs in tool\nmanipulation, with practical amount of human supervision. By analyzing common\ntool manipulation failures, we first demonstrate that open-source LLMs may\nrequire training with usage examples, in-context demonstration and generation\nstyle regulation to resolve failures. These insights motivate us to revisit\nclassical methods in LLM literature, and demonstrate that we can adapt them as\nmodel alignment with programmatic data generation, system prompts and\nin-context demonstration retrievers to enhance open-source LLMs for tool\nmanipulation. To evaluate these techniques, we create the ToolBench, a tool\nmanipulation benchmark consisting of diverse software tools for real-world\ntasks. We demonstrate that our techniques can boost leading open-source LLMs by\nup to 90% success rate, showing capabilities competitive to OpenAI GPT-4 in 4\nout of 8 ToolBench tasks. We show that such enhancement typically requires\nabout one developer day to curate data for each tool, rendering a recipe with\npractical amount of human supervision.", "journal": ""}
{"doi": "10.48550/arXiv.2311.00152", "date": "2023-10-31", "title": "Developing a Tool to Automate Extensions to Support a Flexible Extension Policy", "authors": "Jordan Schwartz, Madison Bohannan, Jacob Yim, Yuerou Tang, Dana Benedicto, Charisse Liu, Armando Fox, Lisa Yan, Narges Norouzi", "abstract": "In this work, we present the development of an automated extension tool to\nassist educators and increase the success and well-being of students by\nimplementing flexible extension policies. Flexible extension policies\nmaterialize in many ways, yet there are similarities in students' interactions\nwith them; students tend to request multi-day long extensions repeatedly. In\ncourses with hundreds or potentially thousands of students, providing a system\nto support this extension request demand is not possible given most currently\navailable resources and limited staff. As such, a tool is necessary to help\nautomate flexible extension processes. The development of this tool should\nreduce staff load while increasing individualized student support, which can be\nused in varying ways for different extension policies. Our research questions\nare: RQ1: Does the extension tool reduce barriers and stigma around asking for\nassistance? RQ2: Does the tool lessen the wait time between requesting and\nreceiving an extension, and how does the tool improve students' learning\nexperience in the course? These questions will help inform us about how an\nautomated tool for flexible extensions helps support growing course sizes and\nstudents who may not otherwise receive the support they need for their success\nand well-being in the course.", "journal": ""}
{"doi": "10.48550/arXiv.2312.01550", "date": "2023-12-04", "title": "Using human and robot synthetic data for training smart hand tools", "authors": "Jose Bendana, Sundar Sripada V. S., Carlos D. Salazar, Sandeep Chinchali, Raul G. Longoria", "abstract": "The future of work does not require a choice between human and robot. Aside\nfrom explicit human-robot collaboration, robotics can play an increasingly\nimportant role in helping train workers as well as the tools they may use,\nespecially in complex tasks that may be difficult to automate or effectively\nroboticize. This paper introduces a form of smart tool for use by human workers\nand shows how training the tool for task recognition, one of the key\nrequirements, can be accomplished. Machine learning (ML) with purely\nhuman-based data can be extremely laborious and time-consuming. First, we show\nhow data synthetically-generated by a robot can be leveraged in the ML training\nprocess. Later, we demonstrate how fine-tuning ML models for individual\nphysical tasks and workers can significantly scale up the benefits of using ML\nto provide this feedback. Experimental results show the effectiveness and\nscalability of our approach, as we test data size versus accuracy. Smart hand\ntools of the type introduced here can provide insights and real-time analytics\non efficient and safe tool usage and operation, thereby enhancing human\nparticipation and skill in a wide range of work environments. Using robotic\nplatforms to help train smart tools will be essential, particularly given the\ndiverse types of applications for which smart hand tools are envisioned for\nhuman use.", "journal": ""}
{"doi": "10.48550/arXiv.2401.10535", "date": "2024-01-19", "title": "The \"Colonial Impulse\" of Natural Language Processing: An Audit of Bengali Sentiment Analysis Tools and Their Identity-based Biases", "authors": "Dipto Das, Shion Guha, Jed Brubaker, Bryan Semaan", "abstract": "While colonization has sociohistorically impacted people's identities across\nvarious dimensions, those colonial values and biases continue to be perpetuated\nby sociotechnical systems. One category of sociotechnical systems--sentiment\nanalysis tools--can also perpetuate colonial values and bias, yet less\nattention has been paid to how such tools may be complicit in perpetuating\ncoloniality, although they are often used to guide various practices (e.g.,\ncontent moderation). In this paper, we explore potential bias in sentiment\nanalysis tools in the context of Bengali communities that have experienced and\ncontinue to experience the impacts of colonialism. Drawing on identity\ncategories most impacted by colonialism amongst local Bengali communities, we\nfocused our analytic attention on gender, religion, and nationality. We\nconducted an algorithmic audit of all sentiment analysis tools for Bengali,\navailable on the Python package index (PyPI) and GitHub. Despite similar\nsemantic content and structure, our analyses showed that in addition to\ninconsistencies in output from different tools, Bengali sentiment analysis\ntools exhibit bias between different identity categories and respond\ndifferently to different ways of identity expression. Connecting our findings\nwith colonially shaped sociocultural structures of Bengali communities, we\ndiscuss the implications of downstream bias of sentiment analysis tools.", "journal": ""}
{"doi": "10.48550/arXiv.2501.15114", "date": "2025-01-25", "title": "Does the Tool Matter? Exploring Some Causes of Threats to Validity in Mining Software Repositories", "authors": "Nicole Hoess, Carlos Paradis, Rick Kazman, Wolfgang Mauerer", "abstract": "Software repositories are an essential source of information for software\nengineering research on topics such as project evolution and developer\ncollaboration. Appropriate mining tools and analysis pipelines are therefore an\nindispensable precondition for many research activities. Ideally, valid results\nshould not depend on technical details of data collection and processing. It\nis, however, widely acknowledged that mining pipelines are complex, with a\nmultitude of implementation decisions made by tool authors based on their\ninterests and assumptions. This raises the questions if (and to what extent)\ntools agree on their results and are interchangeable. In this study, we use two\ntools to extract and analyse ten large software projects, quantitatively and\nqualitatively comparing results and derived data to better understand this\nconcern. We analyse discrepancies from a technical point of view, and adjust\ncode and parametrisation to minimise replication differences. Our results\nindicate that despite similar trends, even simple metrics such as the numbers\nof commits and developers may differ by up to 500%. We find that such\nsubstantial differences are often caused by minor technical details. We show\nhow tool-level and data post-processing changes can overcome these issues, but\nfind they may require considerable efforts. We summarise identified causes in\nour lessons learned to help researchers and practitioners avoid common\npitfalls, and reflect on implementation decisions and their influence in\nensuring obtained data meets explicit and implicit expectations. Our findings\nlead us to hypothesise that similar uncertainties exist in other analysis\ntools, which may limit the validity of conclusions drawn in tool-centric\nresearch.", "journal": ""}
{"doi": "10.48550/arXiv.2503.21491", "date": "2025-03-27", "title": "Data-Driven Contact-Aware Control Method for Real-Time Deformable Tool Manipulation: A Case Study in the Environmental Swabbing", "authors": "Siavash Mahmoudi, Amirreza Davar, Dongyi Wang", "abstract": "Deformable Object Manipulation (DOM) remains a critical challenge in robotics\ndue to the complexities of developing suitable model-based control strategies.\nDeformable Tool Manipulation (DTM) further complicates this task by introducing\nadditional uncertainties between the robot and its environment. While humans\neffortlessly manipulate deformable tools using touch and experience, robotic\nsystems struggle to maintain stability and precision. To address these\nchallenges, we present a novel State-Adaptive Koopman LQR (SA-KLQR) control\nframework for real-time deformable tool manipulation, demonstrated through a\ncase study in environmental swab sampling for food safety. This method\nleverages Koopman operator-based control to linearize nonlinear dynamics while\nadapting to state-dependent variations in tool deformation and contact forces.\nA tactile-based feedback system dynamically estimates and regulates the swab\ntool's angle, contact pressure, and surface coverage, ensuring compliance with\nfood safety standards. Additionally, a sensor-embedded contact pad monitors\nforce distribution to mitigate tool pivoting and deformation, improving\nstability during dynamic interactions. Experimental results validate the\nSA-KLQR approach, demonstrating accurate contact angle estimation, robust\ntrajectory tracking, and reliable force regulation. The proposed framework\nenhances precision, adaptability, and real-time control in deformable tool\nmanipulation, bridging the gap between data-driven learning and optimal control\nin robotic interaction tasks.", "journal": ""}
{"doi": "10.48550/arXiv.2504.03767", "date": "2025-04-02", "title": "MCP Safety Audit: LLMs with the Model Context Protocol Allow Major Security Exploits", "authors": "Brandon Radosevich, John Halloran", "abstract": "To reduce development overhead and enable seamless integration between\npotential components comprising any given generative AI application, the Model\nContext Protocol (MCP) (Anthropic, 2024) has recently been released and\nsubsequently widely adopted. The MCP is an open protocol that standardizes API\ncalls to large language models (LLMs), data sources, and agentic tools. By\nconnecting multiple MCP servers, each defined with a set of tools, resources,\nand prompts, users are able to define automated workflows fully driven by LLMs.\nHowever, we show that the current MCP design carries a wide range of security\nrisks for end users. In particular, we demonstrate that industry-leading LLMs\nmay be coerced into using MCP tools to compromise an AI developer's system\nthrough various attacks, such as malicious code execution, remote access\ncontrol, and credential theft. To proactively mitigate these and related\nattacks, we introduce a safety auditing tool, MCPSafetyScanner, the first\nagentic tool to assess the security of an arbitrary MCP server. MCPScanner uses\nseveral agents to (a) automatically determine adversarial samples given an MCP\nserver's tools and resources; (b) search for related vulnerabilities and\nremediations based on those samples; and (c) generate a security report\ndetailing all findings. Our work highlights serious security issues with\ngeneral-purpose agentic workflows while also providing a proactive tool to\naudit MCP server safety and address detected vulnerabilities before deployment.\n  The described MCP server auditing tool, MCPSafetyScanner, is freely available\nat: https://github.com/johnhalloran321/mcpSafetyScanner", "journal": ""}
{"doi": "10.48550/arXiv.1605.02093", "date": "2016-04-26", "title": "A Perspective Study on Content Management in E-Learning and M-Learning", "authors": "RD. Balaji, Fatma Al-Mahri, R. Malathi", "abstract": "This is the era of Information and Communication Technology (ICT). Nowadays,\nthere is no limit to learn, people can learn anywhere and anytime with the\nenhancement of technology. Electronic Learning (E-learning) and Mobile Learning\n(M-learning) are the two vital buzz terms in modern education particularly in\nEducation Enhanced Technology and Technologies Supported Learning. E-learning\nis defined as the instructional content or learning experience delivered or\nenabled by electronic technologies whereas, M-learning is defined simply as\nlearning via mobile devices such as cell phones, smart phones, palmtops, and\nhandheld computers. There are many similarities between the two technologies as\nboth are modern learning tools. Moreover, the latter is an extension and a\nsubset of the former. However, there are few limitations or differences still\nexist in mobile learning tools, especially in the design, development and the\ntechnology usability. In this paper we have mainly focused on how the digital\ncontent is administrated (Content Management) in these two technologies.\nAdditionally, the content management in E and Mlearning are compared and their\nsimilarities and differences are figured out.", "journal": ""}
{"doi": "10.48550/arXiv.1904.07248", "date": "2019-04-15", "title": "Machine Learning in Astronomy: a practical overview", "authors": "Dalya Baron", "abstract": "Astronomy is experiencing a rapid growth in data size and complexity. This\nchange fosters the development of data-driven science as a useful companion to\nthe common model-driven data analysis paradigm, where astronomers develop\nautomatic tools to mine datasets and extract novel information from them. In\nrecent years, machine learning algorithms have become increasingly popular\namong astronomers, and are now used for a wide variety of tasks. In light of\nthese developments, and the promise and challenges associated with them, the\nIAC Winter School 2018 focused on big data in Astronomy, with a particular\nemphasis on machine learning and deep learning techniques. This document\nsummarizes the topics of supervised and unsupervised learning algorithms\npresented during the school, and provides practical information on the\napplication of such tools to astronomical datasets. In this document I cover\nbasic topics in supervised machine learning, including selection and\npreprocessing of the input dataset, evaluation methods, and three popular\nsupervised learning algorithms, Support Vector Machines, Random Forests, and\nshallow Artificial Neural Networks. My main focus is on unsupervised machine\nlearning algorithms, that are used to perform cluster analysis, dimensionality\nreduction, visualization, and outlier detection. Unsupervised learning\nalgorithms are of particular importance to scientific research, since they can\nbe used to extract new knowledge from existing datasets, and can facilitate new\ndiscoveries.", "journal": ""}
{"doi": "10.48550/arXiv.2105.00324", "date": "2021-05-01", "title": "Neko: a Library for Exploring Neuromorphic Learning Rules", "authors": "Zixuan Zhao, Nathan Wycoff, Neil Getty, Rick Stevens, Fangfang Xia", "abstract": "The field of neuromorphic computing is in a period of active exploration.\nWhile many tools have been developed to simulate neuronal dynamics or convert\ndeep networks to spiking models, general software libraries for learning rules\nremain underexplored. This is partly due to the diverse, challenging nature of\nefforts to design new learning rules, which range from encoding methods to\ngradient approximations, from population approaches that mimic the Bayesian\nbrain to constrained learning algorithms deployed on memristor crossbars. To\naddress this gap, we present Neko, a modular, extensible library with a focus\non aiding the design of new learning algorithms. We demonstrate the utility of\nNeko in three exemplar cases: online local learning, probabilistic learning,\nand analog on-device learning. Our results show that Neko can replicate the\nstate-of-the-art algorithms and, in one case, lead to significant\noutperformance in accuracy and speed. Further, it offers tools including\ngradient comparison that can help develop new algorithmic variants. Neko is an\nopen source Python library that supports PyTorch and TensorFlow backends.", "journal": ""}
{"doi": "10.48550/arXiv.2304.01663", "date": "2023-04-04", "title": "On the Stability-Plasticity Dilemma of Class-Incremental Learning", "authors": "Dongwan Kim, Bohyung Han", "abstract": "A primary goal of class-incremental learning is to strike a balance between\nstability and plasticity, where models should be both stable enough to retain\nknowledge learned from previously seen classes, and plastic enough to learn\nconcepts from new classes. While previous works demonstrate strong performance\non class-incremental benchmarks, it is not clear whether their success comes\nfrom the models being stable, plastic, or a mixture of both. This paper aims to\nshed light on how effectively recent class-incremental learning algorithms\naddress the stability-plasticity trade-off. We establish analytical tools that\nmeasure the stability and plasticity of feature representations, and employ\nsuch tools to investigate models trained with various algorithms on large-scale\nclass-incremental benchmarks. Surprisingly, we find that the majority of\nclass-incremental learning algorithms heavily favor stability over plasticity,\nto the extent that the feature extractor of a model trained on the initial set\nof classes is no less effective than that of the final incremental model. Our\nobservations not only inspire two simple algorithms that highlight the\nimportance of feature representation analysis, but also suggest that\nclass-incremental learning approaches, in general, should strive for better\nfeature representation learning.", "journal": ""}
{"doi": "10.48550/arXiv.2307.05017", "date": "2023-07-11", "title": "Feature Activation Map: Visual Explanation of Deep Learning Models for Image Classification", "authors": "Yi Liao, Yongsheng Gao, Weichuan Zhang", "abstract": "Decisions made by convolutional neural networks(CNN) can be understood and\nexplained by visualizing discriminative regions on images. To this end, Class\nActivation Map (CAM) based methods were proposed as powerful interpretation\ntools, making the prediction of deep learning models more explainable,\ntransparent, and trustworthy. However, all the CAM-based methods (e.g., CAM,\nGrad-CAM, and Relevance-CAM) can only be used for interpreting CNN models with\nfully-connected (FC) layers as a classifier. It is worth noting that many deep\nlearning models classify images without FC layers, e.g., few-shot learning\nimage classification, contrastive learning image classification, and image\nretrieval tasks. In this work, a post-hoc interpretation tool named feature\nactivation map (FAM) is proposed, which can interpret deep learning models\nwithout FC layers as a classifier. In the proposed FAM algorithm, the\nchannel-wise contribution weights are derived from the similarity scores\nbetween two image embeddings. The activation maps are linearly combined with\nthe corresponding normalized contribution weights, forming the explanation map\nfor visualization. The quantitative and qualitative experiments conducted on\nten deep learning models for few-shot image classification, contrastive\nlearning image classification and image retrieval tasks demonstrate the\neffectiveness of the proposed FAM algorithm.", "journal": ""}
{"doi": "10.48550/arXiv.2410.09368", "date": "2024-10-12", "title": "Towards a Domain-Specific Modelling Environment for Reinforcement Learning", "authors": "Natalie Sinani, Sahil Salma, Paul Boutot, Sadaf Mustafiz", "abstract": "In recent years, machine learning technologies have gained immense popularity\nand are being used in a wide range of domains. However, due to the complexity\nassociated with machine learning algorithms, it is a challenge to make it\nuser-friendly, easy to understand and apply. Machine learning applications are\nespecially challenging for users who do not have proficiency in this area.\n  In this paper, we use model-driven engineering (MDE) methods and tools for\ndeveloping a domain-specific modelling environment to contribute towards\nproviding a solution for this problem. We targeted reinforcement learning from\nthe machine learning domain, and evaluated the proposed language, reinforcement\nlearning modelling language (RLML), with multiple applications. The tool\nsupports syntax-directed editing, constraint checking, and automatic generation\nof code from RLML models. The environment also provides support for comparing\nresults generated with multiple RL algorithms. With our proposed MDE approach,\nwe were able to help in abstracting reinforcement learning technologies and\nimprove the learning curve for RL users.", "journal": ""}
{"doi": "10.48550/arXiv.2411.05211", "date": "2024-11-07", "title": "ARLang: An Outdoor Augmented Reality Application for Portuguese Vocabulary Learning", "authors": "Arthur Caetano, Alyssa Lawson, Yimeng Liu, Misha Sra", "abstract": "With recent computer vision techniques and user-generated content, we can\naugment the physical world with metadata that describes attributes, such as\nnames, geo-locations, and visual features of physical objects. To assess the\nbenefits of these potentially ubiquitous labels for foreign vocabulary\nlearning, we built a proof-of-concept system that displays bilingual text and\nsound labels on physical objects outdoors using augmented reality. Established\ntools for language learning have focused on effective content delivery methods\nsuch as books and flashcards. However, recent research and consumer learning\ntools have begun to focus on how learning can become more mobile, ubiquitous,\nand desirable. To test whether our system supports vocabulary learning, we\nconducted a preliminary between-subjects (N=44) study. Our results indicate\nthat participants preferred learning with virtual labels on real-world objects\noutdoors over learning with flashcards. Our findings motivate further\ninvestigation into mobile AR-based learning systems in outdoor settings.", "journal": "In Proceedings of the 2023 ACM Designing Interactive Systems\n  Conference (pp. 1224-1235)"}
{"doi": "10.48550/arXiv.2412.07177", "date": "2024-12-10", "title": "Effective Reward Specification in Deep Reinforcement Learning", "authors": "Julien Roy", "abstract": "In the last decade, Deep Reinforcement Learning has evolved into a powerful\ntool for complex sequential decision-making problems. It combines deep\nlearning's proficiency in processing rich input signals with reinforcement\nlearning's adaptability across diverse control tasks. At its core, an RL agent\nseeks to maximize its cumulative reward, enabling AI algorithms to uncover\nnovel solutions previously unknown to experts. However, this focus on reward\nmaximization also introduces a significant difficulty: improper reward\nspecification can result in unexpected, misaligned agent behavior and\ninefficient learning. The complexity of accurately specifying the reward\nfunction is further amplified by the sequential nature of the task, the\nsparsity of learning signals, and the multifaceted aspects of the desired\nbehavior.\n  In this thesis, we survey the literature on effective reward specification\nstrategies, identify core challenges relating to each of these approaches, and\npropose original contributions addressing the issue of sample efficiency and\nalignment in deep reinforcement learning. Reward specification represents one\nof the most challenging aspects of applying reinforcement learning in\nreal-world domains. Our work underscores the absence of a universal solution to\nthis complex and nuanced challenge; solving it requires selecting the most\nappropriate tools for the specific requirements of each unique application.", "journal": ""}
{"doi": "10.48550/arXiv.1808.04203", "date": "2018-08-09", "title": "Xcos on Web as a promising learning tool for Bachelor's of Electromechanics modeling of technical objects", "authors": "Yevhenii O. Modlo, Serhiy O. Semerikov", "abstract": "Research goals: to identify the perspective learning simulation tool for\nBachelors of Electromechanics. Research objectives: to prove the feasibility of\nusing the simulation system Xcos on Web as a tool of forming of future\nBachelors of Electromechanics competence in modeling of technical objects.\nResearch object: the use of imitative simulation systems to learning the\nBachelors of Electromechanics. Research subject: the use Xcos on Web in\nlearning modeling of technical objects the Bachelors of Electromechanics.\nResearch methods used: the analysis of existing software usage experience.\nResearch results. The imitative simulation system Xcos on Web is a promising\ncloud-based learning tool for Bachelor's of Electromechanics modeling of\ntechnical objects. The main conclusions and recommendations: 1. The use of\nsimulation systems, such as Scilab Xcos, is a necessary part of Bachelor of\nElectromechanics professional training. 2. Cloud-based learning environment\nbuilt on the integrative usage of mobile Internet devices promotes the forming\nof Bachelor's of Electromechanics professional competencies. 3. Implementation\nthe full Scilab Xcos functionality at Xcos on Web creates conditions for\ntransition in Bachelor's of Electromechanics learning the simulation of\ntechnical objects to the use of mobile Internet devices.", "journal": "CEUR Workshop Proceedings 2168 (2018) 34-41"}
{"doi": "10.48550/arXiv.2206.13901", "date": "2022-06-24", "title": "Value Function Decomposition for Iterative Design of Reinforcement Learning Agents", "authors": "James MacGlashan, Evan Archer, Alisa Devlic, Takuma Seno, Craig Sherstan, Peter R. Wurman, Peter Stone", "abstract": "Designing reinforcement learning (RL) agents is typically a difficult process\nthat requires numerous design iterations. Learning can fail for a multitude of\nreasons, and standard RL methods provide too few tools to provide insight into\nthe exact cause. In this paper, we show how to integrate value decomposition\ninto a broad class of actor-critic algorithms and use it to assist in the\niterative agent-design process. Value decomposition separates a reward function\ninto distinct components and learns value estimates for each. These value\nestimates provide insight into an agent's learning and decision-making process\nand enable new training methods to mitigate common problems. As a\ndemonstration, we introduce SAC-D, a variant of soft actor-critic (SAC) adapted\nfor value decomposition. SAC-D maintains similar performance to SAC, while\nlearning a larger set of value predictions. We also introduce\ndecomposition-based tools that exploit this information, including a new reward\ninfluence metric, which measures each reward component's effect on agent\ndecision-making. Using these tools, we provide several demonstrations of\ndecomposition's use in identifying and addressing problems in the design of\nboth environments and agents. Value decomposition is broadly applicable and\neasy to incorporate into existing algorithms and workflows, making it a\npowerful tool in an RL practitioner's toolbox.", "journal": ""}
{"doi": "10.48550/arXiv.2409.02392", "date": "2024-09-04", "title": "Building Math Agents with Multi-Turn Iterative Preference Learning", "authors": "Wei Xiong, Chengshuai Shi, Jiaming Shen, Aviv Rosenberg, Zhen Qin, Daniele Calandriello, Misha Khalman, Rishabh Joshi, Bilal Piot, Mohammad Saleh, Chi Jin, Tong Zhang, Tianqi Liu", "abstract": "Recent studies have shown that large language models' (LLMs) mathematical\nproblem-solving capabilities can be enhanced by integrating external tools,\nsuch as code interpreters, and employing multi-turn Chain-of-Thought (CoT)\nreasoning. While current methods focus on synthetic data generation and\nSupervised Fine-Tuning (SFT), this paper studies the complementary direct\npreference learning approach to further improve model performance. However,\nexisting direct preference learning algorithms are originally designed for the\nsingle-turn chat task, and do not fully address the complexities of multi-turn\nreasoning and external tool integration required for tool-integrated\nmathematical reasoning tasks. To fill in this gap, we introduce a multi-turn\ndirect preference learning framework, tailored for this context, that leverages\nfeedback from code interpreters and optimizes trajectory-level preferences.\nThis framework includes multi-turn DPO and multi-turn KTO as specific\nimplementations. The effectiveness of our framework is validated through\ntraining of various language models using an augmented prompt set from the\nGSM8K and MATH datasets. Our results demonstrate substantial improvements: a\nsupervised fine-tuned Gemma-1.1-it-7B model's performance increased from 77.5%\nto 83.9% on GSM8K and from 46.1% to 51.2% on MATH. Similarly, a Gemma-2-it-9B\nmodel improved from 84.1% to 86.3% on GSM8K and from 51.0% to 54.5% on MATH.", "journal": ""}
{"doi": "10.48550/arXiv.1812.01366", "date": "2018-12-04", "title": "Weakly Supervised Convolutional LSTM Approach for Tool Tracking in Laparoscopic Videos", "authors": "Chinedu Innocent Nwoye, Didier Mutter, Jacques Marescaux, Nicolas Padoy", "abstract": "Purpose: Real-time surgical tool tracking is a core component of the future\nintelligent operating room (OR), because it is highly instrumental to analyze\nand understand the surgical activities. Current methods for surgical tool\ntracking in videos need to be trained on data in which the spatial positions of\nthe tools are manually annotated. Generating such training data is difficult\nand time-consuming. Instead, we propose to use solely binary presence\nannotations to train a tool tracker for laparoscopic videos. Methods: The\nproposed approach is composed of a CNN + Convolutional LSTM (ConvLSTM) neural\nnetwork trained end-to-end, but weakly supervised on tool binary presence\nlabels only. We use the ConvLSTM to model the temporal dependencies in the\nmotion of the surgical tools and leverage its spatio-temporal ability to smooth\nthe class peak activations in the localization heat maps (Lh-maps).\n  Results: We build a baseline tracker on top of the CNN model and demonstrate\nthat our approach based on the ConvLSTM outperforms the baseline in tool\npresence detection, spatial localization, and motion tracking by over 5.0%,\n13.9%, and 12.6%, respectively.\n  Conclusions: In this paper, we demonstrate that binary presence labels are\nsufficient for training a deep learning tracking model using our proposed\nmethod. We also show that the ConvLSTM can leverage the spatio-temporal\ncoherence of consecutive image frames across a surgical video to improve tool\npresence detection, spatial localization, and motion tracking.\n  keywords: Surgical workflow analysis, tool tracking, weak supervision,\nspatio-temporal coherence, ConvLSTM, endoscopic videos", "journal": "International Journal of Computer Assisted Radiology and Surgery\n  14, 1059-1067 (2019)"}
{"doi": "10.48550/arXiv.1508.00671", "date": "2015-08-04", "title": "Adaptive Automation: Leveraging Machine Learning to Support Uninterrupted Automated Testing of Software Applications", "authors": "Rajesh Mathur, Scott Miles, Miao Du", "abstract": "Checking software application suitability using automated software tools has\nbecome a vital element for most organisations irrespective of whether they\nproduce in-house software or simply customise off-the-shelf software\napplications for internal use. As software solutions become ever more complex,\nthe industry becomes increasingly dependent on software automation tools, yet\nthe brittle nature of the available software automation tools limits their\neffectiveness. Companies invest significantly in obtaining and implementing\nautomation software but most of the tools fail to deliver when the cost of\nmaintaining an effective automation test suite exceeds the cost and time that\nwould have otherwise been spent on manual testing. A failing in the current\ngeneration of software automation tools is they do not adapt to unexpected\nmodifications and obstructions without frequent (and time expensive) manual\ninterference. Such issues are commonly acknowledged amongst industry\npractitioners, yet none of the current generation of tools have leveraged the\nadvances in machine learning and artificial intelligence to address these\nproblems.\n  This paper proposes a framework solution that utilises machine learning\nconcepts, namely fuzzy matching and error recovery. The suggested solution\napplies adaptive techniques to recover from unexpected obstructions that would\notherwise have prevented the script from proceeding. Recovery details are\npresented to the user in a report which can be analysed to determine if the\nrecovery procedure was acceptable and the framework will adapt future runs\nbased on the decisions of the user. Using this framework, a practitioner can\nrun the automated suits without human intervention while minimising the risk of\nschedule delays.", "journal": ""}
{"doi": "10.48550/arXiv.1908.06165", "date": "2019-08-08", "title": "Oxford Handbook on AI Ethics Book Chapter on Race and Gender", "authors": "Timnit Gebru", "abstract": "From massive face-recognition-based surveillance and machine-learning-based\ndecision systems predicting crime recidivism rates, to the move towards\nautomated health diagnostic systems, artificial intelligence (AI) is being used\nin scenarios that have serious consequences in people's lives. However, this\nrapid permeation of AI into society has not been accompanied by a thorough\ninvestigation of the sociopolitical issues that cause certain groups of people\nto be harmed rather than advantaged by it. For instance, recent studies have\nshown that commercial face recognition systems have much higher error rates for\ndark skinned women while having minimal errors on light skinned men. A 2016\nProPublica investigation uncovered that machine learning based tools that\nassess crime recidivism rates in the US are biased against African Americans.\nOther studies show that natural language processing tools trained on newspapers\nexhibit societal biases (e.g. finishing the analogy \"Man is to computer\nprogrammer as woman is to X\" by homemaker). At the same time, books such as\nWeapons of Math Destruction and Automated Inequality detail how people in lower\nsocioeconomic classes in the US are subjected to more automated decision making\ntools than those who are in the upper class. Thus, these tools are most often\nused on people towards whom they exhibit the most bias. While many technical\nsolutions have been proposed to alleviate bias in machine learning systems, we\nhave to take a holistic and multifaceted approach. This includes\nstandardization bodies determining what types of systems can be used in which\nscenarios, making sure that automated decision tools are created by people from\ndiverse backgrounds, and understanding the historical and political factors\nthat disadvantage certain groups who are subjected to these tools.", "journal": ""}
{"doi": "10.48550/arXiv.2009.12684", "date": "2020-09-26", "title": "MicroAnalyzer: A Python Tool for Automated Bacterial Analysis with Fluorescence Microscopy", "authors": "Jonathan Reiner, Guy Azran, Gal Hyams", "abstract": "Fluorescence microscopy is a widely used method among cell biologists for\nstudying the localization and co-localization of fluorescent protein. For\nmicrobial cell biologists, these studies often include tedious and\ntime-consuming manual segmentation of bacteria and of the fluorescence clusters\nor working with multiple programs. Here, we present MicroAnalyzer - a tool that\nautomates these tasks by providing an end-to-end platform for microscope image\nanalysis. While such tools do exist, they are costly, black-boxed programs.\nMicroanalyzer offers an open-source alternative to these tools, allowing\nflexibility and expandability by advanced users. MicroAnalyzer provides\naccurate cell and fluorescence cluster segmentation based on state-of-the-art\ndeep-learning segmentation models, combined with ad-hoc post-processing and\nColicoords - an open-source cell image analysis tool for calculating general\ncell and fluorescence measurements. Using these methods, it performs better\nthan generic approaches since the dynamic nature of neural networks allows for\na quick adaptation to experiment restrictions and assumptions. Other existing\ntools do not consider experiment assumptions, nor do they provide fluorescence\ncluster detection without the need for any specialized equipment. The key goal\nof MicroAnalyzer is to automate the entire process of cell and fluorescence\nimage analysis \"from microscope to database\", meaning it does not require any\nfurther input from the researcher except for the initial deep-learning model\ntraining. In this fashion, it allows the researchers to concentrate on the\nbigger picture instead of granular, eye-straining labor", "journal": ""}
{"doi": "10.48550/arXiv.2106.07513", "date": "2021-06-14", "title": "CodeLabeller: A Web-based Code Annotation Tool for Java Design Patterns and Summaries", "authors": "Najam Nazar, Norman Chen, Chun Yong Chong", "abstract": "While constructing supervised learning models, we require labelled examples\nto build a corpus and train a machine learning model. However, most studies\nhave built the labelled dataset manually, which in many occasions is a daunting\ntask. To mitigate this problem, we have built an online tool called\nCodeLabeller. CodeLabeller is a web-based tool that aims to provide an\nefficient approach to handling the process of labelling source code files for\nsupervised learning methods at scale by improving the data collection process\nthroughout. CodeLabeller is tested by constructing a corpus of over a thousand\nsource files obtained from a large collection of open source Java projects and\nlabelling each Java source file with their respective design patterns and\nsummaries. Twenty five experts in the field of software engineering\nparticipated in a usability evaluation of the tool using the standard User\nExperience Questionnaire online survey. The survey results demonstrate that the\ntool achieves the Good standard on hedonic and pragmatic quality standards, is\neasy to use and meets the needs of the annotating the corpus for supervised\nclassifiers. Apart from assisting researchers in crowdsourcing a labelled\ndataset, the tool has practical applicability in software engineering education\nand assists in building expert ratings for software artefacts.", "journal": ""}
{"doi": "10.48550/arXiv.2203.13962", "date": "2022-03-26", "title": "Tutorial: Modern Theoretical Tools for Understanding and Designing Next-generation Information Retrieval System", "authors": "Da Xu, Chuanwei Ruan", "abstract": "In the relatively short history of machine learning, the subtle balance\nbetween engineering and theoretical progress has been proved critical at\nvarious stages. The most recent wave of AI has brought to the IR community\npowerful techniques, particularly for pattern recognition. While many benefits\nfrom the burst of ideas as numerous tasks become algorithmically feasible, the\nbalance is tilting toward the application side. The existing theoretical tools\nin IR can no longer explain, guide, and justify the newly-established\nmethodologies.\n  The consequences can be suffering: in stark contrast to how the IR industry\nhas envisioned modern AI making life easier, many are experiencing increased\nconfusion and costs in data manipulation, model selection, monitoring,\ncensoring, and decision making. This reality is not surprising: without handy\ntheoretical tools, we often lack principled knowledge of the pattern\nrecognition model's expressivity, optimization property, generalization\nguarantee, and our decision-making process has to rely on over-simplified\nassumptions and human judgments from time to time.\n  Time is now to bring the community a systematic tutorial on how we\nsuccessfully adapt those tools and make significant progress in understanding,\ndesigning, and eventually productionize impactful IR systems. We emphasize\nsystematicity because IR is a comprehensive discipline that touches upon\nparticular aspects of learning, causal inference analysis, interactive (online)\ndecision-making, etc. It thus requires systematic calibrations to render the\nactual usefulness of the imported theoretical tools to serve IR problems, as\nthey usually exhibit unique structures and definitions. Therefore, we plan this\ntutorial to systematically demonstrate our learning and successful experience\nof using advanced theoretical tools for understanding and designing IR systems.", "journal": ""}
{"doi": "10.48550/arXiv.2205.12397", "date": "2022-05-24", "title": "Predicting Post-Route Quality of Results Estimates for HLS Designs using Machine Learning", "authors": "Pingakshya Goswami, Dinesh Bhatia", "abstract": "Machine learning (ML) has been widely used to improve the predictability of\nEDA tools. The use of CAD tools that express designs at higher levels of\nabstraction makes machine learning even more important to highlight the\nperformance of various design steps. Behavioral descriptions used during the\nhigh-level synthesis (HLS) are completely technology independent making it hard\nfor designers to interpret how changes in the synthesis options affect the\nresultant circuit. FPGA design flows are completely embracing HLS based\nmethodologies so that software engineers with almost no hardware design skills\ncan easily use their tools. HLS tools allow design space exploration by\nmodifying synthesis options, however, they lack accuracy in the Quality of\nResults (QoR) reported right after HLS. This lack of correctness results in\nsub-optimal designs with problems in timing closure. This paper presents a\nrobust ML based design flow that can accurately predict post-route QoR for a\ngiven behavioral description without the need to synthesize the design. The\nmodel is an important design exploration tool where a designer can quickly view\nthe impact on overall design quality when local and global optimization\ndirectives are changed. The proposed methodology presents two strong\nadvantages: (i) Accurate prediction of the design quality (QoR), and (ii)\ncomplete elimination of the need to execute high-level synthesis for each\ndesign option. We predict three post route parameters, (i). Area, (ii). Latency\nand (iii). Clock Period of a design just by analyzing the high level behavioral\ncode and some intermediate representation codes. We have integrated the\nmethodology with Xilinx HLS tools and have demonstrated accurate estimation on\na variety of FPGA families. Our estimated results are within 10\\% of actual\ncomputed values", "journal": "ISQED 2022"}
{"doi": "10.48550/arXiv.2302.07940", "date": "2023-02-15", "title": "Online Tool Selection with Learned Grasp Prediction Models", "authors": "Khashayar Rohanimanesh, Jake Metzger, William Richards, Aviv Tamar", "abstract": "Deep learning-based grasp prediction models have become an industry standard\nfor robotic bin-picking systems. To maximize pick success, production\nenvironments are often equipped with several end-effector tools that can be\nswapped on-the-fly, based on the target object. Tool-change, however, takes\ntime. Choosing the order of grasps to perform, and corresponding tool-change\nactions, can improve system throughput; this is the topic of our work. The main\nchallenge in planning tool change is uncertainty - we typically cannot see\nobjects in the bin that are currently occluded. Inspired by queuing and\nadmission control problems, we model the problem as a Markov Decision Process\n(MDP), where the goal is to maximize expected throughput, and we pursue an\napproximate solution based on model predictive control, where at each time step\nwe plan based only on the currently visible objects. Special to our method is\nthe idea of void zones, which are geometrical boundaries in which an unknown\nobject will be present, and therefore cannot be accounted for during planning.\nOur planning problem can be solved using integer linear programming (ILP).\nHowever, we find that an approximate solution based on sparse tree search\nyields near optimal performance at a fraction of the time. Another question\nthat we explore is how to measure the performance of tool-change planning: we\nfind that throughput alone can fail to capture delicate and smooth behavior,\nand propose a principled alternative. Finally, we demonstrate our algorithms on\nboth synthetic and real world bin picking tasks.", "journal": ""}
{"doi": "10.48550/arXiv.2407.15012", "date": "2024-06-15", "title": "Transformative Influence of LLM and AI Tools in Student Social Media Engagement: Analyzing Personalization, Communication Efficiency, and Collaborative Learning", "authors": "Masoud Bashiri, Kamran Kowsari", "abstract": "The advent of Large Language Models (LLMs) and Artificial Intelligence (AI)\ntools has revolutionized various facets of our lives, particularly in the realm\nof social media. For students, these advancements have unlocked unprecedented\nopportunities for learning, collaboration, and personal growth. AI-driven\napplications are transforming how students interact with social media, offering\npersonalized content and recommendations, and enabling smarter, more efficient\ncommunication. Recent studies utilizing data from UniversityCube underscore the\nprofound impact of AI tools on students' academic and social experiences. These\nstudies reveal that students engaging with AI-enhanced social media platforms\nreport higher academic performance, enhanced critical thinking skills, and\nincreased engagement in collaborative projects.\n  Moreover, AI tools assist in filtering out distracting content, allowing\nstudents to concentrate more on educational materials and pertinent\ndiscussions. The integration of LLMs in social media has further facilitated\nimproved peer-to-peer communication and mentorship opportunities. AI algorithms\neffectively match students based on shared academic interests and career goals,\nfostering a supportive and intellectually stimulating online community, thereby\ncontributing to increased student satisfaction and retention rates.\n  In this article, we delve into the data provided by UniversityCube to explore\nhow LLMs and AI tools are specifically transforming social media for students.\nThrough case studies and statistical analyses, we offer a comprehensive\nunderstanding of the educational and social benefits these technologies offer.\nOur exploration highlights the potential of AI-driven tools to create a more\nenriched, efficient, and supportive educational environment for students in the\ndigital age.", "journal": ""}
{"doi": "10.48550/arXiv.2409.17939", "date": "2024-09-26", "title": "Predicting Anchored Text from Translation Memories for Machine Translation Using Deep Learning Methods", "authors": "Richard Yue, John E. Ortega", "abstract": "Translation memories (TMs) are the backbone for professional translation\ntools called computer-aided translation (CAT) tools. In order to perform a\ntranslation using a CAT tool, a translator uses the TM to gather translations\nsimilar to the desired segment to translate (s'). Many CAT tools offer a\nfuzzy-match algorithm to locate segments (s) in the TM that are close in\ndistance to s'. After locating two similar segments, the CAT tool will present\nparallel segments (s, t) that contain one segment in the source language along\nwith its translation in the target language. Additionally, CAT tools contain\nfuzzy-match repair (FMR) techniques that will automatically use the parallel\nsegments from the TM to create new TM entries containing a modified version of\nthe original with the idea in mind that it will be the translation of s'. Most\nFMR techniques use machine translation as a way of \"repairing\" those words that\nhave to be modified. In this article, we show that for a large part of those\nwords which are anchored, we can use other techniques that are based on machine\nlearning approaches such as Word2Vec. BERT, and even ChatGPT. Specifically, we\nshow that for anchored words that follow the continuous bag-of-words (CBOW)\nparadigm, Word2Vec, BERT, and GPT-4 can be used to achieve similar and, for\nsome cases, better results than neural machine translation for translating\nanchored words from French to English.", "journal": ""}
{"doi": "10.48550/arXiv.2501.08808", "date": "2025-01-15", "title": "A Bayesian Hierarchical Model for Generating Synthetic Unbalanced Power Distribution Grids", "authors": "Henrique O. Caetano, Rahul K. Gupta, Marco Aiello, Carlos Dias Maciel", "abstract": "The real-world data of power networks is often inaccessible due to privacy\nand security concerns, highlighting the need for tools to generate realistic\nsynthetic network data. Existing methods leverage geographic tools like\nOpenStreetMap with heuristic rules to model system topology and typically focus\non single-phase, balanced systems, limiting their applicability to real-world\ndistribution systems, which are usually unbalanced. This work proposes a\nBayesian Hierarchical Model (BHM) to generate unbalanced three-phase\ndistribution systems learning from existing networks. The scheme takes as input\nthe base topology and aggregated demand per node and outputs a three-phase\nunbalanced system. The proposed scheme achieves a Mean Absolute Percentage\nError (MAPE) of less than $8\\%$ across all phases, with computation times of\n20.4 seconds for model training and 3.1 seconds per sample generation. The tool\nis applied to learn from publicly available SMART-DS dataset and applied to\ngenerate European 906 and IEEE-123 systems. We demonstrate the transfer\nlearning capability of the proposed tool by leveraging a model trained on an\nobserved system to generate a synthetic network for an unobserved system.\nSpecifically, the tool is trained using the publicly available SMART-DS dataset\nand subsequently applied to generate synthetic networks for the European\n906-bus system and the IEEE 123-bus system. This tool allows researchers to\nsimulate realistic unbalanced three-phase power data with high accuracy and\nspeed, enhancing planning and operational analysis for modern power grids.", "journal": ""}
{"doi": "10.48550/arXiv.2502.02880", "date": "2025-02-05", "title": "Learning not cheating: AI assistance can enhance rather than hinder skill development", "authors": "Benjamin Lira, Todd Rogers, Daniel G. Goldstein, Lyle Ungar, Angela L. Duckworth", "abstract": "It is widely believed that outsourcing cognitive work to AI boosts immediate\nproductivity at the expense of long-term human capital development. An\noverlooked possibility is that AI tools can support skill development by\nproviding just-in-time, high-quality, personalized examples. In this\ninvestigation, lay forecasters predicted that practicing writing cover letters\nwith an AI tool would impair learning compared to practicing writing letters\nwithout the tool. However, in a highly-powered pre-registered experiment,\nparticipants randomly assigned to practice writing with AI improved more on a\nwriting test one day later compared to writers assigned to practice without AI.\nNotably, writers given access to the AI tool improved more despite exerting\nless effort, whether measured by time on task, keystrokes, or subjective\nratings. We replicated and extended these results in a second pre-registered\nexperiment, showing that writers given access to the AI tool again outperformed\nthose who practiced on their own -- but performed no better than writers merely\nshown an AI-generated cover letter that they could not edit. Collectively,\nthese findings constitute an existence proof that by providing personalized\nexamples of high-quality work, AI tools can improve, rather than undermine,\nlearning.", "journal": ""}
{"doi": "10.48550/arXiv.2503.00144", "date": "2025-02-28", "title": "Learner and Instructor Needs in AI-Supported Programming Learning Tools: Design Implications for Features and Adaptive Control", "authors": "Zihan Wu, Yicheng Tang, Barbara Ericson", "abstract": "AI-supported tools can help learners overcome challenges in programming\neducation by providing adaptive assistance. However, existing research often\nfocuses on individual tools rather than deriving broader design\nrecommendations. A key challenge in designing these systems is balancing\nlearner control with system-driven guidance. To explore user preferences for\nAI-supported programming learning tools, we conducted a participatory design\nstudy with 15 undergraduate novice programmers and 10 instructors to gather\ninsights on their desired help features and control preferences, as well as a\nfollow-up survey with 172 introductory programming students.\n  Our qualitative findings show that learners prefer help that is encouraging,\nincorporates visual aids, and includes peer-related insights, whereas\ninstructors prioritize scaffolding that reflects learners' progress and\nreinforces best practices. Both groups favor shared control, though learners\ngenerally prefer more autonomy, while instructors lean toward greater system\nguidance to prevent cognitive overload. Additionally, our interviews revealed\nindividual differences in control preferences.\n  Based on our findings, we propose design guidelines for AI-supported\nprogramming tools, particularly regarding user-centered help features and\nadaptive control mechanisms. Our work contributes to the human-centered design\nof AI-supported learning environments by informing the development of systems\nthat effectively balance autonomy and guidance, enhancing AI-supported\neducational tools for programming and beyond.", "journal": ""}
{"doi": "10.48550/arXiv.2505.08119", "date": "2025-05-12", "title": "Will Your Next Pair Programming Partner Be Human? An Empirical Evaluation of Generative AI as a Collaborative Teammate in a Semester-Long Classroom Setting", "authors": "Wenhan Lyu, Yimeng Wang, Yifan Sun, Yixuan Zhang", "abstract": "Generative AI (GenAI), especially Large Language Models (LLMs), is rapidly\nreshaping both programming workflows and computer science education. Many\nprogrammers now incorporate GenAI tools into their workflows, including for\ncollaborative coding tasks such as pair programming. While prior research has\ndemonstrated the benefits of traditional pair programming and begun to explore\nGenAI-assisted coding, the role of LLM-based tools as collaborators in pair\nprogramming remains underexamined. In this work, we conducted a mixed-methods\nstudy with 39 undergraduate students to examine how GenAI influences\ncollaboration, learning, and performance in pair programming. Specifically,\nstudents completed six in-class assignments under three conditions: Traditional\nPair Programming (PP), Pair Programming with GenAI (PAI), and Solo Programming\nwith GenAI (SAI). They used both LLM-based inline completion tools (e.g.,\nGitHub Copilot) and LLM-based conversational tools (e.g., ChatGPT). Our results\nshow that students in PAI achieved the highest assignment scores, whereas those\nin SAI attained the lowest. Additionally, students' attitudes toward LLMs'\nprogramming capabilities improved significantly after collaborating with\nLLM-based tools, and preferences were largely shaped by the perceived\nusefulness for completing assignments and learning programming skills, as well\nas the quality of collaboration. Our qualitative findings further reveal that\nwhile students appreciated LLM-based tools as valuable pair programming\npartners, they also identified limitations and had different expectations\ncompared to human teammates. Our study provides one of the first empirical\nevaluations of GenAI as a pair programming collaborator through a comparison of\nthree conditions (PP, PAI, and SAI). We also discuss the design implications\nand pedagogical considerations for future GenAI-assisted pair programming\napproaches.", "journal": ""}
{"doi": "10.48550/arXiv.1612.04858", "date": "2016-12-14", "title": "Bayesian Optimization for Machine Learning : A Practical Guidebook", "authors": "Ian Dewancker, Michael McCourt, Scott Clark", "abstract": "The engineering of machine learning systems is still a nascent field; relying\non a seemingly daunting collection of quickly evolving tools and best\npractices. It is our hope that this guidebook will serve as a useful resource\nfor machine learning practitioners looking to take advantage of Bayesian\noptimization techniques. We outline four example machine learning problems that\ncan be solved using open source machine learning libraries, and highlight the\nbenefits of using Bayesian optimization in the context of these common machine\nlearning applications.", "journal": ""}
{"doi": "10.48550/arXiv.1705.07538", "date": "2017-05-22", "title": "Infrastructure for Usable Machine Learning: The Stanford DAWN Project", "authors": "Peter Bailis, Kunle Olukotun, Christopher Re, Matei Zaharia", "abstract": "Despite incredible recent advances in machine learning, building machine\nlearning applications remains prohibitively time-consuming and expensive for\nall but the best-trained, best-funded engineering organizations. This expense\ncomes not from a need for new and improved statistical models but instead from\na lack of systems and tools for supporting end-to-end machine learning\napplication development, from data preparation and labeling to\nproductionization and monitoring. In this document, we outline opportunities\nfor infrastructure supporting usable, end-to-end machine learning applications\nin the context of the nascent DAWN (Data Analytics for What's Next) project at\nStanford.", "journal": ""}
{"doi": "10.48550/arXiv.2002.11203", "date": "2020-02-25", "title": "Interactive Summarizing -- Automatic Slide Localization Technology as Generative Learning Tool", "authors": "Lili Yan, Kai Li", "abstract": "Making a summary is a common learning strategy in lecture learning. It is an\neffective way for learners to engage in both traditional and video lectures.\nVideo summarization is an effective technology applied to enhance learners'\nsummarizing experience in a video lecture. In this article, we propose to apply\ncutting-edge automatic slide localization technology to lecture video learning\nexperience. An interactive summarizing model is designed to explain how\nlearners are engaged in the video lecture learning process supported by\nconvolutional neural network and the possibility of related learning analytics.", "journal": ""}
{"doi": "10.48550/arXiv.2002.03614", "date": "2020-02-10", "title": "RDFFrames: Knowledge Graph Access for Machine Learning Tools", "authors": "Aisha Mohamed, Ghadeer Abuoda, Abdurrahman Ghanem, Zoi Kaoudi, Ashraf Aboulnaga", "abstract": "Knowledge graphs represented as RDF datasets are integral to many machine\nlearning applications. RDF is supported by a rich ecosystem of data management\nsystems and tools, most notably RDF database systems that provide a SPARQL\nquery interface. Surprisingly, machine learning tools for knowledge graphs do\nnot use SPARQL, despite the obvious advantages of using a database system. This\nis due to the mismatch between SPARQL and machine learning tools in terms of\ndata model and programming style. Machine learning tools work on data in\ntabular format and process it using an imperative programming style, while\nSPARQL is declarative and has as its basic operation matching graph patterns to\nRDF triples. We posit that a good interface to knowledge graphs from a machine\nlearning software stack should use an imperative, navigational programming\nparadigm based on graph traversal rather than the SPARQL query paradigm based\non graph patterns. In this paper, we present RDFFrames, a framework that\nprovides such an interface. RDFFrames provides an imperative Python API that\ngets internally translated to SPARQL, and it is integrated with the PyData\nmachine learning software stack. RDFFrames enables the user to make a sequence\nof Python calls to define the data to be extracted from a knowledge graph\nstored in an RDF database system, and it translates these calls into a compact\nSPQARL query, executes it on the database system, and returns the results in a\nstandard tabular format. Thus, RDFFrames is a useful tool for data preparation\nthat combines the usability of PyData with the flexibility and performance of\nRDF database systems.", "journal": ""}
{"doi": "10.48550/arXiv.1905.01023", "date": "2019-05-02", "title": "Physicist's Journeys Through the AI World - A Topical Review. There is no royal road to unsupervised learning", "authors": "Imad Alhousseini, Wissam Chemissany, Fatima Kleit, Aly Nasrallah", "abstract": "Artificial Intelligence (AI), defined in its most simple form, is a\ntechnological tool that makes machines intelligent. Since learning is at the\ncore of intelligence, machine learning poses itself as a core sub-field of AI.\nThen there comes a subclass of machine learning, known as deep learning, to\naddress the limitations of their predecessors. AI has generally acquired its\nprominence over the past few years due to its considerable progress in various\nfields. AI has vastly invaded the realm of research. This has led physicists to\nattentively direct their research towards implementing AI tools. Their central\naim has been to gain better understanding and enrich their intuition. This\nreview article is meant to supplement the previously presented efforts to\nbridge the gap between AI and physics, and take a serious step forward to\nfilter out the \"Babelian\" clashes brought about from such gabs. This\nnecessitates first to have fundamental knowledge about common AI tools. To this\nend, the review's primary focus shall be on deep learning models called\nartificial neural networks. They are deep learning models which train\nthemselves through different learning processes. It discusses also the concept\nof Markov decision processes. Finally, shortcut to the main goal, the review\nthoroughly examines how these neural networks are capable to construct a\nphysical theory describing some observations without applying any previous\nphysical knowledge.", "journal": ""}
{"doi": "10.48550/arXiv.1308.5956", "date": "2013-08-27", "title": "Intentional Design for Empowerment", "authors": "Noah Podolefsky", "abstract": "I argue for empowering education, adapting Marx's idea of ownership of the\nmeans of production, and discuss interactive simulations as one example of a\ntool in which intentional design can support student ownership of learning. I\npropose a model that leverages affordances of educational tools to do positive\nwork toward empowering education.", "journal": ""}
{"doi": "10.48550/arXiv.1904.11951", "date": "2019-04-23", "title": "Optical Frequency Comb Noise Characterization Using Machine Learning", "authors": "Giovanni Brajato, Lars Lundberg, Victor Torres-Company, Darko Zibar", "abstract": "A novel tool, based on Bayesian filtering framework and expectation\nmaximization algorithm, is numerically and experimentally demonstrated for\naccurate frequency comb noise characterization. The tool is statistically\noptimum in a mean-square-error-sense, works at wide range of SNRs and offers\nmore accurate noise estimation compared to conventional methods.", "journal": ""}
{"doi": "10.48550/arXiv.2205.11064", "date": "2022-05-23", "title": "WOGAN at the SBST 2022 CPS Tool Competition", "authors": "Jarkko Peltom\u00e4ki, Frankie Spencer, Ivan Porres", "abstract": "WOGAN is an online test generation algorithm based on Wasserstein generative\nadversarial networks. In this note, we present how WOGAN works and summarize\nits performance in the SBST 2022 CPS tool competition concerning the AI of a\nself-driving car.", "journal": ""}
{"doi": "10.48550/arXiv.2206.09141", "date": "2022-06-18", "title": "ToolTango: Common sense Generalization in Predicting Sequential Tool Interactions for Robot Plan Synthesis", "authors": "Shreshth Tuli, Rajas Bansal, Rohan Paul, Mausam", "abstract": "Robots assisting us in environments such as factories or homes must learn to\nmake use of objects as tools to perform tasks, for instance using a tray to\ncarry objects. We consider the problem of learning commonsense knowledge of\nwhen a tool may be useful and how its use may be composed with other tools to\naccomplish a high-level task instructed by a human. Specifically, we introduce\na novel neural model, termed TOOLTANGO, that first predicts the next tool to be\nused, and then uses this information to predict the next action. We show that\nthis joint model can inform learning of a fine-grained policy enabling the\nrobot to use a particular tool in sequence and adds a significant value in\nmaking the model more accurate. TOOLTANGO encodes the world state, comprising\nobjects and symbolic relationships between them, using a graph neural network\nand is trained using demonstrations from human teachers instructing a virtual\nrobot in a physics simulator. The model learns to attend over the scene using\nknowledge of the goal and the action history, finally decoding the symbolic\naction to execute. Crucially, we address generalization to unseen environments\nwhere some known tools are missing, but alternative unseen tools are present.\nWe show that by augmenting the representation of the environment with\npre-trained embeddings derived from a knowledge-base, the model can generalize\neffectively to novel environments. Experimental results show at least\n48.8-58.1% absolute improvement over the baselines in predicting successful\nsymbolic plans for a simulated mobile manipulator in novel environments with\nunseen objects. This work takes a step in the direction of enabling robots to\nrapidly synthesize robust plans for complex tasks, particularly in novel\nsettings", "journal": ""}
{"doi": "10.48550/arXiv.2402.04254", "date": "2023-10-15", "title": "Large Vocabulary Spontaneous Speech Recognition for Tigrigna", "authors": "Ataklti Kahsu, Solomon Teferra", "abstract": "This thesis proposes and describes a research attempt at designing and\ndeveloping a speaker independent spontaneous automatic speech recognition\nsystem for Tigrigna The acoustic model of the Speech Recognition System is\ndeveloped using Carnegie Mellon University Automatic Speech Recognition\ndevelopment tool (Sphinx) while the SRIM tool is used for the development of\nthe language model.\n  Keywords Automatic Speech Recognition Tigrigna language", "journal": ""}
{"doi": "10.48550/arXiv.2410.18051", "date": "2024-10-23", "title": "Real time anomalies detection on video", "authors": "Fabien Poirier", "abstract": "Nowadays, many places use security cameras. Unfortunately, when an incident\noccurs, these technologies are used to show past events. So it can be\nconsidered as a deterrence tool than a detection tool. In this article, we will\npropose a deep learning approach trying to solve this problematic. This\napproach uses convolutional models (CNN) to extract relevant characteristics\nlinked to the video images, theses characteristics will form times series to be\nanalyzed by LSTM / GRU models.", "journal": ""}
{"doi": "10.48550/arXiv.2504.11536", "date": "2025-04-15", "title": "ReTool: Reinforcement Learning for Strategic Tool Use in LLMs", "authors": "Jiazhan Feng, Shijue Huang, Xingwei Qu, Ge Zhang, Yujia Qin, Baoquan Zhong, Chengquan Jiang, Jinxin Chi, Wanjun Zhong", "abstract": "While reasoning models (e.g., DeepSeek R1) trained with reinforcement\nlearning (RL), excel in textual reasoning, they struggle in scenarios requiring\nstructured problem-solving, such as geometric reasoning, concise computation,\nor complex equation solving-areas where computational tools like code\ninterpreters (CI) demonstrate distinct advantages. To bridge this gap, we\npropose ReTool, which enhances long-form reasoning with tool-integrated\nlearning, including two key features: (1) dynamic interleaving of real-time\ncode execution within natural language reasoning processes, and (2) an\nautomated RL paradigm that allows policy rollouts with multi-turn real-time\ncode execution and teaches the model in learning when and how to invoke tools\nbased on outcome feedback. ReTool employs a systematic training framework,\nbeginning with synthetic cold-start data generation to produce code-augmented\nlong-form reasoning traces for fine-tuning base models. Subsequent RL training\nleverages task outcomes as rewards to iteratively refine the model's tool use\nstrategy, enabling autonomous discovery of optimal tool invocation patterns\nwithout human priors. Experiments on the challenging MATH Olympiad benchmark\nAIME demonstrate ReTool's superiority: Our 32B model achieves 67% accuracy with\n400 training steps, outperforming text-based RL baseline (40% accuracy, 1080\nsteps) in efficiency and performance. Remarkably, ReTool-32B attains 72.5%\naccuracy in extended settings, surpassing OpenAI's o1-preview by 27.9%. Further\nanalysis reveals emergent behaviors such as code self-correction, signaling an\n''aha moment'' in which the model autonomously masters adaptive tool use. These\nfindings highlight the promise of outcome-driven tool integration for advancing\ncomplex mathematical reasoning and offer new insights into hybrid\nneuro-symbolic systems.", "journal": ""}
{"doi": "10.48550/arXiv.2007.00914", "date": "2020-07-02", "title": "Federated Learning and Differential Privacy: Software tools analysis, the Sherpa.ai FL framework and methodological guidelines for preserving data privacy", "authors": "Nuria Rodr\u00edguez-Barroso, Goran Stipcich, Daniel Jim\u00e9nez-L\u00f3pez, Jos\u00e9 Antonio Ruiz-Mill\u00e1n, Eugenio Mart\u00ednez-C\u00e1mara, Gerardo Gonz\u00e1lez-Seco, M. Victoria Luz\u00f3n, Miguel \u00c1ngel Veganzones, Francisco Herrera", "abstract": "The high demand of artificial intelligence services at the edges that also\npreserve data privacy has pushed the research on novel machine learning\nparadigms that fit those requirements. Federated learning has the ambition to\nprotect data privacy through distributed learning methods that keep the data in\ntheir data silos. Likewise, differential privacy attains to improve the\nprotection of data privacy by measuring the privacy loss in the communication\namong the elements of federated learning. The prospective matching of federated\nlearning and differential privacy to the challenges of data privacy protection\nhas caused the release of several software tools that support their\nfunctionalities, but they lack of the needed unified vision for those\ntechniques, and a methodological workflow that support their use. Hence, we\npresent the Sherpa.ai Federated Learning framework that is built upon an\nholistic view of federated learning and differential privacy. It results from\nthe study of how to adapt the machine learning paradigm to federated learning,\nand the definition of methodological guidelines for developing artificial\nintelligence services based on federated learning and differential privacy. We\nshow how to follow the methodological guidelines with the Sherpa.ai Federated\nLearning framework by means of a classification and a regression use cases.", "journal": "Information Fusion 64 (2020) 270-292"}
{"doi": "10.48550/arXiv.1603.02766", "date": "2016-03-09", "title": "Leveraging Crowd for Game-based Learning: A Case Study of Privacy Education Game Design and Evaluation by Crowdsourcing", "authors": "Wendy Wang, Yu Tao, Kai Wang, Dominik Jedruszczak, Ben Knutson", "abstract": "As the Internet grows in importance, it is vital to develop methods and\ntechniques for educating end-users to improve their awareness of online\nprivacy. Web-based education tools have been proven effective in many domains\nand have been increasingly adopted by many online professional and educational\nservices. However, the design and development of Web-based education tools for\nonline privacy is still in the early stage. The traditional solutions always\ninvolve privacy experts who have sophisticated expertise. Such involvement can\nmake the tool development costly. Furthermore, it is not clear how inspiring\nand effective these education tools are to general users of varying\nbackgrounds, specially to novice users who have rarely dealt with online\nprivacy issues before. In this paper, we design, develop, and evaluate a\ngame-based privacy learning system by leveraging the wisdom of a crowd of\nnon-experts on Amazon Mechanic Turk. Empirical study demonstrates that the\ncrowd can provide high-quality ideas of designing and developing a practical,\neducational privacy learning game.", "journal": ""}
{"doi": "10.48550/arXiv.1908.08381", "date": "2019-08-20", "title": "ElectroLens: Understanding Atomistic Simulations Through Spatially-resolved Visualization of High-dimensional Features", "authors": "Xiangyun Lei, Fred Hohman, Duen Horng Chau, Andrew J. Medford", "abstract": "In recent years, machine learning (ML) has gained significant popularity in\nthe field of chemical informatics and electronic structure theory. These\ntechniques often require researchers to engineer abstract \"features\" that\nencode chemical concepts into a mathematical form compatible with the input to\nmachine-learning models. However, there is no existing tool to connect these\nabstract features back to the actual chemical system, making it difficult to\ndiagnose failures and to build intuition about the meaning of the features. We\npresent ElectroLens, a new visualization tool for high-dimensional\nspatially-resolved features to tackle this problem. The tool visualizes\nhigh-dimensional data sets for atomistic and electron environment features by a\nseries of linked 3D views and 2D plots. The tool is able to connect different\nderived features and their corresponding regions in 3D via interactive\nselection. It is built to be scalable, and integrate with existing\ninfrastructure.", "journal": ""}
{"doi": "10.48550/arXiv.1703.06076", "date": "2017-03-15", "title": "Machine learning approach for early detection of autism by combining questionnaire and home video screening", "authors": "Halim Abbas, Ford Garberson, Eric Glover, Dennis P Wall", "abstract": "Existing screening tools for early detection of autism are expensive,\ncumbersome, time-intensive, and sometimes fall short in predictive value. In\nthis work, we apply Machine Learning (ML) to gold standard clinical data\nobtained across thousands of children at risk for autism spectrum disorders to\ncreate a low-cost, quick, and easy to apply autism screening tool that performs\nas well or better than most widely used standardized instruments. This new tool\ncombines two screening methods into a single assessment, one based on short,\nstructured parent-report questionnaires and the other on tagging key behaviors\nfrom short, semi-structured home videos of children. To overcome the scarcity,\nsparsity, and imbalance of training data, we apply creative feature selection,\nfeature engineering, and novel feature encoding techniques. We allow for\ninconclusive determination where appropriate in order to boost screening\naccuracy when conclusive. We demonstrate a significant accuracy improvement\nover standard screening tools in a clinical study sample of 162 children.", "journal": ""}
{"doi": "10.48550/arXiv.2003.03268", "date": "2020-03-06", "title": "Learning the Designer's Preferences to Drive Evolution", "authors": "Alberto Alvarez, Jose Font", "abstract": "This paper presents the Designer Preference Model, a data-driven solution\nthat pursues to learn from user generated data in a Quality-Diversity\nMixed-Initiative Co-Creativity (QD MI-CC) tool, with the aims of modelling the\nuser's design style to better assess the tool's procedurally generated content\nwith respect to that user's preferences. Through this approach, we aim for\nincreasing the user's agency over the generated content in a way that neither\nstalls the user-tool reciprocal stimuli loop nor fatigues the user with\nperiodical suggestion handpicking. We describe the details of this novel\nsolution, as well as its implementation in the MI-CC tool the Evolutionary\nDungeon Designer. We present and discuss our findings out of the initial tests\ncarried out, spotting the open challenges for this combined line of research\nthat integrates MI-CC with Procedural Content Generation through Machine\nLearning.", "journal": ""}
{"doi": "10.48550/arXiv.2005.06022", "date": "2020-05-08", "title": "Reputation Agent: Prompting Fair Reviews in Gig Markets", "authors": "Carlos Toxtli, Angela Richmond-Fuller, Saiph Savage", "abstract": "Our study presents a new tool, Reputation Agent, to promote fairer reviews\nfrom requesters (employers or customers) on gig markets. Unfair reviews,\ncreated when requesters consider factors outside of a worker's control, are\nknown to plague gig workers and can result in lost job opportunities and even\ntermination from the marketplace. Our tool leverages machine learning to\nimplement an intelligent interface that: (1) uses deep learning to\nautomatically detect when an individual has included unfair factors into her\nreview (factors outside the worker's control per the policies of the market);\nand (2) prompts the individual to reconsider her review if she has incorporated\nunfair factors. To study the effectiveness of Reputation Agent, we conducted a\ncontrolled experiment over different gig markets. Our experiment illustrates\nthat across markets, Reputation Agent, in contrast with traditional approaches,\nmotivates requesters to review gig workers' performance more fairly. We discuss\nhow tools that bring more transparency to employers about the policies of a gig\nmarket can help build empathy thus resulting in reasoned discussions around\npotential injustices towards workers generated by these interfaces. Our vision\nis that with tools that promote truth and transparency we can bring fairer\ntreatment to gig workers.", "journal": ""}
{"doi": "10.48550/arXiv.2011.05900", "date": "2020-11-11", "title": "A decision-making tool to fine-tune abnormal levels in the complete blood count tests", "authors": "Marta Avalos-Fernandez, Helene Touchais, Marcela Henriquez-Henriquez", "abstract": "The complete blood count (CBC) performed by automated hematology analyzers is\none of the most ordered laboratory tests. It is a first-line tool for assessing\na patient's general health status, or diagnosing and monitoring disease\nprogression. When the analysis does not fit an expected setting, technologists\nmanually review a blood smear using a microscope. The International Consensus\nGroup for Hematology Review published in 2005 a set of criteria for reviewing\nCBCs. Commonly, adjustments are locally needed to account for laboratory\nresources and populations characteristics. Our objective is to provide a\ndecision support tool to identify which CBC variables are associated with\nhigher risks of abnormal smear and at which cutoff values. We propose a\ncost-sensitive Lasso-penalized additive logistic regression combined with\nstability selection. Using simulated and real CBC data, we demonstrate that our\ntool correctly identify the true cutoff values, provided that there is enough\navailable data in their neighbourhood.", "journal": ""}
{"doi": "10.48550/arXiv.2109.02473", "date": "2021-08-30", "title": "A Robust Cybersecurity Topic Classification Tool", "authors": "Elijah Pelofske, Lorie M. Liebrock, Vincent Urias", "abstract": "In this research, we use user defined labels from three internet text sources\n(Reddit, Stackexchange, Arxiv) to train 21 different machine learning models\nfor the topic classification task of detecting cybersecurity discussions in\nnatural text. We analyze the false positive and false negative rates of each of\nthe 21 model's in a cross validation experiment. Then we present a\nCybersecurity Topic Classification (CTC) tool, which takes the majority vote of\nthe 21 trained machine learning models as the decision mechanism for detecting\ncybersecurity related text. We also show that the majority vote mechanism of\nthe CTC tool provides lower false negative and false positive rates on average\nthan any of the 21 individual models. We show that the CTC tool is scalable to\nthe hundreds of thousands of documents with a wall clock time on the order of\nhours.", "journal": ""}
{"doi": "10.48550/arXiv.2205.03790", "date": "2022-05-08", "title": "MLSmellHound: A Context-Aware Code Analysis Tool", "authors": "Jai Kannan, Scott Barnett, Lu\u00eds Cruz, Anj Simmons, Akash Agarwal", "abstract": "Meeting the rise of industry demand to incorporate machine learning (ML)\ncomponents into software systems requires interdisciplinary teams contributing\nto a shared code base. To maintain consistency, reduce defects and ensure\nmaintainability, developers use code analysis tools to aid them in identifying\ndefects and maintaining standards. With the inclusion of machine learning,\ntools must account for the cultural differences within the teams which\nmanifests as multiple programming languages, and conflicting definitions and\nobjectives. Existing tools fail to identify these cultural differences and are\ngeared towards software engineering which reduces their adoption in ML\nprojects. In our approach we attempt to resolve this problem by exploring the\nuse of context which includes i) purpose of the source code, ii) technical\ndomain, iii) problem domain, iv) team norms, v) operational environment, and\nvi) development lifecycle stage to provide contextualised error reporting for\ncode analysis. To demonstrate our approach, we adapt Pylint as an example and\napply a set of contextual transformations to the linting results based on the\ndomain of individual project files under analysis. This allows for\ncontextualised and meaningful error reporting for the end-user.", "journal": ""}
{"doi": "10.48550/arXiv.2304.02491", "date": "2023-04-05", "title": "\"It's Weird That it Knows What I Want\": Usability and Interactions with Copilot for Novice Programmers", "authors": "James Prather, Brent N. Reeves, Paul Denny, Brett A. Becker, Juho Leinonen, Andrew Luxton-Reilly, Garrett Powell, James Finnie-Ansley, Eddie Antonio Santos", "abstract": "Recent developments in deep learning have resulted in code-generation models\nthat produce source code from natural language and code-based prompts with high\naccuracy. This is likely to have profound effects in the classroom, where\nnovices learning to code can now use free tools to automatically suggest\nsolutions to programming exercises and assignments. However, little is\ncurrently known about how novices interact with these tools in practice. We\npresent the first study that observes students at the introductory level using\none such code auto-generating tool, Github Copilot, on a typical introductory\nprogramming (CS1) assignment. Through observations and interviews we explore\nstudent perceptions of the benefits and pitfalls of this technology for\nlearning, present new observed interaction patterns, and discuss cognitive and\nmetacognitive difficulties faced by students. We consider design implications\nof these findings, specifically in terms of how tools like Copilot can better\nsupport and scaffold the novice programming experience.", "journal": ""}
{"doi": "10.48550/arXiv.2306.10304", "date": "2023-06-17", "title": "Understanding Revision Behavior in Adaptive Writing Support Systems for Education", "authors": "Luca Mouchel, Thiemo Wambsganss, Paola Mejia-Domenzain, Tanja K\u00e4ser", "abstract": "Revision behavior in adaptive writing support systems is an important and\nrelatively new area of research that can improve the design and effectiveness\nof these tools, and promote students' self-regulated learning (SRL).\nUnderstanding how these tools are used is key to improving them to better\nsupport learners in their writing and learning processes. In this paper, we\npresent a novel pipeline with insights into the revision behavior of students\nat scale. We leverage a data set of two groups using an adaptive writing\nsupport tool in an educational setting. With our novel pipeline, we show that\nthe tool was effective in promoting revision among the learners. Depending on\nthe writing feedback, we were able to analyze different strategies of learners\nwhen revising their texts, we found that users of the exemplary case improved\nover time and that females tend to be more efficient. Our research contributes\na pipeline for measuring SRL behaviors at scale in writing tasks (i.e.,\nengagement or revision behavior) and informs the design of future adaptive\nwriting support systems for education, with the goal of enhancing their\neffectiveness in supporting student writing. The source code is available at\nhttps://github.com/lucamouchel/Understanding-Revision-Behavior.", "journal": ""}
{"doi": "10.48550/arXiv.2307.07119", "date": "2023-07-14", "title": "DataAssist: A Machine Learning Approach to Data Cleaning and Preparation", "authors": "Kartikay Goyle, Quin Xie, Vakul Goyle", "abstract": "Current automated machine learning (ML) tools are model-centric, focusing on\nmodel selection and parameter optimization. However, the majority of the time\nin data analysis is devoted to data cleaning and wrangling, for which limited\ntools are available. Here we present DataAssist, an automated data preparation\nand cleaning platform that enhances dataset quality using ML-informed methods.\nWe show that DataAssist provides a pipeline for exploratory data analysis and\ndata cleaning, including generating visualization for user-selected variables,\nunifying data annotation, suggesting anomaly removal, and preprocessing data.\nThe exported dataset can be readily integrated with other autoML tools or\nuser-specified model for downstream analysis. Our data-centric tool is\napplicable to a variety of fields, including economics, business, and\nforecasting applications saving over 50% time of the time spent on data\ncleansing and preparation.", "journal": ""}
{"doi": "10.48550/arXiv.2310.14629", "date": "2023-10-23", "title": "Making informed decisions in cutting tool maintenance in milling: A KNN-based model agnostic approach", "authors": "Revati M. Wahul, Aditya M. Rahalkar, Om M. Khare, Abhishek D. Patange, Rohan N. Soman", "abstract": "Tool Condition Monitoring (TCM) is vital for maintaining productivity and\nproduct quality in machining. This study leverages machine learning to analyze\nreal-time force signals collected from experiments under various tool wear\nconditions. Statistical analysis and feature selection using decision trees\nwere followed by classification using a K-Nearest Neighbors (KNN) algorithm,\nwith hyperparameter tuning to enhance performance. While machine learning has\nbeen widely applied in TCM, interpretability remains limited. This work\nintroduces a KNN-based white-box model that enhances transparency in\ndecision-making by revealing how features influence classification. The model\nnot only detects tool wear but also provides insights into the reasoning behind\neach decision, enabling manufacturers to make informed maintenance choices.", "journal": ""}
{"doi": "10.48550/arXiv.2310.16162", "date": "2023-10-24", "title": "Brainchop: Next Generation Web-Based Neuroimaging Application", "authors": "Mohamed Masoud, Pratyush Reddy, Farfalla Hu, Sergey Plis", "abstract": "Performing volumetric image processing directly within the browser,\nparticularly with medical data, presents unprecedented challenges compared to\nconventional backend tools. These challenges arise from limitations inherent in\nbrowser environments, such as constrained computational resources and the\navailability of frontend machine learning libraries. Consequently, there is a\nshortage of neuroimaging frontend tools capable of providing comprehensive\nend-to-end solutions for whole brain preprocessing and segmentation while\npreserving end-user data privacy and residency. In light of this context, we\nintroduce Brainchop (http://www.brainchop.org) as a groundbreaking in-browser\nneuroimaging tool that enables volumetric analysis of structural MRI using\npre-trained full-brain deep learning models, all without requiring technical\nexpertise or intricate setup procedures. Beyond its commitment to data privacy,\nthis frontend tool offers multiple features, including scalability, low\nlatency, user-friendly operation, cross-platform compatibility, and enhanced\naccessibility. This paper outlines the processing pipeline of Brainchop and\nevaluates the performance of models across various software and hardware\nconfigurations. The results demonstrate the practicality of client-side\nprocessing for volumetric data, owing to the robust MeshNet architecture, even\nwithin the resource-constrained environment of web browsers.", "journal": ""}
{"doi": "10.48550/arXiv.2311.14086", "date": "2023-11-23", "title": "Brain MRI Screening Tool with Federated Learning", "authors": "Roman Stoklasa, Ioannis Stathopoulos, Efstratios Karavasilis, Efstathios Efstathopoulos, Marek Dost\u00e1l, Milo\u0161 Ke\u0159kovsk\u00fd, Michal Kozubek, Luigi Serio", "abstract": "In clinical practice, we often see significant delays between MRI scans and\nthe diagnosis made by radiologists, even for severe cases. In some cases, this\nmay be caused by the lack of additional information and clues, so even the\nsevere cases need to wait in the queue for diagnosis. This can be avoided if\nthere is an automatic software tool, which would supplement additional\ninformation, alerting radiologists that the particular patient may be a severe\ncase.\n  We are presenting an automatic brain MRI Screening Tool and we are\ndemonstrating its capabilities for detecting tumor-like pathologies. It is the\nfirst version on the path toward a robust multi-pathology screening solution.\nThe tool supports Federated Learning, so multiple institutions may contribute\nto the model without disclosing their private data.", "journal": ""}
{"doi": "10.48550/arXiv.2402.11145", "date": "2024-02-17", "title": "Supporting Experts with a Multimodal Machine-Learning-Based Tool for Human Behavior Analysis of Conversational Videos", "authors": "Riku Arakawa, Kiyosu Maeda, Hiromu Yakura", "abstract": "Multimodal scene search of conversations is essential for unlocking valuable\ninsights into social dynamics and enhancing our communication. While experts in\nconversational analysis have their own knowledge and skills to find key scenes,\na lack of comprehensive, user-friendly tools that streamline the processing of\ndiverse multimodal queries impedes efficiency and objectivity. To solve it, we\ndeveloped Providence, a visual-programming-based tool based on design\nconsiderations derived from a formative study with experts. It enables experts\nto combine various machine learning algorithms to capture human behavioral cues\nwithout writing code. Our study showed its preferable usability and\nsatisfactory output with less cognitive load imposed in accomplishing scene\nsearch tasks of conversations, verifying the importance of its customizability\nand transparency. Furthermore, through the in-the-wild trial, we confirmed the\nobjectivity and reusability of the tool transform experts' workflow, suggesting\nthe advantage of expert-AI teaming in a highly human-contextual domain.", "journal": ""}
{"doi": "10.48550/arXiv.2405.06221", "date": "2024-05-10", "title": "For the Misgendered Chinese in Gender Bias Research: Multi-Task Learning with Knowledge Distillation for Pinyin Name-Gender Prediction", "authors": "Xiaocong Du, Haipeng Zhang", "abstract": "Achieving gender equality is a pivotal factor in realizing the UN's Global\nGoals for Sustainable Development. Gender bias studies work towards this and\nrely on name-based gender inference tools to assign individual gender labels\nwhen gender information is unavailable. However, these tools often inaccurately\npredict gender for Chinese Pinyin names, leading to potential bias in such\nstudies. With the growing participation of Chinese in international activities,\nthis situation is becoming more severe. Specifically, current tools focus on\npronunciation (Pinyin) information, neglecting the fact that the latent\nconnections between Pinyin and Chinese characters (Hanzi) behind convey\ncritical information. As a first effort, we formulate the Pinyin name-gender\nguessing problem and design a Multi-Task Learning Network assisted by Knowledge\nDistillation that enables the Pinyin embeddings in the model to possess\nsemantic features of Chinese characters and to learn gender information from\nChinese character names. Our open-sourced method surpasses commercial\nname-gender guessing tools by 9.70\\% to 20.08\\% relatively, and also\noutperforms the state-of-the-art algorithms.", "journal": ""}
{"doi": "10.48550/arXiv.2407.01199", "date": "2024-07-01", "title": "Deep Learning Based Tool Wear Estimation Considering Cutting Conditions", "authors": "Zongshuo Li, Markus Meurer, Thomas Bergs", "abstract": "Tool wear conditions impact the final quality of the workpiece. In this\nstudy, we propose a deep learning approach based on a convolutional neural\nnetwork that incorporates cutting conditions as extra model inputs, aiming to\nimprove tool wear estimation accuracy and fulfill industrial demands for\nzero-shot transferability. Through a series of milling experiments under\nvarious cutting parameters, we evaluate the model's performance in terms of\ntool wear estimation accuracy and its transferability to new fixed or variable\ncutting parameters. The results consistently highlight our approach's advantage\nover conventional models that omit cutting conditions, maintaining superior\nperformance irrespective of the stability of the wear development or the\nlimitation of the training dataset. This finding underscores its potential\napplicability in industrial scenarios.", "journal": ""}
{"doi": "10.48550/arXiv.2409.17228", "date": "2024-09-25", "title": "Disk2Planet: A Robust and Automated Machine Learning Tool for Parameter Inference in Disk-Planet Systems", "authors": "Shunyuan Mao, Ruobing Dong, Kwang Moo Yi, Lu Lu, Sifan Wang, Paris Perdikaris", "abstract": "We introduce Disk2Planet, a machine learning-based tool to infer key\nparameters in disk-planet systems from observed protoplanetary disk structures.\nDisk2Planet takes as input the disk structures in the form of two-dimensional\ndensity and velocity maps, and outputs disk and planet properties, that is, the\nShakura--Sunyaev viscosity, the disk aspect ratio, the planet--star mass ratio,\nand the planet's radius and azimuth. We integrate the Covariance Matrix\nAdaptation Evolution Strategy (CMA--ES), an evolutionary algorithm tailored for\ncomplex optimization problems, and the Protoplanetary Disk Operator Network\n(PPDONet), a neural network designed to predict solutions of disk--planet\ninteractions. Our tool is fully automated and can retrieve parameters in one\nsystem in three minutes on an Nvidia A100 graphics processing unit. We\nempirically demonstrate that our tool achieves percent-level or higher\naccuracy, and is able to handle missing data and unknown levels of noise.", "journal": ""}
{"doi": "10.48550/arXiv.2501.17496", "date": "2025-01-29", "title": "SemML: Enhancing Automata-Theoretic LTL Synthesis with Machine Learning", "authors": "Jan Kretinsky, Tobias Meggendorfer, Maximilian Prokop, Ashkan Zarkhah", "abstract": "Synthesizing a reactive system from specifications given in linear temporal\nlogic (LTL) is a classical problem, finding its applications in safety-critical\nsystems design. We present our tool SemML, which won this year's LTL\nrealizability tracks of SYNTCOMP, after years of domination by Strix. While\nboth tools are based on the automata-theoretic approach, ours relies heavily on\n(i) Semantic labelling, additional information of logical nature, coming from\nrecent LTL-to-automata translations and decorating the resulting parity game,\nand (ii) Machine Learning approaches turning this information into a guidance\noracle for on-the-fly exploration of the parity game (whence the name SemML).\nOur tool fills the missing gaps of previous suggestions to use such an oracle\nand provides an efficeint implementation with additional algorithmic\nimprovements. We evaluate SemML both on the entire set of SYNTCOMP as well as a\nsynthetic data set, compare it to Strix, and analyze the advantages and\nlimitations. As SemML solves more instances on SYNTCOMP and does so\nsignificantly faster on larger instances, this demonstrates for the first time\nthat machine-learning-aided approaches can out-perform state-of-the-art tools\nin real LTL synthesis.", "journal": ""}
{"doi": "10.48550/arXiv.1502.03601", "date": "2015-02-12", "title": "A Predictive System for detection of Bankruptcy using Machine Learning techniques", "authors": "Kalyan Nagaraj, Amulyashree Sridhar", "abstract": "Bankruptcy is a legal procedure that claims a person or organization as a\ndebtor. It is essential to ascertain the risk of bankruptcy at initial stages\nto prevent financial losses. In this perspective, different soft computing\ntechniques can be employed to ascertain bankruptcy. This study proposes a\nbankruptcy prediction system to categorize the companies based on extent of\nrisk. The prediction system acts as a decision support tool for detection of\nbankruptcy\n  Keywords: Bankruptcy, soft computing, decision support tool", "journal": "Kalyan Nagaraj, Amulyashree Sridhar (2015). A Predictive System\n  for detection of Bankruptcy using Machine learning techniques. IJDKP. 5(1):\n  29-40"}
{"doi": "10.48550/arXiv.1611.03335", "date": "2016-11-10", "title": "Machine learning methods for nanolaser characterization", "authors": "Darko Zibar, Molly Piels, Ole Winther, Jesper Moerk, Christian Schaeffer", "abstract": "Nanocavity lasers, which are an integral part of an on-chip integrated\nphotonic network, are setting stringent requirements on the sensitivity of the\ntechniques used to characterize the laser performance. Current characterization\ntools cannot provide detailed knowledge about nanolaser noise and dynamics. In\nthis progress article, we will present tools and concepts from the Bayesian\nmachine learning and digital coherent detection that offer novel approaches for\nhighly-sensitive laser noise characterization and inference of laser dynamics.\nThe goal of the paper is to trigger new research directions that combine the\nfields of machine learning and nanophotonics for characterizing nanolasers and\neventually integrated photonic networks", "journal": ""}
{"doi": "10.48550/arXiv.1809.04392", "date": "2018-09-12", "title": "Machine Learning tools for global PDF fits", "authors": "Juan Rojo", "abstract": "The use of machine learning algorithms in theoretical and experimental\nhigh-energy physics has experienced an impressive progress in recent years,\nwith applications from trigger selection to jet substructure classification and\ndetector simulation among many others. In this contribution, we review the\nmachine learning tools used in the NNPDF family of global QCD analyses. These\ninclude multi-layer feed-forward neural networks for the model-independent\nparametrisation of parton distributions and fragmentation functions, genetic\nand covariance matrix adaptation algorithms for training and optimisation, and\nclosure testing for the systematic validation of the fitting methodology.", "journal": ""}
